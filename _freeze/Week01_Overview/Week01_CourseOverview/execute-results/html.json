{
  "hash": "2820610096a601b0e78d7364033de1f3",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Week 1: Course Overview & Computational Foundations\"\nformat:\n  html:\n    toc: true\n    toc-depth: 3\n    number-sections: true\n    code-fold: false\n    code-tools: true\n---\n\n::: {.callout-note icon=false}\n## Learning Objectives\n\nBy the end of this week, you will be able to:\n\n1. **Understand** the **philosophy** of building equations from first principles before using statistical software\n2. **Set up** your **computational environment** for **matrix operations** in R\n3. **Connect** **linear models** to **animal breeding applications** (EPDs, breeding values, BLUP)\n4. **Perform** basic **matrix operations** in R\n5. **Express** the **sample mean** as a simple **linear model**\n:::\n\n## Why Build Our Own Solvers?\n\n### The Black Box Problem\n\nMost students learn to analyze data by running commands like:\n\n```r\nmodel <- lm(y ~ x)\nsummary(model)\n```\n\nWhile this approach gets results quickly, it creates several problems:\n\n- **Lack of understanding**: What is `lm()` actually doing?\n- **Limited flexibility**: Can't modify methods for special situations\n- **Difficulty debugging**: When results seem wrong, where do you look?\n- **Blind trust**: How do you know the software is correct?\n\n### Our Philosophy: Understanding Through Building\n\nIn this course, we take a different approach:\n\n1. **Learn the mathematics** behind each method\n2. **Derive estimators** from first principles\n3. **Build solvers manually** using matrix operations\n4. **Verify our results** against established software\n5. **Then** use software with confidence and understanding\n\n::: {.callout-note}\n## Why This Matters for Animal Breeding\n\nIn animal breeding, we work with:\n\n- **Large datasets** (thousands to millions of animals)\n- **Complex relationships** (pedigrees, genomic data)\n- **Special structures** (repeated records, incomplete data)\n- **Custom models** (not always available in standard software)\n\nUnderstanding how methods work allows you to:\n\n- Adapt methods for your specific problems\n- Recognize when software makes incorrect assumptions\n- Build custom solutions when needed\n- Communicate effectively with statisticians and programmers\n:::\n\n## Linear Models in Animal Breeding\n\nBefore diving into the mathematics, let's see where linear models appear in animal breeding and genetics.\n\n### Best Linear Unbiased Prediction (BLUP)\n\nBLUP is the foundation of modern genetic evaluation. It estimates breeding values by solving a large system of linear equations called **mixed model equations (MME)**.\n\nFor a simple sire model:\n\n$$\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{Z}\\mathbf{u} + \\mathbf{e}\n$$\n\nWhere:\n\n- $\\mathbf{y}$ = vector of observations (e.g., milk yields)\n- $\\mathbf{X}$ = design matrix for fixed effects (e.g., herds, years)\n- $\\boldsymbol{\\beta}$ = vector of fixed effects\n- $\\mathbf{Z}$ = incidence matrix relating observations to sires\n- $\\mathbf{u}$ = vector of sire effects (breeding values)\n- $\\mathbf{e}$ = vector of random errors\n\nWe'll build toward this model throughout the course, starting with simpler fixed effects models.\n\n### Estimated Progeny Differences (EPDs)\n\nEPDs in beef cattle are predictions of genetic merit based on performance data. They come from solving linear models that account for:\n\n- Contemporary groups (fixed effects)\n- Genetic relationships (random effects)\n- Multiple traits (multivariate models)\n\n### Yield Deviations\n\nIn dairy cattle, yield deviations adjust cow records for:\n\n- Herd-year-season effects\n- Age at calving\n- Days in milk\n- Previous lactations\n\nThese adjustments use linear model equations.\n\n### Genomic Evaluations\n\nModern genomic selection uses linear models relating:\n\n- Phenotypes (e.g., growth rate)\n- Genotypes (SNP markers)\n- Relationships (genomic relationship matrix)\n\n::: {.callout-important}\n## All These Methods Share a Common Foundation\n\nEvery method listed above relies on:\n\n1. Expressing the problem as a linear model\n2. Building appropriate design matrices\n3. Solving systems of linear equations\n4. Understanding properties of estimators (BLUE, BLUP)\n\n**This course teaches you those fundamental skills.**\n:::\n\n## Setting Up Your Computing Environment\n\n### Installing R and RStudio\n\nIf you haven't already:\n\n1. **Download R** from [https://cran.r-project.org/](https://cran.r-project.org/)\n2. **Download RStudio** from [https://posit.co/download/rstudio-desktop/](https://posit.co/download/rstudio-desktop/)\n3. **Install both** in that order (R first, then RStudio)\n\n### Required R Packages\n\nWe'll use several R packages throughout the course. Install them now:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Install required packages\ninstall.packages(\"MASS\")      # For generalized inverse: ginv()\ninstall.packages(\"car\")       # For VIF, Type III SS\ninstall.packages(\"emmeans\")   # For adjusted means\ninstall.packages(\"multcomp\")  # For multiple contrasts\ninstall.packages(\"lme4\")      # For mixed models (Week 14)\n```\n:::\n\n\nLoad the main package we'll use in Week 1:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(MASS)  # For ginv() function\n```\n:::\n\n\n### Test Your Installation\n\nRun this simple test:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a simple matrix\nA <- matrix(c(1, 2, 3, 4), nrow = 2, ncol = 2)\nprint(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n```\n\n\n:::\n\n```{.r .cell-code}\n# Compute its inverse\nA_inv <- solve(A)\nprint(A_inv)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]   -2  1.5\n[2,]    1 -0.5\n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify: A %*% A_inv should equal identity matrix\nI <- A %*% A_inv\nprint(round(I, 10))  # Round to remove floating point errors\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n```\n\n\n:::\n:::\n\n\nIf this runs without errors and produces the identity matrix, you're ready to go!\n\n## Review of Matrix Operations\n\nLet's review the matrix operations we'll use throughout this course.\n\n### Creating Matrices in R\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a vector\ny <- c(25, 28, 26, 30, 27)\nprint(y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 25 28 26 30 27\n```\n\n\n:::\n\n```{.r .cell-code}\n# Create a matrix by specifying elements\nX <- matrix(c(1, 1, 1, 1, 1), nrow = 5, ncol = 1)\nprint(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]\n[1,]    1\n[2,]    1\n[3,]    1\n[4,]    1\n[5,]    1\n```\n\n\n:::\n\n```{.r .cell-code}\n# Create a matrix using cbind (column bind)\nX2 <- matrix(c(1, 1, 1, 1, 1,\n               25, 28, 26, 30, 27), nrow = 5, ncol = 2)\nprint(X2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    1   25\n[2,]    1   28\n[3,]    1   26\n[4,]    1   30\n[5,]    1   27\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check dimensions\ndim(y)   # Actually a vector, shown as NULL\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNULL\n```\n\n\n:::\n\n```{.r .cell-code}\ndim(X)   # 5 rows, 1 column\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5 1\n```\n\n\n:::\n\n```{.r .cell-code}\ndim(X2)  # 5 rows, 2 columns\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5 2\n```\n\n\n:::\n:::\n\n\n### Matrix Transpose\n\nThe transpose of a matrix swaps rows and columns.\n\nNotation: $\\mathbf{X}'$ or $\\mathbf{X}^\\top$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- matrix(c(1, 2, 3, 4, 5, 6), nrow = 2, ncol = 3)\nprint(\"Original matrix X (2x3):\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Original matrix X (2x3):\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n```\n\n\n:::\n\n```{.r .cell-code}\nX_transpose <- t(X)\nprint(\"Transposed matrix X' (3x2):\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Transposed matrix X' (3x2):\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(X_transpose)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n[3,]    5    6\n```\n\n\n:::\n:::\n\n\n### Matrix Multiplication\n\nMatrix multiplication follows the rule: $(n \\times k)$ matrix times $(k \\times m)$ matrix equals $(n \\times m)$ matrix.\n\nThe number of columns in the first matrix must equal the number of rows in the second matrix.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example 1: Vector times matrix\nA <- matrix(c(1, 2, 3, 4), nrow = 2, ncol = 2)\nv <- c(5, 6)\n\n# v is 1x2 (row vector), A is 2x2\n# Result should be 1x2\nresult1 <- v %*% A\nprint(result1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]   17   39\n```\n\n\n:::\n\n```{.r .cell-code}\n# Example 2: Matrix times vector\n# A is 2x2, v as column vector is 2x1\n# Result should be 2x1\nresult2 <- A %*% v\nprint(result2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]\n[1,]   23\n[2,]   34\n```\n\n\n:::\n\n```{.r .cell-code}\n# Example 3: Matrix times matrix\nB <- matrix(c(1, 0, 0, 1), nrow = 2, ncol = 2)\nresult3 <- A %*% B\nprint(result3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n```\n\n\n:::\n:::\n\n\n::: {.callout-tip}\n## Matrix Multiplication in R\n\nIn R, use `%*%` for matrix multiplication, NOT `*`.\n\n- `A * B` performs element-wise multiplication\n- `A %*% B` performs matrix multiplication\n:::\n\n### Matrix Addition\n\nMatrices of the same dimensions can be added element-wise.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA <- matrix(c(1, 2, 3, 4), nrow = 2, ncol = 2)\nB <- matrix(c(5, 6, 7, 8), nrow = 2, ncol = 2)\n\nC <- A + B\nprint(C)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    6   10\n[2,]    8   12\n```\n\n\n:::\n:::\n\n\n### Identity Matrix\n\nThe identity matrix $\\mathbf{I}$ is a square matrix with 1's on the diagonal and 0's elsewhere.\n\nProperty: $\\mathbf{A}\\mathbf{I} = \\mathbf{I}\\mathbf{A} = \\mathbf{A}$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create 3x3 identity matrix\nI3 <- diag(3)\nprint(I3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify property\nA <- matrix(c(1, 2, 3, 4, 5, 6, 7, 8, 9), nrow = 3, ncol = 3)\nprint(\"A * I equals A:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"A * I equals A:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A %*% I3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n```\n\n\n:::\n:::\n\n\n## The Sample Mean as a Linear Model\n\nLet's start with the simplest possible linear model: estimating a sample mean.\n\n### The Statistical Model\n\nSuppose we have $n$ observations: $y_1, y_2, \\ldots, y_n$.\n\nWe can write this as:\n\n$$\ny_i = \\mu + e_i, \\quad i = 1, 2, \\ldots, n\n$$ {#eq-mean-model}\n\nWhere:\n\n- $y_i$ = observation $i$ (scalar)\n- $\\mu$ = population mean (scalar, unknown parameter to estimate)\n- $e_i$ = random error for observation $i$ (scalar)\n\n### Assumptions\n\nWe assume:\n\n1. $E(e_i) = 0$ (errors have mean zero)\n2. $Var(e_i) = \\sigma^2$ (constant variance)\n3. $Cov(e_i, e_j) = 0$ for $i \\neq j$ (errors are independent)\n\n### Matrix Form\n\nWe can write this model in matrix form:\n\n$$\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{e}\n$$ {#eq-matrix-form}\n\nWhere:\n\n- $\\mathbf{y}$ is an $n \\times 1$ vector of observations: $\\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}$\n\n- $\\mathbf{X}$ is an $n \\times 1$ design matrix of ones: $\\mathbf{X} = \\begin{bmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix}$\n\n- $\\boldsymbol{\\beta}$ is a $1 \\times 1$ vector (scalar) containing $\\mu$: $\\boldsymbol{\\beta} = [\\mu]$\n\n- $\\mathbf{e}$ is an $n \\times 1$ vector of errors: $\\mathbf{e} = \\begin{bmatrix} e_1 \\\\ e_2 \\\\ \\vdots \\\\ e_n \\end{bmatrix}$\n\n### Least Squares Estimator\n\nThe least squares estimator minimizes the sum of squared errors:\n\n$$\nSSE = \\sum_{i=1}^{n} e_i^2 = \\sum_{i=1}^{n} (y_i - \\mu)^2\n$$\n\nTaking the derivative with respect to $\\mu$ and setting equal to zero:\n\n$$\n\\frac{\\partial SSE}{\\partial \\mu} = -2\\sum_{i=1}^{n}(y_i - \\mu) = 0\n$$\n\nSolving:\n\n$$\n\\sum_{i=1}^{n}y_i = n\\mu\n$$\n\n$$\n\\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^{n}y_i = \\bar{y}\n$$ {#eq-mean-estimate}\n\nThe least squares estimate of $\\mu$ is simply the sample mean!\n\n### Using the Normal Equations\n\nThe general form of the **normal equations** is:\n\n$$\n\\mathbf{X}'\\mathbf{X}\\mathbf{b} = \\mathbf{X}'\\mathbf{y}\n$$ {#eq-normal-equations}\n\nWhere $\\mathbf{b}$ is our estimate of $\\boldsymbol{\\beta}$.\n\nFor our mean model:\n\n$$\n\\mathbf{X}'\\mathbf{X} = \\begin{bmatrix} 1 & 1 & \\cdots & 1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix} = n\n$$\n\nThis is a $1 \\times 1$ matrix (scalar) equal to $n$.\n\n$$\n\\mathbf{X}'\\mathbf{y} = \\begin{bmatrix} 1 & 1 & \\cdots & 1 \\end{bmatrix} \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix} = \\sum_{i=1}^{n} y_i\n$$\n\nThe normal equation becomes:\n\n$$\nn \\cdot b = \\sum_{i=1}^{n} y_i\n$$\n\nSolving:\n\n$$\nb = \\frac{1}{n}\\sum_{i=1}^{n} y_i = \\bar{y}\n$$\n\nSame result!\n\n### Solution Using Matrix Inverse\n\nThe general solution to the normal equations (when $\\mathbf{X}'\\mathbf{X}$ is invertible) is:\n\n$$\n\\mathbf{b} = (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y}\n$$ {#eq-ls-solution}\n\nFor our mean model:\n\n$$\n(\\mathbf{X}'\\mathbf{X})^{-1} = n^{-1} = \\frac{1}{n}\n$$\n\nTherefore:\n\n$$\nb = \\frac{1}{n} \\sum_{i=1}^{n} y_i = \\bar{y}\n$$\n\n::: {.callout-note}\n## NOTATION ESTABLISHED\n\nThis is Week 1, where we establish notation used throughout all 15 weeks:\n\n**Vectors and Matrices:**\n\n- $\\mathbf{y}$ = response/observation vector (lowercase bold)\n- $\\mathbf{X}$ = design matrix (uppercase bold)\n- $\\boldsymbol{\\beta}$ = parameter vector (Greek lowercase bold)\n- $\\mathbf{b}$ = estimate vector (lowercase bold)\n- $\\mathbf{e}$ = error/residual vector (lowercase bold)\n- $\\mathbf{I}$ = identity matrix (uppercase bold)\n\n**Operators:**\n\n- $\\mathbf{A}'$ or $\\mathbf{A}^\\top$ = transpose\n- $\\mathbf{A}^{-1}$ = matrix inverse\n- $\\mathbf{A}^{-}$ = generalized inverse (introduced Week 2)\n\n**Scalars:**\n\n- $n$ = sample size\n- $p$ = number of parameters\n- $\\sigma^2$ = variance\n\n**Statistical Quantities:**\n\n- $SSE$ = sum of squares for error\n- $SST$ = total sum of squares\n- $SSM$ = model sum of squares\n\nThis notation will be maintained consistently. Any extensions will be clearly marked.\n:::\n\n## Small Numerical Example: Dairy Milk Yield\n\nLet's work through a complete example calculating the mean milk yield for 5 Holstein cows.\n\n### Data\n\nDaily milk yield (kg/day) for 5 cows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny <- c(25, 28, 26, 30, 27)\nn <- length(y)\n\nprint(paste(\"Observations:\", paste(y, collapse = \", \")))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Observations: 25, 28, 26, 30, 27\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(paste(\"Sample size n =\", n))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Sample size n = 5\"\n```\n\n\n:::\n:::\n\n\n### Model\n\n$$\ny_i = \\mu + e_i, \\quad i = 1, 2, 3, 4, 5\n$$\n\nWe want to estimate $\\mu$ (the mean milk yield).\n\n### Design Matrix\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- matrix(1, nrow = n, ncol = 1)\nprint(\"Design matrix X:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Design matrix X:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]\n[1,]    1\n[2,]    1\n[3,]    1\n[4,]    1\n[5,]    1\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(paste(\"Dimensions:\", nrow(X), \"x\", ncol(X)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Dimensions: 5 x 1\"\n```\n\n\n:::\n:::\n\n\n### Compute X'X\n\n\n::: {.cell}\n\n```{.r .cell-code}\nXtX <- t(X) %*% X\nprint(\"X'X:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"X'X:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(XtX)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]\n[1,]    5\n```\n\n\n:::\n:::\n\n\nAs expected, $\\mathbf{X}'\\mathbf{X} = n = 5$.\n\n### Compute X'y\n\n\n::: {.cell}\n\n```{.r .cell-code}\nXty <- t(X) %*% y\nprint(\"X'y:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"X'y:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(Xty)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]\n[1,]  136\n```\n\n\n:::\n:::\n\n\nThis equals $\\sum y_i = 25 + 28 + 26 + 30 + 27 = 136$.\n\n### Solve Normal Equations\n\n$$\n\\mathbf{X}'\\mathbf{X} \\mathbf{b} = \\mathbf{X}'\\mathbf{y}\n$$\n$$\n5b = 136\n$$\n$$\nb = \\frac{136}{5} = 27.2\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Method 1: Using matrix inverse\nXtX_inv <- solve(XtX)  # Inverse of X'X\nb <- XtX_inv %*% Xty\nprint(\"Estimate of mu:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Estimate of mu:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]\n[1,] 27.2\n```\n\n\n:::\n\n```{.r .cell-code}\n# Method 2: Direct calculation\nb_direct <- sum(y) / n\nprint(\"Direct calculation (sample mean):\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Direct calculation (sample mean):\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(b_direct)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 27.2\n```\n\n\n:::\n:::\n\n\nBoth methods give $\\hat{\\mu} = 27.2$ kg/day.\n\n### Fitted Values and Residuals\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fitted values: y_hat = X * b\ny_hat <- X %*% b\nprint(\"Fitted values:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Fitted values:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(y_hat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]\n[1,] 27.2\n[2,] 27.2\n[3,] 27.2\n[4,] 27.2\n[5,] 27.2\n```\n\n\n:::\n\n```{.r .cell-code}\n# Residuals: e = y - y_hat\nresiduals <- y - y_hat\nprint(\"Residuals:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Residuals:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(residuals)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]\n[1,] -2.2\n[2,]  0.8\n[3,] -1.2\n[4,]  2.8\n[5,] -0.2\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check: sum of residuals should be zero\nprint(paste(\"Sum of residuals:\", sum(residuals)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Sum of residuals: -1.4210854715202e-14\"\n```\n\n\n:::\n:::\n\n\nEach fitted value is $\\hat{y}_i = \\hat{\\mu} = 27.2$.\n\nThe residuals are: $e_i = y_i - 27.2$.\n\n::: {.callout-tip}\n## Property of Least Squares\n\nFor any model with an intercept, the sum of residuals equals zero (within rounding error):\n\n$$\n\\sum_{i=1}^{n} e_i = 0\n$$\n:::\n\n### Sum of Squares\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Total sum of squares\ny_bar <- mean(y)\nSST <- sum((y - y_bar)^2)\nprint(paste(\"Total SS:\", SST))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Total SS: 14.8\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# Sum of squares for error\nSSE <- sum(residuals^2)\nprint(paste(\"Error SS:\", SSE))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Error SS: 14.8\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# Alternative calculation\nSSE_alt <- t(residuals) %*% residuals\nprint(paste(\"Error SS (matrix form):\", SSE_alt))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Error SS (matrix form): 14.8\"\n```\n\n\n:::\n:::\n\n\n### Verify Against lm()\n\nLet's verify our hand calculations match R's `lm()` function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit using lm()\nmodel <- lm(y ~ 1)  # Model with intercept only\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ 1)\n\nResiduals:\n   1    2    3    4    5 \n-2.2  0.8 -1.2  2.8 -0.2 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  27.2000     0.8602   31.62 5.96e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.924 on 4 degrees of freedom\n```\n\n\n:::\n\n```{.r .cell-code}\n# Compare our estimate to lm()\nprint(\"Our estimate:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Our estimate:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]\n[1,] 27.2\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(\"lm() estimate:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"lm() estimate:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(coef(model))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept) \n       27.2 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Are they equal?\nprint(paste(\"Match:\", all.equal(c(b), coef(model))))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Match: names for current but not for target\"\n```\n\n\n:::\n:::\n\n\nPerfect match!\n\n## R Implementation: Building Our First Solver\n\nLet's create a reusable function to compute the mean using the linear model approach:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Function to estimate mean using linear model framework\nestimate_mean <- function(y) {\n\n  # Sample size\n  n <- length(y)\n\n  # Design matrix (column of ones)\n  X <- matrix(1, nrow = n, ncol = 1)\n\n  # Compute X'X\n  XtX <- t(X) %*% X\n\n  # Compute X'y\n  Xty <- t(X) %*% y\n\n  # Solve normal equations: b = (X'X)^-1 X'y\n  b <- solve(XtX) %*% Xty\n\n  # Compute fitted values\n  y_hat <- X %*% b\n\n  # Compute residuals\n  residuals <- y - y_hat\n\n  # Compute SSE\n  SSE <- sum(residuals^2)\n\n  # Compute variance estimate\n  # sigma^2 = SSE / (n - p) where p = 1\n  sigma2_hat <- SSE / (n - 1)\n\n  # Return results as list\n  results <- list(\n    estimate = c(b),\n    fitted_values = c(y_hat),\n    residuals = c(residuals),\n    SSE = SSE,\n    sigma2 = sigma2_hat,\n    n = n\n  )\n\n  return(results)\n}\n\n# Test our function\ntest_data <- c(25, 28, 26, 30, 27)\nresults <- estimate_mean(test_data)\n\nprint(\"Results from our custom function:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Results from our custom function:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(results)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$estimate\n[1] 27.2\n\n$fitted_values\n[1] 27.2 27.2 27.2 27.2 27.2\n\n$residuals\n[1] -2.2  0.8 -1.2  2.8 -0.2\n\n$SSE\n[1] 14.8\n\n$sigma2\n[1] 3.7\n\n$n\n[1] 5\n```\n\n\n:::\n:::\n\n\nThis function computes everything using matrix operations, just as we'll do for more complex models in later weeks.\n\n## Realistic Livestock Application\n\nLet's apply our method to a larger dairy dataset.\n\n### Scenario\n\nA dairy researcher collects morning milk yield data from 30 Holstein cows to estimate the herd average production level. The data represents cows in similar stages of lactation (60-90 days in milk) under the same management system.\n\n### Generate Realistic Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set seed for reproducibility\nset.seed(123)\n\n# Generate realistic milk yield data (kg/day)\n# Mean around 27 kg, SD around 4 kg (typical for Holsteins)\nn <- 30\ntrue_mean <- 27\ntrue_sd <- 4\n\nmilk_yield <- round(rnorm(n, mean = true_mean, sd = true_sd), 1)\n\n# Ensure no negative values (biologically impossible)\nmilk_yield[milk_yield < 0] <- abs(milk_yield[milk_yield < 0])\n\nprint(\"Milk yield data (kg/day):\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Milk yield data (kg/day):\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(milk_yield)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 24.8 26.1 33.2 27.3 27.5 33.9 28.8 21.9 24.3 25.2 31.9 28.4 28.6 27.4 24.8\n[16] 34.1 29.0 19.1 29.8 25.1 22.7 26.1 22.9 24.1 24.5 20.3 30.4 27.6 22.4 32.0\n```\n\n\n:::\n:::\n\n\n### Exploratory Analysis\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Summary statistics\nsummary(milk_yield)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  19.10   24.35   26.70   26.81   28.95   34.10 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Standard deviation\nsd(milk_yield)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.922696\n```\n\n\n:::\n\n```{.r .cell-code}\n# Histogram\nhist(milk_yield,\n     breaks = 10,\n     main = \"Distribution of Milk Yield\",\n     xlab = \"Milk Yield (kg/day)\",\n     ylab = \"Frequency\",\n     col = \"lightblue\",\n     border = \"white\")\nabline(v = mean(milk_yield), col = \"red\", lwd = 2, lty = 2)\nlegend(\"topright\", legend = \"Sample Mean\", col = \"red\", lty = 2, lwd = 2)\n```\n\n::: {.cell-output-display}\n![Distribution of milk yield for 30 Holstein cows](Week01_CourseOverview_files/figure-html/exploratory-plot-1.png){width=768}\n:::\n:::\n\n\n### Estimate Mean Using Our Method\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Use our custom function\nresults_large <- estimate_mean(milk_yield)\n\nprint(\"Estimated mean milk yield:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Estimated mean milk yield:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(results_large$estimate)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 26.80667\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(\"Estimated variance:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Estimated variance:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(results_large$sigma2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 15.38754\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(\"Standard error of the mean:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Standard error of the mean:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nse_mean <- sqrt(results_large$sigma2 / results_large$n)\nprint(se_mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.7161829\n```\n\n\n:::\n:::\n\n\n### 95% Confidence Interval\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Degrees of freedom\ndf <- n - 1\n\n# Critical t-value (two-tailed, alpha = 0.05)\nt_crit <- qt(0.975, df)\n\n# Confidence interval\nlower <- results_large$estimate - t_crit * se_mean\nupper <- results_large$estimate + t_crit * se_mean\n\nprint(paste(\"95% CI: [\", round(lower, 2), \",\", round(upper, 2), \"]\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"95% CI: [ 25.34 , 28.27 ]\"\n```\n\n\n:::\n:::\n\n\n### Interpretation\n\nThe estimated mean milk yield is 26.81 kg/day with a 95% confidence interval of [25.34, 28.27] kg/day. This represents the average production level for this group of Holstein cows under these management conditions.\n\n### Compare with lm()\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_large <- lm(milk_yield ~ 1)\nsummary(model_large)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = milk_yield ~ 1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.7067 -2.4567 -0.1067  2.1433  7.2933 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  26.8067     0.7162   37.43   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.923 on 29 degrees of freedom\n```\n\n\n:::\n\n```{.r .cell-code}\n# Compare estimates\nprint(\"Our estimate vs lm():\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Our estimate vs lm():\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(c(results_large$estimate, coef(model_large)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            (Intercept) \n   26.80667    26.80667 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Check equality\nall.equal(results_large$estimate, coef(model_large)[[1]])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\nPerfect agreement!\n\n## Connection to Breeding Applications\n\n### How This Relates to Genetic Evaluation\n\nThis simple example introduces concepts we'll build on:\n\n1. **Design matrices**: We constructed $\\mathbf{X}$ from data structure\n2. **Normal equations**: We solved $\\mathbf{X}'\\mathbf{X}\\mathbf{b} = \\mathbf{X}'\\mathbf{y}$\n3. **Estimation**: We obtained BLUE (Best Linear Unbiased Estimator)\n4. **Variance**: We estimated $\\sigma^2$ from residuals\n\nIn genetic evaluation:\n\n- $\\mathbf{X}$ includes multiple fixed effects (herd, year, age, etc.)\n- $\\mathbf{Z}$ relates observations to breeding values\n- Normal equations become **mixed model equations**\n- We estimate both fixed effects and predict breeding values\n\n### Preview: Contemporary Groups\n\nIn real breeding programs, we don't estimate a single overall mean. Instead, we estimate means for **contemporary groups** - animals raised together under similar conditions.\n\nFor example:\n\n- Herd-Year-Season groups in dairy\n- Pen-Sex-Diet groups in swine\n- Flock-Year-Management groups in sheep\n\nEach group gets its own parameter in the design matrix. We'll learn how to build these matrices in Week 3.\n\n## Summary\n\nThis week we:\n\n1. Established the course philosophy: **understand through building**\n2. Connected linear models to animal breeding applications\n3. Set up our computing environment\n4. Reviewed essential matrix operations\n5. Expressed the sample mean as a linear model\n6. Derived the least squares estimator using:\n   - Calculus (minimizing SSE)\n   - Normal equations ($\\mathbf{X}'\\mathbf{X}\\mathbf{b} = \\mathbf{X}'\\mathbf{y}$)\n   - Matrix inverse ($\\mathbf{b} = (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y}$)\n7. Implemented a solver function in R\n8. Verified results against `lm()`\n9. Applied the method to realistic dairy data\n\n### Key Takeaways\n\n::: {.callout-note}\n## What We Learned\n\n- The sample mean is a special case of a linear model\n- Matrix notation provides a unified framework\n- Building solvers manually deepens understanding\n- The methods scale from simple means to complex breeding value prediction\n:::\n\n### Looking Ahead\n\n**Next week (Week 2)**, we'll dive deeper into linear algebra:\n\n- Matrix rank and linear independence\n- Regular and generalized inverses\n- Solving systems of equations\n- Properties critical for understanding estimability\n\nThis foundation will enable us to handle more complex models in subsequent weeks.\n\n## Additional Resources\n\n### R Documentation\n\n- `?matrix` - Creating matrices\n- `?solve` - Matrix inverse\n- `?lm` - Linear models\n\n### Recommended Reading\n\n- @searle1971 - Classic text on linear models\n- @searle2006 - Updated edition with modern examples\n- @henderson1984 - Application to animal breeding\n\n### Practice Dataset\n\nA CSV file with dairy milk yield data is available in the `data/` subdirectory for additional practice.\n\n---\n\n**Next**: [Week 2: Linear Algebra Essentials](../Week02_LinearAlgebra/Week02_LinearAlgebra.qmd)\n",
    "supporting": [
      "Week01_CourseOverview_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}