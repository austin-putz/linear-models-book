{
  "hash": "4b1fd068eec0f801b3dccbe8bb795b9b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Appendix B: Matrix Algebra Review\"\nnumber-sections: true\n---\n\n# Appendix B: Matrix Algebra Review {#sec-appendix-b .unnumbered}\n\n## Introduction {#sec-matrix-intro}\n\n### Purpose of This Appendix\n\nThis appendix serves as a comprehensive reference for matrix algebra concepts used throughout the 15-week linear models course. It is designed to complement **Week 2: Linear Algebra Essentials** by providing:\n\n- **Deeper coverage** of fundamental concepts\n- **Complete property listings** with proofs and examples\n- **Comprehensive R implementations** for every operation\n- **Quick reference tables** for identities and formulas\n- **Cross-references** to specific course weeks where concepts are applied\n\n:::{.callout-note}\n## Relationship to Week 2\n\nWeek 2 introduces essential matrix algebra concepts needed to get started with linear models. This appendix provides the detailed reference material you'll need as you progress through the course and encounter more advanced applications.\n:::\n\n### How to Use This Appendix\n\n- **As a quick reference**: Look up specific properties, formulas, or R functions\n- **For deeper understanding**: Read detailed explanations and work through examples\n- **To verify your work**: Use the provided R code templates to check your calculations\n- **To connect concepts**: Follow cross-references to see how matrix operations appear in different contexts\n\n### Organization\n\nThe appendix is organized into 13 main sections:\n\n1. **Introduction** (this section)\n2. **Basic Matrix Operations** - Addition, multiplication, transpose\n3. **Special Types of Matrices** - Identity, diagonal, symmetric, orthogonal, idempotent\n4. **Matrix Properties** - Rank, trace, determinant\n5. **Matrix Inverses** - Regular, generalized, identities\n6. **Projection Matrices and Quadratic Forms** - Critical for least squares theory\n7. **Eigenvalues and Eigenvectors** - Spectral decomposition, SVD\n8. **Matrix Calculus** - Derivatives, deriving normal equations\n9. **Kronecker Products** - For Animal models and multi-trait analysis\n10. **Matrix Identities** - Quick reference tables\n11. **Computational Considerations** - Stability, efficiency, verification\n12. **Cross-Reference Table** - Concept to week mapping\n13. **R Code Templates** - Ready-to-use functions\n\n:::{.callout-tip}\n## Navigation Tip\n\nUse the table of contents sidebar to quickly jump to any section. Within sections, look for **Cross-references** boxes that link to relevant course weeks.\n:::\n\n### R Package Setup\n\nThroughout this appendix, we'll use the following R packages:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Base R (always available)\n# - Matrix operations: %*%, t(), solve()\n# - Basic functions: matrix(), diag(), det()\n\n# MASS package for generalized inverse\nlibrary(MASS)  # ginv()\n\n# Matrix package for advanced operations\nlibrary(Matrix)  # rankMatrix(), sparse matrices, cond()\n\n# Set display options\noptions(digits = 4)  # Display 4 decimal places\n```\n:::\n\n\n:::{.callout-important}\n## Required Packages\n\nMake sure to install these packages if you haven't already:\n\n```r\ninstall.packages(\"MASS\")\ninstall.packages(\"Matrix\")\n```\n:::\n\n---\n\n## Basic Matrix Operations {#sec-basic-operations}\n\n### Matrix Addition and Scalar Multiplication {#sec-addition}\n\n**Definition:**\n\nFor matrices **A** and **B** of the same dimensions (m × n), matrix addition is element-wise:\n\n$$\n(\\mathbf{A} + \\mathbf{B})_{ij} = a_{ij} + b_{ij}\n$$ {#eq-matrix-addition}\n\nScalar multiplication multiplies every element by a constant c:\n\n$$\n(c\\mathbf{A})_{ij} = c \\cdot a_{ij}\n$$ {#eq-scalar-mult}\n\n**Properties:**\n\n- **Commutative**: $\\mathbf{A} + \\mathbf{B} = \\mathbf{B} + \\mathbf{A}$\n- **Associative**: $(\\mathbf{A} + \\mathbf{B}) + \\mathbf{C} = \\mathbf{A} + (\\mathbf{B} + \\mathbf{C})$\n- **Distributive**: $c(\\mathbf{A} + \\mathbf{B}) = c\\mathbf{A} + c\\mathbf{B}$\n- **Identity element**: $\\mathbf{A} + \\mathbf{0} = \\mathbf{A}$ where **0** is the zero matrix\n\n**R Implementation:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define two 2x3 matrices\nA <- matrix(c(1, 2, 3, 4, 5, 6), nrow = 2, ncol = 3, byrow = TRUE)\nB <- matrix(c(2, 1, 0, 1, 2, 1), nrow = 2, ncol = 3, byrow = TRUE)\n\ncat(\"Matrix A:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMatrix A:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nMatrix B:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nMatrix B:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(B)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]    2    1    0\n[2,]    1    2    1\n```\n\n\n:::\n\n```{.r .cell-code}\n# Addition\ncat(\"\\nA + B:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nA + B:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A + B)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]    3    3    3\n[2,]    5    7    7\n```\n\n\n:::\n\n```{.r .cell-code}\n# Scalar multiplication\nc <- 3\ncat(\"\\n3A:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n3A:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(c * A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]    3    6    9\n[2,]   12   15   18\n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify commutative property\ncat(\"\\nVerify A + B = B + A:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVerify A + B = B + A:\n```\n\n\n:::\n\n```{.r .cell-code}\nall.equal(A + B, B + A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\n**Livestock Example:**\n\nConsider average daily gain (kg/day) for two groups of beef steers on different rations:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Pen 1 ADG (3 steers)\npen1 <- matrix(c(1.2, 1.3, 1.1), nrow = 1)\n\n# Pen 2 ADG (3 steers)\npen2 <- matrix(c(1.0, 1.1, 0.9), nrow = 1)\n\ncat(\"Pen 1 ADG:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPen 1 ADG:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(pen1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]  1.2  1.3  1.1\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nPen 2 ADG:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nPen 2 ADG:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(pen2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]    1  1.1  0.9\n```\n\n\n:::\n\n```{.r .cell-code}\n# Combined average\ncat(\"\\nAverage of both pens:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nAverage of both pens:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint((pen1 + pen2) / 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]  1.1  1.2    1\n```\n\n\n:::\n:::\n\n\n:::{.callout-note}\n## Cross-Reference\n\nMatrix addition is used throughout the course, starting in **Week 1** for basic computations and appearing in every subsequent week.\n:::\n\n---\n\n### Matrix Multiplication {#sec-multiplication}\n\n**Definition:**\n\nFor matrix **A** of dimension (m × n) and matrix **B** of dimension (n × p), the product **AB** is an (m × p) matrix where:\n\n$$\n(\\mathbf{AB})_{ij} = \\sum_{k=1}^{n} a_{ik}b_{kj}\n$$ {#eq-matrix-mult}\n\nThe element in row i, column j of **AB** is the dot product of row i of **A** with column j of **B**.\n\n**Critical Requirement:** The number of columns in **A** must equal the number of rows in **B**.\n\n**Dimension Rule:**\n$$\n\\underbrace{\\mathbf{A}}_{m \\times n} \\times \\underbrace{\\mathbf{B}}_{n \\times p} = \\underbrace{\\mathbf{AB}}_{m \\times p}\n$$ {#eq-dimension-rule}\n\n**Properties:**\n\n1. **Non-commutative**: In general, $\\mathbf{AB} \\neq \\mathbf{BA}$ (even when both are defined)\n2. **Associative**: $(\\mathbf{AB})\\mathbf{C} = \\mathbf{A}(\\mathbf{BC})$\n3. **Distributive**: $\\mathbf{A}(\\mathbf{B} + \\mathbf{C}) = \\mathbf{AB} + \\mathbf{AC}$\n4. **Scalar multiplication**: $(c\\mathbf{A})\\mathbf{B} = c(\\mathbf{AB}) = \\mathbf{A}(c\\mathbf{B})$\n5. **Identity**: $\\mathbf{AI} = \\mathbf{IA} = \\mathbf{A}$ where **I** is identity matrix of appropriate dimension\n\n**R Implementation:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example 1: Basic multiplication\nA <- matrix(c(1, 2, 3, 4, 5, 6), nrow = 2, ncol = 3, byrow = TRUE)\nB <- matrix(c(1, 0, 2, 1, 0, 1), nrow = 3, ncol = 2, byrow = TRUE)\n\ncat(\"Matrix A (2×3):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMatrix A (2×3):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nMatrix B (3×2):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nMatrix B (3×2):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(B)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    1    0\n[2,]    2    1\n[3,]    0    1\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nA %*% B (2×2):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nA %*% B (2×2):\n```\n\n\n:::\n\n```{.r .cell-code}\nAB <- A %*% B  # %*% is matrix multiplication in R\nprint(AB)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    5    5\n[2,]   14   11\n```\n\n\n:::\n\n```{.r .cell-code}\n# Manual calculation of element (1,1)\ncat(\"\\nManual calculation of (AB)[1,1]:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nManual calculation of (AB)[1,1]:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"= A[1,1]*B[1,1] + A[1,2]*B[2,1] + A[1,3]*B[3,1]\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n= A[1,1]*B[1,1] + A[1,2]*B[2,1] + A[1,3]*B[3,1]\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"= %d*%d + %d*%d + %d*%d = %d\\n\",\n            A[1,1], B[1,1], A[1,2], B[2,1], A[1,3], B[3,1],\n            A[1,1]*B[1,1] + A[1,2]*B[2,1] + A[1,3]*B[3,1]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n= 1*1 + 2*2 + 3*0 = 5\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Verify non-commutativity\nBA <- B %*% A  # Different result!\n\ncat(\"B %*% A (3×3):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nB %*% A (3×3):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(BA)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    6    9   12\n[3,]    4    5    6\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nNote: A %*% B is 2×2, but B %*% A is 3×3\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nNote: A %*% B is 2×2, but B %*% A is 3×3\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nMatrix multiplication is NOT commutative!\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nMatrix multiplication is NOT commutative!\n```\n\n\n:::\n:::\n\n\n**Livestock Example - Normal Equations:**\n\nThe most important matrix multiplication in linear models is forming the normal equations **X'Xb = X'y**.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simple regression: milk yield (y) vs days in milk (x)\n# n = 4 dairy cows\ndays <- c(30, 60, 90, 120)\nyield <- c(35, 32, 28, 25)  # kg/day\n\n# Design matrix (n×2): intercept and days\nX <- cbind(1, days)\ny <- matrix(yield, ncol = 1)\n\ncat(\"Design matrix X (4×2):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDesign matrix X (4×2):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       days\n[1,] 1   30\n[2,] 1   60\n[3,] 1   90\n[4,] 1  120\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nResponse vector y (4×1):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nResponse vector y (4×1):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]\n[1,]   35\n[2,]   32\n[3,]   28\n[4,]   25\n```\n\n\n:::\n\n```{.r .cell-code}\n# Form X'X (the \"information matrix\")\nXtX <- t(X) %*% X\ncat(\"\\nX'X (2×2):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nX'X (2×2):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(XtX)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          days\n       4   300\ndays 300 27000\n```\n\n\n:::\n\n```{.r .cell-code}\n# Form X'y (the \"right-hand side\")\nXty <- t(X) %*% y\ncat(\"\\nX'y (2×1):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nX'y (2×1):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(Xty)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]\n      120\ndays 8490\n```\n\n\n:::\n\n```{.r .cell-code}\n# Solve for regression coefficients\nb <- solve(XtX) %*% Xty\ncat(\"\\nRegression coefficients b = (X'X)^(-1)X'y:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nRegression coefficients b = (X'X)^(-1)X'y:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        [,1]\n     38.5000\ndays -0.1133\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"\\nInterpretation: Intercept = %.2f kg/day, Slope = %.4f kg/day per day\\n\",\n            b[1], b[2]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nInterpretation: Intercept = 38.50 kg/day, Slope = -0.1133 kg/day per day\n```\n\n\n:::\n:::\n\n\n**Important Special Cases:**\n\n1. **Quadratic forms**: $\\mathbf{x}'\\mathbf{Ax}$ (scalar result)\n   - Example: Sum of squares = $\\mathbf{e}'\\mathbf{e}$ where **e** is residual vector\n\n2. **Outer product**: For vectors **u** (n×1) and **v** (m×1), $\\mathbf{uv}'$ is n×m matrix\n\n3. **Inner product**: For vectors **u** and **v** (both n×1), $\\mathbf{u}'\\mathbf{v}$ is scalar\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Quadratic form: sum of squares\ne <- c(-1.2, 0.5, 0.8, -0.1)  # Residuals\nSSE <- t(e) %*% e  # Same as sum(e^2)\ncat(\"Sum of squared errors (e'e):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSum of squared errors (e'e):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(SSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]\n[1,] 2.34\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"Verification: sum(e^2) = %.4f\\n\", sum(e^2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nVerification: sum(e^2) = 2.3400\n```\n\n\n:::\n\n```{.r .cell-code}\n# Outer product\nu <- matrix(c(1, 2, 3), ncol = 1)\nv <- matrix(c(4, 5), ncol = 1)\nouter_prod <- u %*% t(v)\ncat(\"\\nOuter product u v' (3×2):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nOuter product u v' (3×2):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(outer_prod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    4    5\n[2,]    8   10\n[3,]   12   15\n```\n\n\n:::\n\n```{.r .cell-code}\n# Inner product (dot product)\nw <- c(1, 2, 3)\nz <- c(4, 5, 6)\ninner_prod <- t(w) %*% z  # Or: sum(w * z)\ncat(\"\\nInner product w'z (scalar):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nInner product w'z (scalar):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(inner_prod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]\n[1,]   32\n```\n\n\n:::\n:::\n\n\n**Livestock Example - Multi-Trait Analysis:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Swine: 3 pigs, 2 traits (backfat mm, loin depth mm)\n# Raw data matrix\nY <- matrix(c(12, 15, 10,   # Backfat for pigs 1, 2, 3\n              65, 70, 68),  # Loin depth for pigs 1, 2, 3\n            nrow = 3, ncol = 2)\ncolnames(Y) <- c(\"Backfat\", \"LoinDepth\")\n\ncat(\"Multi-trait data Y (3 pigs × 2 traits):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMulti-trait data Y (3 pigs × 2 traits):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(Y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     Backfat LoinDepth\n[1,]      12        65\n[2,]      15        70\n[3,]      10        68\n```\n\n\n:::\n\n```{.r .cell-code}\n# Compute trait covariance matrix: (Y'Y) / (n-1)\n# This involves matrix multiplication\nYtY <- t(Y) %*% Y\ncat(\"\\nY'Y (sums of squares and cross-products):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nY'Y (sums of squares and cross-products):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(YtY)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Backfat LoinDepth\nBackfat       469      2510\nLoinDepth    2510     13749\n```\n\n\n:::\n\n```{.r .cell-code}\ncov_matrix <- YtY / (nrow(Y) - 1)\ncat(\"\\nCovariance matrix (2×2):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCovariance matrix (2×2):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(cov_matrix)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Backfat LoinDepth\nBackfat     234.5      1255\nLoinDepth  1255.0      6874\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"\\nInterpretation:\\n\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nInterpretation:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"  Var(Backfat) = %.2f\\n\", cov_matrix[1,1]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Var(Backfat) = 234.50\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"  Var(LoinDepth) = %.2f\\n\", cov_matrix[2,2]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Var(LoinDepth) = 6874.50\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"  Cov(Backfat, LoinDepth) = %.2f\\n\", cov_matrix[1,2]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Cov(Backfat, LoinDepth) = 1255.00\n```\n\n\n:::\n:::\n\n\n:::{.callout-warning}\n## Common Mistakes with Matrix Multiplication\n\n1. **Wrong operator**: Use `%*%` for matrix multiplication, not `*` (which is element-wise)\n2. **Dimension mismatch**: Check that ncol(A) == nrow(B) before computing A %*% B\n3. **Order matters**: AB ≠ BA in general\n4. **Vectors**: R treats vectors flexibly; be explicit with `matrix()` or transpose `t()` to avoid confusion\n:::\n\n:::{.callout-note}\n## Cross-References\n\nMatrix multiplication appears constantly in linear models:\n\n- **Week 3**: Building design matrix **X** and forming **X'X**\n- **Week 4-6**: Normal equations **X'Xb = X'y**\n- **Week 5**: Hat matrix **H = X(X'X)^(-1)X'** involves three matrix multiplications\n- **Week 11**: Computing fitted values **ŷ = Xb** and residuals **e = y - Xb**\n:::\n\n---\n\n### Matrix Transpose {#sec-transpose}\n\n**Definition:**\n\nThe **transpose** of a matrix **A** (denoted **A'** or **A**^T^) is obtained by interchanging rows and columns:\n\n$$\n(\\mathbf{A}')_{ij} = (\\mathbf{A})_{ji}\n$$ {#eq-transpose-def}\n\nIf **A** is m × n, then **A'** is n × m.\n\n**Properties:**\n\n1. **Double transpose**: $(\\mathbf{A}')' = \\mathbf{A}$\n2. **Sum transpose**: $(\\mathbf{A} + \\mathbf{B})' = \\mathbf{A}' + \\mathbf{B}'$\n3. **Product transpose**: $(\\mathbf{AB})' = \\mathbf{B}'\\mathbf{A}'$ (reverse order!)\n4. **Scalar multiply**: $(c\\mathbf{A})' = c\\mathbf{A}'$\n5. **Inverse transpose**: $(\\mathbf{A}^{-1})' = (\\mathbf{A}')^{-1}$ (for invertible matrices)\n\n**Critical Property for Linear Models:**\n\nThe transpose of a product **reverses the order**: $(\\mathbf{ABC})' = \\mathbf{C}'\\mathbf{B}'\\mathbf{A}'$\n\nThis is essential when manipulating expressions like $(\\mathbf{X}'\\mathbf{X})^{-1}$ or $(\\mathbf{y} - \\mathbf{Xb})'(\\mathbf{y} - \\mathbf{Xb})$.\n\n**R Implementation:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a 3x2 matrix\nA <- matrix(c(1, 2, 3, 4, 5, 6), nrow = 3, ncol = 2, byrow = TRUE)\n\ncat(\"Matrix A (3×2):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMatrix A (3×2):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n[3,]    5    6\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nTranspose A' (2×3):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nTranspose A' (2×3):\n```\n\n\n:::\n\n```{.r .cell-code}\nAt <- t(A)\nprint(At)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify double transpose\ncat(\"\\nVerify (A')' = A:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVerify (A')' = A:\n```\n\n\n:::\n\n```{.r .cell-code}\nall.equal(t(At), A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\n# Product transpose rule\nB <- matrix(c(1, 0, 2, 1), nrow = 2, ncol = 2, byrow = TRUE)\ncat(\"\\nMatrix B (2×2):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nMatrix B (2×2):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(B)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    1    0\n[2,]    2    1\n```\n\n\n:::\n\n```{.r .cell-code}\nAB <- A %*% B\ncat(\"\\nA %*% B (3×2):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nA %*% B (3×2):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(AB)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    5    2\n[2,]   11    4\n[3,]   17    6\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\n(AB)':\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n(AB)':\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(t(AB))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]    5   11   17\n[2,]    2    4    6\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nB' %*% A' (should equal (AB)'):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nB' %*% A' (should equal (AB)'):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(t(B) %*% t(A))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]    5   11   17\n[2,]    2    4    6\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nVerify (AB)' = B'A':\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVerify (AB)' = B'A':\n```\n\n\n:::\n\n```{.r .cell-code}\nall.equal(t(AB), t(B) %*% t(A))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\n**Livestock Example - Normal Equations:**\n\nThe normal equations **X'Xb = X'y** rely fundamentally on the transpose operation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Beef cattle: ADG (kg/day) for 5 steers\nadg <- c(1.2, 1.4, 1.1, 1.3, 1.5)\ny <- matrix(adg, ncol = 1)  # Column vector (5×1)\n\ncat(\"Response vector y (5×1):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nResponse vector y (5×1):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]\n[1,]  1.2\n[2,]  1.4\n[3,]  1.1\n[4,]  1.3\n[5,]  1.5\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nTranspose y' (1×5):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nTranspose y' (1×5):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(t(y))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3] [,4] [,5]\n[1,]  1.2  1.4  1.1  1.3  1.5\n```\n\n\n:::\n\n```{.r .cell-code}\n# Simple model: y = μ + e (estimate the mean)\nX <- matrix(1, nrow = 5, ncol = 1)  # Column of ones\n\ncat(\"\\nDesign matrix X (5×1):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nDesign matrix X (5×1):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]\n[1,]    1\n[2,]    1\n[3,]    1\n[4,]    1\n[5,]    1\n```\n\n\n:::\n\n```{.r .cell-code}\n# Form X'X\nXtX <- t(X) %*% X\ncat(\"\\nX'X (1×1 scalar):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nX'X (1×1 scalar):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(XtX)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]\n[1,]    5\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"Interpretation: X'X = %d (the sample size)\\n\", XtX[1,1]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nInterpretation: X'X = 5 (the sample size)\n```\n\n\n:::\n\n```{.r .cell-code}\n# Form X'y\nXty <- t(X) %*% y\ncat(\"\\nX'y (1×1 scalar):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nX'y (1×1 scalar):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(Xty)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]\n[1,]  6.5\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"Interpretation: X'y = %.1f (the sum of observations)\\n\", Xty[1,1]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nInterpretation: X'y = 6.5 (the sum of observations)\n```\n\n\n:::\n\n```{.r .cell-code}\n# Solve: b = (X'X)^(-1) X'y = sum(y) / n\nb <- solve(XtX) %*% Xty\ncat(sprintf(\"\\nEstimated mean: %.3f kg/day\\n\", b[1,1]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nEstimated mean: 1.300 kg/day\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"Verification: mean(y) = %.3f\\n\", mean(adg)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nVerification: mean(y) = 1.300\n```\n\n\n:::\n:::\n\n\n**Symmetric Matrices:**\n\nA matrix is **symmetric** if **A = A'**. This property is crucial in linear models.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# X'X is always symmetric\nX <- matrix(c(1, 1, 1, 1,\n              2, 3, 1, 4), nrow = 4, ncol = 2)\n\ncat(\"Design matrix X (4×2):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDesign matrix X (4×2):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    1    2\n[2,]    1    3\n[3,]    1    1\n[4,]    1    4\n```\n\n\n:::\n\n```{.r .cell-code}\nXtX <- t(X) %*% X\ncat(\"\\nX'X (2×2):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nX'X (2×2):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(XtX)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    4   10\n[2,]   10   30\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\n(X'X)' (transpose of X'X):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n(X'X)' (transpose of X'X):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(t(XtX))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    4   10\n[2,]   10   30\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nVerify X'X is symmetric:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVerify X'X is symmetric:\n```\n\n\n:::\n\n```{.r .cell-code}\nall.equal(XtX, t(XtX))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\n# Why is X'X always symmetric? Proof:\n# (X'X)' = X'(X')' = X'X ✓\n```\n:::\n\n\n**Livestock Example - Sum of Squares:**\n\nThe sum of squared errors **SSE = e'e** uses transpose to convert a column vector to a scalar.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Residuals from a regression\ne <- matrix(c(-0.5, 0.8, -0.3, 0.2, -0.2), ncol = 1)\n\ncat(\"Residual vector e (5×1):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nResidual vector e (5×1):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(e)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]\n[1,] -0.5\n[2,]  0.8\n[3,] -0.3\n[4,]  0.2\n[5,] -0.2\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nTranspose e' (1×5):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nTranspose e' (1×5):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(t(e))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3] [,4] [,5]\n[1,] -0.5  0.8 -0.3  0.2 -0.2\n```\n\n\n:::\n\n```{.r .cell-code}\n# Sum of squared errors\nSSE <- t(e) %*% e  # (1×5) %*% (5×1) = (1×1) scalar\ncat(\"\\nSSE = e'e:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSSE = e'e:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(SSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]\n[1,] 1.06\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"\\nInterpretation: SSE = %.4f\\n\", SSE[1,1]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nInterpretation: SSE = 1.0600\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"Verification: sum(e^2) = %.4f\\n\", sum(e^2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nVerification: sum(e^2) = 1.0600\n```\n\n\n:::\n\n```{.r .cell-code}\n# Equivalent computations\ncat(\"\\nThree ways to compute SSE:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nThree ways to compute SSE:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"1. t(e) %%*%% e = %.4f\\n\", (t(e) %*% e)[1,1]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n1. t(e) %*% e = 1.0600\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"2. sum(e^2) = %.4f\\n\", sum(e^2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n2. sum(e^2) = 1.0600\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"3. crossprod(e) = %.4f\\n\", crossprod(e)[1,1]))  # More efficient\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n3. crossprod(e) = 1.0600\n```\n\n\n:::\n:::\n\n\n**Useful R Functions:**\n\n- `t(A)`: Transpose of matrix A\n- `crossprod(X, y)`: Computes **X'y** efficiently (more efficient than `t(X) %*% y`)\n- `crossprod(X)`: Computes **X'X** efficiently (more efficient than `t(X) %*% X`)\n- `tcrossprod(X, Y)`: Computes **XY'** efficiently\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Efficiency comparison for large matrices\nX <- matrix(rnorm(1000), nrow = 100, ncol = 10)\n\n# Standard way\nsystem.time({\n  XtX_standard <- t(X) %*% X\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n      0       0       0 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Efficient way\nsystem.time({\n  XtX_efficient <- crossprod(X)\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n      0       0       0 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Results are identical:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nResults are identical:\n```\n\n\n:::\n\n```{.r .cell-code}\nall.equal(XtX_standard, XtX_efficient)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nFor normal equations, always use:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nFor normal equations, always use:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  XtX <- crossprod(X)     # Instead of t(X) %*% X\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  XtX <- crossprod(X)     # Instead of t(X) %*% X\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  Xty <- crossprod(X, y)  # Instead of t(X) %*% y\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Xty <- crossprod(X, y)  # Instead of t(X) %*% y\n```\n\n\n:::\n:::\n\n\n:::{.callout-important}\n## Critical for Linear Models\n\nThe transpose appears in virtually every linear model calculation:\n\n1. **Normal equations**: **X'Xb = X'y**\n2. **Sum of squares**: **SSE = (y - Xb)'(y - Xb) = e'e**\n3. **Variance of estimates**: **Var(b) = (X'X)^(-1)σ²**\n4. **Hat matrix**: **H = X(X'X)^(-1)X'**\n\nAlways remember: $(\\mathbf{AB})' = \\mathbf{B}'\\mathbf{A}'$ (reverse the order!)\n:::\n\n:::{.callout-note}\n## Cross-References\n\nThe transpose operation is used in:\n\n- **Week 1-2**: Basic matrix operations\n- **Week 3**: Forming **X'X** from design matrix\n- **Week 4-6**: All regression calculations\n- **Week 5**: Deriving least squares via $\\partial(\\mathbf{e}'\\mathbf{e})/\\partial\\mathbf{β}$\n- **Week 8**: Testing contrasts $\\mathbf{c}'\\mathbf{b}$\n- **Week 11**: Computing residuals and diagnostics\n:::\n\n---\n\n## Special Types of Matrices {#sec-special-matrices}\n\n### Identity Matrix {#sec-identity}\n\n**Definition:**\n\nThe **identity matrix** **I**~n~ is an n × n square matrix with 1's on the main diagonal and 0's elsewhere:\n\n$$\n\\mathbf{I}_n = \\begin{bmatrix}\n1 & 0 & 0 & \\cdots & 0 \\\\\n0 & 1 & 0 & \\cdots & 0 \\\\\n0 & 0 & 1 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & 1\n\\end{bmatrix}\n$$ {#eq-identity-matrix}\n\nEquivalently: $(\\mathbf{I})_{ij} = \\begin{cases} 1 & \\text{if } i = j \\\\ 0 & \\text{if } i \\neq j \\end{cases}$\n\n**Key Properties:**\n\n1. **Multiplicative identity**: $\\mathbf{AI} = \\mathbf{IA} = \\mathbf{A}$ for any matrix **A** of compatible dimension\n2. **Inverse of itself**: $\\mathbf{I}^{-1} = \\mathbf{I}$\n3. **Idempotent**: $\\mathbf{I}^2 = \\mathbf{I}$\n4. **Symmetric**: $\\mathbf{I}' = \\mathbf{I}$\n5. **Trace**: $\\text{tr}(\\mathbf{I}_n) = n$\n6. **Determinant**: $\\det(\\mathbf{I}_n) = 1$\n7. **Rank**: $r(\\mathbf{I}_n) = n$ (full rank)\n\n**R Implementation:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create identity matrices of various sizes\nI2 <- diag(2)\nI3 <- diag(3)\nI5 <- diag(5)\n\ncat(\"I_2 (2×2 identity):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nI_2 (2×2 identity):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(I2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nI_3 (3×3 identity):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nI_3 (3×3 identity):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(I3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify multiplicative identity property\nA <- matrix(c(1, 2, 3, 4, 5, 6), nrow = 2, ncol = 3)\ncat(\"\\nMatrix A (2×3):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nMatrix A (2×3):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nI_2 %*% A (should equal A):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nI_2 %*% A (should equal A):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(I2 %*% A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nA %*% I_3 (should equal A):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nA %*% I_3 (should equal A):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A %*% I3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nVerify IA = A:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVerify IA = A:\n```\n\n\n:::\n\n```{.r .cell-code}\nall.equal(I2 %*% A, A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nVerify AI = A:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVerify AI = A:\n```\n\n\n:::\n\n```{.r .cell-code}\nall.equal(A %*% I3, A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\n**Role in Linear Models:**\n\nThe identity matrix appears frequently in variance-covariance structures and residual matrices.\n\n**1. Homoscedastic, Independent Errors:**\n\nThe assumption $\\text{Var}(\\mathbf{e}) = \\sigma^2\\mathbf{I}_n$ means:\n- All errors have the same variance $\\sigma^2$ (homoscedasticity)\n- Errors are uncorrelated (independence)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example: n = 4 observations\nn <- 4\nsigma_sq <- 2.5\n\n# Variance-covariance matrix of errors\nVar_e <- sigma_sq * diag(n)\n\ncat(\"Var(e) = σ²I where σ² = 2.5:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nVar(e) = σ²I where σ² = 2.5:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(Var_e)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3] [,4]\n[1,]  2.5  0.0  0.0  0.0\n[2,]  0.0  2.5  0.0  0.0\n[3,]  0.0  0.0  2.5  0.0\n[4,]  0.0  0.0  0.0  2.5\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nInterpretation:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nInterpretation:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  - Diagonal elements = 2.5 (common variance)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  - Diagonal elements = 2.5 (common variance)\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  - Off-diagonal elements = 0 (no correlation)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  - Off-diagonal elements = 0 (no correlation)\n```\n\n\n:::\n:::\n\n\n**2. Residual Projection:**\n\nThe matrix $\\mathbf{I} - \\mathbf{H}$ projects observations onto the residual space (see @sec-projection-matrices).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simple example with n=3\nX <- cbind(1, c(1, 2, 3))  # Intercept and slope\nn <- nrow(X)\n\n# Hat matrix\nH <- X %*% solve(t(X) %*% X) %*% t(X)\n\ncat(\"Hat matrix H (3×3):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nHat matrix H (3×3):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(H)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        [,1]   [,2]    [,3]\n[1,]  0.8333 0.3333 -0.1667\n[2,]  0.3333 0.3333  0.3333\n[3,] -0.1667 0.3333  0.8333\n```\n\n\n:::\n\n```{.r .cell-code}\n# Residual projection matrix\nI_minus_H <- diag(n) - H\n\ncat(\"\\nI - H (3×3):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nI - H (3×3):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(I_minus_H)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        [,1]    [,2]    [,3]\n[1,]  0.1667 -0.3333  0.1667\n[2,] -0.3333  0.6667 -0.3333\n[3,]  0.1667 -0.3333  0.1667\n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify idempotence\ncat(\"\\nVerify (I-H)² = I-H:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVerify (I-H)² = I-H:\n```\n\n\n:::\n\n```{.r .cell-code}\nall.equal((I_minus_H) %*% (I_minus_H), I_minus_H)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\n**Livestock Example - Weighted Least Squares:**\n\nWhen observations have different precisions, we use $\\mathbf{W} = \\text{diag}(w_1, ..., w_n)$ instead of $\\mathbf{I}$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Pen-average ADG with different pen sizes\npen_avg <- c(1.2, 1.3, 1.1, 1.4)  # kg/day\npen_size <- c(10, 15, 8, 12)      # Number of pigs per pen\n\n# Weights proportional to pen size (more pigs = more precise)\nweights <- pen_size / mean(pen_size)  # Standardized weights\n\ncat(\"Pen averages and sizes:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPen averages and sizes:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(data.frame(ADG = pen_avg, PenSize = pen_size, Weight = weights))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  ADG PenSize Weight\n1 1.2      10 0.8889\n2 1.3      15 1.3333\n3 1.1       8 0.7111\n4 1.4      12 1.0667\n```\n\n\n:::\n\n```{.r .cell-code}\n# Weight matrix (diagonal)\nW <- diag(weights)\ncat(\"\\nWeight matrix W (4×4):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nWeight matrix W (4×4):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(W)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       [,1]  [,2]   [,3]  [,4]\n[1,] 0.8889 0.000 0.0000 0.000\n[2,] 0.0000 1.333 0.0000 0.000\n[3,] 0.0000 0.000 0.7111 0.000\n[4,] 0.0000 0.000 0.0000 1.067\n```\n\n\n:::\n\n```{.r .cell-code}\n# Weighted mean: (1'W1)^(-1) 1'Wy\nones <- matrix(1, nrow = 4, ncol = 1)\ny <- matrix(pen_avg, ncol = 1)\n\nweighted_mean <- solve(t(ones) %*% W %*% ones) %*% t(ones) %*% W %*% y\ncat(sprintf(\"\\nWeighted mean ADG: %.3f kg/day\\n\", weighted_mean[1,1]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nWeighted mean ADG: 1.269 kg/day\n```\n\n\n:::\n\n```{.r .cell-code}\n# Compare with unweighted mean\ncat(sprintf(\"Unweighted mean ADG: %.3f kg/day\\n\", mean(pen_avg)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nUnweighted mean ADG: 1.250 kg/day\n```\n\n\n:::\n:::\n\n\n:::{.callout-note}\n## Creating Identity Matrices in R\n\nThree equivalent ways:\n```r\nI <- diag(n)           # Most common\nI <- diag(1, n)        # Explicit\nI <- diag(rep(1, n))   # Using rep()\n```\n:::\n\n:::{.callout-note}\n## Cross-References\n\nIdentity matrix appears in:\n\n- **Week 5**: Variance assumption $\\text{Var}(\\mathbf{e}) = \\sigma^2\\mathbf{I}$\n- **Week 6**: Residual projection matrix $\\mathbf{I} - \\mathbf{H}$\n- **Week 10**: Weighted least squares with $\\mathbf{W}$ replacing $\\mathbf{I}$\n- **Week 14**: Generalizations to non-identity covariance structures\n:::\n\n---\n\n### Diagonal Matrices {#sec-diagonal}\n\n**Definition:**\n\nA **diagonal matrix** **D** is a square matrix with non-zero elements only on the main diagonal:\n\n$$\n\\mathbf{D} = \\begin{bmatrix}\nd_1 & 0 & 0 & \\cdots & 0 \\\\\n0 & d_2 & 0 & \\cdots & 0 \\\\\n0 & 0 & d_3 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & d_n\n\\end{bmatrix}\n$$ {#eq-diagonal-matrix}\n\nNotation: $\\mathbf{D} = \\text{diag}(d_1, d_2, ..., d_n)$\n\nEquivalently: $(\\mathbf{D})_{ij} = \\begin{cases} d_i & \\text{if } i = j \\\\ 0 & \\text{if } i \\neq j \\end{cases}$\n\n**Key Properties:**\n\n1. **Multiplication is commutative**: $\\mathbf{D}_1\\mathbf{D}_2 = \\mathbf{D}_2\\mathbf{D}_1$ (rare for matrices!)\n2. **Simple inverse**: $\\mathbf{D}^{-1} = \\text{diag}(1/d_1, 1/d_2, ..., 1/d_n)$ if all $d_i \\neq 0$\n3. **Determinant**: $\\det(\\mathbf{D}) = \\prod_{i=1}^{n} d_i$ (product of diagonal elements)\n4. **Trace**: $\\text{tr}(\\mathbf{D}) = \\sum_{i=1}^{n} d_i$ (sum of diagonal elements)\n5. **Rank**: $r(\\mathbf{D})$ = number of non-zero $d_i$\n6. **Powers**: $\\mathbf{D}^k = \\text{diag}(d_1^k, d_2^k, ..., d_n^k)$\n7. **Symmetric**: $\\mathbf{D}' = \\mathbf{D}$\n\n**R Implementation:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create diagonal matrices\nD1 <- diag(c(2, 3, 5))\nD2 <- diag(c(1, 4, 2))\n\ncat(\"D1:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nD1:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(D1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]    2    0    0\n[2,]    0    3    0\n[3,]    0    0    5\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nD2:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nD2:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(D2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    4    0\n[3,]    0    0    2\n```\n\n\n:::\n\n```{.r .cell-code}\n# Multiplication is commutative\ncat(\"\\nD1 %*% D2:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nD1 %*% D2:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(D1 %*% D2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]    2    0    0\n[2,]    0   12    0\n[3,]    0    0   10\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nD2 %*% D1:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nD2 %*% D1:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(D2 %*% D1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]    2    0    0\n[2,]    0   12    0\n[3,]    0    0   10\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nVerify D1*D2 = D2*D1:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVerify D1*D2 = D2*D1:\n```\n\n\n:::\n\n```{.r .cell-code}\nall.equal(D1 %*% D2, D2 %*% D1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\n# Inverse\nD1_inv <- solve(D1)\ncat(\"\\nD1^(-1):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nD1^(-1):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(D1_inv)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]   [,2] [,3]\n[1,]  0.5 0.0000  0.0\n[2,]  0.0 0.3333  0.0\n[3,]  0.0 0.0000  0.2\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nManual inverse: diag(1/2, 1/3, 1/5):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nManual inverse: diag(1/2, 1/3, 1/5):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(diag(1/c(2, 3, 5)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]   [,2] [,3]\n[1,]  0.5 0.0000  0.0\n[2,]  0.0 0.3333  0.0\n[3,]  0.0 0.0000  0.2\n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify inverse\ncat(\"\\nVerify D1 * D1^(-1) = I:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVerify D1 * D1^(-1) = I:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(D1 %*% D1_inv)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n```\n\n\n:::\n:::\n\n\n**Scaling Rows and Columns:**\n\nDiagonal matrices are useful for scaling. Pre-multiplying scales rows; post-multiplying scales columns.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example matrix\nA <- matrix(c(1, 2, 3, 4, 5, 6, 7, 8, 9), nrow = 3, ncol = 3)\ncat(\"Matrix A (3×3):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMatrix A (3×3):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n```\n\n\n:::\n\n```{.r .cell-code}\n# Scale rows by diag(2, 3, 4)\nD_row <- diag(c(2, 3, 4))\nDA <- D_row %*% A\n\ncat(\"\\nD %*% A (scales rows):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nD %*% A (scales rows):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(DA)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]    2    8   14\n[2,]    6   15   24\n[3,]   12   24   36\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Row 1 scaled by 2, Row 2 by 3, Row 3 by 4\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRow 1 scaled by 2, Row 2 by 3, Row 3 by 4\n```\n\n\n:::\n\n```{.r .cell-code}\n# Scale columns by diag(10, 20, 30)\nD_col <- diag(c(10, 20, 30))\nAD <- A %*% D_col\n\ncat(\"\\nA %*% D (scales columns):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nA %*% D (scales columns):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(AD)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]   10   80  210\n[2,]   20  100  240\n[3,]   30  120  270\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Column 1 scaled by 10, Column 2 by 20, Column 3 by 30\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nColumn 1 scaled by 10, Column 2 by 20, Column 3 by 30\n```\n\n\n:::\n:::\n\n\n**Livestock Example - Weighted Least Squares:**\n\nIn weighted regression, observations with different variances get different weights.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Swine ADG data with heterogeneous variances\n# 4 treatment groups with different precisions\nadg <- c(0.85, 0.90, 0.88, 0.92)\nn_pigs <- c(20, 30, 15, 25)  # Group sizes\n\n# Variance inversely proportional to group size\n# Var(mean) = σ²/n, so weight = n/σ²\nweights <- n_pigs / mean(n_pigs)  # Standardized\n\ncat(\"Treatment data:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTreatment data:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(data.frame(Treatment = 1:4, ADG = adg, n = n_pigs, Weight = round(weights, 2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Treatment  ADG  n Weight\n1         1 0.85 20   0.89\n2         2 0.90 30   1.33\n3         3 0.88 15   0.67\n4         4 0.92 25   1.11\n```\n\n\n:::\n\n```{.r .cell-code}\n# Weight matrix\nW <- diag(weights)\ncat(\"\\nWeight matrix W (4×4):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nWeight matrix W (4×4):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(W, 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3] [,4]\n[1,] 0.89 0.00 0.00 0.00\n[2,] 0.00 1.33 0.00 0.00\n[3,] 0.00 0.00 0.67 0.00\n[4,] 0.00 0.00 0.00 1.11\n```\n\n\n:::\n\n```{.r .cell-code}\n# Weighted least squares: (X'WX)^(-1) X'Wy\nX <- matrix(1, nrow = 4, ncol = 1)  # Just intercept (estimate overall mean)\ny <- matrix(adg, ncol = 1)\n\n# Normal equations with weights\nXtWX <- t(X) %*% W %*% X\nXtWy <- t(X) %*% W %*% y\n\nb_weighted <- solve(XtWX) %*% XtWy\nb_unweighted <- mean(adg)\n\ncat(sprintf(\"\\nWeighted mean: %.4f kg/day\\n\", b_weighted[1,1]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nWeighted mean: 0.8911 kg/day\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"Unweighted mean: %.4f kg/day\\n\", b_unweighted))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nUnweighted mean: 0.8875 kg/day\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nWeighted mean gives more influence to groups with larger n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nWeighted mean gives more influence to groups with larger n\n```\n\n\n:::\n:::\n\n\n**Livestock Example - Variance Components:**\n\nDiagonal matrices represent variances when effects are independent.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Genetic evaluation: 3 sires, assume independent\n# Sire breeding values have different reliabilities\nreliability <- c(0.90, 0.75, 0.85)\ngenetic_variance <- 10  # Additive genetic variance\n\n# Variance of estimated breeding value = (1-r²) * σ²_a\nvar_ebv <- (1 - reliability^2) * genetic_variance\n\ncat(\"Sire reliabilities and EBV variances:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSire reliabilities and EBV variances:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(data.frame(Sire = 1:3, Reliability = reliability,\n                 Var_EBV = round(var_ebv, 2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Sire Reliability Var_EBV\n1    1        0.90    1.90\n2    2        0.75    4.38\n3    3        0.85    2.78\n```\n\n\n:::\n\n```{.r .cell-code}\n# Variance-covariance matrix (diagonal because sires are independent)\nV <- diag(var_ebv)\ncat(\"\\nVariance-covariance matrix V (3×3):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVariance-covariance matrix V (3×3):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(V, 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]  1.9 0.00 0.00\n[2,]  0.0 4.38 0.00\n[3,]  0.0 0.00 2.78\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nInterpretation:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nInterpretation:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  - Diagonal: variances of each sire's EBV\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  - Diagonal: variances of each sire's EBV\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  - Off-diagonal zeros: sires are unrelated (independent)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  - Off-diagonal zeros: sires are unrelated (independent)\n```\n\n\n:::\n:::\n\n\n**Extracting and Creating Diagonals:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract diagonal from matrix\nA <- matrix(c(1, 2, 3, 4, 5, 6, 7, 8, 9), nrow = 3)\ncat(\"Matrix A:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMatrix A:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n```\n\n\n:::\n\n```{.r .cell-code}\ndiag_A <- diag(A)\ncat(\"\\nDiagonal of A:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nDiagonal of A:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(diag_A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1 5 9\n```\n\n\n:::\n\n```{.r .cell-code}\n# Create diagonal matrix from vector\nv <- c(10, 20, 30)\nD <- diag(v)\ncat(\"\\nDiagonal matrix from vector:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nDiagonal matrix from vector:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(D)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]   10    0    0\n[2,]    0   20    0\n[3,]    0    0   30\n```\n\n\n:::\n\n```{.r .cell-code}\n# Replace diagonal\nA_new <- A\ndiag(A_new) <- c(99, 99, 99)\ncat(\"\\nA with diagonal replaced:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nA with diagonal replaced:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A_new)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]   99    4    7\n[2,]    2   99    8\n[3,]    3    6   99\n```\n\n\n:::\n:::\n\n\n**Special Case - Singular Diagonal Matrix:**\n\nIf any $d_i = 0$, the matrix is singular (not invertible).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Diagonal matrix with a zero\nD_singular <- diag(c(2, 0, 5))\ncat(\"Singular diagonal matrix:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSingular diagonal matrix:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(D_singular)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]    2    0    0\n[2,]    0    0    0\n[3,]    0    0    5\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"\\nDeterminant: %.1f\\n\", det(D_singular)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nDeterminant: 0.0\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"Rank: %d (less than 3)\\n\", qr(D_singular)$rank))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRank: 2 (less than 3)\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nThis matrix is not invertible because det = 0\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nThis matrix is not invertible because det = 0\n```\n\n\n:::\n\n```{.r .cell-code}\n# But we can compute generalized inverse\nlibrary(MASS)\nD_ginv <- ginv(D_singular)\ncat(\"\\nGeneralized inverse:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nGeneralized inverse:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(D_ginv)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]  0.5    0  0.0\n[2,]  0.0    0  0.0\n[3,]  0.0    0  0.2\n```\n\n\n:::\n:::\n\n\n:::{.callout-tip}\n## Computational Advantages\n\nDiagonal matrices are computationally efficient:\n\n1. **Storage**: Store only n values instead of n²\n2. **Multiplication**: O(n) instead of O(n³)\n3. **Inversion**: O(n) instead of O(n³)\n4. **Determinant**: Simple product instead of complex calculation\n\nFor large-scale problems, use sparse matrix representations.\n:::\n\n:::{.callout-note}\n## Cross-References\n\nDiagonal matrices appear in:\n\n- **Week 5**: Variance matrix $\\mathbf{Var}(\\mathbf{e}) = \\sigma^2\\mathbf{I}$ (special case)\n- **Week 10**: Weighted least squares with weight matrix $\\mathbf{W}$\n- **Week 11**: Leverage values (diagonal of hat matrix)\n- **Week 14**: Variance components in genetic evaluation\n:::\n\n---\n\n### Symmetric Matrices {#sec-symmetric}\n\n**Definition:**\n\nA matrix **A** is **symmetric** if $\\mathbf{A} = \\mathbf{A}'$ (equals its own transpose).\n\nThis requires:\n1. **A** must be square (n × n)\n2. $a_{ij} = a_{ji}$ for all i, j\n\n$$\n\\mathbf{A} = \\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{12} & a_{22} & a_{23} \\\\\na_{13} & a_{23} & a_{33}\n\\end{bmatrix}\n$$ {#eq-symmetric-matrix}\n\n**Key Properties:**\n\n1. **Transpose**: $\\mathbf{A}' = \\mathbf{A}$ (by definition)\n2. **Sum**: If **A** and **B** are symmetric, then $\\mathbf{A} + \\mathbf{B}$ is symmetric\n3. **Scalar multiple**: If **A** is symmetric, then $c\\mathbf{A}$ is symmetric\n4. **Product with transpose**: $\\mathbf{X}'\\mathbf{X}$ and $\\mathbf{XX}'$ are always symmetric for any **X**\n5. **Inverse**: If **A** is symmetric and invertible, then $\\mathbf{A}^{-1}$ is symmetric\n6. **Eigenvalues**: All eigenvalues are real\n7. **Eigenvectors**: Eigenvectors corresponding to distinct eigenvalues are orthogonal\n\n**Why Symmetric Matrices Are Crucial for Linear Models:**\n\nThe matrix $\\mathbf{X}'\\mathbf{X}$ is **always** symmetric, and this matrix appears in every linear model calculation.\n\n**Proof that X'X is symmetric:**\n$$\n(\\mathbf{X}'\\mathbf{X})' = \\mathbf{X}'(\\mathbf{X}')' = \\mathbf{X}'\\mathbf{X} \\quad \\checkmark\n$$\n\n**R Implementation:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a symmetric matrix\nA <- matrix(c(4, 2, 1,\n              2, 5, 3,\n              1, 3, 6), nrow = 3, ncol = 3, byrow = TRUE)\n\ncat(\"Symmetric matrix A:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSymmetric matrix A:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]    4    2    1\n[2,]    2    5    3\n[3,]    1    3    6\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nTranspose A':\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nTranspose A':\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(t(A))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]    4    2    1\n[2,]    2    5    3\n[3,]    1    3    6\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nVerify A = A':\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVerify A = A':\n```\n\n\n:::\n\n```{.r .cell-code}\nall.equal(A, t(A))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check if a matrix is symmetric\nis_symmetric <- function(M) {\n  isTRUE(all.equal(M, t(M)))\n}\n\ncat(\"\\nIs A symmetric?\", is_symmetric(A), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nIs A symmetric? TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\n# Non-symmetric matrix\nB <- matrix(c(1, 2, 3, 4, 5, 6, 7, 8, 9), nrow = 3, ncol = 3, byrow = TRUE)\ncat(\"\\nNon-symmetric matrix B:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nNon-symmetric matrix B:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(B)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n[3,]    7    8    9\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Is B symmetric?\", is_symmetric(B), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nIs B symmetric? FALSE \n```\n\n\n:::\n:::\n\n\n**Creating Symmetric Matrices - X'X:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Design matrix for simple regression\n# n = 5 observations\nX <- cbind(1, c(10, 20, 30, 40, 50))\n\ncat(\"Design matrix X (5×2):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDesign matrix X (5×2):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    1   10\n[2,]    1   20\n[3,]    1   30\n[4,]    1   40\n[5,]    1   50\n```\n\n\n:::\n\n```{.r .cell-code}\n# Form X'X\nXtX <- t(X) %*% X\n\ncat(\"\\nX'X (2×2):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nX'X (2×2):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(XtX)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    5  150\n[2,]  150 5500\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nVerify X'X is symmetric:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVerify X'X is symmetric:\n```\n\n\n:::\n\n```{.r .cell-code}\nall.equal(XtX, t(XtX))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nStructure of X'X:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nStructure of X'X:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  [1,1] = sum of 1² = n = 5\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  [1,1] = sum of 1² = n = 5\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  [1,2] = [2,1] = sum of x = 150\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  [1,2] = [2,1] = sum of x = 150\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  [2,2] = sum of x² = 5500\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  [2,2] = sum of x² = 5500\n```\n\n\n:::\n:::\n\n\n**Livestock Example - Variance-Covariance Matrix:**\n\nSample covariance matrices are always symmetric.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Broiler data: 6 birds, 3 traits (body weight, breast yield, leg yield)\ntraits <- matrix(c(\n  2.5, 0.85, 0.45,  # Bird 1\n  2.8, 0.90, 0.48,  # Bird 2\n  2.3, 0.80, 0.42,  # Bird 3\n  2.7, 0.88, 0.47,  # Bird 4\n  2.6, 0.87, 0.46,  # Bird 5\n  2.9, 0.92, 0.50   # Bird 6\n), nrow = 6, ncol = 3, byrow = TRUE)\ncolnames(traits) <- c(\"BodyWt_kg\", \"BreastYield\", \"LegYield\")\n\ncat(\"Broiler trait data (6 birds × 3 traits):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBroiler trait data (6 birds × 3 traits):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(traits)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     BodyWt_kg BreastYield LegYield\n[1,]       2.5        0.85     0.45\n[2,]       2.8        0.90     0.48\n[3,]       2.3        0.80     0.42\n[4,]       2.7        0.88     0.47\n[5,]       2.6        0.87     0.46\n[6,]       2.9        0.92     0.50\n```\n\n\n:::\n\n```{.r .cell-code}\n# Compute covariance matrix\ncov_matrix <- cov(traits)\n\ncat(\"\\nCovariance matrix (3×3):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCovariance matrix (3×3):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(cov_matrix, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            BodyWt_kg BreastYield LegYield\nBodyWt_kg      0.0467      0.0090   0.0059\nBreastYield    0.0090      0.0018   0.0011\nLegYield       0.0059      0.0011   0.0007\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nVerify symmetry:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVerify symmetry:\n```\n\n\n:::\n\n```{.r .cell-code}\nall.equal(cov_matrix, t(cov_matrix))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nInterpretation:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nInterpretation:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"  Var(BodyWt) = %.4f\\n\", cov_matrix[1,1]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Var(BodyWt) = 0.0467\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"  Cov(BodyWt, BreastYield) = %.4f\\n\", cov_matrix[1,2]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Cov(BodyWt, BreastYield) = 0.0090\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"  Note: cov[1,2] = cov[2,1] = %.4f (symmetric!)\\n\", cov_matrix[1,2]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Note: cov[1,2] = cov[2,1] = 0.0090 (symmetric!)\n```\n\n\n:::\n:::\n\n\n**Symmetric Matrices in ANOVA:**\n\nFor a one-way ANOVA with balanced data, $\\mathbf{X}'\\mathbf{X}$ has special structure.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# One-way ANOVA: 3 breeds, 2 observations per breed\nbreed <- factor(rep(1:3, each = 2))\nX <- model.matrix(~ breed - 1)  # Cell means model\n\ncat(\"Design matrix X (6×3):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDesign matrix X (6×3):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  breed1 breed2 breed3\n1      1      0      0\n2      1      0      0\n3      0      1      0\n4      0      1      0\n5      0      0      1\n6      0      0      1\nattr(,\"assign\")\n[1] 1 1 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$breed\n[1] \"contr.treatment\"\n```\n\n\n:::\n\n```{.r .cell-code}\nXtX <- t(X) %*% X\n\ncat(\"\\nX'X (3×3):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nX'X (3×3):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(XtX)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       breed1 breed2 breed3\nbreed1      2      0      0\nbreed2      0      2      0\nbreed3      0      0      2\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nVerify X'X is symmetric:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVerify X'X is symmetric:\n```\n\n\n:::\n\n```{.r .cell-code}\nall.equal(XtX, t(XtX))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nFor balanced design, X'X is diagonal:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nFor balanced design, X'X is diagonal:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Each breed has n=2 observations, so diagonal = 2\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEach breed has n=2 observations, so diagonal = 2\n```\n\n\n:::\n:::\n\n\n**Inverse of Symmetric Matrix:**\n\nIf **A** is symmetric, then $\\mathbf{A}^{-1}$ is also symmetric.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Symmetric matrix\nA <- matrix(c(4, 1, 1, 3), nrow = 2, ncol = 2, byrow = TRUE)\n\ncat(\"Symmetric matrix A:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSymmetric matrix A:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    4    1\n[2,]    1    3\n```\n\n\n:::\n\n```{.r .cell-code}\n# Inverse\nA_inv <- solve(A)\n\ncat(\"\\nInverse A^(-1):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nInverse A^(-1):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A_inv)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         [,1]     [,2]\n[1,]  0.27273 -0.09091\n[2,] -0.09091  0.36364\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nVerify A^(-1) is symmetric:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVerify A^(-1) is symmetric:\n```\n\n\n:::\n\n```{.r .cell-code}\nall.equal(A_inv, t(A_inv))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nVerify A * A^(-1) = I:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVerify A * A^(-1) = I:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(A %*% A_inv, 10))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n```\n\n\n:::\n:::\n\n\n**Livestock Example - Normal Equations:**\n\nThe entire normal equation system is symmetric.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Dairy cow milk yield (kg/day) vs days in milk\ndays <- c(30, 60, 90, 120, 150)\nyield <- c(35, 32, 28, 25, 22)\n\n# Design matrix (n×2): intercept and days\nX <- cbind(1, days)\ny <- matrix(yield, ncol = 1)\n\ncat(\"Design matrix X (5×2):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDesign matrix X (5×2):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       days\n[1,] 1   30\n[2,] 1   60\n[3,] 1   90\n[4,] 1  120\n[5,] 1  150\n```\n\n\n:::\n\n```{.r .cell-code}\n# Normal equations: X'Xb = X'y\nXtX <- t(X) %*% X\nXty <- t(X) %*% y\n\ncat(\"\\nX'X (symmetric coefficient matrix):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nX'X (symmetric coefficient matrix):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(XtX)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          days\n       5   450\ndays 450 49500\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nX'y (right-hand side):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nX'y (right-hand side):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(Xty)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      [,1]\n       142\ndays 11790\n```\n\n\n:::\n\n```{.r .cell-code}\n# Solve\nb <- solve(XtX) %*% Xty\n\ncat(\"\\nSolution b:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSolution b:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      [,1]\n     38.30\ndays -0.11\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"\\nRegression equation: y = %.2f + %.4f * days\\n\", b[1], b[2]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nRegression equation: y = 38.30 + -0.1100 * days\n```\n\n\n:::\n\n```{.r .cell-code}\n# Variance of estimates: (X'X)^(-1) σ²\nXtX_inv <- solve(XtX)\n\ncat(\"\\n(X'X)^(-1) (also symmetric):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n(X'X)^(-1) (also symmetric):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(XtX_inv, 6))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                days\n      1.10 -0.010000\ndays -0.01  0.000111\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nVerify (X'X)^(-1) is symmetric:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVerify (X'X)^(-1) is symmetric:\n```\n\n\n:::\n\n```{.r .cell-code}\nall.equal(XtX_inv, t(XtX_inv))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\n**Checking Symmetry Numerically:**\n\nDue to floating-point arithmetic, sometimes need to check \"near symmetry.\"\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create matrix with small numerical errors\nA_perfect <- matrix(c(1, 0.5, 0.5, 2), nrow = 2, ncol = 2)\nA_noisy <- A_perfect\nA_noisy[1,2] <- A_noisy[1,2] + 1e-15  # Tiny numerical error\n\ncat(\"Matrix with numerical noise:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMatrix with numerical noise:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A_noisy, digits = 20)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]                  [,2]\n[1,]  1.0 0.5000000000000009992\n[2,]  0.5 2.0000000000000000000\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nExact equality fails:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nExact equality fails:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(identical(A_noisy, t(A_noisy)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] FALSE\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nBut all.equal() handles tolerance:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nBut all.equal() handles tolerance:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(all.equal(A_noisy, t(A_noisy)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\n# Force exact symmetry\nmake_symmetric <- function(M) {\n  (M + t(M)) / 2\n}\n\nA_fixed <- make_symmetric(A_noisy)\ncat(\"\\nForced symmetric matrix:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nForced symmetric matrix:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A_fixed, digits = 20)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                       [,1]                   [,2]\n[1,] 1.00000000000000000000 0.50000000000000044409\n[2,] 0.50000000000000044409 2.00000000000000000000\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nNow exactly symmetric:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nNow exactly symmetric:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(identical(A_fixed, t(A_fixed)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\n:::{.callout-important}\n## X'X is Always Symmetric\n\nIn every linear model calculation, the matrix $\\mathbf{X}'\\mathbf{X}$ is symmetric. This property:\n\n1. Reduces computational cost (only need to compute upper or lower triangle)\n2. Guarantees real eigenvalues\n3. Enables specialized algorithms (Cholesky decomposition)\n4. Ensures variance-covariance matrix $\\text{Var}(\\mathbf{b}) = (\\mathbf{X}'\\mathbf{X})^{-1}\\sigma^2$ is symmetric\n\nAlways verify symmetry when implementing linear model solvers!\n:::\n\n:::{.callout-note}\n## Cross-References\n\nSymmetric matrices are central to:\n\n- **Week 2**: Properties of $\\mathbf{X}'\\mathbf{X}$\n- **Week 5**: Variance-covariance matrices\n- **Week 6**: Correlation matrices\n- **Week 8**: Testing contrasts with $\\mathbf{c}'(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{c}$\n- **Week 14**: Genetic covariance matrices **G** and **A**\n:::\n\n---\n\n### Orthogonal Matrices {#sec-orthogonal}\n\n**Definition:**\n\nA square matrix **Q** is **orthogonal** if its columns are orthonormal (orthogonal unit vectors).\n\nEquivalently: $\\mathbf{Q}'\\mathbf{Q} = \\mathbf{QQ}' = \\mathbf{I}$\n\nThis means: $\\mathbf{Q}' = \\mathbf{Q}^{-1}$ (transpose equals inverse!)\n\n**Orthonormal Columns:**\n\nThe columns $\\mathbf{q}_1, \\mathbf{q}_2, ..., \\mathbf{q}_n$ satisfy:\n$$\n\\mathbf{q}_i'\\mathbf{q}_j = \\begin{cases}\n1 & \\text{if } i = j \\text{ (unit length)} \\\\\n0 & \\text{if } i \\neq j \\text{ (orthogonal)}\n\\end{cases}\n$$ {#eq-orthonormal}\n\n**Key Properties:**\n\n1. **Inverse is transpose**: $\\mathbf{Q}^{-1} = \\mathbf{Q}'$ (very fast to compute!)\n2. **Preserves lengths**: $||\\mathbf{Qx}|| = ||\\mathbf{x}||$ for any vector **x**\n3. **Preserves angles**: Inner products preserved\n4. **Determinant**: $\\det(\\mathbf{Q}) = \\pm 1$\n5. **Product**: If **Q**₁ and **Q**₂ are orthogonal, so is **Q**₁**Q**₂\n6. **Eigenvalues**: All eigenvalues have magnitude 1\n\n**R Implementation:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simple 2×2 rotation matrix (rotation by 45°)\ntheta <- pi/4  # 45 degrees\nQ <- matrix(c(cos(theta), sin(theta),\n              -sin(theta), cos(theta)), nrow = 2, ncol = 2, byrow = TRUE)\n\ncat(\"Orthogonal matrix Q (2×2 rotation):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOrthogonal matrix Q (2×2 rotation):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(Q, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        [,1]   [,2]\n[1,]  0.7071 0.7071\n[2,] -0.7071 0.7071\n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify Q'Q = I\nQtQ <- t(Q) %*% Q\ncat(\"\\nQ'Q (should be I):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nQ'Q (should be I):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(QtQ, 10))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify QQ' = I\nQQt <- Q %*% t(Q)\ncat(\"\\nQQ' (should be I):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nQQ' (should be I):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(QQt, 10))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify Q' = Q^(-1)\nQ_inv <- solve(Q)\ncat(\"\\nQ^(-1):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nQ^(-1):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(Q_inv, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       [,1]    [,2]\n[1,] 0.7071 -0.7071\n[2,] 0.7071  0.7071\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nQ' (should equal Q^(-1)):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nQ' (should equal Q^(-1)):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(t(Q), 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       [,1]    [,2]\n[1,] 0.7071 -0.7071\n[2,] 0.7071  0.7071\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nVerify Q' = Q^(-1):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVerify Q' = Q^(-1):\n```\n\n\n:::\n\n```{.r .cell-code}\nall.equal(t(Q), Q_inv)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\n**Creating Orthogonal Matrices - Gram-Schmidt:**\n\nConvert a set of vectors into orthonormal vectors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Start with two non-orthogonal vectors\nv1 <- c(1, 1, 0)\nv2 <- c(1, 0, 1)\n\ncat(\"Original vectors:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOriginal vectors:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"v1 =\", v1, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nv1 = 1 1 0 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"v2 =\", v2, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nv2 = 1 0 1 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Gram-Schmidt orthogonalization\n# Step 1: Normalize v1\nq1 <- v1 / sqrt(sum(v1^2))\ncat(\"\\nq1 (normalized v1):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nq1 (normalized v1):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(q1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.7071 0.7071 0.0000\n```\n\n\n:::\n\n```{.r .cell-code}\n# Step 2: Remove component of v2 in direction of q1\nv2_perp <- v2 - sum(v2 * q1) * q1\ncat(\"\\nv2_perp (v2 minus projection onto q1):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nv2_perp (v2 minus projection onto q1):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(v2_perp)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  0.5 -0.5  1.0\n```\n\n\n:::\n\n```{.r .cell-code}\n# Step 3: Normalize\nq2 <- v2_perp / sqrt(sum(v2_perp^2))\ncat(\"\\nq2 (normalized v2_perp):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nq2 (normalized v2_perp):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(q2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  0.4082 -0.4082  0.8165\n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify orthonormality\ncat(\"\\nq1'q1 (should be 1):\", sum(q1 * q1), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nq1'q1 (should be 1): 1 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"q2'q2 (should be 1):\", sum(q2 * q2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nq2'q2 (should be 1): 1 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"q1'q2 (should be 0):\", sum(q1 * q2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nq1'q2 (should be 0): 1.11e-16 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Built-in QR decomposition does this automatically\nA <- cbind(v1, v2)\nQR <- qr(A)\nQ_auto <- qr.Q(QR)\ncat(\"\\nQ from qr() decomposition:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nQ from qr() decomposition:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(Q_auto)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        [,1]    [,2]\n[1,] -0.7071  0.4082\n[2,] -0.7071 -0.4082\n[3,]  0.0000  0.8165\n```\n\n\n:::\n:::\n\n\n**Length Preservation:**\n\nOrthogonal transformations preserve vector lengths.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Vector\nx <- c(3, 4)\ncat(\"Original vector x:\", x, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOriginal vector x: 3 4 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Length ||x||:\", sqrt(sum(x^2)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLength ||x||: 5 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Rotate by 45°\nQ <- matrix(c(cos(pi/4), sin(pi/4),\n              -sin(pi/4), cos(pi/4)), nrow = 2, ncol = 2, byrow = TRUE)\n\n# Transform\nQx <- Q %*% x\ncat(\"\\nTransformed vector Qx:\", Qx, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nTransformed vector Qx: 4.95 0.7071 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Length ||Qx||:\", sqrt(sum(Qx^2)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLength ||Qx||: 5 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nLengths are preserved!\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nLengths are preserved!\n```\n\n\n:::\n:::\n\n\n**Orthogonal Matrices in Linear Models - QR Decomposition:**\n\nAny matrix **X** can be factored as **X = QR** where **Q** is orthogonal and **R** is upper triangular.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Design matrix for simple regression\nX <- cbind(1, c(1, 2, 3, 4, 5))\n\ncat(\"Design matrix X (5×2):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDesign matrix X (5×2):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    1    1\n[2,]    1    2\n[3,]    1    3\n[4,]    1    4\n[5,]    1    5\n```\n\n\n:::\n\n```{.r .cell-code}\n# QR decomposition\nqr_obj <- qr(X)\nQ <- qr.Q(qr_obj)\nR <- qr.R(qr_obj)\n\ncat(\"\\nQ (orthogonal, 5×2):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nQ (orthogonal, 5×2):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(Q, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        [,1]    [,2]\n[1,] -0.4472 -0.6325\n[2,] -0.4472 -0.3162\n[3,] -0.4472  0.0000\n[4,] -0.4472  0.3162\n[5,] -0.4472  0.6325\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nR (upper triangular, 2×2):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nR (upper triangular, 2×2):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(R, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       [,1]   [,2]\n[1,] -2.236 -6.708\n[2,]  0.000  3.162\n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify X = QR\ncat(\"\\nVerify X = QR:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVerify X = QR:\n```\n\n\n:::\n\n```{.r .cell-code}\nQR_product <- Q %*% R\nprint(round(QR_product, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    1    1\n[2,]    1    2\n[3,]    1    3\n[4,]    1    4\n[5,]    1    5\n```\n\n\n:::\n\n```{.r .cell-code}\nall.equal(X, QR_product, check.attributes = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify Q'Q = I\ncat(\"\\nVerify Q'Q = I:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVerify Q'Q = I:\n```\n\n\n:::\n\n```{.r .cell-code}\nQtQ <- t(Q) %*% Q\nprint(round(QtQ, 10))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n```\n\n\n:::\n:::\n\n\n**Solving Normal Equations with QR:**\n\nIf **X = QR**, then **X'X = R'Q'QR = R'R** (since Q'Q = I).\n\nThe normal equations simplify: **R'Rb = R'Q'y**\n\nOr: **Rb = Q'y** (much more stable numerically!)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example data\ny <- c(2, 4, 6, 8, 10)\n\n# Traditional method: (X'X)^(-1)X'y\nXtX <- t(X) %*% X\nXty <- t(X) %*% y\nb_traditional <- solve(XtX) %*% Xty\n\ncat(\"Traditional solution b:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTraditional solution b:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(b_traditional)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]\n[1,]    0\n[2,]    2\n```\n\n\n:::\n\n```{.r .cell-code}\n# QR method: R^(-1)Q'y\nb_qr <- solve(R) %*% t(Q) %*% y\n\ncat(\"\\nQR solution b:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nQR solution b:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(b_qr)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]\n[1,]    0\n[2,]    2\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nVerify both methods give same answer:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVerify both methods give same answer:\n```\n\n\n:::\n\n```{.r .cell-code}\nall.equal(b_traditional, b_qr, check.attributes = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nQR method is numerically more stable for ill-conditioned X'X\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nQR method is numerically more stable for ill-conditioned X'X\n```\n\n\n:::\n:::\n\n\n**Livestock Example - Orthogonal Contrasts:**\n\nIn balanced ANOVA, orthogonal contrasts correspond to orthogonal directions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Three feed types, n=3 per feed\n# Treatment means (estimated)\nmu <- c(25, 28, 22)  # kg, body weight gain\n\ncat(\"Treatment means:\", mu, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTreatment means: 25 28 22 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Two orthogonal contrasts\n# C1: Treatment 1 vs Treatment 2\nc1 <- c(1, -1, 0)\n\n# C2: Average of (1,2) vs Treatment 3\nc2 <- c(1, 1, -2)\n\ncat(\"\\nContrast 1:\", c1, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nContrast 1: 1 -1 0 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Contrast 2:\", c2, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nContrast 2: 1 1 -2 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Check orthogonality: c1'c2 = 0\ncat(\"\\nc1'c2 =\", sum(c1 * c2), \"(orthogonal!)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nc1'c2 = 0 (orthogonal!)\n```\n\n\n:::\n\n```{.r .cell-code}\n# Estimates\npsi1 <- sum(c1 * mu)\npsi2 <- sum(c2 * mu)\n\ncat(sprintf(\"\\nContrast 1 estimate: %.1f kg (T1 - T2)\\n\", psi1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nContrast 1 estimate: -3.0 kg (T1 - T2)\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"Contrast 2 estimate: %.1f kg (Avg(T1,T2) - T3)\\n\", psi2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nContrast 2 estimate: 9.0 kg (Avg(T1,T2) - T3)\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nOrthogonal contrasts partition the treatment sum of squares\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nOrthogonal contrasts partition the treatment sum of squares\n```\n\n\n:::\n:::\n\n\n**Orthogonal Polynomials:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Five time points\ntime <- 1:5\n\n# Orthogonal polynomials (up to degree 4)\npoly_matrix <- poly(time, degree = 4)\n\ncat(\"Orthogonal polynomial matrix (5×4):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOrthogonal polynomial matrix (5×4):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(poly_matrix, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           1       2       3       4\n[1,] -0.6325  0.5345 -0.3162  0.1195\n[2,] -0.3162 -0.2673  0.6325 -0.4781\n[3,]  0.0000 -0.5345  0.0000  0.7171\n[4,]  0.3162 -0.2673 -0.6325 -0.4781\n[5,]  0.6325  0.5345  0.3162  0.1195\nattr(,\"coefs\")\nattr(,\"coefs\")$alpha\n[1] 3 3 3 3\n\nattr(,\"coefs\")$norm2\n[1]  1.000  5.000 10.000 14.000 14.400  8.229\n\nattr(,\"degree\")\n[1] 1 2 3 4\nattr(,\"class\")\n[1] \"poly\"   \"matrix\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify orthonormality\ncat(\"\\nX'X (should be identity):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nX'X (should be identity):\n```\n\n\n:::\n\n```{.r .cell-code}\nXtX <- t(poly_matrix) %*% poly_matrix\nprint(round(XtX, 10))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  1 2 3 4\n1 1 0 0 0\n2 0 1 0 0\n3 0 0 1 0\n4 0 0 0 1\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nThese orthogonal polynomials reduce collinearity in polynomial regression\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nThese orthogonal polynomials reduce collinearity in polynomial regression\n```\n\n\n:::\n:::\n\n\n:::{.callout-tip}\n## Why Orthogonal Matrices Matter\n\n1. **Numerical stability**: $\\mathbf{Q}^{-1} = \\mathbf{Q}'$ is trivial to compute\n2. **QR decomposition**: More stable than solving $(X'X)^{-1}$ directly\n3. **Geometry**: Represents rotations and reflections\n4. **Condition number**: $\\kappa(\\mathbf{Q}) = 1$ (perfectly conditioned)\n5. **Orthogonal contrasts**: Partition sums of squares cleanly\n:::\n\n:::{.callout-note}\n## Cross-References\n\nOrthogonal matrices appear in:\n\n- **Week 2**: QR decomposition\n- **Week 5**: Stable solution of normal equations\n- **Week 8**: Orthogonal contrasts in balanced ANOVA\n- **Week 14**: Polynomial regression with orthogonal polynomials\n- **Appendix**: Eigenvalue decomposition **A = QΛQ'** for symmetric **A**\n:::\n\n---\n\n### Idempotent Matrices {#sec-idempotent}\n\n**Definition:**\n\nA matrix **P** is **idempotent** if $\\mathbf{P}^2 = \\mathbf{P}$ (applying it twice gives same result as applying once).\n\nEquivalently: $\\mathbf{PP} = \\mathbf{P}$\n\n**Geometric Interpretation:**\n\nIdempotent matrices represent **projections**. Projecting onto a subspace twice is the same as projecting once.\n\n**Key Properties:**\n\n1. **Repeated application**: $\\mathbf{P}^k = \\mathbf{P}$ for all $k \\geq 1$\n2. **Eigenvalues**: Only 0 or 1 (no other values possible!)\n3. **Rank equals trace**: $r(\\mathbf{P}) = \\text{tr}(\\mathbf{P})$ (very useful!)\n4. **Complement is idempotent**: If **P** is idempotent, so is $\\mathbf{I} - \\mathbf{P}$\n5. **Orthogonal projections**: If **P** is also symmetric, it's an orthogonal projection\n\n**Proof that I - P is idempotent:**\n$$\n(\\mathbf{I} - \\mathbf{P})^2 = \\mathbf{I}^2 - 2\\mathbf{P} + \\mathbf{P}^2 = \\mathbf{I} - 2\\mathbf{P} + \\mathbf{P} = \\mathbf{I} - \\mathbf{P} \\quad \\checkmark\n$$\n\n**Most Important Examples in Linear Models:**\n\n1. **Hat matrix**: $\\mathbf{H} = \\mathbf{X}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'$ (projects onto column space of **X**)\n2. **Residual matrix**: $\\mathbf{I} - \\mathbf{H}$ (projects onto orthogonal complement)\n3. **Centering matrix**: $\\mathbf{C} = \\mathbf{I} - n^{-1}\\mathbf{J}_n$ where $\\mathbf{J}_n = \\mathbf{11}'$\n\n**R Implementation:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simple example: projection onto first coordinate\nP <- matrix(c(1, 0, 0, 0, 0, 0, 0, 0, 0), nrow = 3, ncol = 3, byrow = TRUE)\n\ncat(\"Projection matrix P:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nProjection matrix P:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(P)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    0    0\n[3,]    0    0    0\n```\n\n\n:::\n\n```{.r .cell-code}\n# Test idempotence\nP2 <- P %*% P\n\ncat(\"\\nP² (should equal P):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nP² (should equal P):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(P2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    0    0\n[3,]    0    0    0\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nVerify P² = P:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVerify P² = P:\n```\n\n\n:::\n\n```{.r .cell-code}\nall.equal(P2, P)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\n# Apply to vector\nx <- c(3, 4, 5)\nPx <- P %*% x\n\ncat(\"\\nOriginal vector x:\", x, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nOriginal vector x: 3 4 5 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Projected vector Px:\", Px, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nProjected vector Px: 3 0 0 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"(Projects onto first coordinate, zeros out others)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Projects onto first coordinate, zeros out others)\n```\n\n\n:::\n\n```{.r .cell-code}\n# Apply twice\nPPx <- P %*% Px\ncat(\"P(Px) (should equal Px):\", PPx, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nP(Px) (should equal Px): 3 0 0 \n```\n\n\n:::\n:::\n\n\n**Hat Matrix - THE Central Example:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simple regression: n=4 observations\nX <- cbind(1, c(1, 2, 3, 4))\n\ncat(\"Design matrix X (4×2):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDesign matrix X (4×2):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    1    1\n[2,]    1    2\n[3,]    1    3\n[4,]    1    4\n```\n\n\n:::\n\n```{.r .cell-code}\n# Construct hat matrix\nH <- X %*% solve(t(X) %*% X) %*% t(X)\n\ncat(\"\\nHat matrix H (4×4):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nHat matrix H (4×4):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(H, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3] [,4]\n[1,]  0.7  0.4  0.1 -0.2\n[2,]  0.4  0.3  0.2  0.1\n[3,]  0.1  0.2  0.3  0.4\n[4,] -0.2  0.1  0.4  0.7\n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify idempotence: H² = H\nH2 <- H %*% H\n\ncat(\"\\nH² (should equal H):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nH² (should equal H):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(H2, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3] [,4]\n[1,]  0.7  0.4  0.1 -0.2\n[2,]  0.4  0.3  0.2  0.1\n[3,]  0.1  0.2  0.3  0.4\n[4,] -0.2  0.1  0.4  0.7\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nVerify H² = H:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVerify H² = H:\n```\n\n\n:::\n\n```{.r .cell-code}\nall.equal(H2, H)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify rank = trace\nrank_H <- qr(H)$rank\ntrace_H <- sum(diag(H))\n\ncat(sprintf(\"\\nrank(H) = %d\\n\", rank_H))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nrank(H) = 2\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"trace(H) = %.4f\\n\", trace_H))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntrace(H) = 2.0000\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"rank(H) = trace(H) ✓\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nrank(H) = trace(H) ✓\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nInterpretation: trace(H) = p = 2 (number of parameters)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nInterpretation: trace(H) = p = 2 (number of parameters)\n```\n\n\n:::\n:::\n\n\n**Residual Matrix - Orthogonal Complement:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Residual matrix\nI <- diag(4)\nM <- I - H\n\ncat(\"Residual matrix I - H (4×4):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nResidual matrix I - H (4×4):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(M, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3] [,4]\n[1,]  0.3 -0.4 -0.1  0.2\n[2,] -0.4  0.7 -0.2 -0.1\n[3,] -0.1 -0.2  0.7 -0.4\n[4,]  0.2 -0.1 -0.4  0.3\n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify idempotence: (I-H)² = I-H\nM2 <- M %*% M\n\ncat(\"\\n(I-H)² (should equal I-H):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n(I-H)² (should equal I-H):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(M2, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3] [,4]\n[1,]  0.3 -0.4 -0.1  0.2\n[2,] -0.4  0.7 -0.2 -0.1\n[3,] -0.1 -0.2  0.7 -0.4\n[4,]  0.2 -0.1 -0.4  0.3\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nVerify (I-H)² = I-H:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVerify (I-H)² = I-H:\n```\n\n\n:::\n\n```{.r .cell-code}\nall.equal(M2, M)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify rank = trace\nrank_M <- qr(M)$rank\ntrace_M <- sum(diag(M))\n\ncat(sprintf(\"\\nrank(I-H) = %d\\n\", rank_M))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nrank(I-H) = 2\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"trace(I-H) = %.4f\\n\", trace_M))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntrace(I-H) = 2.0000\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"rank(I-H) = trace(I-H) ✓\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nrank(I-H) = trace(I-H) ✓\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"\\nInterpretation: trace(I-H) = n-p = %d - %d = %d (degrees of freedom for error)\\n\",\n            nrow(X), ncol(X), nrow(X) - ncol(X)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nInterpretation: trace(I-H) = n-p = 4 - 2 = 2 (degrees of freedom for error)\n```\n\n\n:::\n:::\n\n\n**Orthogonal Decomposition:**\n\nThe hat matrix **H** and residual matrix **I-H** provide an orthogonal decomposition of any vector.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Any observation vector y\ny <- c(2, 4, 5, 7)\n\ncat(\"Observation vector y:\", y, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nObservation vector y: 2 4 5 7 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Fitted values (projection onto column space of X)\ny_hat <- H %*% y\ncat(\"\\nFitted values ŷ = Hy:\", as.vector(y_hat), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nFitted values ŷ = Hy: 2.1 3.7 5.3 6.9 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Residuals (projection onto orthogonal complement)\ne <- (I - H) %*% y\ncat(\"Residuals e = (I-H)y:\", as.vector(e), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nResiduals e = (I-H)y: -0.1 0.3 -0.3 0.1 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify y = ŷ + e\ncat(\"\\nVerify y = ŷ + e:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVerify y = ŷ + e:\n```\n\n\n:::\n\n```{.r .cell-code}\nall.equal(y, as.vector(y_hat + e))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify orthogonality: ŷ'e = 0\ncat(sprintf(\"\\nŷ'e = %.10f (should be 0)\\n\", sum(y_hat * e)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nŷ'e = -0.0000000000 (should be 0)\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nThis is the fundamental decomposition: y = Hy + (I-H)y\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nThis is the fundamental decomposition: y = Hy + (I-H)y\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Fitted values + Residuals = Observations\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFitted values + Residuals = Observations\n```\n\n\n:::\n:::\n\n\n**Centering Matrix:**\n\nThe centering matrix removes the mean from data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Data vector\nx <- c(10, 20, 30, 40, 50)\nn <- length(x)\n\ncat(\"Data x:\", x, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nData x: 10 20 30 40 50 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Mean:\", mean(x), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMean: 30 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Centering matrix: C = I - (1/n)J where J = 11'\nJ <- matrix(1, nrow = n, ncol = n)  # All ones\nC <- diag(n) - (1/n) * J\n\ncat(\"\\nCentering matrix C (5×5):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCentering matrix C (5×5):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(C, 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3] [,4] [,5]\n[1,]  0.8 -0.2 -0.2 -0.2 -0.2\n[2,] -0.2  0.8 -0.2 -0.2 -0.2\n[3,] -0.2 -0.2  0.8 -0.2 -0.2\n[4,] -0.2 -0.2 -0.2  0.8 -0.2\n[5,] -0.2 -0.2 -0.2 -0.2  0.8\n```\n\n\n:::\n\n```{.r .cell-code}\n# Apply centering matrix\nx_centered <- C %*% x\n\ncat(\"\\nCentered data Cx:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCentered data Cx:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(x_centered)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]\n[1,]  -20\n[2,]  -10\n[3,]    0\n[4,]   10\n[5,]   20\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nVerify mean of centered data is 0:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVerify mean of centered data is 0:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"Mean = %.10f\\n\", mean(x_centered)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMean = 0.0000000000\n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify idempotence\nC2 <- C %*% C\ncat(\"\\nVerify C² = C:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVerify C² = C:\n```\n\n\n:::\n\n```{.r .cell-code}\nall.equal(C2, C)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nCentering is a projection (idempotent operation)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCentering is a projection (idempotent operation)\n```\n\n\n:::\n:::\n\n\n**Livestock Example - Sum of Squares Decomposition:**\n\nIdempotent matrices decompose total sum of squares into components.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Broiler weights (kg)\nweight <- c(2.1, 2.3, 2.5, 2.7, 2.9)\ny <- matrix(weight, ncol = 1)\nn <- length(weight)\n\ncat(\"Broiler weights (kg):\", weight, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBroiler weights (kg): 2.1 2.3 2.5 2.7 2.9 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Design matrix (just intercept - estimate mean)\nX <- matrix(1, nrow = n, ncol = 1)\n\n# Hat matrix\nH <- X %*% solve(t(X) %*% X) %*% t(X)\n# For intercept-only: H = (1/n)J (all elements = 1/n)\n\n# Centering matrix\nJ_n <- matrix(1/n, nrow = n, ncol = n)\nC <- diag(n) - J_n\n\ncat(\"\\nHat matrix H (all elements 1/5 = 0.2):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nHat matrix H (all elements 1/5 = 0.2):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(H, 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3] [,4] [,5]\n[1,]  0.2  0.2  0.2  0.2  0.2\n[2,]  0.2  0.2  0.2  0.2  0.2\n[3,]  0.2  0.2  0.2  0.2  0.2\n[4,]  0.2  0.2  0.2  0.2  0.2\n[5,]  0.2  0.2  0.2  0.2  0.2\n```\n\n\n:::\n\n```{.r .cell-code}\n# Total sum of squares: y'Cy\nSST <- t(y) %*% C %*% y\ncat(sprintf(\"\\nSST = y'(I - n⁻¹J)y = %.4f\\n\", SST[1,1]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSST = y'(I - n⁻¹J)y = 0.4000\n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify with formula\nSST_formula <- sum((weight - mean(weight))^2)\ncat(sprintf(\"Verification: Σ(y - ȳ)² = %.4f\\n\", SST_formula))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nVerification: Σ(y - ȳ)² = 0.4000\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nThe centering matrix C = I - n⁻¹J is idempotent and computes SST\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nThe centering matrix C = I - n⁻¹J is idempotent and computes SST\n```\n\n\n:::\n:::\n\n\n**Properties of Symmetric Idempotent Matrices:**\n\nIf **P** is symmetric and idempotent, it's an **orthogonal projection**.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Hat matrix is both symmetric and idempotent\nX <- cbind(1, c(1, 2, 3))\nH <- X %*% solve(t(X) %*% X) %*% t(X)\n\ncat(\"Hat matrix H:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nHat matrix H:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(H, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        [,1]   [,2]    [,3]\n[1,]  0.8333 0.3333 -0.1667\n[2,]  0.3333 0.3333  0.3333\n[3,] -0.1667 0.3333  0.8333\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nCheck symmetry:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCheck symmetry:\n```\n\n\n:::\n\n```{.r .cell-code}\nall.equal(H, t(H))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nCheck idempotence:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCheck idempotence:\n```\n\n\n:::\n\n```{.r .cell-code}\nall.equal(H %*% H, H)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nH is symmetric AND idempotent → orthogonal projection\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nH is symmetric AND idempotent → orthogonal projection\n```\n\n\n:::\n\n```{.r .cell-code}\n# Eigenvalues are only 0 or 1\neigenvalues <- eigen(H)$values\ncat(\"\\nEigenvalues of H:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nEigenvalues of H:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(eigenvalues, 10))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1 1 0\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Only 0's and 1's (characteristic of idempotent matrices)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOnly 0's and 1's (characteristic of idempotent matrices)\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"Number of 1's = %d = rank(H) = trace(H)\\n\", sum(eigenvalues > 0.5)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNumber of 1's = 2 = rank(H) = trace(H)\n```\n\n\n:::\n:::\n\n\n**Why Idempotent Matrices Matter:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example: repeated projection doesn't change result\nX <- cbind(1, 1:5)\ny <- c(2, 4, 6, 8, 10)\nH <- X %*% solve(t(X) %*% X) %*% t(X)\n\n# Fitted values\ny_hat1 <- H %*% y\ny_hat2 <- H %*% y_hat1  # Project fitted values again\ny_hat3 <- H %*% y_hat2  # And again\n\ncat(\"Original y:\", y, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOriginal y: 2 4 6 8 10 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Hy (1st projection):\", round(as.vector(y_hat1), 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nHy (1st projection): 2 4 6 8 10 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"H(Hy) (2nd projection):\", round(as.vector(y_hat2), 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nH(Hy) (2nd projection): 2 4 6 8 10 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"H(H(Hy)) (3rd projection):\", round(as.vector(y_hat3), 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nH(H(Hy)) (3rd projection): 2 4 6 8 10 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nAll identical! Projection is idempotent.\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nAll identical! Projection is idempotent.\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Once you're in the subspace, projecting again does nothing.\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOnce you're in the subspace, projecting again does nothing.\n```\n\n\n:::\n:::\n\n\n:::{.callout-important}\n## Hat Matrix is Idempotent\n\nThe hat matrix $\\mathbf{H} = \\mathbf{X}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'$ is:\n\n1. **Symmetric**: $\\mathbf{H}' = \\mathbf{H}$\n2. **Idempotent**: $\\mathbf{H}^2 = \\mathbf{H}$\n3. **Rank = Trace**: $r(\\mathbf{H}) = \\text{tr}(\\mathbf{H}) = p$ (number of parameters)\n\nThe residual matrix $\\mathbf{I} - \\mathbf{H}$ has the same properties with:\n- $r(\\mathbf{I} - \\mathbf{H}) = \\text{tr}(\\mathbf{I} - \\mathbf{H}) = n - p$ (error degrees of freedom)\n\nThis is why trace gives degrees of freedom!\n:::\n\n:::{.callout-note}\n## Cross-References\n\nIdempotent matrices are fundamental to:\n\n- **Week 5**: Hat matrix **H** and fitted values $\\hat{\\mathbf{y}} = \\mathbf{Hy}$\n- **Week 5**: Residuals $\\mathbf{e} = (\\mathbf{I} - \\mathbf{H})\\mathbf{y}$\n- **Week 5**: Degrees of freedom = $\\text{tr}(\\mathbf{I} - \\mathbf{H}) = n - p$\n- **Week 6**: Sum of squares decomposition using centering matrix\n- **Week 11**: Leverage values (diagonal of **H**)\n- **Section @sec-projection-matrices**: Complete coverage of projection matrices\n:::\n\n---\n\n## Matrix Properties {#sec-properties}\n\nUnderstanding the fundamental properties of matrices - rank, trace, and determinant - is essential for working with linear models. These properties determine invertibility, degrees of freedom, and the structure of solutions.\n\n### Rank {#sec-rank}\n\nThe **rank** of a matrix is one of its most important properties, especially in linear models where it determines whether we can uniquely solve for parameters.\n\n#### Definition\n\nThe **rank** of an m × n matrix **A**, denoted r(**A**), is:\n\n- The **maximum number of linearly independent rows**, OR\n- The **maximum number of linearly independent columns**, OR\n- The dimension of the **column space** (or row space) of **A**\n\nThese three definitions are equivalent.\n\n**Key fact**: For any matrix **A**, $r(\\mathbf{A}^{\\text{rows}}) = r(\\mathbf{A}^{\\text{columns}})$\n\n#### Properties and Theorems\n\n**1. Basic Properties**\n\n- $0 \\leq r(\\mathbf{A}) \\leq \\min(m, n)$ for m × n matrix\n- $r(\\mathbf{A}) = r(\\mathbf{A}')$ (rank of transpose equals rank)\n- $r(c\\mathbf{A}) = r(\\mathbf{A})$ for any scalar $c \\neq 0$\n\n**2. Full Rank**\n\nAn m × n matrix **A** is **full rank** if:\n- $r(\\mathbf{A}) = \\min(m, n)$\n- If m < n: full row rank (r = m)\n- If m > n: full column rank (r = n)\n- If m = n: **A** is square and invertible\n\n**3. Rank Deficient**\n\nA matrix is **rank deficient** if $r(\\mathbf{A}) < \\min(m, n)$\n\nFor square n × n matrices:\n- Full rank: r(**A**) = n (invertible, det(**A**) ≠ 0)\n- Rank deficient: r(**A**) < n (singular, det(**A**) = 0)\n\n**4. Product Rule**\n\n$$\nr(\\mathbf{AB}) \\leq \\min(r(\\mathbf{A}), r(\\mathbf{B}))\n$$ {#eq-rank-product}\n\nProof: The columns of **AB** are linear combinations of columns of **A**, so column space of **AB** is a subspace of column space of **A**.\n\n**5. Sum Rule**\n\n$$\nr(\\mathbf{A} + \\mathbf{B}) \\leq r(\\mathbf{A}) + r(\\mathbf{B})\n$$ {#eq-rank-sum}\n\n**6. Critical Result for Linear Models**\n\n$$\nr(\\mathbf{X}'\\mathbf{X}) = r(\\mathbf{X}\\mathbf{X}') = r(\\mathbf{X})\n$$ {#eq-rank-xtx}\n\nThis is **fundamental** for understanding when normal equations have unique solutions.\n\n**Proof**: We need to show $r(\\mathbf{X}'\\mathbf{X}) = r(\\mathbf{X})$.\n\nFirst, $r(\\mathbf{X}'\\mathbf{X}) \\leq r(\\mathbf{X})$ by the product rule.\n\nFor the reverse inequality, suppose $\\mathbf{X}\\mathbf{c} = \\mathbf{0}$ for some vector **c**. Then:\n$$\n\\mathbf{c}'\\mathbf{X}'\\mathbf{X}\\mathbf{c} = (\\mathbf{X}\\mathbf{c})'(\\mathbf{X}\\mathbf{c}) = \\mathbf{0}'\\mathbf{0} = 0\n$$\n\nThis implies $\\mathbf{X}'\\mathbf{X}\\mathbf{c} = \\mathbf{0}$ (since the quadratic form equals zero).\n\nTherefore, null space of **X** equals null space of **X'X**, which means they have the same rank.\n\n:::{.callout-important}\n## Why This Matters for Linear Models\n\nThe result $r(\\mathbf{X}'\\mathbf{X}) = r(\\mathbf{X})$ tells us:\n\n- If **X** is full column rank (r = p), then **X'X** is invertible\n- If **X** is rank deficient, **X'X** is also rank deficient (singular)\n- This determines whether normal equations have a unique solution\n\nThis is central to understanding **Week 12: Non-Full Rank Models**!\n:::\n\n#### Computing Rank in R\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Method 1: QR decomposition (most reliable)\nA <- matrix(c(1, 2, 3,\n              2, 4, 6,\n              1, 1, 1), nrow = 3, byrow = TRUE)\n\ncat(\"Matrix A:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMatrix A:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    2    4    6\n[3,]    1    1    1\n```\n\n\n:::\n\n```{.r .cell-code}\nrank_qr <- qr(A)$rank\ncat(\"\\nRank using QR:\", rank_qr, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nRank using QR: 2 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Method 2: Using Matrix package\nlibrary(Matrix)\nrank_matrix <- rankMatrix(A)\ncat(\"Rank using rankMatrix:\", rank_matrix[1], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRank using rankMatrix: 2 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Check: Second row is 2 times first row, so rank < 3\ncat(\"\\nRow 2 = 2 * Row 1?\", all.equal(A[2,], 2 * A[1,]), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nRow 2 = 2 * Row 1? TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Therefore rank should be 2 (not 3)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTherefore rank should be 2 (not 3)\n```\n\n\n:::\n:::\n\n\n#### Livestock Example: Design Matrix Rank\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example: Sheep fleece weight by breed (unbalanced data)\nbreed <- c(rep(\"Merino\", 3), rep(\"Suffolk\", 2), rep(\"Dorset\", 1))\nweight <- c(5.2, 5.4, 5.3, 4.8, 5.0, 5.6)\n\n# Cell means model (always full rank)\nX_cell <- model.matrix(~ breed - 1)\ncat(\"Cell Means Model Design Matrix:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCell Means Model Design Matrix:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(X_cell)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  breedDorset breedMerino breedSuffolk\n1           0           1            0\n2           0           1            0\n3           0           1            0\n4           0           0            1\n5           0           0            1\n6           1           0            0\nattr(,\"assign\")\n[1] 1 1 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$breed\n[1] \"contr.treatment\"\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Rank:\", qr(X_cell)$rank, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRank: 3 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Dimensions:\", nrow(X_cell), \"×\", ncol(X_cell), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDimensions: 6 × 3 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Full column rank?\", qr(X_cell)$rank == ncol(X_cell), \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFull column rank? TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\n# Effects model (overparameterized, not full rank)\nX_effects <- model.matrix(~ breed)\ncat(\"Effects Model Design Matrix:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEffects Model Design Matrix:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(X_effects)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  (Intercept) breedMerino breedSuffolk\n1           1           1            0\n2           1           1            0\n3           1           1            0\n4           1           0            1\n5           1           0            1\n6           1           0            0\nattr(,\"assign\")\n[1] 0 1 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$breed\n[1] \"contr.treatment\"\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Rank:\", qr(X_effects)$rank, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRank: 3 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Dimensions:\", nrow(X_effects), \"×\", ncol(X_effects), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDimensions: 6 × 3 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Full column rank?\", qr(X_effects)$rank == ncol(X_effects), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFull column rank? TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Rank deficient by:\", ncol(X_effects) - qr(X_effects)$rank, \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRank deficient by: 0 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify X'X has same rank as X\nXtX_cell <- t(X_cell) %*% X_cell\nXtX_effects <- t(X_effects) %*% X_effects\n\ncat(\"Rank verification:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRank verification:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"X (cell means): rank =\", qr(X_cell)$rank, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nX (cell means): rank = 3 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"X'X (cell means): rank =\", qr(XtX_cell)$rank, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nX'X (cell means): rank = 3 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Agreement:\", qr(X_cell)$rank == qr(XtX_cell)$rank, \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAgreement: TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"X (effects): rank =\", qr(X_effects)$rank, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nX (effects): rank = 3 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"X'X (effects): rank =\", qr(XtX_effects)$rank, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nX'X (effects): rank = 3 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Agreement:\", qr(X_effects)$rank == qr(XtX_effects)$rank, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAgreement: TRUE \n```\n\n\n:::\n:::\n\n\n#### Implications for Estimability\n\nWhen **X** is rank deficient:\n\n- **X'Xb = X'y** has **infinitely many solutions**\n- Individual parameters $\\beta_j$ are **not uniquely estimable**\n- But **linear combinations** $\\mathbf{c}'\\boldsymbol{\\beta}$ may still be estimable if **c** is in the row space of **X**\n\nExample: In effects model μ + α₁ + α₂ + α₃ with $\\sum \\alpha_i = 0$ constraint:\n- μ is not uniquely estimable (confounded with αᵢ)\n- Individual αᵢ are not uniquely estimable\n- **Contrasts** α₁ - α₂ **are uniquely estimable**\n\n:::{.callout-note}\n## Cross-References\n\n- **Week 2: Linear Algebra Essentials** - Introduction to rank\n- **Week 7: One-Way ANOVA** - Full rank cell means vs. rank deficient effects model\n- **Week 8: Contrasts and Estimable Functions** - What's estimable when rank deficient\n- **Week 12: Non-Full Rank Models** - Complete treatment of rank deficiency\n:::\n\n---\n\n### Trace {#sec-trace}\n\nThe **trace** of a square matrix is the sum of its diagonal elements. It has elegant properties that make it useful in linear models.\n\n#### Definition\n\nFor an n × n matrix **A**:\n\n$$\n\\text{tr}(\\mathbf{A}) = \\sum_{i=1}^n a_{ii}\n$$ {#eq-trace-def}\n\nThe trace is a **scalar** (single number).\n\n#### Properties\n\n**1. Linearity**\n\n$$\n\\text{tr}(\\mathbf{A} + \\mathbf{B}) = \\text{tr}(\\mathbf{A}) + \\text{tr}(\\mathbf{B})\n$$\n\n$$\n\\text{tr}(c\\mathbf{A}) = c \\cdot \\text{tr}(\\mathbf{A})\n$$\n\n**2. Cyclic Property** (Most Important!)\n\n$$\n\\text{tr}(\\mathbf{ABC}) = \\text{tr}(\\mathbf{BCA}) = \\text{tr}(\\mathbf{CAB})\n$$ {#eq-trace-cyclic}\n\nSpecial cases:\n- $\\text{tr}(\\mathbf{AB}) = \\text{tr}(\\mathbf{BA})$ (even if **AB** ≠ **BA**)\n- $\\text{tr}(\\mathbf{A}'\\mathbf{A}) = \\text{tr}(\\mathbf{A}\\mathbf{A}') = \\sum_{i,j} a_{ij}^2$ (sum of all squared elements)\n\n**3. Trace of Transpose**\n\n$$\n\\text{tr}(\\mathbf{A}') = \\text{tr}(\\mathbf{A})\n$$\n\n**4. Trace and Eigenvalues**\n\n$$\n\\text{tr}(\\mathbf{A}) = \\sum_{i=1}^n \\lambda_i\n$$ {#eq-trace-eigenvalues}\n\nwhere λᵢ are the eigenvalues of **A** (including multiplicities).\n\n**5. Trace of Idempotent Matrix**\n\nFor an idempotent matrix **P** (where **P²** = **P**):\n\n$$\n\\text{tr}(\\mathbf{P}) = \\text{rank}(\\mathbf{P})\n$$ {#eq-trace-rank-idempotent}\n\nThis is **crucial** for projection matrices!\n\n**Proof**: Since **P** is idempotent, its eigenvalues are 0 or 1. If **P** has rank r, then r eigenvalues equal 1 and n - r equal 0. Therefore:\n$$\n\\text{tr}(\\mathbf{P}) = \\sum \\lambda_i = r \\cdot 1 + (n-r) \\cdot 0 = r = \\text{rank}(\\mathbf{P})\n$$\n\n#### Applications in Linear Models\n\n**1. Degrees of Freedom**\n\nFor hat matrix $\\mathbf{H} = \\mathbf{X}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'$:\n\n$$\n\\text{tr}(\\mathbf{H}) = p\n$$\n\nwhere p = rank(**X**) = number of parameters.\n\nThis is the **model degrees of freedom**.\n\nFor residual projection $\\mathbf{I} - \\mathbf{H}$:\n\n$$\n\\text{tr}(\\mathbf{I} - \\mathbf{H}) = n - p\n$$\n\nThis is the **error degrees of freedom**.\n\n**2. Sum of Squared Elements**\n\n$$\n\\text{tr}(\\mathbf{A}'\\mathbf{A}) = \\sum_{i=1}^m \\sum_{j=1}^n a_{ij}^2 = ||\\mathbf{A}||_F^2\n$$\n\nThis is the squared **Frobenius norm** of **A**.\n\n**3. Quadratic Forms**\n\nFor any matrices **A** (n × n) and **B** (n × n):\n\n$$\n\\text{tr}(\\mathbf{AB}) = \\text{tr}(\\mathbf{BA})\n$$\n\nSpecial case: If **y** is n × 1 and **A** is n × n:\n\n$$\n\\mathbf{y}'\\mathbf{A}\\mathbf{y} = \\text{tr}(\\mathbf{A}\\mathbf{y}\\mathbf{y}')\n$$ {#eq-trace-quadratic}\n\nThis connects quadratic forms to traces.\n\n#### R Examples\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define trace function (R doesn't have built-in)\ntr <- function(A) sum(diag(A))\n\n# Example 1: Basic trace\nA <- matrix(c(1, 2, 3, 4), 2, 2)\ncat(\"Matrix A:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMatrix A:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Trace:\", tr(A), \"= 1 + 4 =\", 1 + 4, \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrace: 5 = 1 + 4 = 5 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Example 2: Cyclic property tr(AB) = tr(BA)\nB <- matrix(c(5, 6, 7, 8), 2, 2)\nAB <- A %*% B\nBA <- B %*% A\n\ncat(\"Cyclic property:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCyclic property:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"tr(AB) =\", tr(AB), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntr(AB) = 69 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"tr(BA) =\", tr(BA), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntr(BA) = 69 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Equal?\", all.equal(tr(AB), tr(BA)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEqual? TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"But AB ≠ BA:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBut AB ≠ BA:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"AB =\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAB =\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(AB)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]   23   31\n[2,]   34   46\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"BA =\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBA =\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(BA)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]   19   43\n[2,]   22   50\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\n\")\n```\n\n```{.r .cell-code}\n# Example 3: Trace of A'A = sum of squared elements\ncat(\"tr(A'A) = sum of all squared elements:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntr(A'A) = sum of all squared elements:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"tr(A'A) =\", tr(t(A) %*% A), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntr(A'A) = 30 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Sum of squared elements =\", sum(A^2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSum of squared elements = 30 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Agreement:\", all.equal(tr(t(A) %*% A), sum(A^2)), \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAgreement: TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\n# Example 4: Trace of idempotent matrix = rank\n# Use projection matrix from earlier\nX <- cbind(1, c(1, 2, 3, 4, 5))  # 5x2 design matrix\nH <- X %*% solve(t(X) %*% X) %*% t(X)\n\ncat(\"Projection matrix H:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nProjection matrix H:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Trace(H) =\", tr(H), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrace(H) = 2 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Rank(H) =\", qr(H)$rank, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRank(H) = 2 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Number of parameters p =\", ncol(X), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNumber of parameters p = 2 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"All equal?\", all.equal(tr(H), qr(H)$rank), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAll equal? TRUE \n```\n\n\n:::\n:::\n\n\n#### Livestock Example: Degrees of Freedom\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Beef cattle: ADG for 10 steers, 2 breeds (5 per breed)\nadg <- c(1.2, 1.3, 1.1, 1.4, 1.2,  # Angus\n         1.0, 1.1, 0.9, 1.2, 1.0)  # Hereford\nbreed <- factor(rep(c(\"Angus\", \"Hereford\"), each = 5))\n\n# Design matrix (cell means model)\nX <- model.matrix(~ breed - 1)\nn <- nrow(X)\np <- ncol(X)\n\n# Compute projection matrices\nH <- X %*% solve(t(X) %*% X) %*% t(X)\nI_minus_H <- diag(n) - H\n\ncat(\"Sample size n =\", n, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSample size n = 10 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Number of parameters p =\", p, \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNumber of parameters p = 2 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Model degrees of freedom:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel degrees of freedom:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  tr(H) =\", tr(H), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  tr(H) = 2 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  rank(H) =\", qr(H)$rank, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  rank(H) = 2 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  p =\", p, \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  p = 2 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Error degrees of freedom:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nError degrees of freedom:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  tr(I - H) =\", tr(I_minus_H), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  tr(I - H) = 8 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  rank(I - H) =\", qr(I_minus_H)$rank, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  rank(I - H) = 8 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  n - p =\", n - p, \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  n - p = 8 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Verification: tr(H) + tr(I-H) = n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nVerification: tr(H) + tr(I-H) = n\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  \", tr(H), \"+\", tr(I_minus_H), \"=\", tr(H) + tr(I_minus_H), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   2 + 8 = 10 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  n =\", n, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  n = 10 \n```\n\n\n:::\n:::\n\n\n:::{.callout-note}\n## Cross-References\n\n- **Week 2: Linear Algebra Essentials** - Introduction to trace\n- **Week 5: Least Squares Theory** - Uses tr(H) = p for degrees of freedom\n- **Week 6: Multiple Regression** - Degrees of freedom in ANOVA tables\n- **Week 11: Model Diagnostics** - Leverage as diagonal elements of H\n:::\n\n---\n\n### Determinant {#sec-determinant}\n\nThe **determinant** is a scalar value computed from a square matrix that provides information about invertibility, volume scaling, and system solvability.\n\n#### Definition\n\nFor a square n × n matrix **A**, the determinant det(**A**) or |**A**| is defined recursively:\n\n**1 × 1 matrix**: det([a]) = a\n\n**2 × 2 matrix**:\n$$\n\\det\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix} = ad - bc\n$$ {#eq-det-2x2}\n\n**n × n matrix**: Using cofactor expansion along row i:\n$$\n\\det(\\mathbf{A}) = \\sum_{j=1}^n (-1)^{i+j} a_{ij} M_{ij}\n$$\n\nwhere $M_{ij}$ is the minor (determinant of (n-1) × (n-1) submatrix obtained by deleting row i and column j).\n\n#### Properties\n\n**1. Invertibility**\n\n$$\n\\det(\\mathbf{A}) \\neq 0 \\iff \\mathbf{A} \\text{ is invertible}\n$$ {#eq-det-invertible}\n\n$$\n\\det(\\mathbf{A}) = 0 \\iff \\mathbf{A} \\text{ is singular}\n$$\n\n**2. Product Rule**\n\n$$\n\\det(\\mathbf{AB}) = \\det(\\mathbf{A}) \\cdot \\det(\\mathbf{B})\n$$ {#eq-det-product}\n\n**3. Inverse**\n\n$$\n\\det(\\mathbf{A}^{-1}) = \\frac{1}{\\det(\\mathbf{A})}\n$$ {#eq-det-inverse}\n\n**4. Transpose**\n\n$$\n\\det(\\mathbf{A}') = \\det(\\mathbf{A})\n$$\n\n**5. Scalar Multiplication**\n\nFor n × n matrix:\n$$\n\\det(c\\mathbf{A}) = c^n \\det(\\mathbf{A})\n$$ {#eq-det-scalar}\n\n**6. Determinant and Eigenvalues**\n\n$$\n\\det(\\mathbf{A}) = \\prod_{i=1}^n \\lambda_i\n$$ {#eq-det-eigenvalues}\n\nwhere λᵢ are the eigenvalues.\n\n**7. Geometric Interpretation**\n\n|det(**A**)| is the volume (or area in 2D) of the parallelepiped formed by the column vectors of **A**.\n\nIf det(**A**) < 0, the transformation reverses orientation.\n\n#### Computing Determinants in R\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example 1: 2x2 matrix\nA_2x2 <- matrix(c(3, 1, 2, 4), 2, 2)\ncat(\"2×2 Matrix A:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n2×2 Matrix A:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A_2x2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    3    2\n[2,]    1    4\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"det(A) =\", det(A_2x2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ndet(A) = 10 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Manual: (3)(4) - (1)(2) =\", 3*4 - 1*2, \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nManual: (3)(4) - (1)(2) = 10 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Example 2: Singular matrix (det = 0)\nA_sing <- matrix(c(1, 2, 2, 4), 2, 2)  # Row 2 = 2 * Row 1\ncat(\"Singular Matrix (row 2 = 2*row 1):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSingular Matrix (row 2 = 2*row 1):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A_sing)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    1    2\n[2,]    2    4\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"det(A) =\", det(A_sing), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ndet(A) = 0 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Is singular?\", abs(det(A_sing)) < 1e-10, \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nIs singular? TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\n# Example 3: Product rule\nB_2x2 <- matrix(c(1, 3, 2, 1), 2, 2)\ncat(\"Product rule: det(AB) = det(A) * det(B)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nProduct rule: det(AB) = det(A) * det(B)\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"det(A) =\", det(A_2x2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ndet(A) = 10 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"det(B) =\", det(B_2x2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ndet(B) = -5 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"det(A) * det(B) =\", det(A_2x2) * det(B_2x2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ndet(A) * det(B) = -50 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"det(AB) =\", det(A_2x2 %*% B_2x2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ndet(AB) = -50 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Agreement:\", all.equal(det(A_2x2) * det(B_2x2),\n                            det(A_2x2 %*% B_2x2)), \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAgreement: TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\n# Example 4: Inverse rule\ncat(\"Inverse rule: det(A^{-1}) = 1/det(A)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nInverse rule: det(A^{-1}) = 1/det(A)\n```\n\n\n:::\n\n```{.r .cell-code}\nA_inv <- solve(A_2x2)\ncat(\"det(A) =\", det(A_2x2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ndet(A) = 10 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"1/det(A) =\", 1/det(A_2x2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n1/det(A) = 0.1 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"det(A^{-1}) =\", det(A_inv), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ndet(A^{-1}) = 0.1 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Agreement:\", all.equal(1/det(A_2x2), det(A_inv)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAgreement: TRUE \n```\n\n\n:::\n:::\n\n\n#### Application: Testing Rank Deficiency\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create rank-deficient design matrix (overparameterized ANOVA)\ntreatment <- factor(rep(1:3, each = 3))\nX_full <- model.matrix(~ treatment)  # Intercept + 2 treatment effects\n\ncat(\"Full effects model design matrix:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFull effects model design matrix:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(X_full)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  (Intercept) treatment2 treatment3\n1           1          0          0\n2           1          0          0\n3           1          0          0\n4           1          1          0\n5           1          1          0\n6           1          1          0\n7           1          0          1\n8           1          0          1\n9           1          0          1\nattr(,\"assign\")\n[1] 0 1 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$treatment\n[1] \"contr.treatment\"\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Rank:\", qr(X_full)$rank, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRank: 3 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Columns:\", ncol(X_full), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nColumns: 3 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Rank deficient?\", qr(X_full)$rank < ncol(X_full), \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRank deficient? FALSE \n```\n\n\n:::\n\n```{.r .cell-code}\n# Compute X'X\nXtX <- t(X_full) %*% X_full\ncat(\"X'X:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nX'X:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(XtX)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            (Intercept) treatment2 treatment3\n(Intercept)           9          3          3\ntreatment2            3          3          0\ntreatment3            3          0          3\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"det(X'X) =\", det(XtX), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ndet(X'X) = 27 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Is X'X singular?\", abs(det(XtX)) < 1e-10, \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nIs X'X singular? FALSE \n```\n\n\n:::\n\n```{.r .cell-code}\n# Compare with full rank (cell means model)\nX_cell <- model.matrix(~ treatment - 1)\nXtX_cell <- t(X_cell) %*% X_cell\ncat(\"Cell means model:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCell means model:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"X'X:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nX'X:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(XtX_cell)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           treatment1 treatment2 treatment3\ntreatment1          3          0          0\ntreatment2          0          3          0\ntreatment3          0          0          3\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"det(X'X) =\", det(XtX_cell), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ndet(X'X) = 27 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Is X'X invertible?\", abs(det(XtX_cell)) > 1e-10, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nIs X'X invertible? TRUE \n```\n\n\n:::\n:::\n\n\n#### Livestock Example: Checking Invertibility\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Swine growth: 6 pigs, measure weight at 3 time points\n# Question: Can we fit polynomial growth curve?\n\npig_id <- rep(1:6, each = 3)\nage_days <- rep(c(30, 60, 90), 6)\nweight <- c(15, 35, 60,   # Pig 1\n            14, 33, 58,   # Pig 2\n            16, 36, 62,   # Pig 3\n            15, 34, 59,   # Pig 4\n            14, 35, 61,   # Pig 5\n            16, 37, 63)   # Pig 6\n\n# Try to fit quadratic model: weight ~ age + age²\n# Design matrix for one pig's data\nage_one_pig <- c(30, 60, 90)\nX_quad <- cbind(1, age_one_pig, age_one_pig^2)\n\ncat(\"Quadratic model design matrix (one pig):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nQuadratic model design matrix (one pig):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(X_quad)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       age_one_pig     \n[1,] 1          30  900\n[2,] 1          60 3600\n[3,] 1          90 8100\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Rank:\", qr(X_quad)$rank, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRank: 3 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Dimensions:\", nrow(X_quad), \"×\", ncol(X_quad), \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDimensions: 3 × 3 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Can we fit quadratic (3 parameters) with 3 observations?\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCan we fit quadratic (3 parameters) with 3 observations?\n```\n\n\n:::\n\n```{.r .cell-code}\nXtX_quad <- t(X_quad) %*% X_quad\ncat(\"X'X:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nX'X:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(XtX_quad)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                  age_one_pig         \n                3         180    12600\nage_one_pig   180       12600   972000\n            12600      972000 79380000\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"det(X'X) =\", det(XtX_quad), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ndet(X'X) = 2.916e+09 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Is X'X invertible?\", abs(det(XtX_quad)) > 1e-10, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nIs X'X invertible? TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nYes! Exactly identified (3 parameters, 3 observations)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nYes! Exactly identified (3 parameters, 3 observations)\n```\n\n\n:::\n\n```{.r .cell-code}\n# What about cubic?\nX_cubic <- cbind(X_quad, age_one_pig^3)\ncat(\"\\nCubic model (4 parameters) with 3 observations:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCubic model (4 parameters) with 3 observations:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Rank:\", qr(X_cubic)$rank, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRank: 3 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Dimensions:\", nrow(X_cubic), \"×\", ncol(X_cubic), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDimensions: 3 × 4 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Can estimate 4 parameters with 3 observations? No - rank deficient\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCan estimate 4 parameters with 3 observations? No - rank deficient\n```\n\n\n:::\n:::\n\n\n:::{.callout-warning}\n## Numerical Issues\n\nFor large matrices or nearly singular matrices, computed determinants can be unreliable due to:\n\n1. **Overflow/underflow**: det(**A**) can be extremely large or small\n2. **Rounding errors**: Near-zero determinants may not be exactly zero\n\n**Better approaches**:\n- Use rank to check singularity: `qr(A)$rank < ncol(A)`\n- Use condition number to check \"near singularity\": `kappa(A)`\n- Don't use det() for large matrices (computational cost is O(n³))\n:::\n\n:::{.callout-note}\n## Cross-References\n\n- **Week 2: Linear Algebra Essentials** - Introduction to determinants\n- **Week 12: Non-Full Rank Models** - Determinant zero for rank-deficient X'X\n- **Week 14: Computational Considerations** - Numerical stability issues\n:::\n\n:::{.callout-important}\n## Summary: Matrix Properties\n\n**Rank**: Determines uniqueness of solutions\n- r(**X'X**) = r(**X**) - fundamental result\n- Full rank → unique least squares solution\n- Rank deficient → need constraints or estimable functions\n\n**Trace**: Sum of diagonal elements\n- tr(**H**) = p (model degrees of freedom)\n- tr(**I - H**) = n - p (error degrees of freedom)\n- tr(**P**) = rank(**P**) for idempotent **P**\n\n**Determinant**: Tests invertibility\n- det(**X'X**) ≠ 0 → **X'X** invertible\n- det(**X'X**) = 0 → rank deficient\n- Use rank for numerical stability\n:::\n\n---\n\n## Matrix Inverses {#sec-inverses}\n\nMatrix inverses are fundamental for solving linear systems, including the normal equations in linear models. When **X** is rank deficient, we need generalized inverses.\n\n### Regular Inverse {#sec-regular-inverse}\n\nThe **regular inverse** (or simply \"inverse\") of a matrix exists only for square, full-rank matrices.\n\n#### Definition\n\nFor a square n × n matrix **A**, the inverse **A**⁻¹ (if it exists) satisfies:\n\n$$\n\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}\n$$ {#eq-inverse-def}\n\n**Existence conditions:**\n- **A** must be **square** (n × n)\n- **A** must be **full rank** (rank n)\n- Equivalently: det(**A**) ≠ 0\n\n#### Properties\n\n**1. Uniqueness**: If **A**⁻¹ exists, it is unique\n\n**2. Reverse order**:\n$$\n(\\mathbf{AB})^{-1} = \\mathbf{B}^{-1}\\mathbf{A}^{-1}\n$$ {#eq-inverse-product}\n\n**3. Transpose**:\n$$\n(\\mathbf{A}')^{-1} = (\\mathbf{A}^{-1})'\n$$ {#eq-inverse-transpose}\n\n**4. Inverse of inverse**:\n$$\n(\\mathbf{A}^{-1})^{-1} = \\mathbf{A}\n$$\n\n**5. Scalar multiplication**:\n$$\n(c\\mathbf{A})^{-1} = \\frac{1}{c}\\mathbf{A}^{-1}, \\quad c \\neq 0\n$$\n\n#### Formula for 2 × 2 Matrix\n\nFor quick reference:\n$$\n\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}^{-1} = \\frac{1}{ad-bc} \\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}\n$$ {#eq-inverse-2x2}\n\nprovided $ad - bc \\neq 0$ (determinant is non-zero).\n\n#### Computing Inverses in R\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example 1: 2x2 matrix\nA <- matrix(c(3, 1, 2, 4), 2, 2)\ncat(\"Matrix A:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMatrix A:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    3    2\n[2,]    1    4\n```\n\n\n:::\n\n```{.r .cell-code}\n# Compute inverse\nA_inv <- solve(A)\ncat(\"\\nA inverse:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nA inverse:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A_inv)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]  0.4 -0.2\n[2,] -0.1  0.3\n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify AA^{-1} = I\nI_check <- A %*% A_inv\ncat(\"\\nA * A^{-1} =\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nA * A^{-1} =\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(I_check, 10))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nIs identity?\", all.equal(I_check, diag(2)), \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nIs identity? TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\n# Example 2: Verify manual 2x2 formula\ndet_A <- det(A)\nA_inv_manual <- (1/det_A) * matrix(c(4, -1, -2, 3), 2, 2)\ncat(\"Manual 2x2 inverse formula:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nManual 2x2 inverse formula:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A_inv_manual)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]  0.4 -0.2\n[2,] -0.1  0.3\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Agreement with solve():\", all.equal(A_inv, A_inv_manual), \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAgreement with solve(): TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\n# Example 3: Transpose property (A')^{-1} = (A^{-1})'\nAt_inv <- solve(t(A))\nAinv_t <- t(A_inv)\ncat(\"Transpose property:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTranspose property:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"(A')^{-1} = (A^{-1})'?\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(A')^{-1} = (A^{-1})'?\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(all.equal(At_inv, Ainv_t), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTRUE \n```\n\n\n:::\n:::\n\n\n#### Application: Solving Normal Equations\n\nWhen **X** is full rank, the normal equations $\\mathbf{X}'\\mathbf{X}\\mathbf{b} = \\mathbf{X}'\\mathbf{y}$ have unique solution:\n\n$$\n\\mathbf{b} = (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y}\n$$ {#eq-ls-solution-inverse}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simple regression example\nx <- c(1, 2, 3, 4, 5)\ny <- c(2.1, 3.9, 6.2, 7.8, 10.1)\n\n# Design matrix\nX <- cbind(1, x)\ncat(\"Design matrix X:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDesign matrix X:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       x\n[1,] 1 1\n[2,] 1 2\n[3,] 1 3\n[4,] 1 4\n[5,] 1 5\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Rank:\", qr(X)$rank, \"Columns:\", ncol(X), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRank: 2 Columns: 2 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Full rank?\", qr(X)$rank == ncol(X), \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFull rank? TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\n# Normal equations: X'Xb = X'y\nXtX <- t(X) %*% X\nXty <- t(X) %*% y\n\ncat(\"X'X:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nX'X:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(XtX)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      x\n   5 15\nx 15 55\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"det(X'X) =\", det(XtX), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ndet(X'X) = 50 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Is X'X invertible?\", abs(det(XtX)) > 1e-10, \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nIs X'X invertible? TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\n# Solve using inverse\nXtX_inv <- solve(XtX)\nb <- XtX_inv %*% Xty\n\ncat(\"Solution b = (X'X)^{-1}X'y:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSolution b = (X'X)^{-1}X'y:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  [,1]\n  0.05\nx 1.99\n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify with lm()\nfit <- lm(y ~ x)\ncat(\"\\nComparison with lm():\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nComparison with lm():\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Our b:\", c(b), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOur b: 0.05 1.99 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"lm() coef:\", coef(fit), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nlm() coef: 0.05 1.99 \n```\n\n\n:::\n:::\n\n\n:::{.callout-note}\n## Cross-References\n\n- **Week 2: Linear Algebra Essentials** - Introduction to matrix inverses\n- **Week 4-6: Regression** - Uses (**X'X**)⁻¹ throughout\n- **Week 5: Least Squares** - Derives b = (**X'X**)⁻¹**X'y**\n:::\n\n---\n\n### Generalized Inverse {#sec-generalized-inverse}\n\nWhen a matrix is not square or not full rank, the regular inverse doesn't exist. **Generalized inverses** extend the concept of inversion to such matrices.\n\n#### Definition: Reflexive Generalized Inverse\n\nA matrix **A**⁻ is a **generalized inverse** (or **g-inverse**) of **A** if:\n\n$$\n\\mathbf{A}\\mathbf{A}^{-}\\mathbf{A} = \\mathbf{A}\n$$ {#eq-ginv-def}\n\n**Key properties:**\n- **A**⁻ is **not unique** (many g-inverses exist)\n- If **A** is square and invertible, then **A**⁻ = **A**⁻¹ (unique)\n- Works for any m × n matrix **A** (doesn't need to be square)\n\n#### Moore-Penrose Inverse\n\nThe **Moore-Penrose inverse** **A**⁺ is a **unique** generalized inverse satisfying four conditions:\n\n1. $\\mathbf{A}\\mathbf{A}^{+}\\mathbf{A} = \\mathbf{A}$ (reflexive)\n2. $\\mathbf{A}^{+}\\mathbf{A}\\mathbf{A}^{+} = \\mathbf{A}^{+}$ (reflexive for **A**⁺)\n3. $(\\mathbf{A}\\mathbf{A}^{+})' = \\mathbf{A}\\mathbf{A}^{+}$ (symmetric)\n4. $(\\mathbf{A}^{+}\\mathbf{A})' = \\mathbf{A}^{+}\\mathbf{A}$ (symmetric)\n\n**A**⁺ is the **unique** matrix satisfying all four conditions.\n\n#### Computing Generalized Inverses\n\n**Method 1: Using SVD (Most Stable)**\n\nIf $\\mathbf{A} = \\mathbf{U}\\mathbf{D}\\mathbf{V}'$ is the singular value decomposition, then:\n\n$$\n\\mathbf{A}^{+} = \\mathbf{V}\\mathbf{D}^{+}\\mathbf{U}'\n$$ {#eq-ginv-svd}\n\nwhere **D**⁺ is formed by taking reciprocals of non-zero diagonal elements of **D** and transposing.\n\n**Method 2: Using `ginv()` from MASS package**\n\n```r\nlibrary(MASS)\nA_ginv <- ginv(A)\n```\n\n#### Properties of Generalized Inverses\n\n**1. Not unique** (except Moore-Penrose):\n\nDifferent g-inverses give different solutions to **Ax** = **b**, BUT:\n\n**2. Estimable functions are unique**:\n\nIf $\\mathbf{c}'\\boldsymbol{\\beta}$ is estimable (i.e., **c**' is in row space of **X**), then $\\mathbf{c}'\\mathbf{b}$ is the **same** for all g-inverses used to compute **b** = (**X'X**)⁻**X'y**.\n\nThis is **crucial** for rank-deficient models!\n\n**3. For symmetric **A**:**\n- (**A**⁻)' is also a g-inverse of **A**\n- **AA**⁻ and **A**⁻**A** are symmetric and idempotent (projection matrices)\n\n#### R Examples\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(MASS)\n\n# Example 1: Rank-deficient matrix\nA_rd <- matrix(c(1, 2, 3,\n                 2, 4, 6,\n                 1, 1, 1), 3, 3, byrow = TRUE)\n\ncat(\"Rank-deficient matrix A:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRank-deficient matrix A:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A_rd)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    2    4    6\n[3,]    1    1    1\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Rank:\", qr(A_rd)$rank, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRank: 2 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Dimensions: 3 x 3\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDimensions: 3 x 3\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"det(A) =\", det(A_rd), \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ndet(A) = 0 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Try regular inverse (will fail)\ncat(\"Attempting regular inverse...\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAttempting regular inverse...\n```\n\n\n:::\n\n```{.r .cell-code}\ntryCatch({\n  A_rd_inv <- solve(A_rd)\n  cat(\"Success!\\n\")\n}, error = function(e) {\n  cat(\"Error:\", conditionMessage(e), \"\\n\")\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nError: Lapack routine dgesv: system is exactly singular: U[3,3] = 0 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Compute generalized inverse\nA_rd_ginv <- ginv(A_rd)\ncat(\"\\nGeneralized inverse A^-:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nGeneralized inverse A^-:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(A_rd_ginv, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]    [,3]\n[1,] -0.1 -0.2  1.3333\n[2,]  0.0  0.0  0.3333\n[3,]  0.1  0.2 -0.6667\n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify AA^-A = A\nAAminusA <- A_rd %*% A_rd_ginv %*% A_rd\ncat(\"\\nVerify AA^-A = A:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVerify AA^-A = A:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Max difference:\", max(abs(AAminusA - A_rd)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMax difference: 4.441e-15 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Agreement:\", all.equal(AAminusA, A_rd), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAgreement: TRUE \n```\n\n\n:::\n:::\n\n\n#### Application: Non-Full Rank Design Matrix\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Three-breed ANOVA with cell means model\nbreed <- factor(rep(c(\"A\", \"B\", \"C\"), each = 3))\ny <- c(10, 11, 12,  # Breed A\n       15, 16, 14,  # Breed B\n       18, 19, 20)  # Breed C\n\n# Cell means model: y = μ_breed + e (one mean per breed)\nX <- model.matrix(~ breed - 1)  # -1 removes intercept\ncat(\"Design matrix (cell means model):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDesign matrix (cell means model):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  breedA breedB breedC\n1      1      0      0\n2      1      0      0\n3      1      0      0\n4      0      1      0\n5      0      1      0\n6      0      1      0\n7      0      0      1\n8      0      0      1\n9      0      0      1\nattr(,\"assign\")\n[1] 1 1 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$breed\n[1] \"contr.treatment\"\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Rank:\", qr(X)$rank, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRank: 3 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Columns:\", ncol(X), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nColumns: 3 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Full rank?\", qr(X)$rank == ncol(X), \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFull rank? TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\n# Compute X'X (will be full rank for cell means model)\nXtX <- t(X) %*% X\ncat(\"X'X:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nX'X:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(XtX)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       breedA breedB breedC\nbreedA      3      0      0\nbreedB      0      3      0\nbreedC      0      0      3\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"det(X'X) =\", det(XtX), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ndet(X'X) = 27 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Is invertible?\", abs(det(XtX)) > 1e-10, \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nIs invertible? TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\n# Solve for breed means\nXtX_inv <- solve(XtX)\nXty <- t(X) %*% y\nb <- XtX_inv %*% Xty\n\ncat(\"Breed means (μ_A, μ_B, μ_C):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBreed means (μ_A, μ_B, μ_C):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       [,1]\nbreedA   11\nbreedB   15\nbreedC   19\n```\n\n\n:::\n\n```{.r .cell-code}\n# Contrasts are directly estimable\n# Contrast: μ_A - μ_B (breed A vs breed B)\ncontrast <- c(1, -1, 0)  # μ_A - μ_B\ncontrast_estimate <- t(contrast) %*% b\ncat(\"\\nContrast μ_A - μ_B =\", c(contrast_estimate), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nContrast μ_A - μ_B = -4 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify this equals difference in breed means\nmean_A <- mean(y[breed == \"A\"])\nmean_B <- mean(y[breed == \"B\"])\ncat(\"Direct calculation:\", mean_A - mean_B, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDirect calculation: -4 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Agreement:\", all.equal(c(contrast_estimate), mean_A - mean_B), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAgreement: TRUE \n```\n\n\n:::\n:::\n\n\n#### Computing Moore-Penrose via SVD\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Demonstrate computing Moore-Penrose inverse manually\nA <- matrix(c(1, 2, 2, 4), 2, 2)  # Rank 1 matrix\n\ncat(\"Matrix A (rank deficient):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMatrix A (rank deficient):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    1    2\n[2,]    2    4\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Rank:\", qr(A)$rank, \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRank: 1 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Compute SVD\nsvd_A <- svd(A)\nU <- svd_A$u\nD_diag <- svd_A$d\nV <- svd_A$v\n\ncat(\"Singular values:\", D_diag, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSingular values: 5 1.986e-16 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Number of non-zero singular values:\", sum(D_diag > 1e-10), \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNumber of non-zero singular values: 1 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Create D^+ (reciprocals of non-zero singular values)\nD_plus_diag <- ifelse(D_diag > 1e-10, 1/D_diag, 0)\nD_plus <- diag(D_plus_diag)\n\n# A^+ = V D^+ U'\nA_plus_manual <- V %*% D_plus %*% t(U)\n\ncat(\"Moore-Penrose inverse (manual via SVD):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMoore-Penrose inverse (manual via SVD):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(A_plus_manual, 6))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,] 0.04 0.08\n[2,] 0.08 0.16\n```\n\n\n:::\n\n```{.r .cell-code}\n# Compare with ginv()\nA_plus_ginv <- ginv(A)\ncat(\"\\nMoore-Penrose inverse (ginv):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nMoore-Penrose inverse (ginv):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(A_plus_ginv, 6))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,] 0.04 0.08\n[2,] 0.08 0.16\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nAgreement:\", all.equal(A_plus_manual, A_plus_ginv, tolerance = 1e-6), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nAgreement: TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify four Moore-Penrose conditions\ncat(\"\\n Verifying four Moore-Penrose conditions:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n Verifying four Moore-Penrose conditions:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"1. AA^+A = A?\", all.equal(A %*% A_plus_ginv %*% A, A), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n1. AA^+A = A? TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"2. A^+AA^+ = A^+?\", all.equal(A_plus_ginv %*% A %*% A_plus_ginv, A_plus_ginv), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n2. A^+AA^+ = A^+? TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"3. (AA^+)' = AA^+?\", all.equal(t(A %*% A_plus_ginv), A %*% A_plus_ginv), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n3. (AA^+)' = AA^+? TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"4. (A^+A)' = A^+A?\", all.equal(t(A_plus_ginv %*% A), A_plus_ginv %*% A), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n4. (A^+A)' = A^+A? TRUE \n```\n\n\n:::\n:::\n\n\n:::{.callout-important}\n## Key Insight: Estimability\n\nWhen using generalized inverses for rank-deficient **X**:\n\n- **Non-estimable functions**: $\\mathbf{c}'\\mathbf{b}$ depends on which g-inverse is used\n- **Estimable functions**: $\\mathbf{c}'\\mathbf{b}$ is **unique** regardless of g-inverse\n\n**Example**: In effects model μ + αᵢ:\n- μ alone: NOT estimable (changes with g-inverse)\n- αᵢ alone: NOT estimable\n- αᵢ - αⱼ: **ESTIMABLE** (unique across all g-inverses)\n\nThis is why we focus on **contrasts** in Week 8!\n:::\n\n:::{.callout-note}\n## Cross-References\n\n- **Week 2: Linear Algebra Essentials** - Introduction to generalized inverses\n- **Week 8: Contrasts and Estimable Functions** - Which functions are estimable\n- **Week 12: Non-Full Rank Models** - Extensive use of g-inverses\n- **Week 13: Special Topics I** - Different g-inverse choices\n:::\n\n---\n\n### Useful Inverse Identities {#sec-inverse-identities}\n\nSeveral matrix identities make working with inverses more efficient, especially for structured matrices.\n\n#### Woodbury Matrix Identity\n\nAlso called **matrix inversion lemma**:\n\n$$\n(\\mathbf{A} + \\mathbf{UCV})^{-1} = \\mathbf{A}^{-1} - \\mathbf{A}^{-1}\\mathbf{U}(\\mathbf{C}^{-1} + \\mathbf{V}\\mathbf{A}^{-1}\\mathbf{U})^{-1}\\mathbf{V}\\mathbf{A}^{-1}\n$$ {#eq-woodbury}\n\n**Special cases:**\n\n**Sherman-Morrison Formula** (rank-1 update):\n\nIf **A** is n × n invertible and **u**, **v** are n × 1 vectors:\n\n$$\n(\\mathbf{A} + \\mathbf{uv}')^{-1} = \\mathbf{A}^{-1} - \\frac{\\mathbf{A}^{-1}\\mathbf{uv}'\\mathbf{A}^{-1}}{1 + \\mathbf{v}'\\mathbf{A}^{-1}\\mathbf{u}}\n$$ {#eq-sherman-morrison}\n\nThis is useful when updating an inverse after a rank-1 change.\n\n#### Block Matrix Inversion\n\nFor a block matrix:\n$$\n\\mathbf{M} = \\begin{pmatrix} \\mathbf{A} & \\mathbf{B} \\\\ \\mathbf{C} & \\mathbf{D} \\end{pmatrix}\n$$\n\nIf **A** and $\\mathbf{D} - \\mathbf{CA}^{-1}\\mathbf{B}$ (the Schur complement) are invertible:\n\n$$\n\\mathbf{M}^{-1} = \\begin{pmatrix}\n\\mathbf{A}^{-1} + \\mathbf{A}^{-1}\\mathbf{B}\\mathbf{S}^{-1}\\mathbf{CA}^{-1} & -\\mathbf{A}^{-1}\\mathbf{B}\\mathbf{S}^{-1} \\\\\n-\\mathbf{S}^{-1}\\mathbf{CA}^{-1} & \\mathbf{S}^{-1}\n\\end{pmatrix}\n$$ {#eq-block-inverse}\n\nwhere $\\mathbf{S} = \\mathbf{D} - \\mathbf{CA}^{-1}\\mathbf{B}$ is the Schur complement.\n\n#### Inverse of Sum\n\nIn general, $(\\mathbf{A} + \\mathbf{B})^{-1} \\neq \\mathbf{A}^{-1} + \\mathbf{B}^{-1}$\n\nBut if **A** and **B** commute (**AB** = **BA**):\n$$\n(\\mathbf{A} + \\mathbf{B})^{-1}\\mathbf{A} = \\mathbf{A}(\\mathbf{A} + \\mathbf{B})^{-1}\n$$\n\n#### R Examples\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example 1: Sherman-Morrison formula\nA <- matrix(c(4, 1, 1, 3), 2, 2)\nu <- c(1, 2)\nv <- c(3, 1)\n\ncat(\"Sherman-Morrison Formula:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSherman-Morrison Formula:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"A =\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nA =\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    4    1\n[2,]    1    3\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"u =\", u, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nu = 1 2 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"v =\", v, \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nv = 3 1 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Direct computation\nA_plus_uv <- A + u %*% t(v)\nA_plus_uv_inv_direct <- solve(A_plus_uv)\n\ncat(\"Direct: (A + uv')^{-1} =\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDirect: (A + uv')^{-1} =\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(A_plus_uv_inv_direct, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        [,1]    [,2]\n[1,]  0.2381 -0.0952\n[2,] -0.3333  0.3333\n```\n\n\n:::\n\n```{.r .cell-code}\n# Sherman-Morrison formula\nA_inv <- solve(A)\nnumerator <- A_inv %*% u %*% t(v) %*% A_inv\ndenominator <- 1 + c(t(v) %*% A_inv %*% u)\nA_plus_uv_inv_SM <- A_inv - numerator / denominator\n\ncat(\"\\nSherman-Morrison: (A + uv')^{-1} =\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSherman-Morrison: (A + uv')^{-1} =\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(A_plus_uv_inv_SM, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        [,1]    [,2]\n[1,]  0.2381 -0.0952\n[2,] -0.3333  0.3333\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nAgreement:\", all.equal(A_plus_uv_inv_direct, A_plus_uv_inv_SM), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nAgreement: TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nComputational advantage: Don't need to invert (A + uv'),\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nComputational advantage: Don't need to invert (A + uv'),\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"just use existing A^{-1} with vector operations\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\njust use existing A^{-1} with vector operations\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example 2: Block matrix inversion\n# Useful for partitioned models\nA_block <- matrix(c(2, 0, 0, 3), 2, 2)\nB_block <- matrix(c(1, 0, 0, 1), 2, 2)\nC_block <- matrix(c(0, 1, 1, 0), 2, 2)\nD_block <- matrix(c(4, 1, 1, 5), 2, 2)\n\nM <- rbind(cbind(A_block, B_block),\n           cbind(C_block, D_block))\n\ncat(\"Block matrix M:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBlock matrix M:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(M)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3] [,4]\n[1,]    2    0    1    0\n[2,]    0    3    0    1\n[3,]    0    1    4    1\n[4,]    1    0    1    5\n```\n\n\n:::\n\n```{.r .cell-code}\n# Direct inversion\nM_inv_direct <- solve(M)\ncat(\"\\nDirect M^{-1}:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nDirect M^{-1}:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(M_inv_direct, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        [,1]    [,2]    [,3]    [,4]\n[1,]  0.4915  0.0424 -0.1271  0.0169\n[2,]  0.0339  0.3305  0.0085 -0.0678\n[3,]  0.0169 -0.0847  0.2542 -0.0339\n[4,] -0.1017  0.0085 -0.0254  0.2034\n```\n\n\n:::\n\n```{.r .cell-code}\n# Block inversion formula\nA_inv <- solve(A_block)\nS <- D_block - C_block %*% A_inv %*% B_block  # Schur complement\nS_inv <- solve(S)\n\nM11 <- A_inv + A_inv %*% B_block %*% S_inv %*% C_block %*% A_inv\nM12 <- -A_inv %*% B_block %*% S_inv\nM21 <- -S_inv %*% C_block %*% A_inv\nM22 <- S_inv\n\nM_inv_block <- rbind(cbind(M11, M12),\n                     cbind(M21, M22))\n\ncat(\"\\nBlock formula M^{-1}:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nBlock formula M^{-1}:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(M_inv_block, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        [,1]    [,2]    [,3]    [,4]\n[1,]  0.4915  0.0424 -0.1271  0.0169\n[2,]  0.0339  0.3305  0.0085 -0.0678\n[3,]  0.0169 -0.0847  0.2542 -0.0339\n[4,] -0.1017  0.0085 -0.0254  0.2034\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nAgreement:\", all.equal(M_inv_direct, M_inv_block), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nAgreement: TRUE \n```\n\n\n:::\n:::\n\n\n:::{.callout-tip}\n## When to Use These Identities\n\n**Sherman-Morrison**:\n- Updating regression after adding one observation\n- Sequential estimation\n- Rank-1 modifications to covariance matrices\n\n**Woodbury**:\n- Weighted least squares\n- Ridge regression\n- Kalman filtering\n\n**Block inversion**:\n- Partitioned regression models\n- Testing subsets of parameters\n- Mixed models (Week 14 preview)\n\nThese identities can dramatically reduce computation when you have structure to exploit!\n:::\n\n:::{.callout-note}\n## Cross-References\n\n- **Week 14: Special Topics II** - Weighted least squares uses these identities\n- Advanced mixed models courses - Block inversion for MME\n:::\n\n:::{.callout-important}\n## Summary: Matrix Inverses\n\n**Regular Inverse**:\n- Only for square, full-rank matrices\n- Unique: **AA**⁻¹ = **A**⁻¹**A** = **I**\n- R: `solve(A)`\n\n**Generalized Inverse**:\n- For any matrix (rank deficient OK)\n- Not unique: **AA**⁻**A** = **A**\n- Moore-Penrose **A**⁺ is unique\n- R: `ginv(A)` from MASS\n- **Key**: Estimable functions are unique across all g-inverses\n\n**Identities**:\n- Sherman-Morrison: Rank-1 updates\n- Woodbury: Low-rank updates\n- Block inversion: Partitioned matrices\n:::\n\n---\n\n## Projection Matrices and Quadratic Forms {#sec-projections}\n\n:::{.callout-important}\n## Critical Section\n\nThis section is foundational for understanding least squares theory in Week 5. It receives extra emphasis with additional examples and detailed coverage. The concepts here are central to all of linear models.\n:::\n\n### Projection Matrices {#sec-projection-matrices}\n\nProjection matrices are fundamental to least squares theory. They provide the geometric interpretation of fitting linear models and decomposing variation into model and error components.\n\n#### Definition\n\nA **projection matrix** **P** is a square matrix satisfying:\n\n$$\n\\mathbf{P}^2 = \\mathbf{P}\n$$ {#eq-idempotent}\n\nThis property is called **idempotence**. If additionally **P** is symmetric ($\\mathbf{P}' = \\mathbf{P}$), then **P** is an **orthogonal projection matrix**.\n\n:::{.callout-note}\n## Geometric Interpretation\n\nAn orthogonal projection matrix projects vectors onto a subspace and is \"idempotent\" because projecting twice is the same as projecting once - you're already in the subspace after the first projection.\n:::\n\n#### The Hat Matrix **H**\n\nThe most important projection matrix in linear models is the **hat matrix**:\n\n$$\n\\mathbf{H} = \\mathbf{X}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\n$$ {#eq-hat-matrix}\n\nwhere **X** is the n × p design matrix with full column rank (rank p).\n\n**Why \"hat matrix\"?** Because it puts the \"hat\" on **y**:\n\n$$\n\\hat{\\mathbf{y}} = \\mathbf{H}\\mathbf{y}\n$$ {#eq-y-hat}\n\nwhere $\\hat{\\mathbf{y}}$ is the vector of fitted values.\n\n#### Properties of the Hat Matrix\n\nThe hat matrix **H** has several critical properties:\n\n1. **Symmetric**: $\\mathbf{H}' = \\mathbf{H}$\n\n   Proof: $\\mathbf{H}' = [\\mathbf{X}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}']' = \\mathbf{X}[(\\mathbf{X}'\\mathbf{X})^{-1}]'\\mathbf{X}' = \\mathbf{X}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}' = \\mathbf{H}$\n\n2. **Idempotent**: $\\mathbf{H}^2 = \\mathbf{H}$\n\n   Proof:\n   $$\\begin{align}\n   \\mathbf{H}^2 &= \\mathbf{X}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}' \\cdot \\mathbf{X}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}' \\\\\n   &= \\mathbf{X}(\\mathbf{X}'\\mathbf{X})^{-1}[\\mathbf{X}'\\mathbf{X}](\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}' \\\\\n   &= \\mathbf{X}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}' = \\mathbf{H}\n   \\end{align}$$\n\n3. **Rank and Trace**: $\\text{rank}(\\mathbf{H}) = \\text{tr}(\\mathbf{H}) = p$\n\n   For idempotent matrices, rank equals trace. Since **H** projects onto the p-dimensional column space of **X**, its rank is p.\n\n4. **Eigenvalues**: All eigenvalues of **H** are either 0 or 1\n\n   - p eigenvalues equal 1 (corresponding to column space of **X**)\n   - n - p eigenvalues equal 0 (corresponding to orthogonal complement)\n\n5. **Projects onto column space of X**: $\\mathbf{H}\\mathbf{X} = \\mathbf{X}$\n\n   Proof: $\\mathbf{H}\\mathbf{X} = \\mathbf{X}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{X} = \\mathbf{X}$\n\n#### The Residual Projection Matrix **I - H**\n\nThe complement of the hat matrix is equally important:\n\n$$\n\\mathbf{I} - \\mathbf{H}\n$$ {#eq-residual-matrix}\n\nThis matrix projects onto the space orthogonal to the column space of **X**.\n\n**Properties of I - H:**\n\n1. **Symmetric**: $(\\mathbf{I} - \\mathbf{H})' = \\mathbf{I} - \\mathbf{H}$\n\n2. **Idempotent**: $(\\mathbf{I} - \\mathbf{H})^2 = \\mathbf{I} - \\mathbf{H}$\n\n   Proof:\n   $$\\begin{align}\n   (\\mathbf{I} - \\mathbf{H})^2 &= \\mathbf{I} - 2\\mathbf{H} + \\mathbf{H}^2 \\\\\n   &= \\mathbf{I} - 2\\mathbf{H} + \\mathbf{H} \\\\\n   &= \\mathbf{I} - \\mathbf{H}\n   \\end{align}$$\n\n3. **Rank and Trace**: $\\text{rank}(\\mathbf{I} - \\mathbf{H}) = \\text{tr}(\\mathbf{I} - \\mathbf{H}) = n - p$\n\n4. **Produces residuals**: $\\mathbf{e} = (\\mathbf{I} - \\mathbf{H})\\mathbf{y}$\n\n5. **Orthogonal to X**: $(\\mathbf{I} - \\mathbf{H})\\mathbf{X} = \\mathbf{0}$\n\n   Proof: $(\\mathbf{I} - \\mathbf{H})\\mathbf{X} = \\mathbf{X} - \\mathbf{H}\\mathbf{X} = \\mathbf{X} - \\mathbf{X} = \\mathbf{0}$\n\n#### Orthogonality of **H** and **I - H**\n\nA key relationship:\n\n$$\n\\mathbf{H}(\\mathbf{I} - \\mathbf{H}) = \\mathbf{0}\n$$ {#eq-orthogonal}\n\nProof:\n$$\\begin{align}\n\\mathbf{H}(\\mathbf{I} - \\mathbf{H}) &= \\mathbf{H} - \\mathbf{H}^2 \\\\\n&= \\mathbf{H} - \\mathbf{H} = \\mathbf{0}\n\\end{align}$$\n\nThis shows that fitted values $\\hat{\\mathbf{y}} = \\mathbf{H}\\mathbf{y}$ and residuals $\\mathbf{e} = (\\mathbf{I} - \\mathbf{H})\\mathbf{y}$ are orthogonal:\n\n$$\n\\hat{\\mathbf{y}}'\\mathbf{e} = (\\mathbf{H}\\mathbf{y})'[(\\mathbf{I} - \\mathbf{H})\\mathbf{y}] = \\mathbf{y}'\\mathbf{H}'(\\mathbf{I} - \\mathbf{H})\\mathbf{y} = 0\n$$ {#eq-fitted-resid-orthogonal}\n\n#### Hat Values (Leverage)\n\nThe **diagonal elements** of **H** are called **hat values** or **leverage values**:\n\n$$\nh_{ii} = [\\mathbf{H}]_{ii}\n$$ {#eq-leverage}\n\nProperties:\n- $0 \\leq h_{ii} \\leq 1$ for all i\n- $\\sum_{i=1}^n h_{ii} = \\text{tr}(\\mathbf{H}) = p$\n- Average hat value: $\\bar{h} = p/n$\n\n**Interpretation**: $h_{ii}$ measures the leverage or influence that observation i has on its own fitted value. High leverage points have the potential to strongly influence the regression.\n\n**Rule of thumb**: Observations with $h_{ii} > 2p/n$ or $h_{ii} > 3p/n$ are considered high leverage.\n\n:::{.callout-note}\n## Cross-Reference\n\nHat values are used extensively in **Week 11: Model Diagnostics** to identify influential observations.\n:::\n\n#### R Implementation\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example: Simple regression with 5 observations\n# Dairy cow milk yield (kg/day) vs days in milk\n\ndays <- c(30, 60, 90, 120, 150)\nyield <- c(28, 32, 30, 27, 24)\n\n# Build design matrix\nX <- cbind(1, days)  # Intercept and slope\nn <- nrow(X)\np <- ncol(X)\n\ncat(\"Design matrix X:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDesign matrix X:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       days\n[1,] 1   30\n[2,] 1   60\n[3,] 1   90\n[4,] 1  120\n[5,] 1  150\n```\n\n\n:::\n\n```{.r .cell-code}\n# Compute hat matrix H = X(X'X)^{-1}X'\nXtX <- t(X) %*% X\nXtX_inv <- solve(XtX)\nH <- X %*% XtX_inv %*% t(X)\n\ncat(\"\\nHat matrix H:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nHat matrix H:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(H, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3] [,4] [,5]\n[1,]  0.6  0.4  0.2  0.0 -0.2\n[2,]  0.4  0.3  0.2  0.1  0.0\n[3,]  0.2  0.2  0.2  0.2  0.2\n[4,]  0.0  0.1  0.2  0.3  0.4\n[5,] -0.2  0.0  0.2  0.4  0.6\n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify H is symmetric\ncat(\"\\nIs H symmetric?\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nIs H symmetric?\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(all.equal(H, t(H)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTRUE \n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify H is idempotent (H^2 = H)\ncat(\"\\nIs H idempotent (H^2 = H)?\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nIs H idempotent (H^2 = H)?\n```\n\n\n:::\n\n```{.r .cell-code}\nH2 <- H %*% H\ncat(all.equal(H, H2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTRUE \n```\n\n\n:::\n\n```{.r .cell-code}\n# Check rank and trace\ncat(\"\\nRank of H:\", qr(H)$rank, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nRank of H: 2 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Trace of H:\", sum(diag(H)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrace of H: 2 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Expected (p):\", p, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nExpected (p): 2 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Compute I - H\nI_minus_H <- diag(n) - H\n\ncat(\"\\nResidual projection matrix (I - H):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nResidual projection matrix (I - H):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(I_minus_H, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3] [,4] [,5]\n[1,]  0.4 -0.4 -0.2  0.0  0.2\n[2,] -0.4  0.7 -0.2 -0.1  0.0\n[3,] -0.2 -0.2  0.8 -0.2 -0.2\n[4,]  0.0 -0.1 -0.2  0.7 -0.4\n[5,]  0.2  0.0 -0.2 -0.4  0.4\n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify (I-H) is idempotent\nI_minus_H2 <- I_minus_H %*% I_minus_H\ncat(\"\\nIs (I-H) idempotent?\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nIs (I-H) idempotent?\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(all.equal(I_minus_H, I_minus_H2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTRUE \n```\n\n\n:::\n\n```{.r .cell-code}\n# Check trace of I-H\ncat(\"\\nTrace of (I-H):\", sum(diag(I_minus_H)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nTrace of (I-H): 3 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Expected (n-p):\", n - p, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nExpected (n-p): 3 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify orthogonality: H(I-H) = 0\nH_times_IminusH <- H %*% I_minus_H\ncat(\"\\nIs H(I-H) = 0?\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nIs H(I-H) = 0?\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(all(abs(H_times_IminusH) < 1e-10), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTRUE \n```\n\n\n:::\n\n```{.r .cell-code}\n# Compute fitted values and residuals\ny_hat <- H %*% yield\ne <- (I_minus_H) %*% yield\n\ncat(\"\\nFitted values:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nFitted values:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(y_hat, 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]\n[1,] 30.8\n[2,] 29.5\n[3,] 28.2\n[4,] 26.9\n[5,] 25.6\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nResiduals:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nResiduals:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(e, 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]\n[1,] -2.8\n[2,]  2.5\n[3,]  1.8\n[4,]  0.1\n[5,] -1.6\n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify fitted and residuals are orthogonal\ncat(\"\\nAre fitted and residuals orthogonal (sum to 0)?\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nAre fitted and residuals orthogonal (sum to 0)?\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(round(sum(y_hat * e), 10), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Hat values (leverage)\nh_ii <- diag(H)\ncat(\"\\nHat values (leverage):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nHat values (leverage):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(h_ii, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.6 0.3 0.2 0.3 0.6\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nAverage leverage:\", p/n, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nAverage leverage: 0.4 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"High leverage threshold (2p/n):\", 2*p/n, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nHigh leverage threshold (2p/n): 0.8 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Observations with high leverage:\", which(h_ii > 2*p/n), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nObservations with high leverage:  \n```\n\n\n:::\n:::\n\n\n#### Extended Example: Multiple Regression\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Beef cattle: ADG (kg/day) vs initial weight (kg) and days on feed\n# n = 8 steers\n\ninitial_wt <- c(250, 260, 245, 270, 255, 265, 240, 275)\ndays_feed <- c(120, 120, 120, 120, 150, 150, 150, 150)\nadg <- c(1.2, 1.3, 1.1, 1.4, 1.0, 1.1, 0.9, 1.2)\n\n# Design matrix: intercept, initial weight, days on feed\nX <- cbind(1, initial_wt, days_feed)\nn <- nrow(X)\np <- ncol(X)\n\ncat(\"Design matrix X (first 4 rows):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDesign matrix X (first 4 rows):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(head(X, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       initial_wt days_feed\n[1,] 1        250       120\n[2,] 1        260       120\n[3,] 1        245       120\n[4,] 1        270       120\n```\n\n\n:::\n\n```{.r .cell-code}\n# Compute H\nH <- X %*% solve(t(X) %*% X) %*% t(X)\n\n# Verify all properties\ncat(\"\\nVerification of H properties:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVerification of H properties:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"1. Symmetric:\", all.equal(H, t(H)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n1. Symmetric: TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"2. Idempotent:\", all.equal(H, H %*% H), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n2. Idempotent: TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"3. Rank:\", qr(H)$rank, \"= p =\", p, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n3. Rank: 3 = p = 3 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"4. Trace:\", round(sum(diag(H)), 6), \"= p =\", p, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n4. Trace: 3 = p = 3 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Hat values\nh_ii <- diag(H)\ncat(\"\\nHat values:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nHat values:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(h_ii, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2877 0.2636 0.3720 0.4322 0.2636 0.2877 0.5889 0.5045\n```\n\n\n:::\n\n```{.r .cell-code}\n# Identify high leverage points\nthreshold <- 2 * p / n\ncat(\"\\nLeverage threshold (2p/n):\", round(threshold, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nLeverage threshold (2p/n): 0.75 \n```\n\n\n:::\n\n```{.r .cell-code}\nhigh_leverage <- which(h_ii > threshold)\ncat(\"High leverage observations:\", high_leverage, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nHigh leverage observations:  \n```\n\n\n:::\n\n```{.r .cell-code}\n# Compute fitted and residual\ny_hat <- H %*% adg\ne <- adg - y_hat\n\ncat(\"\\nModel fit:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nModel fit:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"R² =\", 1 - sum(e^2) / sum((adg - mean(adg))^2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR² = 0.9799 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Residual sum of squares =\", sum(e^2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nResidual sum of squares = 0.003614 \n```\n\n\n:::\n:::\n\n\n:::{.callout-tip}\n## Computational Note\n\nIn practice, never compute **H** explicitly for large datasets. Instead:\n\n1. Use `lm()` and `hatvalues()` to get leverage values\n2. Compute projections via QR decomposition (more stable)\n3. For fitted values: $\\hat{\\mathbf{y}} = \\mathbf{X}\\mathbf{b}$ (no need for **H**)\n4. For residuals: $\\mathbf{e} = \\mathbf{y} - \\mathbf{X}\\mathbf{b}$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Practical approach\nfit <- lm(adg ~ initial_wt + days_feed)\n\n# Get leverage directly\nh_practical <- hatvalues(fit)\ncat(\"Leverage from lm():\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLeverage from lm():\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(h_practical, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     1      2      3      4      5      6      7      8 \n0.2877 0.2636 0.3720 0.4322 0.2636 0.2877 0.5889 0.5045 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Compare with our manual calculation\ncat(\"\\nAgreement with manual H:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nAgreement with manual H:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(all.equal(diag(H), h_practical, check.attributes = FALSE), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTRUE \n```\n\n\n:::\n:::\n\n:::\n\n---\n\n### Quadratic Forms {#sec-quadratic-forms}\n\nA **quadratic form** is a scalar-valued function of a vector obtained by \"sandwiching\" a matrix between the vector and its transpose:\n\n$$\nq = \\mathbf{y}'\\mathbf{A}\\mathbf{y}\n$$ {#eq-quadratic-form}\n\nwhere **y** is n × 1 and **A** is n × n symmetric.\n\n#### Properties of Quadratic Forms\n\n1. **Result is a scalar**: $\\mathbf{y}'\\mathbf{A}\\mathbf{y}$ is a 1 × 1 matrix (scalar)\n\n2. **Symmetry matters**: For $\\mathbf{y}'\\mathbf{A}\\mathbf{y}$ to be well-defined, **A** should be symmetric\n   - If **A** is not symmetric, only $(\\mathbf{A} + \\mathbf{A}')/2$ contributes to the quadratic form\n\n3. **Trace relationship**:\n   $$\n   \\mathbf{y}'\\mathbf{A}\\mathbf{y} = \\text{tr}(\\mathbf{A}\\mathbf{y}\\mathbf{y}')\n   $$ {#eq-qf-trace}\n\n4. **Expected value**: If $\\mathbf{y} \\sim N(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$, then:\n   $$\n   E(\\mathbf{y}'\\mathbf{A}\\mathbf{y}) = \\text{tr}(\\mathbf{A}\\boldsymbol{\\Sigma}) + \\boldsymbol{\\mu}'\\mathbf{A}\\boldsymbol{\\mu}\n   $$ {#eq-qf-expectation}\n\n5. **Variance**: If $\\mathbf{y} \\sim N(\\boldsymbol{\\mu}, \\sigma^2\\mathbf{I})$ and **A** is symmetric:\n   $$\n   \\text{Var}(\\mathbf{y}'\\mathbf{A}\\mathbf{y}) = 2\\sigma^4 \\text{tr}(\\mathbf{A}^2) + 4\\sigma^2\\boldsymbol{\\mu}'\\mathbf{A}^2\\boldsymbol{\\mu}\n   $$ {#eq-qf-variance}\n\n#### Distribution of Quadratic Forms\n\n**Theorem**: If $\\mathbf{y} \\sim N(\\mathbf{0}, \\sigma^2\\mathbf{I})$ and **A** is symmetric and idempotent with rank r, then:\n\n$$\n\\frac{\\mathbf{y}'\\mathbf{A}\\mathbf{y}}{\\sigma^2} \\sim \\chi^2_r\n$$ {#eq-qf-chi-squared}\n\nThis is **Cochran's Theorem** and is fundamental for hypothesis testing in linear models.\n\n#### Quadratic Forms in Linear Models\n\nQuadratic forms appear everywhere in linear models analysis. The key decomposition is:\n\n$$\n\\mathbf{y}'\\mathbf{y} = \\mathbf{y}'\\mathbf{H}\\mathbf{y} + \\mathbf{y}'(\\mathbf{I} - \\mathbf{H})\\mathbf{y}\n$$ {#eq-qf-decomposition}\n\nThis leads to the sum of squares decomposition:\n\n**Total Sum of Squares (SST)**:\n$$\n\\text{SST} = \\mathbf{y}'(\\mathbf{I} - n^{-1}\\mathbf{1}\\mathbf{1}')\\mathbf{y} = \\sum_{i=1}^n (y_i - \\bar{y})^2\n$$ {#eq-sst}\n\nwhere $\\mathbf{1}$ is an n × 1 vector of ones and $\\bar{y} = n^{-1}\\mathbf{1}'\\mathbf{y}$.\n\n**Model Sum of Squares (SSM or SSR)**:\n$$\n\\text{SSM} = \\mathbf{y}'[\\mathbf{H} - n^{-1}\\mathbf{1}\\mathbf{1}']\\mathbf{y} = \\sum_{i=1}^n (\\hat{y}_i - \\bar{y})^2\n$$ {#eq-ssm}\n\nAlternatively:\n$$\n\\text{SSM} = \\mathbf{b}'\\mathbf{X}'\\mathbf{y} - n\\bar{y}^2\n$$ {#eq-ssm-alt}\n\n**Error Sum of Squares (SSE or RSS)**:\n$$\n\\text{SSE} = \\mathbf{y}'(\\mathbf{I} - \\mathbf{H})\\mathbf{y} = \\mathbf{e}'\\mathbf{e} = \\sum_{i=1}^n e_i^2\n$$ {#eq-sse}\n\nAlternatively:\n$$\n\\text{SSE} = \\mathbf{y}'\\mathbf{y} - \\mathbf{b}'\\mathbf{X}'\\mathbf{y}\n$$ {#eq-sse-alt}\n\n**Decomposition**:\n$$\n\\text{SST} = \\text{SSM} + \\text{SSE}\n$$ {#eq-ss-decomposition}\n\nwith degrees of freedom:\n- SST: n - 1\n- SSM: p - 1 (or p if no intercept)\n- SSE: n - p\n\n#### Variance Estimation via Quadratic Forms\n\nThe variance estimate is:\n\n$$\n\\hat{\\sigma}^2 = \\frac{\\text{SSE}}{n - p} = \\frac{\\mathbf{y}'(\\mathbf{I} - \\mathbf{H})\\mathbf{y}}{n - p}\n$$ {#eq-sigma-hat}\n\nUnder normality, $\\text{SSE}/\\sigma^2 \\sim \\chi^2_{n-p}$, so:\n\n$$\nE(\\hat{\\sigma}^2) = \\sigma^2\n$$ {#eq-sigma-hat-unbiased}\n\n#### Variance of Estimates\n\nThe variance of the least squares estimates **b** is also a quadratic form:\n\n$$\n\\text{Var}(\\mathbf{b}) = (\\mathbf{X}'\\mathbf{X})^{-1}\\sigma^2\n$$ {#eq-var-b}\n\nFor a linear combination $\\mathbf{c}'\\mathbf{b}$:\n\n$$\n\\text{Var}(\\mathbf{c}'\\mathbf{b}) = \\mathbf{c}'(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{c} \\cdot \\sigma^2\n$$ {#eq-var-contrast}\n\nThis is a quadratic form in **c**.\n\n#### R Implementation\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Continue with dairy cow example from earlier\ndays <- c(30, 60, 90, 120, 150)\nyield <- c(28, 32, 30, 27, 24)\nn <- length(yield)\n\n# Design matrix\nX <- cbind(1, days)\np <- ncol(X)\n\n# Compute projection matrices\nH <- X %*% solve(t(X) %*% X) %*% t(X)\nI_minus_H <- diag(n) - H\nJ_n <- matrix(1/n, n, n)  # n^{-1}11'\n\n# Demonstrate y'y = y'Hy + y'(I-H)y\nyty <- t(yield) %*% yield\nytHy <- t(yield) %*% H %*% yield\nyt_IminusH_y <- t(yield) %*% I_minus_H %*% yield\n\ncat(\"Decomposition of y'y:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDecomposition of y'y:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"y'y =\", c(yty), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ny'y = 4013 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"y'Hy =\", c(ytHy), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ny'Hy = 3993 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"y'(I-H)y =\", c(yt_IminusH_y), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ny'(I-H)y = 19.9 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"y'Hy + y'(I-H)y =\", c(ytHy + yt_IminusH_y), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ny'Hy + y'(I-H)y = 4013 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Check:\", all.equal(c(yty), c(ytHy + yt_IminusH_y)), \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCheck: TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\n# Sum of squares decomposition\ny_bar <- mean(yield)\n\n# SST = y'(I - n^{-1}11')y\nSST <- t(yield) %*% (diag(n) - J_n) %*% yield\nSST_alt <- sum((yield - y_bar)^2)\n\ncat(\"Total Sum of Squares (SST):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTotal Sum of Squares (SST):\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Quadratic form:\", c(SST), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nQuadratic form: 36.8 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Direct calculation:\", SST_alt, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDirect calculation: 36.8 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Check:\", all.equal(c(SST), SST_alt), \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCheck: TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\n# SSM = y'(H - n^{-1}11')y\nSSM <- t(yield) %*% (H - J_n) %*% yield\ny_hat <- H %*% yield\nSSM_alt <- sum((y_hat - y_bar)^2)\n\ncat(\"Model Sum of Squares (SSM):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel Sum of Squares (SSM):\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Quadratic form:\", c(SSM), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nQuadratic form: 16.9 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Direct calculation:\", SSM_alt, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDirect calculation: 16.9 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Check:\", all.equal(c(SSM), SSM_alt, tolerance = 1e-10), \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCheck: TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\n# SSE = y'(I-H)y\nSSE <- t(yield) %*% I_minus_H %*% yield\ne <- (I_minus_H) %*% yield\nSSE_alt <- sum(e^2)\n\ncat(\"Error Sum of Squares (SSE):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nError Sum of Squares (SSE):\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Quadratic form:\", c(SSE), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nQuadratic form: 19.9 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Direct calculation:\", SSE_alt, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDirect calculation: 19.9 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Check:\", all.equal(c(SSE), SSE_alt), \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCheck: TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify SST = SSM + SSE\ncat(\"Sum of Squares Decomposition:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSum of Squares Decomposition:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"SST =\", c(SST), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSST = 36.8 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"SSM + SSE =\", c(SSM + SSE), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSSM + SSE = 36.8 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Check:\", all.equal(c(SST), c(SSM + SSE)), \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCheck: TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\n# Degrees of freedom\ndf_total <- n - 1\ndf_model <- p - 1\ndf_error <- n - p\n\ncat(\"Degrees of Freedom:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDegrees of Freedom:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Total:\", df_total, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTotal: 4 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Model:\", df_model, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel: 1 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Error:\", df_error, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nError: 3 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Check:\", df_model + df_error, \"=\", df_total, \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCheck: 4 = 4 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Variance estimate\nsigma2_hat <- c(SSE) / df_error\ncat(\"Variance estimate: sigma^2 =\", sigma2_hat, \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nVariance estimate: sigma^2 = 6.633 \n```\n\n\n:::\n\n```{.r .cell-code}\n# R-squared\nR2 <- c(SSM) / c(SST)\nR2_alt <- 1 - c(SSE) / c(SST)\ncat(\"R-squared:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR-squared:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"SSM/SST =\", R2, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSSM/SST = 0.4592 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"1 - SSE/SST =\", R2_alt, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n1 - SSE/SST = 0.4592 \n```\n\n\n:::\n:::\n\n\n#### Complete Example: Beef Cattle ADG\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Beef cattle: ADG (kg/day) for 8 steers\n# Model: ADG ~ breed (Angus vs Hereford, 4 steers each)\n\nadg <- c(1.2, 1.3, 1.1, 1.4,  # Angus\n         1.0, 1.1, 0.9, 1.2)  # Hereford\nbreed <- factor(rep(c(\"Angus\", \"Hereford\"), each = 4))\nn <- length(adg)\n\n# Cell means model: X has 2 columns (one per breed)\nX <- model.matrix(~ breed - 1)\np <- ncol(X)\n\ncat(\"Design matrix X:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDesign matrix X:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  breedAngus breedHereford\n1          1             0\n2          1             0\n3          1             0\n4          1             0\n5          0             1\n6          0             1\n7          0             1\n8          0             1\nattr(,\"assign\")\n[1] 1 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$breed\n[1] \"contr.treatment\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# Compute all needed matrices\nH <- X %*% solve(t(X) %*% X) %*% t(X)\nI_minus_H <- diag(n) - H\n\n# Compute sum of squares\ny_bar <- mean(adg)\nSST <- sum((adg - y_bar)^2)\n\ny_hat <- H %*% adg\nSSM <- sum((y_hat - y_bar)^2)\n\ne <- adg - y_hat\nSSE <- sum(e^2)\n\ncat(\"\\nANOVA Table:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nANOVA Table:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Source       SS      df    MS       F\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSource       SS      df    MS       F\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"-------------------------------------------\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n-------------------------------------------\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"Breed     %6.4f   %2d   %6.4f   %6.2f\\n\",\n            SSM, p-1, SSM/(p-1), (SSM/(p-1))/(SSE/(n-p))))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBreed     0.0800    1   0.0800     4.80\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"Error     %6.4f   %2d   %6.4f\\n\",\n            SSE, n-p, SSE/(n-p)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nError     0.1000    6   0.0167\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"Total     %6.4f   %2d\\n\", SST, n-1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTotal     0.1800    7\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"-------------------------------------------\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n-------------------------------------------\n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify with lm()\nfit <- lm(adg ~ breed - 1)\ncat(\"\\nComparison with lm():\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nComparison with lm():\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(anova(lm(adg ~ breed)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: adg\n          Df Sum Sq Mean Sq F value Pr(>F)  \nbreed      1   0.08  0.0800     4.8  0.071 .\nResiduals  6   0.10  0.0167                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\n# Demonstrate quadratic form properties\ncat(\"\\n\\nQuadratic Form Verification:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\nQuadratic Form Verification:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"1. SSE = y'(I-H)y = e'e:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n1. SSE = y'(I-H)y = e'e:\n```\n\n\n:::\n\n```{.r .cell-code}\nSSE_qf <- c(t(adg) %*% I_minus_H %*% adg)\nSSE_ee <- c(t(e) %*% e)\ncat(\"   Quadratic form:\", SSE_qf, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Quadratic form: 0.1 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"   e'e:\", SSE_ee, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   e'e: 0.1 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\n2. SSM = y'(H - n^{-1}11')y:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n2. SSM = y'(H - n^{-1}11')y:\n```\n\n\n:::\n\n```{.r .cell-code}\nJ_n <- matrix(1/n, n, n)\nSSM_qf <- c(t(adg) %*% (H - J_n) %*% adg)\ncat(\"   Quadratic form:\", SSM_qf, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Quadratic form: 0.08 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"   Sum of squared deviations:\", SSM, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Sum of squared deviations: 0.08 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\n3. SST = SSM + SSE:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n3. SST = SSM + SSE:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"   SST:\", SST, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   SST: 0.18 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"   SSM + SSE:\", SSM + SSE, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   SSM + SSE: 0.18 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"   Agreement:\", all.equal(SST, SSM + SSE), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Agreement: TRUE \n```\n\n\n:::\n:::\n\n\n:::{.callout-important}\n## Key Takeaways\n\n1. **Hat matrix H** projects **y** onto the column space of **X** to give fitted values\n2. **I - H** projects onto the orthogonal complement to give residuals\n3. **Both are idempotent and symmetric** - essential for distribution theory\n4. **Quadratic forms** express sums of squares in matrix notation\n5. **Sum of squares decomposition** SST = SSM + SSE follows from orthogonality of projections\n6. **Cochran's Theorem** provides the distributional basis for F-tests and t-tests\n\nThese concepts are the mathematical foundation of **everything** in linear models!\n:::\n\n:::{.callout-note}\n## Cross-References\n\n- **Week 5: Least Squares Theory** - Full derivation of these results\n- **Week 6: Multiple Regression** - Application to multiple predictors\n- **Week 7-10: ANOVA models** - Using quadratic forms for hypothesis tests\n- **Week 11: Model Diagnostics** - Leverage values and residual analysis\n:::\n\n---\n\n## Eigenvalues and Eigenvectors {#sec-eigenvalues}\n\n### Definitions {#sec-eigen-definitions}\n\n**Definition:**\n\nLet **A** be an n × n square matrix. A scalar λ (lambda) is an **eigenvalue** of **A** if there exists a non-zero vector **v** such that:\n\n$$\n\\mathbf{Av} = \\lambda\\mathbf{v}\n$$ {#eq-eigenvalue-def}\n\nThe vector **v** is called an **eigenvector** corresponding to eigenvalue λ.\n\n**Geometric Interpretation:**\n\nWhen **A** acts on eigenvector **v**, it simply scales **v** by factor λ (no rotation, just scaling).\n\n**Characteristic Equation:**\n\nRearranging: $\\mathbf{Av} = \\lambda\\mathbf{v} \\Rightarrow (\\mathbf{A} - \\lambda\\mathbf{I})\\mathbf{v} = \\mathbf{0}$\n\nFor non-trivial solution (**v** ≠ 0), the matrix $(\\mathbf{A} - \\lambda\\mathbf{I})$ must be singular:\n\n$$\n\\det(\\mathbf{A} - \\lambda\\mathbf{I}) = 0\n$$ {#eq-characteristic}\n\nThis **characteristic equation** is a polynomial of degree n in λ, yielding n eigenvalues (counting multiplicity).\n\n**Key Properties:**\n\n1. **Trace**: $\\sum_{i=1}^{n} \\lambda_i = \\text{tr}(\\mathbf{A})$ (sum of eigenvalues equals trace)\n2. **Determinant**: $\\prod_{i=1}^{n} \\lambda_i = \\det(\\mathbf{A})$ (product of eigenvalues equals determinant)\n3. **Eigenvectors for distinct eigenvalues**: Linearly independent\n4. **Matrix power**: If $\\mathbf{Av} = \\lambda\\mathbf{v}$, then $\\mathbf{A}^k\\mathbf{v} = \\lambda^k\\mathbf{v}$\n\n**R Implementation:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simple 2x2 example\nA <- matrix(c(4, 1, 2, 3), nrow = 2, ncol = 2, byrow = TRUE)\n\ncat(\"Matrix A:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMatrix A:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    4    1\n[2,]    2    3\n```\n\n\n:::\n\n```{.r .cell-code}\n# Compute eigenvalues and eigenvectors\neigen_A <- eigen(A)\n\neigenvalues <- eigen_A$values\neigenvectors <- eigen_A$vectors\n\ncat(\"\\nEigenvalues:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nEigenvalues:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(eigenvalues)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5 2\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nEigenvectors (as columns):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nEigenvectors (as columns):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(eigenvectors)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       [,1]    [,2]\n[1,] 0.7071 -0.4472\n[2,] 0.7071  0.8944\n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify: Av = λv for first eigenvalue\nlambda1 <- eigenvalues[1]\nv1 <- eigenvectors[, 1]\n\nAv1 <- A %*% v1\nlambda_v1 <- lambda1 * v1\n\ncat(sprintf(\"\\nVerify Av = λv for λ₁ = %.4f:\\n\", lambda1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVerify Av = λv for λ₁ = 5.0000:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Av₁:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAv₁:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(Av1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      [,1]\n[1,] 3.536\n[2,] 3.536\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"λ₁v₁:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nλ₁v₁:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(lambda_v1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.536 3.536\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nAre they equal?\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nAre they equal?\n```\n\n\n:::\n\n```{.r .cell-code}\nall.equal(Av1, lambda_v1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Attributes: < Modes: list, NULL >\"                   \n[2] \"Attributes: < Lengths: 1, 0 >\"                       \n[3] \"Attributes: < names for target but not for current >\"\n[4] \"Attributes: < current is not list-like >\"            \n[5] \"target is matrix, current is numeric\"                \n```\n\n\n:::\n:::\n\n\n**Characteristic Equation Example:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 2×2 matrix\nA <- matrix(c(5, 2, 2, 5), nrow = 2, ncol = 2, byrow = TRUE)\n\ncat(\"Matrix A:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMatrix A:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    5    2\n[2,]    2    5\n```\n\n\n:::\n\n```{.r .cell-code}\n# Characteristic equation: det(A - λI) = 0\n# For 2×2: det([5-λ, 2; 2, 5-λ]) = (5-λ)² - 4 = 0\n# λ² - 10λ + 21 = 0\n# (λ-7)(λ-3) = 0\n# λ = 7 or λ = 3\n\neigen_A <- eigen(A)\ncat(\"\\nEigenvalues (computed):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nEigenvalues (computed):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(eigen_A$values)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7 3\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nManual calculation:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nManual calculation:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"det(A - λI) = (5-λ)² - 4 = λ² - 10λ + 21 = 0\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ndet(A - λI) = (5-λ)² - 4 = λ² - 10λ + 21 = 0\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Solutions: λ = 7 and λ = 3\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSolutions: λ = 7 and λ = 3\n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify trace and determinant properties\ncat(sprintf(\"\\nSum of eigenvalues = %.1f\\n\", sum(eigen_A$values)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSum of eigenvalues = 10.0\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"Trace of A = %.1f\\n\", sum(diag(A))))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrace of A = 10.0\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"\\nProduct of eigenvalues = %.1f\\n\", prod(eigen_A$values)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nProduct of eigenvalues = 21.0\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"Determinant of A = %.1f\\n\", det(A)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDeterminant of A = 21.0\n```\n\n\n:::\n:::\n\n\n**Finding Eigenvectors:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Using eigenvalue λ = 7, find eigenvector\nA <- matrix(c(5, 2, 2, 5), nrow = 2, ncol = 2, byrow = TRUE)\nlambda <- 7\n\n# Solve (A - λI)v = 0\nA_minus_lambdaI <- A - lambda * diag(2)\n\ncat(\"A - 7I:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nA - 7I:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A_minus_lambdaI)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]   -2    2\n[2,]    2   -2\n```\n\n\n:::\n\n```{.r .cell-code}\n# Null space of this matrix gives eigenvector\n# Row 1: -2v₁ + 2v₂ = 0 → v₁ = v₂\n# So v = [1, 1]' (or any multiple)\n\nv <- c(1, 1)\ncat(\"\\nEigenvector v = [1, 1]':\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nEigenvector v = [1, 1]':\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(v)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1 1\n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify Av = 7v\nAv <- A %*% v\ncat(\"\\nAv:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nAv:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(Av)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]\n[1,]    7\n[2,]    7\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\n7v:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n7v:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(7 * v)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7 7\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nAv = 7v ✓\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nAv = 7v ✓\n```\n\n\n:::\n:::\n\n\n**Livestock Example - Growth Model:**\n\nEigenvalues describe long-term behavior of dynamic systems.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simplified population model: 2 age classes (juveniles, adults)\n# Transition matrix: rows = next state, columns = current state\nA <- matrix(c(\n  0.2, 2.0,   # Juveniles: 20% survive, 2 offspring per adult\n  0.5, 0.8    # Adults: 50% juveniles mature, 80% adults survive\n), nrow = 2, ncol = 2, byrow = TRUE)\n\ncat(\"Population transition matrix A:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPopulation transition matrix A:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]  0.2  2.0\n[2,]  0.5  0.8\n```\n\n\n:::\n\n```{.r .cell-code}\n# Eigenvalues\neigen_A <- eigen(A)\nlambda_max <- max(eigen_A$values)\n\ncat(sprintf(\"\\nDominant eigenvalue: %.4f\\n\", lambda_max))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nDominant eigenvalue: 1.5440\n```\n\n\n:::\n\n```{.r .cell-code}\nif (lambda_max > 1) {\n  cat(\"λ > 1: Population grows\\n\")\n} else if (lambda_max < 1) {\n  cat(\"λ < 1: Population declines\\n\")\n} else {\n  cat(\"λ = 1: Population stable\\n\")\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nλ > 1: Population grows\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"\\nGrowth rate: %.2f%% per time period\\n\", (lambda_max - 1) * 100))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nGrowth rate: 54.40% per time period\n```\n\n\n:::\n\n```{.r .cell-code}\n# Stable age distribution (eigenvector for dominant eigenvalue)\nv_stable <- eigen_A$vectors[, 1]\nv_stable <- v_stable / sum(v_stable)  # Normalize to sum to 1\n\ncat(\"\\nStable age distribution:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nStable age distribution:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"  Juveniles: %.2f%%\\n\", v_stable[1] * 100))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Juveniles: 59.81%\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"  Adults: %.2f%%\\n\", v_stable[2] * 100))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Adults: 40.19%\n```\n\n\n:::\n:::\n\n\n**Diagonalization:**\n\nIf **A** has n linearly independent eigenvectors, we can write:\n\n$$\n\\mathbf{A} = \\mathbf{V\\Lambda V}^{-1}\n$$ {#eq-diagonalization}\n\nwhere:\n- **V** = matrix with eigenvectors as columns\n- **Λ** = diagonal matrix with eigenvalues on diagonal\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA <- matrix(c(4, 1, 2, 3), nrow = 2, ncol = 2, byrow = TRUE)\n\neigen_A <- eigen(A)\nV <- eigen_A$vectors\nLambda <- diag(eigen_A$values)\n\ncat(\"Matrix A:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMatrix A:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    4    1\n[2,]    2    3\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nEigenvector matrix V:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nEigenvector matrix V:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(V)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       [,1]    [,2]\n[1,] 0.7071 -0.4472\n[2,] 0.7071  0.8944\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nEigenvalue matrix Λ:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nEigenvalue matrix Λ:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(Lambda)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    5    0\n[2,]    0    2\n```\n\n\n:::\n\n```{.r .cell-code}\n# Reconstruct A\nA_reconstructed <- V %*% Lambda %*% solve(V)\n\ncat(\"\\nReconstructed A = VΛV⁻¹:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nReconstructed A = VΛV⁻¹:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A_reconstructed)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    4    1\n[2,]    2    3\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nVerify reconstruction:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVerify reconstruction:\n```\n\n\n:::\n\n```{.r .cell-code}\nall.equal(A, A_reconstructed)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\n**Matrix Powers via Eigenvalues:**\n\nComputing **A**^k^ is easy with diagonalization: $\\mathbf{A}^k = \\mathbf{V\\Lambda}^k\\mathbf{V}^{-1}$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA <- matrix(c(0.8, 0.2, 0.3, 0.7), nrow = 2, ncol = 2, byrow = TRUE)\n\ncat(\"Transition matrix A:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTransition matrix A:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]  0.8  0.2\n[2,]  0.3  0.7\n```\n\n\n:::\n\n```{.r .cell-code}\n# Compute A^10 directly (slow for large k)\nA10_direct <- A\nfor (i in 1:9) A10_direct <- A10_direct %*% A\n\ncat(\"\\nA^10 (direct calculation):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nA^10 (direct calculation):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(A10_direct, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       [,1]   [,2]\n[1,] 0.6004 0.3996\n[2,] 0.5994 0.4006\n```\n\n\n:::\n\n```{.r .cell-code}\n# Compute A^10 via eigendecomposition (fast!)\neigen_A <- eigen(A)\nV <- eigen_A$vectors\nLambda <- diag(eigen_A$values)\n\nLambda10 <- diag(eigen_A$values^10)  # Just raise eigenvalues to power 10\nA10_eigen <- V %*% Lambda10 %*% solve(V)\n\ncat(\"\\nA^10 (via eigenvalues):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nA^10 (via eigenvalues):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(A10_eigen, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       [,1]   [,2]\n[1,] 0.6004 0.3996\n[2,] 0.5994 0.4006\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nVerify both methods agree:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVerify both methods agree:\n```\n\n\n:::\n\n```{.r .cell-code}\nall.equal(A10_direct, A10_eigen)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nNote: As k → ∞, matrix stabilizes (dominant eigenvalue < 1)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nNote: As k → ∞, matrix stabilizes (dominant eigenvalue < 1)\n```\n\n\n:::\n:::\n\n\n:::{.callout-note}\n## Cross-References\n\nEigenvalues and eigenvectors appear in:\n\n- **Week 2**: Conceptual introduction\n- **Week 14**: Variance components in genetic evaluation\n- **Section @sec-symmetric-eigen**: Special properties for symmetric matrices\n- **Section @sec-positive-definite**: Positive definite matrices (all eigenvalues > 0)\n- **Section @sec-svd**: Singular value decomposition\n:::\n\n---\n\n### Properties for Symmetric Matrices {#sec-symmetric-eigen}\n\nSymmetric matrices have **very special** eigenvalue properties that make them particularly important in linear models.\n\n**Spectral Theorem for Symmetric Matrices:**\n\nIf **A** is a real symmetric matrix (n × n), then:\n\n1. **All eigenvalues are real** (no complex numbers!)\n2. **Eigenvectors corresponding to distinct eigenvalues are orthogonal**\n3. **A** can be diagonalized by an orthogonal matrix: $\\mathbf{A} = \\mathbf{Q\\Lambda Q}'$\n\nwhere:\n- **Q** is orthogonal ($\\mathbf{Q}'\\mathbf{Q} = \\mathbf{I}$)\n- **Λ** is diagonal with eigenvalues\n\nThis is called the **spectral decomposition** or **eigenvalue decomposition**.\n\n$$\n\\mathbf{A} = \\mathbf{Q\\Lambda Q}' = \\sum_{i=1}^{n} \\lambda_i \\mathbf{q}_i\\mathbf{q}_i'\n$$ {#eq-spectral-decomposition}\n\n**Why This Matters for Linear Models:**\n\nSince $\\mathbf{X}'\\mathbf{X}$ is symmetric, it has:\n- Real eigenvalues only\n- Orthogonal eigenvectors\n- Clean spectral decomposition\n\n**R Implementation:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Symmetric matrix\nA <- matrix(c(4, 2, 1,\n              2, 5, 3,\n              1, 3, 6), nrow = 3, ncol = 3, byrow = TRUE)\n\ncat(\"Symmetric matrix A:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSymmetric matrix A:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]    4    2    1\n[2,]    2    5    3\n[3,]    1    3    6\n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify symmetry\ncat(\"\\nIs A symmetric?\", isTRUE(all.equal(A, t(A))), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nIs A symmetric? TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\n# Eigenvalues and eigenvectors\neigen_A <- eigen(A)\n\ncat(\"\\nEigenvalues (all real!):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nEigenvalues (all real!):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(eigen_A$values)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 9.348 3.730 1.921\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nEigenvectors Q:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nEigenvectors Q:\n```\n\n\n:::\n\n```{.r .cell-code}\nQ <- eigen_A$vectors\nprint(round(Q, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        [,1]    [,2]    [,3]\n[1,] -0.3651  0.7759  0.5145\n[2,] -0.6366  0.1953 -0.7461\n[3,] -0.6793 -0.5999  0.4226\n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify Q is orthogonal\ncat(\"\\nVerify Q'Q = I:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVerify Q'Q = I:\n```\n\n\n:::\n\n```{.r .cell-code}\nQtQ <- t(Q) %*% Q\nprint(round(QtQ, 10))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify Q orthogonal\ncat(\"\\nIs Q orthogonal?\", isTRUE(all.equal(QtQ, diag(3))), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nIs Q orthogonal? TRUE \n```\n\n\n:::\n:::\n\n\n**Orthogonality of Eigenvectors:**\n\nFor symmetric matrices, eigenvectors for different eigenvalues are orthogonal.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Symmetric matrix\nA <- matrix(c(3, 1, 1, 3), nrow = 2, ncol = 2, byrow = TRUE)\n\ncat(\"Symmetric matrix A:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSymmetric matrix A:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    3    1\n[2,]    1    3\n```\n\n\n:::\n\n```{.r .cell-code}\neigen_A <- eigen(A)\n\ncat(\"\\nEigenvalues:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nEigenvalues:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(eigen_A$values)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4 2\n```\n\n\n:::\n\n```{.r .cell-code}\nv1 <- eigen_A$vectors[, 1]\nv2 <- eigen_A$vectors[, 2]\n\ncat(\"\\nEigenvector 1:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nEigenvector 1:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(v1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.7071 0.7071\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nEigenvector 2:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nEigenvector 2:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(v2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.7071  0.7071\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check orthogonality\ncat(sprintf(\"\\nv1'v2 = %.10f (should be 0)\\n\", sum(v1 * v2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nv1'v2 = 0.0000000000 (should be 0)\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check unit length\ncat(sprintf(\"||v1|| = %.4f\\n\", sqrt(sum(v1^2))))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n||v1|| = 1.0000\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"||v2|| = %.4f\\n\", sqrt(sum(v2^2))))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n||v2|| = 1.0000\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nEigenvectors are orthonormal (orthogonal + unit length)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nEigenvectors are orthonormal (orthogonal + unit length)\n```\n\n\n:::\n:::\n\n\n**Spectral Decomposition:**\n\n$\\mathbf{A} = \\mathbf{Q\\Lambda Q}'$ where **Q'** = **Q**^(-1)^\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA <- matrix(c(4, 2, 2, 3), nrow = 2, ncol = 2, byrow = TRUE)\n\ncat(\"Symmetric matrix A:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSymmetric matrix A:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    4    2\n[2,]    2    3\n```\n\n\n:::\n\n```{.r .cell-code}\neigen_A <- eigen(A)\nQ <- eigen_A$vectors\nLambda <- diag(eigen_A$values)\n\ncat(\"\\nOrthogonal matrix Q:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nOrthogonal matrix Q:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(Q, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        [,1]    [,2]\n[1,] -0.7882  0.6154\n[2,] -0.6154 -0.7882\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nEigenvalue matrix Λ:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nEigenvalue matrix Λ:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(Lambda, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      [,1]  [,2]\n[1,] 5.562 0.000\n[2,] 0.000 1.438\n```\n\n\n:::\n\n```{.r .cell-code}\n# Reconstruct A using A = QΛQ'\nA_reconstructed <- Q %*% Lambda %*% t(Q)\n\ncat(\"\\nReconstructed A = QΛQ':\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nReconstructed A = QΛQ':\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(A_reconstructed, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    4    2\n[2,]    2    3\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nVerify reconstruction:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVerify reconstruction:\n```\n\n\n:::\n\n```{.r .cell-code}\nall.equal(A, A_reconstructed)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nNote: For symmetric matrices, Q' = Q⁻¹ (transpose = inverse!)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nNote: For symmetric matrices, Q' = Q⁻¹ (transpose = inverse!)\n```\n\n\n:::\n:::\n\n\n**Livestock Example - Covariance Matrix:**\n\nCovariance matrices are symmetric, so they have real eigenvalues and orthogonal eigenvectors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Broiler data: body weight (kg), breast yield (proportion), leg yield\n# Covariance matrix (realistic values)\nSigma <- matrix(c(\n  0.16,  0.04,  0.03,  # Var(BW) = 0.16, Cov(BW, Breast) = 0.04, Cov(BW, Leg) = 0.03\n  0.04,  0.02,  0.01,  # Cov(Breast, BW) = 0.04, Var(Breast) = 0.02, Cov(Breast, Leg) = 0.01\n  0.03,  0.01,  0.02   # Cov(Leg, BW) = 0.03, Cov(Leg, Breast) = 0.01, Var(Leg) = 0.02\n), nrow = 3, ncol = 3, byrow = TRUE)\n\ncolnames(Sigma) <- rownames(Sigma) <- c(\"BodyWt\", \"Breast\", \"Leg\")\n\ncat(\"Covariance matrix Σ:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCovariance matrix Σ:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(Sigma, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       BodyWt Breast  Leg\nBodyWt   0.16   0.04 0.03\nBreast   0.04   0.02 0.01\nLeg      0.03   0.01 0.02\n```\n\n\n:::\n\n```{.r .cell-code}\n# Eigendecomposition\neigen_Sigma <- eigen(Sigma)\n\ncat(\"\\nEigenvalues (variances of principal components):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nEigenvalues (variances of principal components):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(eigen_Sigma$values, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1770 0.0144 0.0086\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nEigenvectors (loadings of principal components):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nEigenvectors (loadings of principal components):\n```\n\n\n:::\n\n```{.r .cell-code}\nPC_loadings <- eigen_Sigma$vectors\ncolnames(PC_loadings) <- c(\"PC1\", \"PC2\", \"PC3\")\nrownames(PC_loadings) <- c(\"BodyWt\", \"Breast\", \"Leg\")\nprint(round(PC_loadings, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          PC1     PC2     PC3\nBodyWt 0.9469  0.2707  0.1734\nBreast 0.2539 -0.2990 -0.9199\nLeg    0.1972 -0.9151  0.3518\n```\n\n\n:::\n\n```{.r .cell-code}\n# Proportion of variance explained\nprop_var <- eigen_Sigma$values / sum(eigen_Sigma$values)\n\ncat(\"\\nProportion of variance explained:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nProportion of variance explained:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"  PC1: %.2f%%\\n\", prop_var[1] * 100))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  PC1: 88.48%\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"  PC2: %.2f%%\\n\", prop_var[2] * 100))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  PC2: 7.20%\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"  PC3: %.2f%%\\n\", prop_var[3] * 100))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  PC3: 4.32%\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nThis is the basis of Principal Component Analysis (PCA)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nThis is the basis of Principal Component Analysis (PCA)\n```\n\n\n:::\n:::\n\n\n**Application to X'X Matrix:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Design matrix for simple regression\nX <- cbind(1, c(1, 2, 3, 4, 5))\n\ncat(\"Design matrix X (5×2):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDesign matrix X (5×2):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    1    1\n[2,]    1    2\n[3,]    1    3\n[4,]    1    4\n[5,]    1    5\n```\n\n\n:::\n\n```{.r .cell-code}\n# Form X'X\nXtX <- t(X) %*% X\n\ncat(\"\\nX'X (symmetric):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nX'X (symmetric):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(XtX)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    5   15\n[2,]   15   55\n```\n\n\n:::\n\n```{.r .cell-code}\n# Eigendecomposition\neigen_XtX <- eigen(XtX)\n\ncat(\"\\nEigenvalues of X'X:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nEigenvalues of X'X:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(eigen_XtX$values, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 59.1548  0.8452\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nBoth eigenvalues > 0 (X'X is positive definite)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nBoth eigenvalues > 0 (X'X is positive definite)\n```\n\n\n:::\n\n```{.r .cell-code}\n# Condition number (ratio of largest to smallest eigenvalue)\nkappa <- max(eigen_XtX$values) / min(eigen_XtX$values)\ncat(sprintf(\"\\nCondition number κ(X'X) = %.2f\\n\", kappa))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCondition number κ(X'X) = 69.99\n```\n\n\n:::\n\n```{.r .cell-code}\nif (kappa < 30) {\n  cat(\"Well-conditioned matrix (good numerical stability)\\n\")\n} else if (kappa < 1000) {\n  cat(\"Moderately conditioned (acceptable)\\n\")\n} else {\n  cat(\"Ill-conditioned (numerical problems likely)\\n\")\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModerately conditioned (acceptable)\n```\n\n\n:::\n:::\n\n\n**Sum of Outer Products Form:**\n\nThe spectral decomposition can be written as a sum of rank-1 matrices:\n\n$$\n\\mathbf{A} = \\sum_{i=1}^{n} \\lambda_i \\mathbf{q}_i\\mathbf{q}_i'\n$$ {#eq-outer-product-sum}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA <- matrix(c(5, 2, 2, 2), nrow = 2, ncol = 2, byrow = TRUE)\n\ncat(\"Symmetric matrix A:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSymmetric matrix A:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    5    2\n[2,]    2    2\n```\n\n\n:::\n\n```{.r .cell-code}\neigen_A <- eigen(A)\n\n# Reconstruct using sum of outer products\nlambda1 <- eigen_A$values[1]\nlambda2 <- eigen_A$values[2]\nq1 <- eigen_A$vectors[, 1]\nq2 <- eigen_A$vectors[, 2]\n\nterm1 <- lambda1 * (q1 %*% t(q1))\nterm2 <- lambda2 * (q2 %*% t(q2))\n\ncat(\"\\nλ₁q₁q₁' (first term):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nλ₁q₁q₁' (first term):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(term1, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]  4.8  2.4\n[2,]  2.4  1.2\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nλ₂q₂q₂' (second term):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nλ₂q₂q₂' (second term):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(term2, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]  0.2 -0.4\n[2,] -0.4  0.8\n```\n\n\n:::\n\n```{.r .cell-code}\nA_reconstructed <- term1 + term2\n\ncat(\"\\nA = λ₁q₁q₁' + λ₂q₂q₂':\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nA = λ₁q₁q₁' + λ₂q₂q₂':\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(A_reconstructed, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    5    2\n[2,]    2    2\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nVerify reconstruction:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVerify reconstruction:\n```\n\n\n:::\n\n```{.r .cell-code}\nall.equal(A, A_reconstructed)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\n**Numerical Verification:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Random symmetric matrix\nset.seed(123)\nB <- matrix(rnorm(9), nrow = 3)\nA <- (B + t(B)) / 2  # Force symmetry\n\ncat(\"Random symmetric matrix A:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRandom symmetric matrix A:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(A, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        [,1]    [,2]    [,3]\n[1,] -0.5605 -0.0798  1.0098\n[2,] -0.0798  0.1293  0.2250\n[3,]  1.0098  0.2250 -0.6869\n```\n\n\n:::\n\n```{.r .cell-code}\neigen_A <- eigen(A)\n\n# Check all eigenvalues are real (imaginary part = 0)\ncat(\"\\nImaginary parts of eigenvalues (should all be 0):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nImaginary parts of eigenvalues (should all be 0):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(Im(eigen_A$values))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0 0 0\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nAll eigenvalues are real ✓\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nAll eigenvalues are real ✓\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check eigenvectors are orthogonal\nQ <- eigen_A$vectors\nQtQ <- t(Q) %*% Q\n\ncat(\"\\nQ'Q (should be identity):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nQ'Q (should be identity):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(QtQ, 10))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nEigenvectors are orthogonal ✓\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nEigenvectors are orthogonal ✓\n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify spectral decomposition\nLambda <- diag(eigen_A$values)\nA_check <- Q %*% Lambda %*% t(Q)\n\ncat(\"\\nVerify A = QΛQ':\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVerify A = QΛQ':\n```\n\n\n:::\n\n```{.r .cell-code}\nall.equal(A, A_check)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\n:::{.callout-important}\n## Critical for X'X in Linear Models\n\nSince $\\mathbf{X}'\\mathbf{X}$ is symmetric:\n\n1. **All eigenvalues are real** (no complex numbers to worry about)\n2. **Eigenvectors are orthogonal** (clean geometry)\n3. **Spectral decomposition**: $\\mathbf{X}'\\mathbf{X} = \\mathbf{Q\\Lambda Q}'$\n4. **Condition number**: $\\kappa = \\lambda_{\\max}/\\lambda_{\\min}$ measures numerical stability\n5. **Rank = number of non-zero eigenvalues**\n\nIf any eigenvalue is zero, $\\mathbf{X}'\\mathbf{X}$ is singular (rank deficient).\n:::\n\n:::{.callout-note}\n## Cross-References\n\nProperties of symmetric matrices are central to:\n\n- **Week 2**: Understanding structure of $\\mathbf{X}'\\mathbf{X}$\n- **Week 5**: Variance-covariance matrix $\\text{Var}(\\mathbf{b}) = (\\mathbf{X}'\\mathbf{X})^{-1}\\sigma^2$\n- **Week 11**: Condition number and numerical stability\n- **Section @sec-positive-definite**: When all eigenvalues > 0\n- **Section @sec-computation**: Numerical considerations for eigenvalue computation\n:::\n\n---\n\n### Positive Definite Matrices {#sec-positive-definite}\n\n**Definition:**\n\nA symmetric matrix **A** (n × n) is **positive definite** if:\n\n$$\n\\mathbf{x}'\\mathbf{Ax} > 0 \\quad \\text{for all non-zero vectors } \\mathbf{x}\n$$ {#eq-positive-definite}\n\nThe quadratic form $\\mathbf{x}'\\mathbf{Ax}$ is always strictly positive (except when **x** = 0).\n\n**Related Definitions:**\n\n- **Positive semi-definite**: $\\mathbf{x}'\\mathbf{Ax} \\geq 0$ for all **x** (allows zero)\n- **Negative definite**: $\\mathbf{x}'\\mathbf{Ax} < 0$ for all non-zero **x**\n- **Indefinite**: $\\mathbf{x}'\\mathbf{Ax}$ can be positive or negative depending on **x**\n\n**Equivalent Characterizations:**\n\nFor a symmetric matrix **A**, the following are **equivalent**:\n\n1. **A** is positive definite\n2. All eigenvalues of **A** are positive: $\\lambda_i > 0$ for all i\n3. All leading principal minors are positive (Sylvester's criterion)\n4. There exists a non-singular matrix **B** such that $\\mathbf{A} = \\mathbf{B}'\\mathbf{B}$\n5. **A** is invertible (det(**A**) > 0)\n\n**Why This Matters for Linear Models:**\n\nFor full-rank design matrix **X**, the matrix $\\mathbf{X}'\\mathbf{X}$ is **positive definite**:\n- All eigenvalues > 0\n- Invertible (unique solution exists!)\n- Numerically stable\n\n**R Implementation:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Positive definite matrix\nA <- matrix(c(4, 1, 1, 3), nrow = 2, ncol = 2, byrow = TRUE)\n\ncat(\"Matrix A:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMatrix A:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    4    1\n[2,]    1    3\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check eigenvalues\neigen_A <- eigen(A)\ncat(\"\\nEigenvalues:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nEigenvalues:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(eigen_A$values)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4.618 2.382\n```\n\n\n:::\n\n```{.r .cell-code}\nall_positive <- all(eigen_A$values > 0)\ncat(sprintf(\"\\nAll eigenvalues > 0? %s\\n\", all_positive))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nAll eigenvalues > 0? TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\nif (all_positive) {\n  cat(\"A is positive definite!\\n\")\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nA is positive definite!\n```\n\n\n:::\n\n```{.r .cell-code}\n# Test quadratic form for several vectors\ntest_vectors <- list(\n  c(1, 0),\n  c(0, 1),\n  c(1, 1),\n  c(1, -1),\n  c(2, 3)\n)\n\ncat(\"\\nQuadratic forms x'Ax:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nQuadratic forms x'Ax:\n```\n\n\n:::\n\n```{.r .cell-code}\nfor (i in seq_along(test_vectors)) {\n  x <- test_vectors[[i]]\n  qf <- t(x) %*% A %*% x\n  cat(sprintf(\"x = [%d, %d]: x'Ax = %.4f\\n\", x[1], x[2], qf[1,1]))\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nx = [1, 0]: x'Ax = 4.0000\nx = [0, 1]: x'Ax = 3.0000\nx = [1, 1]: x'Ax = 9.0000\nx = [1, -1]: x'Ax = 5.0000\nx = [2, 3]: x'Ax = 55.0000\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nAll values > 0 (positive definite) ✓\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nAll values > 0 (positive definite) ✓\n```\n\n\n:::\n:::\n\n\n**Cholesky Decomposition:**\n\nEvery positive definite matrix **A** can be uniquely factored as:\n\n$$\n\\mathbf{A} = \\mathbf{L}\\mathbf{L}'\n$$ {#eq-cholesky}\n\nwhere **L** is lower triangular with positive diagonal elements.\n\nThis is the **Cholesky decomposition** - extremely useful for solving linear systems efficiently.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA <- matrix(c(4, 2, 2, 5), nrow = 2, ncol = 2, byrow = TRUE)\n\ncat(\"Positive definite matrix A:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPositive definite matrix A:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    4    2\n[2,]    2    5\n```\n\n\n:::\n\n```{.r .cell-code}\n# Cholesky decomposition\nL <- chol(A)  # Note: R's chol() returns UPPER triangular\n# So we need to transpose to get lower triangular\n\nL_lower <- t(L)  # Lower triangular\n\ncat(\"\\nLower triangular matrix L:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nLower triangular matrix L:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(L_lower, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    2    0\n[2,]    1    2\n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify A = LL'\nA_reconstructed <- L_lower %*% t(L_lower)\n\ncat(\"\\nReconstructed A = LL':\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nReconstructed A = LL':\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(A_reconstructed, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    4    2\n[2,]    2    5\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nVerify reconstruction:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVerify reconstruction:\n```\n\n\n:::\n\n```{.r .cell-code}\nall.equal(A, A_reconstructed)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nCholesky is fast and numerically stable for positive definite matrices\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCholesky is fast and numerically stable for positive definite matrices\n```\n\n\n:::\n:::\n\n\n**Testing Positive Definiteness:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Function to test positive definiteness\nis_positive_definite <- function(A, tol = 1e-8) {\n  # Check symmetry\n  if (!isTRUE(all.equal(A, t(A)))) {\n    cat(\"Not symmetric!\\n\")\n    return(FALSE)\n  }\n\n  # Check eigenvalues\n  eigenvalues <- eigen(A, symmetric = TRUE, only.values = TRUE)$values\n\n  if (all(eigenvalues > tol)) {\n    return(TRUE)\n  } else {\n    return(FALSE)\n  }\n}\n\n# Test various matrices\nA1 <- matrix(c(2, 1, 1, 2), nrow = 2, ncol = 2, byrow = TRUE)\nA2 <- matrix(c(2, 3, 3, 2), nrow = 2, ncol = 2, byrow = TRUE)  # Not PD\nA3 <- matrix(c(1, 0, 0, 1), nrow = 2, ncol = 2, byrow = TRUE)  # Identity (PD)\nA4 <- matrix(c(1, 1, 1, 1), nrow = 2, ncol = 2, byrow = TRUE)  # Singular (not PD)\n\ncat(\"Matrix A1:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMatrix A1:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    2    1\n[2,]    1    2\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Eigenvalues:\", eigen(A1)$values, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEigenvalues: 3 1 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Positive definite?\", is_positive_definite(A1), \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPositive definite? TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Matrix A2:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMatrix A2:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    2    3\n[2,]    3    2\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Eigenvalues:\", round(eigen(A2)$values, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEigenvalues: 5 -1 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Positive definite?\", is_positive_definite(A2), \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPositive definite? FALSE \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Matrix A3 (Identity):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMatrix A3 (Identity):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Eigenvalues:\", eigen(A3)$values, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEigenvalues: 1 1 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Positive definite?\", is_positive_definite(A3), \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPositive definite? TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Matrix A4 (Singular):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMatrix A4 (Singular):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    1    1\n[2,]    1    1\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Eigenvalues:\", eigen(A4)$values, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEigenvalues: 2 0 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Positive definite?\", is_positive_definite(A4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPositive definite? FALSE \n```\n\n\n:::\n:::\n\n\n**Positive Semi-Definite:**\n\n$\\mathbf{x}'\\mathbf{Ax} \\geq 0$ (allows zero). Eigenvalues $\\geq$ 0 (some may be zero).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Positive semi-definite (but not positive definite)\nA <- matrix(c(1, 1, 1, 1), nrow = 2, ncol = 2, byrow = TRUE)\n\ncat(\"Matrix A:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMatrix A:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    1    1\n[2,]    1    1\n```\n\n\n:::\n\n```{.r .cell-code}\neigen_A <- eigen(A)\ncat(\"\\nEigenvalues:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nEigenvalues:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(eigen_A$values)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2 0\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nOne eigenvalue = 0 (positive semi-definite but not positive definite)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nOne eigenvalue = 0 (positive semi-definite but not positive definite)\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"This matrix is singular (not invertible)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nThis matrix is singular (not invertible)\n```\n\n\n:::\n\n```{.r .cell-code}\n# Rank\ncat(sprintf(\"Rank: %d (less than 2)\\n\", qr(A)$rank))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRank: 1 (less than 2)\n```\n\n\n:::\n:::\n\n\n**Livestock Example - X'X is Positive Definite:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Full-rank design matrix\nX <- cbind(1, c(1, 2, 3, 4, 5), c(2, 3, 1, 5, 4))\n\ncat(\"Design matrix X (5×3):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDesign matrix X (5×3):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]    1    1    2\n[2,]    1    2    3\n[3,]    1    3    1\n[4,]    1    4    5\n[5,]    1    5    4\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"\\nRank of X: %d (full rank!)\\n\", qr(X)$rank))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nRank of X: 3 (full rank!)\n```\n\n\n:::\n\n```{.r .cell-code}\n# Form X'X\nXtX <- t(X) %*% X\n\ncat(\"\\nX'X (3×3):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nX'X (3×3):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(XtX)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]    5   15   15\n[2,]   15   55   51\n[3,]   15   51   55\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check positive definiteness\neigen_XtX <- eigen(XtX)\ncat(\"\\nEigenvalues of X'X:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nEigenvalues of X'X:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(eigen_XtX$values, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 110.2745   4.0000   0.7255\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nAll eigenvalues > 0 ✓\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nAll eigenvalues > 0 ✓\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"X'X is positive definite (because X has full column rank)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nX'X is positive definite (because X has full column rank)\n```\n\n\n:::\n\n```{.r .cell-code}\n# This guarantees unique solution to normal equations\ncat(\"\\nConsequence: Normal equations X'Xb = X'y have unique solution\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nConsequence: Normal equations X'Xb = X'y have unique solution\n```\n\n\n:::\n:::\n\n\n**Condition Number:**\n\nFor positive definite matrices, the **condition number** measures numerical stability:\n\n$$\n\\kappa(\\mathbf{A}) = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}}\n$$ {#eq-condition-number}\n\n- $\\kappa = 1$: Perfectly conditioned (e.g., identity matrix)\n- $\\kappa < 30$: Well-conditioned\n- $\\kappa > 1000$: Ill-conditioned (numerical problems)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Well-conditioned matrix\nA1 <- diag(c(10, 9, 8))\n\neigen_A1 <- eigen(A1)\nkappa1 <- max(eigen_A1$values) / min(eigen_A1$values)\n\ncat(\"Well-conditioned matrix A1:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nWell-conditioned matrix A1:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]   10    0    0\n[2,]    0    9    0\n[3,]    0    0    8\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"\\nCondition number: %.2f (well-conditioned)\\n\", kappa1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCondition number: 1.25 (well-conditioned)\n```\n\n\n:::\n\n```{.r .cell-code}\n# Ill-conditioned matrix\nA2 <- matrix(c(1, 0.999, 0.999, 1), nrow = 2, ncol = 2, byrow = TRUE)\n\neigen_A2 <- eigen(A2)\nkappa2 <- max(eigen_A2$values) / min(eigen_A2$values)\n\ncat(\"\\n\\nIll-conditioned matrix A2:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\nIll-conditioned matrix A2:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      [,1]  [,2]\n[1,] 1.000 0.999\n[2,] 0.999 1.000\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"\\nCondition number: %.2f (ill-conditioned!)\\n\", kappa2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCondition number: 1999.00 (ill-conditioned!)\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nHigh condition number means:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nHigh condition number means:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  - Small changes in data cause large changes in solution\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  - Small changes in data cause large changes in solution\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  - Numerical errors magnified\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  - Numerical errors magnified\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  - Avoid if possible (center predictors, remove collinearity)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  - Avoid if possible (center predictors, remove collinearity)\n```\n\n\n:::\n:::\n\n\n**Livestock Example - Multicollinearity:**\n\nHighly correlated predictors lead to near-singular $\\mathbf{X}'\\mathbf{X}$ (large condition number).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Swine data: body weight and two highly correlated measures\n# x1 = backfat (mm), x2 = backfat measured at different location (highly correlated!)\nx1 <- c(10, 12, 11, 13, 14)\nx2 <- x1 + rnorm(5, 0, 0.1)  # Nearly identical to x1\n\nX <- cbind(1, x1, x2)\n\ncat(\"Design matrix X with multicollinearity:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDesign matrix X with multicollinearity:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(X, 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       x1    x2\n[1,] 1 10  9.96\n[2,] 1 12 12.12\n[3,] 1 11 11.04\n[4,] 1 13 13.04\n[5,] 1 14 14.01\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nCorrelation between x1 and x2:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCorrelation between x1 and x2:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"r = %.4f (very high!)\\n\", cor(x1, x2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nr = 0.9994 (very high!)\n```\n\n\n:::\n\n```{.r .cell-code}\nXtX <- t(X) %*% X\n\ncat(\"\\nX'X:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nX'X:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(XtX, 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            x1     x2\n    5.00  60.0  60.16\nx1 60.00 730.0 732.09\nx2 60.16 732.1 734.21\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check condition number\neigen_XtX <- eigen(XtX)\nkappa <- max(eigen_XtX$values) / min(eigen_XtX$values)\n\ncat(\"\\nEigenvalues:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nEigenvalues:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(eigen_XtX$values, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1469.1341    0.0694    0.0065\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"\\nCondition number: %.2f\\n\", kappa))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCondition number: 226498.60\n```\n\n\n:::\n\n```{.r .cell-code}\nif (kappa > 30) {\n  cat(\"WARNING: Ill-conditioned matrix due to multicollinearity!\\n\")\n  cat(\"Solution:\\n\")\n  cat(\"  - Remove one of the highly correlated predictors\\n\")\n  cat(\"  - Use ridge regression\\n\")\n  cat(\"  - Use principal components\\n\")\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nWARNING: Ill-conditioned matrix due to multicollinearity!\nSolution:\n  - Remove one of the highly correlated predictors\n  - Use ridge regression\n  - Use principal components\n```\n\n\n:::\n:::\n\n\n**Using Cholesky for Solving Linear Systems:**\n\nFor positive definite **A**, solve $\\mathbf{Ax = b}$ using Cholesky:\n\n1. Factor: $\\mathbf{A = LL}'$\n2. Solve $\\mathbf{Ly = b}$ (forward substitution)\n3. Solve $\\mathbf{L'x = y}$ (backward substitution)\n\nThis is **faster** and **more stable** than direct inversion.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Positive definite system\nA <- matrix(c(4, 2, 2, 5), nrow = 2, ncol = 2, byrow = TRUE)\nb <- c(6, 8)\n\ncat(\"System Ax = b:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSystem Ax = b:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"A:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nA:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    4    2\n[2,]    2    5\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"b:\", b, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nb: 6 8 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Method 1: Direct inverse (slower, less stable)\nx1 <- solve(A) %*% b\ncat(\"\\nSolution via A^(-1)b:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSolution via A^(-1)b:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(x1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      [,1]\n[1,] 0.875\n[2,] 1.250\n```\n\n\n:::\n\n```{.r .cell-code}\n# Method 2: Cholesky (faster, more stable)\nL <- t(chol(A))  # Lower triangular\n\n# Forward substitution: Ly = b\ny <- forwardsolve(L, b)\n\n# Backward substitution: L'x = y\nx2 <- backsolve(t(L), y)\n\ncat(\"\\nSolution via Cholesky:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSolution via Cholesky:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(x2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.875 1.250\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nBoth methods give same result:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nBoth methods give same result:\n```\n\n\n:::\n\n```{.r .cell-code}\nall.equal(x1, x2, check.attributes = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"target is matrix, current is numeric\"\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nBut Cholesky is faster for large systems!\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nBut Cholesky is faster for large systems!\n```\n\n\n:::\n:::\n\n\n**Livestock Example - Variance-Covariance Matrix:**\n\nVariance-covariance matrices are always positive semi-definite (positive definite if full rank).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Multi-trait data: 10 broilers, 3 traits\nset.seed(456)\nn <- 10\ntraits <- matrix(c(\n  rnorm(n, 2.5, 0.3),   # Body weight (kg)\n  rnorm(n, 0.85, 0.05),  # Breast yield (proportion)\n  rnorm(n, 0.45, 0.03)   # Leg yield (proportion)\n), nrow = n, ncol = 3)\n\ncolnames(traits) <- c(\"BodyWt\", \"Breast\", \"Leg\")\n\ncat(\"First 5 observations:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFirst 5 observations:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(head(traits, 5), 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     BodyWt Breast   Leg\n[1,]  2.097  0.804 0.436\n[2,]  2.687  0.916 0.398\n[3,]  2.740  0.899 0.407\n[4,]  2.083  0.933 0.456\n[5,]  2.286  0.778 0.449\n```\n\n\n:::\n\n```{.r .cell-code}\n# Covariance matrix\nSigma <- cov(traits)\n\ncat(\"\\nCovariance matrix Σ:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCovariance matrix Σ:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(Sigma, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        BodyWt Breast     Leg\nBodyWt  0.0726 0.0087 -0.0017\nBreast  0.0087 0.0039  0.0005\nLeg    -0.0017 0.0005  0.0010\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check positive definiteness\neigen_Sigma <- eigen(Sigma)\n\ncat(\"\\nEigenvalues:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nEigenvalues:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(eigen_Sigma$values, 6))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.073707 0.002969 0.000704\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nAll eigenvalues > 0 ✓\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nAll eigenvalues > 0 ✓\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Σ is positive definite (sample covariance with n > p)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nΣ is positive definite (sample covariance with n > p)\n```\n\n\n:::\n\n```{.r .cell-code}\n# Cholesky decomposition\nL <- t(chol(Sigma))\n\ncat(\"\\nCholesky factor L:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCholesky factor L:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(L, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        BodyWt Breast    Leg\nBodyWt  0.2694 0.0000 0.0000\nBreast  0.0325 0.0529 0.0000\nLeg    -0.0063 0.0124 0.0275\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nThis allows us to simulate correlated data: X = LZ where Z ~ N(0, I)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nThis allows us to simulate correlated data: X = LZ where Z ~ N(0, I)\n```\n\n\n:::\n:::\n\n\n:::{.callout-important}\n## X'X and Positive Definiteness\n\nFor linear models:\n\n1. **Full rank X** → $\\mathbf{X}'\\mathbf{X}$ is **positive definite**\n   - All eigenvalues > 0\n   - Invertible (unique solution exists)\n   - Use Cholesky for fast, stable solution\n\n2. **Rank deficient X** → $\\mathbf{X}'\\mathbf{X}$ is **positive semi-definite**\n   - Some eigenvalues = 0\n   - Not invertible (need generalized inverse)\n   - Infinite solutions (use constraints)\n\n**Rule**: Check eigenvalues of $\\mathbf{X}'\\mathbf{X}$ to detect rank deficiency!\n:::\n\n:::{.callout-note}\n## Cross-References\n\nPositive definite matrices appear in:\n\n- **Week 2**: Understanding when $\\mathbf{X}'\\mathbf{X}$ is invertible\n- **Week 5**: Variance-covariance matrix $\\text{Var}(\\mathbf{b})$ is positive definite\n- **Week 11**: Condition number for numerical stability\n- **Week 12**: Rank deficiency and positive semi-definite matrices\n- **Section @sec-computation**: Cholesky decomposition for solving systems\n:::\n\n---\n\n### Singular Value Decomposition (SVD) {#sec-svd}\n\n**Definition:**\n\nEvery m × n matrix **A** (rectangular or square) can be decomposed as:\n\n$$\n\\mathbf{A} = \\mathbf{U\\Sigma V}'\n$$ {#eq-svd}\n\nwhere:\n- **U** is m × m orthogonal matrix (left singular vectors)\n- **Σ** is m × n diagonal matrix with singular values σ₁ ≥ σ₂ ≥ ... ≥ σᵣ ≥ 0\n- **V** is n × n orthogonal matrix (right singular vectors)\n\n**Key Properties:**\n\n1. **Always exists** (even for non-square, rank-deficient matrices!)\n2. **Singular values**: σᵢ² are eigenvalues of $\\mathbf{A}'\\mathbf{A}$ (or $\\mathbf{AA}'$)\n3. **Rank**: r(**A**) = number of non-zero singular values\n4. **Condition number**: κ(**A**) = σ₁/σᵣ (ratio of largest to smallest non-zero singular value)\n5. **Moore-Penrose inverse**: $\\mathbf{A}^+ = \\mathbf{V\\Sigma}^+\\mathbf{U}'$ where Σ⁺ has 1/σᵢ for non-zero σᵢ\n\n**Why SVD Matters for Linear Models:**\n\n- Works for **any** matrix (square, rectangular, full rank, rank deficient)\n- Provides **most stable** way to compute generalized inverses\n- Reveals **numerical rank** of design matrix **X**\n- Used in **principal component regression**\n- Foundation for many modern algorithms\n\n**R Implementation:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example matrix\nA <- matrix(c(4, 2, 2, 3), nrow = 2, ncol = 2, byrow = TRUE)\n\ncat(\"Matrix A (2×2):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMatrix A (2×2):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    4    2\n[2,]    2    3\n```\n\n\n:::\n\n```{.r .cell-code}\n# Singular value decomposition\nsvd_A <- svd(A)\n\nU <- svd_A$u\nSigma_values <- svd_A$d\nV <- svd_A$v\n\ncat(\"\\nLeft singular vectors U (2×2):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nLeft singular vectors U (2×2):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(U, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        [,1]    [,2]\n[1,] -0.7882 -0.6154\n[2,] -0.6154  0.7882\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nSingular values:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSingular values:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(Sigma_values, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5.562 1.438\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nRight singular vectors V (2×2):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nRight singular vectors V (2×2):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(V, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        [,1]    [,2]\n[1,] -0.7882 -0.6154\n[2,] -0.6154  0.7882\n```\n\n\n:::\n\n```{.r .cell-code}\n# Reconstruct A\nSigma <- diag(Sigma_values)\nA_reconstructed <- U %*% Sigma %*% t(V)\n\ncat(\"\\nReconstructed A = UΣV':\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nReconstructed A = UΣV':\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(A_reconstructed, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    4    2\n[2,]    2    3\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nVerify reconstruction:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVerify reconstruction:\n```\n\n\n:::\n\n```{.r .cell-code}\nall.equal(A, A_reconstructed)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\n**SVD for Rectangular Matrices:**\n\nSVD works perfectly for non-square matrices (unlike eigenvalue decomposition).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Design matrix (5×3)\nX <- matrix(c(\n  1, 1, 2,\n  1, 2, 3,\n  1, 3, 1,\n  1, 4, 5,\n  1, 5, 4\n), nrow = 5, ncol = 3, byrow = TRUE)\n\ncat(\"Design matrix X (5×3):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDesign matrix X (5×3):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]    1    1    2\n[2,]    1    2    3\n[3,]    1    3    1\n[4,]    1    4    5\n[5,]    1    5    4\n```\n\n\n:::\n\n```{.r .cell-code}\n# SVD\nsvd_X <- svd(X)\n\ncat(\"\\nSingular values:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSingular values:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(svd_X$d, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 10.5012  2.0000  0.8517\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"\\nRank: %d (number of non-zero singular values)\\n\", sum(svd_X$d > 1e-10)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nRank: 3 (number of non-zero singular values)\n```\n\n\n:::\n\n```{.r .cell-code}\n# U is 5×5, V is 3×3\ncat(sprintf(\"\\nDimensions: U is %d×%d, V is %d×%d\\n\",\n            nrow(svd_X$u), ncol(svd_X$u),\n            nrow(svd_X$v), ncol(svd_X$v)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nDimensions: U is 5×3, V is 3×3\n```\n\n\n:::\n:::\n\n\n**Relationship to Eigenvalues:**\n\nFor symmetric matrix **A**, SVD and eigenvalue decomposition are closely related.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA <- matrix(c(4, 2, 2, 3), nrow = 2, ncol = 2, byrow = TRUE)\n\n# SVD\nsvd_A <- svd(A)\ncat(\"Singular values from SVD:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSingular values from SVD:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(svd_A$d, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5.562 1.438\n```\n\n\n:::\n\n```{.r .cell-code}\n# Eigenvalues\neigen_A <- eigen(A)\ncat(\"\\nEigenvalues from eigen():\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nEigenvalues from eigen():\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(eigen_A$values, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5.562 1.438\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nFor symmetric matrices, singular values ≈ |eigenvalues|\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nFor symmetric matrices, singular values ≈ |eigenvalues|\n```\n\n\n:::\n:::\n\n\n**Computing Moore-Penrose Inverse via SVD:**\n\n$\\mathbf{A}^+ = \\mathbf{V\\Sigma}^+\\mathbf{U}'$ where $(\\Sigma^+)_{ii} = 1/\\sigma_i$ if $\\sigma_i > 0$, else 0.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Rank-deficient matrix\nA <- matrix(c(1, 2, 2, 4), nrow = 2, ncol = 2, byrow = TRUE)\n\ncat(\"Singular matrix A:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSingular matrix A:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    1    2\n[2,]    2    4\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"Determinant: %.1f (singular!)\\n\", det(A)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDeterminant: 0.0 (singular!)\n```\n\n\n:::\n\n```{.r .cell-code}\n# SVD\nsvd_A <- svd(A)\n\ncat(\"\\nSingular values:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSingular values:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(svd_A$d)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5.000e+00 1.986e-16\n```\n\n\n:::\n\n```{.r .cell-code}\n# Moore-Penrose inverse via SVD\ntol <- 1e-10\nSigma_plus <- ifelse(svd_A$d > tol, 1/svd_A$d, 0)\n\nA_plus <- svd_A$v %*% diag(Sigma_plus) %*% t(svd_A$u)\n\ncat(\"\\nMoore-Penrose inverse A⁺:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nMoore-Penrose inverse A⁺:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(A_plus, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,] 0.04 0.08\n[2,] 0.08 0.16\n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify AA⁺A = A\ncat(\"\\nVerify AA⁺A = A:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVerify AA⁺A = A:\n```\n\n\n:::\n\n```{.r .cell-code}\nAAminusA <- A %*% A_plus %*% A\nprint(round(AAminusA, 10))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    1    2\n[2,]    2    4\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nCompare with ginv():\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCompare with ginv():\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(MASS)\nA_ginv <- ginv(A)\nprint(round(A_ginv, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,] 0.04 0.08\n[2,] 0.08 0.16\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nBoth give Moore-Penrose inverse ✓\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nBoth give Moore-Penrose inverse ✓\n```\n\n\n:::\n:::\n\n\n**Condition Number via SVD:**\n\nThe condition number measures how close a matrix is to being singular.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Well-conditioned matrix\nA1 <- diag(c(10, 9, 8))\nsvd1 <- svd(A1)\nkappa1 <- max(svd1$d) / min(svd1$d)\n\ncat(\"Well-conditioned matrix A1:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nWell-conditioned matrix A1:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]   10    0    0\n[2,]    0    9    0\n[3,]    0    0    8\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"Condition number: %.2f\\n\", kappa1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCondition number: 1.25\n```\n\n\n:::\n\n```{.r .cell-code}\n# Ill-conditioned matrix\nA2 <- matrix(c(1, 1, 1.001, 1.001), nrow = 2, ncol = 2, byrow = TRUE)\nsvd2 <- svd(A2)\nkappa2 <- max(svd2$d) / min(svd2$d)\n\ncat(\"\\n\\nIll-conditioned matrix A2:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\nIll-conditioned matrix A2:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      [,1]  [,2]\n[1,] 1.000 1.000\n[2,] 1.001 1.001\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Singular values:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSingular values:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(svd2$d, 6))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.001 0.000\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"Condition number: %.2f (very high!)\\n\", kappa2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCondition number: Inf (very high!)\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nSmall singular value indicates near-singularity\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSmall singular value indicates near-singularity\n```\n\n\n:::\n:::\n\n\n**Livestock Example - Detecting Rank Deficiency:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# ANOVA with missing cells: 3 breeds × 2 sexes, but some combinations missing\n# breed 1, sex 1: observations 1, 2\n# breed 1, sex 2: observation 3\n# breed 2, sex 1: observations 4, 5\n# breed 2, sex 2: missing!\n# breed 3, sex 1: observation 6\n# breed 3, sex 2: observations 7, 8\n\n# Cell means model (will be rank deficient due to missing cell)\nX <- matrix(0, nrow = 8, ncol = 6)\n# Columns: B1S1, B1S2, B2S1, B2S2, B3S1, B3S2\n\nX[1:2, 1] <- 1   # breed 1, sex 1\nX[3, 2] <- 1     # breed 1, sex 2\nX[4:5, 3] <- 1   # breed 2, sex 1\n# Column 4 (B2S2) has no observations!\nX[6, 5] <- 1     # breed 3, sex 1\nX[7:8, 6] <- 1   # breed 3, sex 2\n\ncat(\"Design matrix X (8×6) with missing cell:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDesign matrix X (8×6) with missing cell:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]    1    0    0    0    0    0\n[2,]    1    0    0    0    0    0\n[3,]    0    1    0    0    0    0\n[4,]    0    0    1    0    0    0\n[5,]    0    0    1    0    0    0\n[6,]    0    0    0    0    1    0\n[7,]    0    0    0    0    0    1\n[8,]    0    0    0    0    0    1\n```\n\n\n:::\n\n```{.r .cell-code}\n# SVD to detect rank\nsvd_X <- svd(X)\n\ncat(\"\\nSingular values:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSingular values:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(svd_X$d, 6))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.414 1.414 1.414 1.000 1.000 0.000\n```\n\n\n:::\n\n```{.r .cell-code}\n# Determine numerical rank\ntol <- 1e-10\nrank_X <- sum(svd_X$d > tol)\n\ncat(sprintf(\"\\nNumerical rank: %d (out of 6 parameters)\\n\", rank_X))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nNumerical rank: 5 (out of 6 parameters)\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Matrix is rank deficient due to missing cell!\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMatrix is rank deficient due to missing cell!\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nConsequence: Must use generalized inverse or constraints\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nConsequence: Must use generalized inverse or constraints\n```\n\n\n:::\n:::\n\n\n**Truncated SVD (Low-Rank Approximation):**\n\nKeep only largest k singular values to approximate **A**.\n\n$$\n\\mathbf{A}_k = \\sum_{i=1}^{k} \\sigma_i \\mathbf{u}_i\\mathbf{v}_i'\n$$ {#eq-truncated-svd}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create noisy data matrix\nset.seed(789)\ntrue_signal <- matrix(c(1, 2, 3, 2, 4, 6), nrow = 3, ncol = 2)\nnoise <- matrix(rnorm(6, 0, 0.1), nrow = 3, ncol = 2)\nA <- true_signal + noise\n\ncat(\"Noisy matrix A (3×2):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNoisy matrix A (3×2):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(A, 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,] 1.05 2.02\n[2,] 1.77 3.96\n[3,] 3.00 5.95\n```\n\n\n:::\n\n```{.r .cell-code}\n# Full SVD\nsvd_A <- svd(A)\n\ncat(\"\\nSingular values:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSingular values:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(svd_A$d, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 8.2715 0.1774\n```\n\n\n:::\n\n```{.r .cell-code}\n# Keep only largest singular value (rank-1 approximation)\nA1 <- svd_A$d[1] * svd_A$u[, 1] %*% t(svd_A$v[, 1])\n\ncat(\"\\nRank-1 approximation (truncated SVD):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nRank-1 approximation (truncated SVD):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(A1, 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,] 1.00 2.04\n[2,] 1.91 3.90\n[3,] 2.93 5.99\n```\n\n\n:::\n\n```{.r .cell-code}\n# Error\nerror_norm <- norm(A - A1, \"F\")\ncat(sprintf(\"\\nFrobenius norm of error: %.4f\\n\", error_norm))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nFrobenius norm of error: 0.1774\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nTruncated SVD removes noise, keeps signal\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nTruncated SVD removes noise, keeps signal\n```\n\n\n:::\n:::\n\n\n**Livestock Example - Principal Component Regression:**\n\nSVD provides principal components for regression.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Swine data: highly correlated predictors\n# y = average daily gain, X = backfat, loin depth, weight (all correlated)\nset.seed(234)\nn <- 20\n\n# Create correlated predictors\nbackfat <- rnorm(n, 15, 2)\nloin <- 70 - 2*backfat + rnorm(n, 0, 1)  # Negatively correlated\nweight <- 100 + 3*backfat + 0.5*loin + rnorm(n, 0, 2)\n\n# Response\nadg <- 0.8 + 0.01*backfat - 0.005*loin + 0.002*weight + rnorm(n, 0, 0.05)\n\n# Design matrix (centered)\nX <- scale(cbind(backfat, loin, weight), center = TRUE, scale = FALSE)\n\ncat(\"Correlation matrix of predictors:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCorrelation matrix of predictors:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(cor(X), 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        backfat   loin weight\nbackfat   1.000 -0.983  0.933\nloin     -0.983  1.000 -0.883\nweight    0.933 -0.883  1.000\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nHigh correlations → multicollinearity!\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nHigh correlations → multicollinearity!\n```\n\n\n:::\n\n```{.r .cell-code}\n# SVD of X\nsvd_X <- svd(X)\n\ncat(\"\\nSingular values:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSingular values:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(svd_X$d, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 28.899  6.779  1.155\n```\n\n\n:::\n\n```{.r .cell-code}\n# Condition number\nkappa <- max(svd_X$d) / min(svd_X$d)\ncat(sprintf(\"\\nCondition number: %.2f (ill-conditioned)\\n\", kappa))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCondition number: 25.02 (ill-conditioned)\n```\n\n\n:::\n\n```{.r .cell-code}\n# Principal components\nPC <- X %*% svd_X$v\ncolnames(PC) <- c(\"PC1\", \"PC2\", \"PC3\")\n\ncat(\"\\nPrincipal components (first 3 observations):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nPrincipal components (first 3 observations):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(head(PC, 3), 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        PC1    PC2    PC3\n[1,] -4.732 -0.984  0.049\n[2,] 12.676 -0.159 -0.494\n[3,]  6.694 -2.581  0.242\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nPCs are uncorrelated (orthogonal):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nPCs are uncorrelated (orthogonal):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(cor(PC), 6))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    PC1 PC2 PC3\nPC1   1   0   0\nPC2   0   1   0\nPC3   0   0   1\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nCan regress on PCs instead of original X to avoid multicollinearity\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCan regress on PCs instead of original X to avoid multicollinearity\n```\n\n\n:::\n:::\n\n\n**Comparing Decompositions:**\n\n| Method | Matrix Type | Output | Use Case |\n|--------|------------|--------|----------|\n| **Eigenvalue** | Square only | $\\mathbf{A} = \\mathbf{Q\\Lambda Q}'$ | Symmetric matrices, $\\mathbf{X}'\\mathbf{X}$ |\n| **SVD** | Any (m×n) | $\\mathbf{A} = \\mathbf{U\\Sigma V}'$ | Rank deficiency, generalized inverse |\n| **Cholesky** | Pos. definite | $\\mathbf{A} = \\mathbf{LL}'$ | Fast solving, simulation |\n| **QR** | Any (m×n) | $\\mathbf{A} = \\mathbf{QR}$ | Stable LS solution |\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA <- matrix(c(4, 2, 2, 5), nrow = 2, ncol = 2, byrow = TRUE)\n\ncat(\"Symmetric positive definite matrix A:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSymmetric positive definite matrix A:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    4    2\n[2,]    2    5\n```\n\n\n:::\n\n```{.r .cell-code}\n# Eigenvalue decomposition\neigen_A <- eigen(A)\ncat(\"\\nEigenvalues:\", round(eigen_A$values, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nEigenvalues: 6.562 2.438 \n```\n\n\n:::\n\n```{.r .cell-code}\n# SVD\nsvd_A <- svd(A)\ncat(\"Singular values:\", round(svd_A$d, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSingular values: 6.562 2.438 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nFor symmetric matrices: singular values ≈ |eigenvalues|\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nFor symmetric matrices: singular values ≈ |eigenvalues|\n```\n\n\n:::\n\n```{.r .cell-code}\n# Cholesky\nL <- t(chol(A))\ncat(\"\\nCholesky factor L:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCholesky factor L:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(L, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    2    0\n[2,]    1    2\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nAll decompositions reveal different aspects of same matrix\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nAll decompositions reveal different aspects of same matrix\n```\n\n\n:::\n:::\n\n\n:::{.callout-important}\n## SVD is THE Most General Decomposition\n\nSVD works for **any** matrix:\n- Square or rectangular\n- Full rank or rank deficient\n- Symmetric or non-symmetric\n\n**Applications in Linear Models:**\n\n1. **Detect rank deficiency**: Count non-zero singular values\n2. **Compute generalized inverse**: $\\mathbf{A}^+ = \\mathbf{V\\Sigma}^+\\mathbf{U}'$\n3. **Assess numerical stability**: Condition number = σ₁/σᵣ\n4. **Principal component regression**: Use **V** to transform predictors\n5. **Data compression**: Truncated SVD for dimensionality reduction\n\n**Computational Note:** SVD is more expensive than eigenvalue decomposition but more robust.\n:::\n\n:::{.callout-note}\n## Cross-References\n\nSVD appears in:\n\n- **Week 2**: Generalized inverse computation\n- **Week 11**: Detecting numerical problems via condition number\n- **Week 12**: Rank deficiency and generalized inverses\n- **Week 14**: Principal component analysis and regression\n- **Section @sec-computation**: Numerical considerations\n:::\n\n---\n\n## Matrix Calculus for Linear Models {#sec-matrix-calculus}\n\n:::{.callout-important}\n## Critical Section\n\nThis section provides the mathematical foundation for deriving normal equations. It receives extra emphasis with detailed step-by-step derivations. Understanding these derivatives is essential for understanding where least squares estimates come from.\n:::\n\nMatrix calculus extends ordinary calculus to functions involving vectors and matrices. In linear models, we use matrix derivatives primarily to minimize the sum of squared errors and derive the normal equations.\n\n### Vector and Matrix Derivatives {#sec-derivatives}\n\n#### Notation and Conventions\n\nWhen taking derivatives with respect to vectors, we must be careful about **layout conventions**. There are two main conventions:\n\n1. **Numerator layout** (preferred in this course): Result has same orientation as numerator\n2. **Denominator layout**: Result has same orientation as denominator\n\nWe use **numerator layout** throughout this course.\n\n#### Scalar Function of a Vector\n\nLet $f: \\mathbb{R}^n \\to \\mathbb{R}$ be a scalar-valued function of a vector $\\mathbf{x} = [x_1, x_2, \\ldots, x_n]'$.\n\nThe **gradient** is:\n\n$$\n\\frac{\\partial f}{\\partial \\mathbf{x}} = \\begin{bmatrix}\n\\frac{\\partial f}{\\partial x_1} \\\\\n\\frac{\\partial f}{\\partial x_2} \\\\\n\\vdots \\\\\n\\frac{\\partial f}{\\partial x_n}\n\\end{bmatrix}\n$$ {#eq-gradient}\n\nThis is an n × 1 **column vector** (numerator layout).\n\n**Example**: If $f(\\mathbf{x}) = x_1^2 + 2x_2$, then:\n\n$$\n\\frac{\\partial f}{\\partial \\mathbf{x}} = \\begin{bmatrix} 2x_1 \\\\ 2 \\end{bmatrix}\n$$\n\n#### Dimensions Matter\n\nAlways check dimensions:\n- If $\\mathbf{x}$ is n × 1 and $f$ is scalar, then $\\frac{\\partial f}{\\partial \\mathbf{x}}$ is n × 1\n- If $\\mathbf{y}$ is m × 1 and $\\mathbf{x}$ is n × 1, then $\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}$ is m × n (Jacobian)\n\n#### Second Derivatives (Hessian Matrix)\n\nThe **Hessian** is the matrix of second partial derivatives:\n\n$$\n\\frac{\\partial^2 f}{\\partial \\mathbf{x} \\partial \\mathbf{x}'} = \\mathbf{H} = \\begin{bmatrix}\n\\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} & \\cdots \\\\\n\\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} & \\cdots \\\\\n\\vdots & \\vdots & \\ddots\n\\end{bmatrix}\n$$ {#eq-hessian}\n\nThis is an n × n matrix. If $f$ has continuous second derivatives, **H** is symmetric.\n\n:::{.callout-note}\n## Caution: Not the Hat Matrix!\n\nDon't confuse the Hessian matrix **H** (from calculus) with the hat matrix **H** (from projections). Context makes it clear which is meant.\n:::\n\n---\n\n### Basic Derivative Rules {#sec-derivative-rules}\n\nHere are the essential matrix derivative rules for linear models:\n\n#### Rule Table\n\n| Expression | $\\frac{\\partial}{\\partial \\mathbf{x}}$ | Dimensions | Notes |\n|------------|---------------------------------------|------------|-------|\n| $\\mathbf{a}'\\mathbf{x}$ | $\\mathbf{a}$ | n × 1 | **a** constant n × 1 |\n| $\\mathbf{x}'\\mathbf{a}$ | $\\mathbf{a}$ | n × 1 | Same as above |\n| $\\mathbf{x}'\\mathbf{x}$ | $2\\mathbf{x}$ | n × 1 | Quadratic |\n| $\\mathbf{x}'\\mathbf{A}\\mathbf{x}$ | $2\\mathbf{A}\\mathbf{x}$ | n × 1 | **A** symmetric n × n |\n| $\\mathbf{x}'\\mathbf{A}\\mathbf{x}$ | $(\\mathbf{A} + \\mathbf{A}')\\mathbf{x}$ | n × 1 | **A** not symmetric |\n| $\\mathbf{A}\\mathbf{x}$ | $\\mathbf{A}$ | m × n | **A** is m × n |\n| $\\mathbf{x}'\\mathbf{A}$ | $\\mathbf{A}'$ | n × m | Result w.r.t. **x** |\n\n**Most important for linear models**: $\\frac{\\partial}{\\partial \\boldsymbol{\\beta}}[\\mathbf{y}'\\mathbf{X}\\boldsymbol{\\beta}] = \\mathbf{X}'\\mathbf{y}$\n\nwhere:\n- $\\mathbf{y}$ is n × 1 (data, constant w.r.t. $\\boldsymbol{\\beta}$)\n- $\\mathbf{X}$ is n × p (design matrix, constant)\n- $\\boldsymbol{\\beta}$ is p × 1 (parameters, variable)\n- Result is p × 1\n\n#### Detailed Derivations\n\n**1. Linear form: $f(\\mathbf{x}) = \\mathbf{a}'\\mathbf{x}$**\n\n$$\nf(\\mathbf{x}) = a_1 x_1 + a_2 x_2 + \\cdots + a_n x_n\n$$\n\n$$\n\\frac{\\partial f}{\\partial \\mathbf{x}} = \\begin{bmatrix} a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_n \\end{bmatrix} = \\mathbf{a}\n$$ {#eq-deriv-linear}\n\n**2. Quadratic form: $f(\\mathbf{x}) = \\mathbf{x}'\\mathbf{x}$**\n\n$$\nf(\\mathbf{x}) = x_1^2 + x_2^2 + \\cdots + x_n^2\n$$\n\n$$\n\\frac{\\partial f}{\\partial \\mathbf{x}} = \\begin{bmatrix} 2x_1 \\\\ 2x_2 \\\\ \\vdots \\\\ 2x_n \\end{bmatrix} = 2\\mathbf{x}\n$$ {#eq-deriv-quadratic}\n\n**3. Quadratic form with matrix: $f(\\mathbf{x}) = \\mathbf{x}'\\mathbf{A}\\mathbf{x}$ where **A** symmetric**\n\nFirst, note that:\n$$\nf(\\mathbf{x}) = \\sum_{i=1}^n \\sum_{j=1}^n a_{ij} x_i x_j\n$$\n\nTaking derivative with respect to $x_k$:\n$$\n\\frac{\\partial f}{\\partial x_k} = \\sum_{i=1}^n a_{ik} x_i + \\sum_{j=1}^n a_{kj} x_j\n$$\n\nSince **A** is symmetric ($a_{ij} = a_{ji}$):\n$$\n\\frac{\\partial f}{\\partial x_k} = 2\\sum_{i=1}^n a_{ki} x_i = 2[\\mathbf{A}\\mathbf{x}]_k\n$$\n\nTherefore:\n$$\n\\frac{\\partial (\\mathbf{x}'\\mathbf{A}\\mathbf{x})}{\\partial \\mathbf{x}} = 2\\mathbf{A}\\mathbf{x}\n$$ {#eq-deriv-qf}\n\n**4. Product rule for $\\mathbf{y}'\\mathbf{X}\\boldsymbol{\\beta}$ with respect to $\\boldsymbol{\\beta}$**\n\nTreating **y** and **X** as constants:\n$$\n\\mathbf{y}'\\mathbf{X}\\boldsymbol{\\beta} = \\sum_{i=1}^n \\sum_{j=1}^p y_i X_{ij} \\beta_j = \\sum_{j=1}^p \\left(\\sum_{i=1}^n y_i X_{ij}\\right) \\beta_j\n$$\n\nThis is linear in $\\boldsymbol{\\beta}$, so:\n$$\n\\frac{\\partial (\\mathbf{y}'\\mathbf{X}\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}} = \\begin{bmatrix}\n\\sum_{i=1}^n y_i X_{i1} \\\\\n\\sum_{i=1}^n y_i X_{i2} \\\\\n\\vdots \\\\\n\\sum_{i=1}^n y_i X_{ip}\n\\end{bmatrix} = \\mathbf{X}'\\mathbf{y}\n$$ {#eq-deriv-xty}\n\n#### R Verification with Numerical Derivatives\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Verify derivative rules numerically using finite differences\n\n# Function to compute numerical gradient\nnumerical_gradient <- function(f, x, h = 1e-8) {\n  n <- length(x)\n  grad <- numeric(n)\n  for (i in 1:n) {\n    x_plus <- x\n    x_plus[i] <- x[i] + h\n    x_minus <- x\n    x_minus[i] <- x[i] - h\n    grad[i] <- (f(x_plus) - f(x_minus)) / (2 * h)\n  }\n  return(grad)\n}\n\n# Example 1: f(x) = a'x\na <- c(2, 3, 4)\nf1 <- function(x) sum(a * x)\n\nx0 <- c(1, 2, 3)\nanalytical1 <- a\nnumerical1 <- numerical_gradient(f1, x0)\n\ncat(\"Example 1: f(x) = a'x\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nExample 1: f(x) = a'x\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Analytical gradient:\", analytical1, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalytical gradient: 2 3 4 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Numerical gradient:\", numerical1, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNumerical gradient: 2 3 4 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Agreement:\", all.equal(analytical1, numerical1), \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAgreement: Mean relative difference: 3.298e-08 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Example 2: f(x) = x'x\nf2 <- function(x) sum(x^2)\n\nanalytical2 <- 2 * x0\nnumerical2 <- numerical_gradient(f2, x0)\n\ncat(\"Example 2: f(x) = x'x\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nExample 2: f(x) = x'x\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Analytical gradient:\", analytical2, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalytical gradient: 2 4 6 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Numerical gradient:\", numerical2, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNumerical gradient: 2 4 6 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Agreement:\", all.equal(analytical2, numerical2), \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAgreement: TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\n# Example 3: f(x) = x'Ax with A symmetric\nA <- matrix(c(2, 1, 1, 3), 2, 2)  # 2x2 symmetric\nf3 <- function(x) c(t(x) %*% A %*% x)\n\nx0_2 <- c(1, 2)\nanalytical3 <- 2 * A %*% x0_2\nnumerical3 <- numerical_gradient(f3, x0_2)\n\ncat(\"Example 3: f(x) = x'Ax (A symmetric)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nExample 3: f(x) = x'Ax (A symmetric)\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Analytical gradient:\", analytical3, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalytical gradient: 8 14 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Numerical gradient:\", numerical3, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNumerical gradient: 8 14 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Agreement:\", all.equal(c(analytical3), numerical3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAgreement: TRUE \n```\n\n\n:::\n:::\n\n\n---\n\n### Deriving Normal Equations {#sec-deriving-normal-equations}\n\nThis is **THE fundamental derivation** in linear models. We'll go step-by-step from minimizing sum of squared errors to the normal equations.\n\n#### Setup\n\nGiven:\n- Response vector $\\mathbf{y}$: n × 1 (observed data)\n- Design matrix $\\mathbf{X}$: n × p (predictors, full rank)\n- Parameter vector $\\boldsymbol{\\beta}$: p × 1 (unknown)\n- Model: $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$\n\n**Goal**: Find $\\boldsymbol{\\beta}$ that minimizes sum of squared errors.\n\n#### Step 1: Define the Sum of Squares Function\n\nThe **residual vector** is:\n$$\n\\mathbf{e} = \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\n$$ {#eq-residual-vector}\n\nThe **sum of squared errors** is:\n$$\nS(\\boldsymbol{\\beta}) = \\mathbf{e}'\\mathbf{e} = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})'(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\n$$ {#eq-sse-function}\n\nThis is a scalar-valued function of the p × 1 vector $\\boldsymbol{\\beta}$.\n\n#### Step 2: Expand the Sum of Squares\n\nExpand using $(\\mathbf{a} - \\mathbf{b})'(\\mathbf{a} - \\mathbf{b}) = \\mathbf{a}'\\mathbf{a} - 2\\mathbf{a}'\\mathbf{b} + \\mathbf{b}'\\mathbf{b}$:\n\n$$\n\\begin{align}\nS(\\boldsymbol{\\beta}) &= (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})'(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) \\\\\n&= \\mathbf{y}'\\mathbf{y} - \\mathbf{y}'\\mathbf{X}\\boldsymbol{\\beta} - \\boldsymbol{\\beta}'\\mathbf{X}'\\mathbf{y} + \\boldsymbol{\\beta}'\\mathbf{X}'\\mathbf{X}\\boldsymbol{\\beta}\n\\end{align}\n$$\n\nSince $\\mathbf{y}'\\mathbf{X}\\boldsymbol{\\beta}$ is a scalar, it equals its transpose:\n$$\n\\mathbf{y}'\\mathbf{X}\\boldsymbol{\\beta} = (\\mathbf{y}'\\mathbf{X}\\boldsymbol{\\beta})' = \\boldsymbol{\\beta}'\\mathbf{X}'\\mathbf{y}\n$$\n\nTherefore:\n$$\nS(\\boldsymbol{\\beta}) = \\mathbf{y}'\\mathbf{y} - 2\\boldsymbol{\\beta}'\\mathbf{X}'\\mathbf{y} + \\boldsymbol{\\beta}'\\mathbf{X}'\\mathbf{X}\\boldsymbol{\\beta}\n$$ {#eq-sse-expanded}\n\n#### Step 3: Take the Derivative with Respect to $\\boldsymbol{\\beta}$\n\nApply derivative rules to each term:\n\n1. $\\frac{\\partial}{\\partial \\boldsymbol{\\beta}}[\\mathbf{y}'\\mathbf{y}] = \\mathbf{0}$ (constant)\n\n2. $\\frac{\\partial}{\\partial \\boldsymbol{\\beta}}[-2\\boldsymbol{\\beta}'\\mathbf{X}'\\mathbf{y}] = -2\\mathbf{X}'\\mathbf{y}$ (linear in $\\boldsymbol{\\beta}$)\n\n3. $\\frac{\\partial}{\\partial \\boldsymbol{\\beta}}[\\boldsymbol{\\beta}'\\mathbf{X}'\\mathbf{X}\\boldsymbol{\\beta}] = 2\\mathbf{X}'\\mathbf{X}\\boldsymbol{\\beta}$ (quadratic, $\\mathbf{X}'\\mathbf{X}$ is symmetric)\n\nCombining:\n$$\n\\frac{\\partial S(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}} = \\mathbf{0} - 2\\mathbf{X}'\\mathbf{y} + 2\\mathbf{X}'\\mathbf{X}\\boldsymbol{\\beta} = -2\\mathbf{X}'\\mathbf{y} + 2\\mathbf{X}'\\mathbf{X}\\boldsymbol{\\beta}\n$$ {#eq-sse-derivative}\n\n#### Step 4: Set Derivative to Zero\n\nFor a minimum, set the gradient to zero:\n$$\n-2\\mathbf{X}'\\mathbf{y} + 2\\mathbf{X}'\\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{0}\n$$\n\nDivide by 2:\n$$\n-\\mathbf{X}'\\mathbf{y} + \\mathbf{X}'\\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{0}\n$$\n\nRearrange:\n$$\n\\mathbf{X}'\\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{X}'\\mathbf{y}\n$$ {#eq-normal-equations}\n\nThese are the **normal equations**!\n\n#### Step 5: Solve for $\\boldsymbol{\\beta}$\n\nIf $\\mathbf{X}'\\mathbf{X}$ is invertible (i.e., **X** has full column rank):\n$$\n\\mathbf{b} = (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y}\n$$ {#eq-ls-solution}\n\nThis is the **least squares estimator**.\n\n#### Step 6: Verify it's a Minimum (Second Derivative Test)\n\nThe Hessian (matrix of second derivatives) is:\n$$\n\\frac{\\partial^2 S(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta} \\partial \\boldsymbol{\\beta}'} = 2\\mathbf{X}'\\mathbf{X}\n$$ {#eq-hessian-sse}\n\nFor a minimum, the Hessian must be **positive definite**. Since $\\mathbf{X}'\\mathbf{X}$ is:\n- Symmetric (always)\n- Positive semi-definite (always): For any $\\mathbf{v}$, $\\mathbf{v}'\\mathbf{X}'\\mathbf{X}\\mathbf{v} = (\\mathbf{X}\\mathbf{v})'(\\mathbf{X}\\mathbf{v}) = ||\\mathbf{X}\\mathbf{v}||^2 \\geq 0$\n- Positive definite if **X** has full rank: $\\mathbf{v}'\\mathbf{X}'\\mathbf{X}\\mathbf{v} > 0$ for all $\\mathbf{v} \\neq \\mathbf{0}$\n\nTherefore, $S(\\boldsymbol{\\beta})$ is convex and has a unique global minimum at $\\mathbf{b} = (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y}$.\n\n:::{.callout-important}\n## Key Result\n\nThe normal equations $\\mathbf{X}'\\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{X}'\\mathbf{y}$ are derived by:\n\n1. **Minimizing** $S(\\boldsymbol{\\beta}) = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})'(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})$\n2. **Taking derivative** with respect to $\\boldsymbol{\\beta}$\n3. **Setting to zero**: $\\frac{\\partial S}{\\partial \\boldsymbol{\\beta}} = \\mathbf{0}$\n\nThis is the foundation of least squares estimation!\n:::\n\n#### Complete Example: Simple Regression\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simple regression: y = β₀ + β₁x + e\n# Derive normal equations step-by-step\n\n# Data: Broiler weight (kg) vs age (days)\nage <- c(21, 28, 35, 42, 49)\nweight <- c(0.5, 0.9, 1.4, 1.9, 2.3)\nn <- length(age)\n\n# Design matrix\nX <- cbind(1, age)\ny <- weight\np <- ncol(X)\n\ncat(\"Step 1: Set up\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStep 1: Set up\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"y:\", y, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ny: 0.5 0.9 1.4 1.9 2.3 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"X:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nX:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       age\n[1,] 1  21\n[2,] 1  28\n[3,] 1  35\n[4,] 1  42\n[5,] 1  49\n```\n\n\n:::\n\n```{.r .cell-code}\n# Step 2: Define S(β) = (y - Xβ)'(y - Xβ)\n# We'll evaluate S at several β values to visualize\n\n# For visualization, use grid of β values\nbeta0_seq <- seq(-2, 2, length = 50)\nbeta1_seq <- seq(0, 0.08, length = 50)\nS_grid <- matrix(NA, length(beta0_seq), length(beta1_seq))\n\nfor (i in 1:length(beta0_seq)) {\n  for (j in 1:length(beta1_seq)) {\n    beta <- c(beta0_seq[i], beta1_seq[j])\n    e <- y - X %*% beta\n    S_grid[i, j] <- sum(e^2)\n  }\n}\n\ncat(\"\\n\\nStep 3: Compute X'X and X'y\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\nStep 3: Compute X'X and X'y\n```\n\n\n:::\n\n```{.r .cell-code}\nXtX <- t(X) %*% X\nXty <- t(X) %*% y\n\ncat(\"X'X =\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nX'X =\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(XtX)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         age\n      5  175\nage 175 6615\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nX'y =\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nX'y =\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(Xty)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]\n      7.0\nage 277.2\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\n\\nStep 4: Solve normal equations X'Xb = X'y\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\nStep 4: Solve normal equations X'Xb = X'y\n```\n\n\n:::\n\n```{.r .cell-code}\nb <- solve(XtX) %*% Xty\ncat(\"b = (X'X)^(-1) X'y =\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nb = (X'X)^(-1) X'y =\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        [,1]\n    -0.90000\nage  0.06571\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\n\\nStep 5: Verify this minimizes S(β)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\nStep 5: Verify this minimizes S(β)\n```\n\n\n:::\n\n```{.r .cell-code}\ne <- y - X %*% b\nSSE <- sum(e^2)\ncat(\"SSE at b:\", SSE, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSSE at b: 0.004 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Check derivative at solution is zero\ngrad_at_b <- -2 * Xty + 2 * XtX %*% b\ncat(\"\\nGradient at b (should be 0):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nGradient at b (should be 0):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(grad_at_b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         [,1]\n    1.954e-14\nage 5.684e-13\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Max absolute value:\", max(abs(grad_at_b)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMax absolute value: 5.684e-13 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify second derivative is positive definite\nhess <- 2 * XtX\ncat(\"\\nHessian = 2X'X =\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nHessian = 2X'X =\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(hess)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          age\n     10   350\nage 350 13230\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Eigenvalues (should be > 0):\", eigen(hess)$values, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEigenvalues (should be > 0): 13239 0.7402 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Compare with lm()\nfit <- lm(weight ~ age)\ncat(\"\\n\\nComparison with lm():\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\nComparison with lm():\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Our b:\", c(b), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOur b: -0.9 0.06571 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"lm() coefficients:\", coef(fit), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nlm() coefficients: -0.9 0.06571 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Agreement:\", all.equal(c(b), unname(coef(fit))), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAgreement: TRUE \n```\n\n\n:::\n:::\n\n\n#### Numerical Verification: Gradient is Zero at Solution\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define S(β) as a function\nS_function <- function(beta, X, y) {\n  e <- y - X %*% beta\n  return(sum(e^2))\n}\n\n# Compute numerical gradient at solution\nh <- 1e-8\ngrad_numerical <- numeric(p)\nfor (i in 1:p) {\n  b_plus <- b\n  b_plus[i] <- b[i] + h\n  b_minus <- b\n  b_minus[i] <- b[i] - h\n  grad_numerical[i] <- (S_function(b_plus, X, y) - S_function(b_minus, X, y)) / (2*h)\n}\n\ncat(\"Numerical gradient at b:\", grad_numerical, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNumerical gradient at b: 4.337e-10 -1.344e-09 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Should be approximately zero\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nShould be approximately zero\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Max absolute value:\", max(abs(grad_numerical)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMax absolute value: 1.344e-09 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Analytical gradient at solution (should be exactly 0)\ngrad_analytical <- -2 * t(X) %*% y + 2 * t(X) %*% X %*% b\ncat(\"\\nAnalytical gradient:\", c(grad_analytical), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nAnalytical gradient: 1.954e-14 5.684e-13 \n```\n\n\n:::\n:::\n\n\n:::{.callout-tip}\n## Understanding the Derivation\n\nThe normal equations emerge from three fundamental calculus steps:\n\n1. **Express the objective**: $S(\\boldsymbol{\\beta}) = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})'(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})$\n2. **Take the gradient**: $\\nabla S = -2\\mathbf{X}'\\mathbf{y} + 2\\mathbf{X}'\\mathbf{X}\\boldsymbol{\\beta}$\n3. **Set to zero**: $\\mathbf{X}'\\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{X}'\\mathbf{y}$\n\nThis is why least squares \"works\" - it's a direct application of calculus to find the minimum of a convex function.\n:::\n\n---\n\n### Useful Identities Table {#sec-calculus-identities}\n\nHere are additional matrix calculus identities useful in linear models:\n\n#### General Identities\n\n| Expression | Derivative w.r.t. $\\mathbf{x}$ | Notes |\n|------------|-------------------------------|-------|\n| $\\mathbf{c}$ | $\\mathbf{0}$ | Constant vector |\n| $\\mathbf{A}\\mathbf{x}$ | $\\mathbf{A}$ | **A** constant m × n |\n| $\\mathbf{x}'\\mathbf{A}$ | $\\mathbf{A}'$ | **A** constant |\n| $\\mathbf{a}'\\mathbf{x}$ | $\\mathbf{a}$ | Linear |\n| $\\mathbf{x}'\\mathbf{A}\\mathbf{x}$ | $(\\mathbf{A} + \\mathbf{A}')\\mathbf{x}$ | General **A** |\n| $\\mathbf{x}'\\mathbf{A}\\mathbf{x}$ | $2\\mathbf{A}\\mathbf{x}$ | **A** symmetric |\n| $(\\mathbf{A}\\mathbf{x})'\\mathbf{B}\\mathbf{x}$ | $\\mathbf{A}'\\mathbf{B}\\mathbf{x} + \\mathbf{B}'\\mathbf{A}\\mathbf{x}$ | Product rule |\n| $\\mathbf{x}'\\mathbf{A}\\mathbf{x}\\mathbf{b}$ | $2\\mathbf{A}\\mathbf{x}\\mathbf{b}'$ | Outer product |\n\n#### Specific to Least Squares\n\n| Quantity | Formula | Derivative w.r.t. $\\boldsymbol{\\beta}$ |\n|----------|---------|---------------------------------------|\n| $\\mathbf{e}$ | $\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}$ | $-\\mathbf{X}$ |\n| $\\mathbf{e}'\\mathbf{e}$ | SSE | $-2\\mathbf{X}'\\mathbf{e} = -2\\mathbf{X}'\\mathbf{y} + 2\\mathbf{X}'\\mathbf{X}\\boldsymbol{\\beta}$ |\n| $\\hat{\\mathbf{y}}$ | $\\mathbf{X}\\boldsymbol{\\beta}$ | $\\mathbf{X}$ |\n| $\\mathbf{y}'\\mathbf{X}\\boldsymbol{\\beta}$ | Linear in $\\boldsymbol{\\beta}$ | $\\mathbf{X}'\\mathbf{y}$ |\n| $\\boldsymbol{\\beta}'\\mathbf{X}'\\mathbf{X}\\boldsymbol{\\beta}$ | Quadratic in $\\boldsymbol{\\beta}$ | $2\\mathbf{X}'\\mathbf{X}\\boldsymbol{\\beta}$ |\n\n#### Chain Rule\n\nIf $\\mathbf{u} = \\mathbf{u}(\\mathbf{x})$ and $f = f(\\mathbf{u})$, then:\n\n$$\n\\frac{\\partial f}{\\partial \\mathbf{x}} = \\frac{\\partial \\mathbf{u}}{\\partial \\mathbf{x}} \\frac{\\partial f}{\\partial \\mathbf{u}}\n$$\n\nwhere $\\frac{\\partial \\mathbf{u}}{\\partial \\mathbf{x}}$ is the Jacobian matrix.\n\n**Example**: If $f = \\mathbf{e}'\\mathbf{e}$ where $\\mathbf{e} = \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}$:\n\n$$\n\\frac{\\partial f}{\\partial \\boldsymbol{\\beta}} = \\frac{\\partial \\mathbf{e}}{\\partial \\boldsymbol{\\beta}} \\frac{\\partial (\\mathbf{e}'\\mathbf{e})}{\\partial \\mathbf{e}} = (-\\mathbf{X})' (2\\mathbf{e}) = -2\\mathbf{X}'\\mathbf{e}\n$$\n\n:::{.callout-note}\n## Cross-References\n\n- **Week 5: Least Squares Theory** - Uses these derivatives extensively\n- **Week 6: Multiple Regression** - Applies to multiple predictors\n- **Week 8: Contrasts** - Derivatives of contrast functions\n:::\n\n:::{.callout-important}\n## Summary: Matrix Calculus Essentials\n\nThe three key results for linear models are:\n\n1. $\\frac{\\partial}{\\partial \\boldsymbol{\\beta}}[\\mathbf{a}'\\boldsymbol{\\beta}] = \\mathbf{a}$ - Linear terms\n2. $\\frac{\\partial}{\\partial \\boldsymbol{\\beta}}[\\boldsymbol{\\beta}'\\mathbf{A}\\boldsymbol{\\beta}] = 2\\mathbf{A}\\boldsymbol{\\beta}$ - Quadratic terms (**A** symmetric)\n3. $\\frac{\\partial}{\\partial \\boldsymbol{\\beta}}[(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})'(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})] = -2\\mathbf{X}'\\mathbf{y} + 2\\mathbf{X}'\\mathbf{X}\\boldsymbol{\\beta}$ - Normal equations\n\nMaster these three, and you understand the calculus foundation of linear models!\n:::\n\n---\n\n## Kronecker Products {#sec-kronecker}\n\n:::{.callout-note}\n## Foundation for Advanced Topics\n\nThis section prepares you for Animal models and multi-trait analysis covered in future courses. Kronecker products provide the mathematical structure for variance-covariance matrices in multi-trait genetic evaluations.\n:::\n\n### Definition and Basic Properties {#sec-kronecker-definition}\n\n**Definition:**\n\nThe **Kronecker product** (also called direct product or tensor product) of matrix **A** (m × n) and matrix **B** (p × q) is:\n\n$$\n\\mathbf{A} \\otimes \\mathbf{B} = \\begin{bmatrix}\na_{11}\\mathbf{B} & a_{12}\\mathbf{B} & \\cdots & a_{1n}\\mathbf{B} \\\\\na_{21}\\mathbf{B} & a_{22}\\mathbf{B} & \\cdots & a_{2n}\\mathbf{B} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1}\\mathbf{B} & a_{m2}\\mathbf{B} & \\cdots & a_{mn}\\mathbf{B}\n\\end{bmatrix}\n$$ {#eq-kronecker-product}\n\nThe result is an (mp) × (nq) matrix formed by replacing each element a~ij~ of **A** with the block a~ij~**B**.\n\n**Simple Example:**\n\n$$\n\\begin{bmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{bmatrix} \\otimes \\begin{bmatrix} b_{11} & b_{12} \\\\ b_{21} & b_{22} \\end{bmatrix} =\n\\begin{bmatrix}\na_{11}b_{11} & a_{11}b_{12} & a_{12}b_{11} & a_{12}b_{12} \\\\\na_{11}b_{21} & a_{11}b_{22} & a_{12}b_{21} & a_{12}b_{22} \\\\\na_{21}b_{11} & a_{21}b_{12} & a_{22}b_{11} & a_{22}b_{12} \\\\\na_{21}b_{21} & a_{21}b_{22} & a_{22}b_{21} & a_{22}b_{22}\n\\end{bmatrix}\n$$ {#eq-kronecker-example}\n\n**Basic Properties:**\n\n1. **Dimension**: If **A** is m × n and **B** is p × q, then $\\mathbf{A} \\otimes \\mathbf{B}$ is (mp) × (nq)\n2. **NOT commutative**: $\\mathbf{A} \\otimes \\mathbf{B} \\neq \\mathbf{B} \\otimes \\mathbf{A}$ (generally)\n3. **Associative**: $(\\mathbf{A} \\otimes \\mathbf{B}) \\otimes \\mathbf{C} = \\mathbf{A} \\otimes (\\mathbf{B} \\otimes \\mathbf{C})$\n4. **Distributive**: $(\\mathbf{A} + \\mathbf{B}) \\otimes \\mathbf{C} = \\mathbf{A} \\otimes \\mathbf{C} + \\mathbf{B} \\otimes \\mathbf{C}$\n\n**R Implementation:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simple 2×2 matrices\nA <- matrix(c(1, 2, 3, 4), nrow = 2, ncol = 2, byrow = TRUE)\nB <- matrix(c(5, 6, 7, 8), nrow = 2, ncol = 2, byrow = TRUE)\n\ncat(\"Matrix A (2×2):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMatrix A (2×2):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nMatrix B (2×2):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nMatrix B (2×2):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(B)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    5    6\n[2,]    7    8\n```\n\n\n:::\n\n```{.r .cell-code}\n# Kronecker product\nC <- kronecker(A, B)  # or A %x% B\n\ncat(\"\\nA ⊗ B (4×4):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nA ⊗ B (4×4):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(C)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3] [,4]\n[1,]    5    6   10   12\n[2,]    7    8   14   16\n[3,]   15   18   20   24\n[4,]   21   24   28   32\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"\\nDimension: %d×%d matrix (A is 2×2, B is 2×2 → product is 4×4)\\n\",\n            nrow(C), ncol(C)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nDimension: 4×4 matrix (A is 2×2, B is 2×2 → product is 4×4)\n```\n\n\n:::\n:::\n\n\n**Non-Commutativity:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# A ⊗ B\nAkronB <- A %x% B\n\n# B ⊗ A\nBkronA <- B %x% A\n\ncat(\"A ⊗ B:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nA ⊗ B:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(AkronB)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3] [,4]\n[1,]    5    6   10   12\n[2,]    7    8   14   16\n[3,]   15   18   20   24\n[4,]   21   24   28   32\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nB ⊗ A:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nB ⊗ A:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(BkronA)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3] [,4]\n[1,]    5   10    6   12\n[2,]   15   20   18   24\n[3,]    7   14    8   16\n[4,]   21   28   24   32\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nA ⊗ B ≠ B ⊗ A (not commutative!)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nA ⊗ B ≠ B ⊗ A (not commutative!)\n```\n\n\n:::\n:::\n\n\n**Kronecker Product with Vectors:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\na <- c(1, 2)\nb <- c(10, 20, 30)\n\n# Kronecker product of vectors\nc <- kronecker(a, b)\n\ncat(\"Vector a:\", a, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nVector a: 1 2 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Vector b:\", b, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nVector b: 10 20 30 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\na ⊗ b:\", c, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\na ⊗ b: 10 20 30 20 40 60 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nResult is a vector of length 2 × 3 = 6\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nResult is a vector of length 2 × 3 = 6\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Interpretation: [1*10, 1*20, 1*30, 2*10, 2*20, 2*30]\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nInterpretation: [1*10, 1*20, 1*30, 2*10, 2*20, 2*30]\n```\n\n\n:::\n:::\n\n\n**Identity Property:**\n\n$\\mathbf{I}_m \\otimes \\mathbf{I}_n = \\mathbf{I}_{mn}$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nI2 <- diag(2)\nI3 <- diag(3)\n\nI_kron <- I2 %x% I3\n\ncat(\"I₂ ⊗ I₃:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nI₂ ⊗ I₃:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(I_kron)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]    1    0    0    0    0    0\n[2,]    0    1    0    0    0    0\n[3,]    0    0    1    0    0    0\n[4,]    0    0    0    1    0    0\n[5,]    0    0    0    0    1    0\n[6,]    0    0    0    0    0    1\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nResult is I₆ (6×6 identity matrix)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nResult is I₆ (6×6 identity matrix)\n```\n\n\n:::\n\n```{.r .cell-code}\nall.equal(I_kron, diag(6))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\n---\n\n### Kronecker Product Rules {#sec-kronecker-rules}\n\nThe Kronecker product has many useful algebraic properties that simplify calculations in multi-trait models.\n\n**Comprehensive Property Table:**\n\n| Property | Rule | Condition |\n|----------|------|-----------|\n| **Transpose** | $(\\mathbf{A} \\otimes \\mathbf{B})' = \\mathbf{A}' \\otimes \\mathbf{B}'$ | Always |\n| **Product** | $(\\mathbf{A} \\otimes \\mathbf{B})(\\mathbf{C} \\otimes \\mathbf{D}) = (\\mathbf{AC}) \\otimes (\\mathbf{BD})$ | Dimensions compatible |\n| **Sum** | $(\\mathbf{A} + \\mathbf{B}) \\otimes \\mathbf{C} = \\mathbf{A} \\otimes \\mathbf{C} + \\mathbf{B} \\otimes \\mathbf{C}$ | A, B same size |\n| **Scalar** | $(c\\mathbf{A}) \\otimes \\mathbf{B} = \\mathbf{A} \\otimes (c\\mathbf{B}) = c(\\mathbf{A} \\otimes \\mathbf{B})$ | Always |\n| **Inverse** | $(\\mathbf{A} \\otimes \\mathbf{B})^{-1} = \\mathbf{A}^{-1} \\otimes \\mathbf{B}^{-1}$ | A, B invertible |\n| **Trace** | $\\text{tr}(\\mathbf{A} \\otimes \\mathbf{B}) = \\text{tr}(\\mathbf{A}) \\cdot \\text{tr}(\\mathbf{B})$ | A, B square |\n| **Determinant** | $\\det(\\mathbf{A} \\otimes \\mathbf{B}) = \\det(\\mathbf{A})^q \\cdot \\det(\\mathbf{B})^m$ | A (m×m), B (q×q) |\n| **Rank** | $r(\\mathbf{A} \\otimes \\mathbf{B}) = r(\\mathbf{A}) \\cdot r(\\mathbf{B})$ | Always |\n| **Eigenvalues** | If λ eigenvalue of A, μ eigenvalue of B, then λμ eigenvalue of A ⊗ B | A, B square |\n\n**Verification of Key Properties:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA <- matrix(c(2, 1, 1, 2), nrow = 2, ncol = 2, byrow = TRUE)\nB <- matrix(c(3, 0, 0, 1), nrow = 2, ncol = 2, byrow = TRUE)\n\ncat(\"Matrix A (2×2):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMatrix A (2×2):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    2    1\n[2,]    1    2\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nMatrix B (2×2):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nMatrix B (2×2):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(B)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    3    0\n[2,]    0    1\n```\n\n\n:::\n\n```{.r .cell-code}\n# Property 1: Transpose\nAkronB <- A %x% B\ntrans_AkronB <- t(A %x% B)\nA_trans_kron_B_trans <- t(A) %x% t(B)\n\ncat(\"\\n1. Transpose property:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n1. Transpose property:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"(A ⊗ B)' equals A' ⊗ B'?\", all.equal(trans_AkronB, A_trans_kron_B_trans), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(A ⊗ B)' equals A' ⊗ B'? TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\n# Property 2: Trace\ntr_AkronB <- sum(diag(A %x% B))\ntr_A_times_tr_B <- sum(diag(A)) * sum(diag(B))\n\ncat(\"\\n2. Trace property:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n2. Trace property:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"tr(A ⊗ B) = %.1f\\n\", tr_AkronB))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntr(A ⊗ B) = 16.0\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"tr(A) × tr(B) = %.1f × %.1f = %.1f\\n\",\n            sum(diag(A)), sum(diag(B)), tr_A_times_tr_B))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntr(A) × tr(B) = 4.0 × 4.0 = 16.0\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Equal?\", all.equal(tr_AkronB, tr_A_times_tr_B), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEqual? TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\n# Property 3: Determinant\ndet_AkronB <- det(A %x% B)\ndet_formula <- det(A)^2 * det(B)^2  # A is 2×2, B is 2×2\n\ncat(\"\\n3. Determinant property:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n3. Determinant property:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"det(A ⊗ B) = %.1f\\n\", det_AkronB))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ndet(A ⊗ B) = 81.0\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"det(A)² × det(B)² = %.1f² × %.1f² = %.1f\\n\",\n            det(A), det(B), det_formula))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ndet(A)² × det(B)² = 3.0² × 3.0² = 81.0\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Equal?\", all.equal(det_AkronB, det_formula), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEqual? TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\n# Property 4: Rank\nrank_AkronB <- qr(A %x% B)$rank\nrank_product <- qr(A)$rank * qr(B)$rank\n\ncat(\"\\n4. Rank property:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n4. Rank property:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"r(A ⊗ B) = %d\\n\", rank_AkronB))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nr(A ⊗ B) = 4\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"r(A) × r(B) = %d × %d = %d\\n\",\n            qr(A)$rank, qr(B)$rank, rank_product))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nr(A) × r(B) = 2 × 2 = 4\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Equal?\", rank_AkronB == rank_product, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEqual? TRUE \n```\n\n\n:::\n:::\n\n\n**Inverse Property (Most Important!):**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA <- matrix(c(4, 1, 1, 3), nrow = 2, ncol = 2, byrow = TRUE)\nB <- matrix(c(2, 0, 0, 5), nrow = 2, ncol = 2, byrow = TRUE)\n\ncat(\"Invertible matrix A:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nInvertible matrix A:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    4    1\n[2,]    1    3\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nInvertible matrix B:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nInvertible matrix B:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(B)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    2    0\n[2,]    0    5\n```\n\n\n:::\n\n```{.r .cell-code}\n# Form A ⊗ B\nAkronB <- A %x% B\n\n# Inverse of A ⊗ B (direct calculation)\nAkronB_inv <- solve(A %x% B)\n\n# Using property: (A ⊗ B)^(-1) = A^(-1) ⊗ B^(-1)\nAinv_kron_Binv <- solve(A) %x% solve(B)\n\ncat(\"\\n(A ⊗ B)^(-1) computed directly:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n(A ⊗ B)^(-1) computed directly:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(AkronB_inv, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        [,1]    [,2]    [,3]    [,4]\n[1,]  0.1364  0.0000 -0.0455  0.0000\n[2,]  0.0000  0.0545  0.0000 -0.0182\n[3,] -0.0455  0.0000  0.1818  0.0000\n[4,]  0.0000 -0.0182  0.0000  0.0727\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nA^(-1) ⊗ B^(-1):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nA^(-1) ⊗ B^(-1):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(Ainv_kron_Binv, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        [,1]    [,2]    [,3]    [,4]\n[1,]  0.1364  0.0000 -0.0455  0.0000\n[2,]  0.0000  0.0545  0.0000 -0.0182\n[3,] -0.0455  0.0000  0.1818  0.0000\n[4,]  0.0000 -0.0182  0.0000  0.0727\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nVerify they're equal:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVerify they're equal:\n```\n\n\n:::\n\n```{.r .cell-code}\nall.equal(AkronB_inv, Ainv_kron_Binv)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nThis property is CRUCIAL for inverting multi-trait covariance matrices!\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nThis property is CRUCIAL for inverting multi-trait covariance matrices!\n```\n\n\n:::\n:::\n\n\n**Product Rule (Mixed-Product Property):**\n\n$(\\mathbf{A} \\otimes \\mathbf{B})(\\mathbf{C} \\otimes \\mathbf{D}) = (\\mathbf{AC}) \\otimes (\\mathbf{BD})$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA <- matrix(c(1, 2, 3, 4), nrow = 2, ncol = 2, byrow = TRUE)\nB <- matrix(c(5, 6, 7, 8), nrow = 2, ncol = 2, byrow = TRUE)\nC <- matrix(c(2, 0, 0, 1), nrow = 2, ncol = 2, byrow = TRUE)\nD <- matrix(c(1, 1, 1, 1), nrow = 2, ncol = 2, byrow = TRUE)\n\n# Left side: (A ⊗ B)(C ⊗ D)\nleft_side <- (A %x% B) %*% (C %x% D)\n\n# Right side: (AC) ⊗ (BD)\nright_side <- (A %*% C) %x% (B %*% D)\n\ncat(\"(A ⊗ B)(C ⊗ D) equals (AC) ⊗ (BD)?\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(A ⊗ B)(C ⊗ D) equals (AC) ⊗ (BD)?\n```\n\n\n:::\n\n```{.r .cell-code}\nall.equal(left_side, right_side)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nThis property simplifies matrix multiplications dramatically!\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nThis property simplifies matrix multiplications dramatically!\n```\n\n\n:::\n:::\n\n\n**Eigenvalue Property:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA <- matrix(c(3, 1, 1, 3), nrow = 2, ncol = 2, byrow = TRUE)\nB <- matrix(c(2, 0, 0, 5), nrow = 2, ncol = 2, byrow = TRUE)\n\n# Eigenvalues of A and B\neigen_A <- eigen(A)$values\neigen_B <- eigen(B)$values\n\ncat(\"Eigenvalues of A:\", round(eigen_A, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEigenvalues of A: 4 2 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Eigenvalues of B:\", round(eigen_B, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEigenvalues of B: 5 2 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Eigenvalues of A ⊗ B should be all products λ_i * μ_j\nAkronB <- A %x% B\neigen_AkronB <- eigen(AkronB)$values\n\ncat(\"\\nEigenvalues of A ⊗ B:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nEigenvalues of A ⊗ B:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(sort(eigen_AkronB, decreasing = TRUE), 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 20 10  8  4\n```\n\n\n:::\n\n```{.r .cell-code}\n# All products\nall_products <- sort(c(outer(eigen_A, eigen_B)), decreasing = TRUE)\ncat(\"\\nAll products λ_i × μ_j:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nAll products λ_i × μ_j:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(all_products, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 20 10  8  4\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nThey match! ✓\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nThey match! ✓\n```\n\n\n:::\n:::\n\n\n**Vec Operator Property:**\n\nThe vec operator stacks matrix columns into a vector. Key property:\n\n$\\text{vec}(\\mathbf{ABC}) = (\\mathbf{C}' \\otimes \\mathbf{A})\\text{vec}(\\mathbf{B})$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Small example\nA <- matrix(c(1, 2, 3, 4), nrow = 2, ncol = 2, byrow = TRUE)\nB <- matrix(c(5, 6, 7, 8), nrow = 2, ncol = 2, byrow = TRUE)\nC <- matrix(c(2, 0, 0, 1), nrow = 2, ncol = 2, byrow = TRUE)\n\n# Compute ABC\nABC <- A %*% B %*% C\n\n# vec(ABC) - stack columns\nvec_ABC <- c(ABC)\n\ncat(\"Matrix ABC:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMatrix ABC:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(ABC)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]   38   22\n[2,]   86   50\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nvec(ABC) (stacking columns):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nvec(ABC) (stacking columns):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(vec_ABC)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 38 86 22 50\n```\n\n\n:::\n\n```{.r .cell-code}\n# Using property: vec(ABC) = (C' ⊗ A) vec(B)\nvec_B <- c(B)\nresult <- (t(C) %x% A) %*% vec_B\n\ncat(\"\\n(C' ⊗ A) vec(B):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n(C' ⊗ A) vec(B):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(result)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]\n[1,]   38\n[2,]   86\n[3,]   22\n[4,]   50\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nVerify they're equal:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVerify they're equal:\n```\n\n\n:::\n\n```{.r .cell-code}\nall.equal(vec_ABC, c(result))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nThis property is used in vectorizing matrix equations\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nThis property is used in vectorizing matrix equations\n```\n\n\n:::\n:::\n\n\n:::{.callout-tip}\n## Key Properties for Linear Models\n\nThe most important properties for linear models are:\n\n1. **Inverse property**: $(\\mathbf{A} \\otimes \\mathbf{B})^{-1} = \\mathbf{A}^{-1} \\otimes \\mathbf{B}^{-1}$\n   - Used for inverting multi-trait covariance matrices\n\n2. **Mixed-product property**: $(\\mathbf{A} \\otimes \\mathbf{B})(\\mathbf{C} \\otimes \\mathbf{D}) = (\\mathbf{AC}) \\otimes (\\mathbf{BD})$\n   - Simplifies matrix multiplications in mixed model equations\n\n3. **Vec operator**: $\\text{vec}(\\mathbf{ABC}) = (\\mathbf{C}' \\otimes \\mathbf{A})\\text{vec}(\\mathbf{B})$\n   - Used in derivative calculations and equation manipulation\n:::\n\n---\n\n### Applications in Linear Models {#sec-kronecker-applications}\n\nKronecker products are essential for structuring variance-covariance matrices in advanced linear models.\n\n**1. Multi-Trait Genetic Evaluation:**\n\nFor t traits measured on n animals, the variance-covariance structure is often:\n\n$$\n\\mathbf{Var}(\\mathbf{y}) = \\mathbf{G} \\otimes \\mathbf{A}\n$$ {#eq-multitrait-variance}\n\nwhere:\n- **G** is t × t genetic covariance matrix between traits\n- **A** is n × n additive relationship matrix between animals\n- Result is (nt) × (nt) covariance matrix\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simple example: 2 traits, 3 animals\n# Genetic covariance matrix (2×2)\nG <- matrix(c(\n  10,  3,   # Var(trait1) = 10, Cov(trait1, trait2) = 3\n   3,  5    # Var(trait2) = 5\n), nrow = 2, ncol = 2, byrow = TRUE)\n\n# Relationship matrix (3×3) - simplified\nA <- matrix(c(\n  1.0, 0.5, 0.0,  # Animal 1 related to itself (1.0) and animal 2 (0.5)\n  0.5, 1.0, 0.25, # Animal 2 related to animals 1, 2, 3\n  0.0, 0.25, 1.0  # Animal 3\n), nrow = 3, ncol = 3, byrow = TRUE)\n\ncat(\"Genetic covariance matrix G (2×2):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nGenetic covariance matrix G (2×2):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(G)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]   10    3\n[2,]    3    5\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nRelationship matrix A (3×3):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nRelationship matrix A (3×3):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]  1.0 0.50 0.00\n[2,]  0.5 1.00 0.25\n[3,]  0.0 0.25 1.00\n```\n\n\n:::\n\n```{.r .cell-code}\n# Full variance-covariance matrix\nV <- G %x% A\n\ncat(\"\\nFull covariance matrix V = G ⊗ A (6×6):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nFull covariance matrix V = G ⊗ A (6×6):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(V, 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]  [,2]  [,3] [,4] [,5] [,6]\n[1,] 10.0  5.00  0.00  3.0 1.50 0.00\n[2,]  5.0 10.00  2.50  1.5 3.00 0.75\n[3,]  0.0  2.50 10.00  0.0 0.75 3.00\n[4,]  3.0  1.50  0.00  5.0 2.50 0.00\n[5,]  1.5  3.00  0.75  2.5 5.00 1.25\n[6,]  0.0  0.75  3.00  0.0 1.25 5.00\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nInterpretation:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nInterpretation:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  - 6×6 matrix for 3 animals × 2 traits\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  - 6×6 matrix for 3 animals × 2 traits\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  - Block structure reflects genetic correlations\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  - Block structure reflects genetic correlations\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  - Animals related by pedigree have correlated breeding values\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  - Animals related by pedigree have correlated breeding values\n```\n\n\n:::\n:::\n\n\n**2. Repeated Measures / Longitudinal Data:**\n\nFor measurements repeated over time, the covariance structure might be:\n\n$$\n\\mathbf{Var}(\\mathbf{y}) = \\mathbf{I}_n \\otimes \\mathbf{\\Sigma}_t\n$$ {#eq-repeated-measures}\n\nwhere:\n- **I**~n~ is n × n identity (animals are independent)\n- **Σ**~t~ is t × t covariance matrix across time points\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 4 animals, 3 time points each\nn_animals <- 4\nn_times <- 3\n\n# Time covariance (AR(1) structure: correlation decreases with time)\nrho <- 0.7  # Autocorrelation\nSigma_t <- matrix(c(\n  1.0,  rho,  rho^2,\n  rho,  1.0,  rho,\n  rho^2, rho, 1.0\n), nrow = 3, ncol = 3, byrow = TRUE)\n\ncat(\"Time covariance matrix Σ_t (3×3):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTime covariance matrix Σ_t (3×3):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(Sigma_t, 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,] 1.00  0.7 0.49\n[2,] 0.70  1.0 0.70\n[3,] 0.49  0.7 1.00\n```\n\n\n:::\n\n```{.r .cell-code}\n# Full covariance: animals independent, but measurements within animal correlated\nV <- diag(n_animals) %x% Sigma_t\n\ncat(\"\\nFull covariance V = I_4 ⊗ Σ_t (12×12):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nFull covariance V = I_4 ⊗ Σ_t (12×12):\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"(Showing first 6×6 block)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Showing first 6×6 block)\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(V[1:6, 1:6], 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,] 1.00  0.7 0.49 0.00  0.0 0.00\n[2,] 0.70  1.0 0.70 0.00  0.0 0.00\n[3,] 0.49  0.7 1.00 0.00  0.0 0.00\n[4,] 0.00  0.0 0.00 1.00  0.7 0.49\n[5,] 0.00  0.0 0.00 0.70  1.0 0.70\n[6,] 0.00  0.0 0.00 0.49  0.7 1.00\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nBlock-diagonal structure: each 3×3 block is Σ_t for one animal\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nBlock-diagonal structure: each 3×3 block is Σ_t for one animal\n```\n\n\n:::\n:::\n\n\n**3. Multi-Trait Animal Model Preview:**\n\nThe mixed model equations for multi-trait evaluation are:\n\n$$\n\\begin{bmatrix}\n\\mathbf{X}'(\\mathbf{R} \\otimes \\mathbf{I})^{-1}\\mathbf{X} & \\mathbf{X}'(\\mathbf{R} \\otimes \\mathbf{I})^{-1}\\mathbf{Z} \\\\\n\\mathbf{Z}'(\\mathbf{R} \\otimes \\mathbf{I})^{-1}\\mathbf{X} & \\mathbf{Z}'(\\mathbf{R} \\otimes \\mathbf{I})^{-1}\\mathbf{Z} + (\\mathbf{G} \\otimes \\mathbf{A})^{-1}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\hat{\\mathbf{b}} \\\\\n\\hat{\\mathbf{u}}\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\mathbf{X}'(\\mathbf{R} \\otimes \\mathbf{I})^{-1}\\mathbf{y} \\\\\n\\mathbf{Z}'(\\mathbf{R} \\otimes \\mathbf{I})^{-1}\\mathbf{y}\n\\end{bmatrix}\n$$ {#eq-multitrait-mme}\n\nThe inverse property is crucial: $(\\mathbf{G} \\otimes \\mathbf{A})^{-1} = \\mathbf{G}^{-1} \\otimes \\mathbf{A}^{-1}$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simplified example: 2 traits, 4 animals\n# Genetic covariance\nG <- matrix(c(10, 3, 3, 5), nrow = 2, ncol = 2, byrow = TRUE)\n\n# Relationship matrix (4 animals)\nA <- matrix(c(\n  1.00, 0.50, 0.25, 0.00,\n  0.50, 1.00, 0.50, 0.25,\n  0.25, 0.50, 1.00, 0.50,\n  0.00, 0.25, 0.50, 1.00\n), nrow = 4, ncol = 4, byrow = TRUE)\n\ncat(\"G (2×2 genetic covariance):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nG (2×2 genetic covariance):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(G)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]   10    3\n[2,]    3    5\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nA (4×4 relationship matrix):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nA (4×4 relationship matrix):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3] [,4]\n[1,] 1.00 0.50 0.25 0.00\n[2,] 0.50 1.00 0.50 0.25\n[3,] 0.25 0.50 1.00 0.50\n[4,] 0.00 0.25 0.50 1.00\n```\n\n\n:::\n\n```{.r .cell-code}\n# Form G ⊗ A\nG_kron_A <- G %x% A\n\ncat(\"\\nG ⊗ A (8×8):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nG ⊗ A (8×8):\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"(Showing first 4×4 block)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Showing first 4×4 block)\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(G_kron_A[1:4, 1:4], 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3] [,4]\n[1,] 10.0  5.0  2.5  0.0\n[2,]  5.0 10.0  5.0  2.5\n[3,]  2.5  5.0 10.0  5.0\n[4,]  0.0  2.5  5.0 10.0\n```\n\n\n:::\n\n```{.r .cell-code}\n# Inverse using property\nG_inv <- solve(G)\nA_inv <- solve(A)\ninv_direct <- solve(G_kron_A)\ninv_property <- G_inv %x% A_inv\n\ncat(\"\\nVerify (G ⊗ A)^(-1) = G^(-1) ⊗ A^(-1):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVerify (G ⊗ A)^(-1) = G^(-1) ⊗ A^(-1):\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Max difference:\", max(abs(inv_direct - inv_property)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMax difference: 2.22e-16 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nThis property makes inversion computationally feasible!\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nThis property makes inversion computationally feasible!\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Instead of inverting 8×8, we invert 2×2 and 4×4 separately.\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nInstead of inverting 8×8, we invert 2×2 and 4×4 separately.\n```\n\n\n:::\n:::\n\n\n**4. Residual Covariance Structure:**\n\nFor heterogeneous residual variances across traits:\n\n$$\n\\mathbf{R} = \\mathbf{R}_0 \\otimes \\mathbf{I}_n\n$$ {#eq-residual-structure}\n\nwhere **R**~0~ is the residual covariance between traits.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 3 traits with different residual variances and covariances\nR0 <- matrix(c(\n  4.0, 0.5, 0.2,  # Trait 1: var = 4.0\n  0.5, 2.0, 0.3,  # Trait 2: var = 2.0\n  0.2, 0.3, 3.0   # Trait 3: var = 3.0\n), nrow = 3, ncol = 3, byrow = TRUE)\n\nn <- 5  # 5 animals\n\ncat(\"Residual covariance between traits R₀ (3×3):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nResidual covariance between traits R₀ (3×3):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(R0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]  4.0  0.5  0.2\n[2,]  0.5  2.0  0.3\n[3,]  0.2  0.3  3.0\n```\n\n\n:::\n\n```{.r .cell-code}\n# Full residual covariance\nR <- R0 %x% diag(n)\n\ncat(\"\\nFull residual covariance R = R₀ ⊗ I₅ (15×15):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nFull residual covariance R = R₀ ⊗ I₅ (15×15):\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"(Showing first 6×6 block)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Showing first 6×6 block)\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(R[1:6, 1:6], 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]  4.0    0    0    0    0  0.5\n[2,]  0.0    4    0    0    0  0.0\n[3,]  0.0    0    4    0    0  0.0\n[4,]  0.0    0    0    4    0  0.0\n[5,]  0.0    0    0    0    4  0.0\n[6,]  0.5    0    0    0    0  2.0\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nStructure: Traits are correlated, but residuals across animals are independent\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nStructure: Traits are correlated, but residuals across animals are independent\n```\n\n\n:::\n:::\n\n\n**5. Kronecker Structure in Design Matrices:**\n\nFor factorial designs, the design matrix can often be written using Kronecker products.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 2×3 factorial: 2 levels of factor A, 3 levels of factor B\n# 2 replicates per cell\n\n# Factor A design (2 levels)\nX_A <- matrix(c(1, 0,\n                0, 1), nrow = 2, ncol = 2, byrow = TRUE)\n\n# Factor B design (3 levels)\nX_B <- matrix(c(1, 0, 0,\n                0, 1, 0,\n                0, 0, 1), nrow = 3, ncol = 3, byrow = TRUE)\n\ncat(\"Factor A design:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFactor A design:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(X_A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nFactor B design:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nFactor B design:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(X_B)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n```\n\n\n:::\n\n```{.r .cell-code}\n# Full factorial design (one observation per cell)\nX_full <- X_A %x% X_B\n\ncat(\"\\nFull factorial design X_A ⊗ X_B (6×6):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nFull factorial design X_A ⊗ X_B (6×6):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(X_full)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]    1    0    0    0    0    0\n[2,]    0    1    0    0    0    0\n[3,]    0    0    1    0    0    0\n[4,]    0    0    0    1    0    0\n[5,]    0    0    0    0    1    0\n[6,]    0    0    0    0    0    1\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nEach row represents one cell in the 2×3 factorial\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nEach row represents one cell in the 2×3 factorial\n```\n\n\n:::\n:::\n\n\n**Livestock Example - Multi-Trait Dairy Evaluation:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simplified dairy evaluation: 2 traits (milk yield, fat %), 6 cows\nn_cows <- 6\nn_traits <- 2\n\n# Genetic covariance matrix\n# Milk and fat % are genetically correlated\nG <- matrix(c(\n  100,  -5,   # Var(milk) = 100, Cov = -5 (negative: higher milk → lower fat %)\n   -5,   4    # Var(fat%) = 4\n), nrow = 2, ncol = 2, byrow = TRUE)\n\n# Pedigree relationship (6 cows, 3 half-sib families)\nA <- matrix(c(\n  1.00, 0.00, 0.00, 0.25, 0.25, 0.00,\n  0.00, 1.00, 0.50, 0.25, 0.25, 0.00,\n  0.00, 0.50, 1.00, 0.25, 0.25, 0.00,\n  0.25, 0.25, 0.25, 1.00, 0.50, 0.50,\n  0.25, 0.25, 0.25, 0.50, 1.00, 0.50,\n  0.00, 0.00, 0.00, 0.50, 0.50, 1.00\n), nrow = 6, ncol = 6, byrow = TRUE)\n\ncat(\"Genetic covariance G:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nGenetic covariance G:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(G)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]  100   -5\n[2,]   -5    4\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nAdditive relationship matrix A (6 cows):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nAdditive relationship matrix A (6 cows):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(A, 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,] 1.00 0.00 0.00 0.25 0.25  0.0\n[2,] 0.00 1.00 0.50 0.25 0.25  0.0\n[3,] 0.00 0.50 1.00 0.25 0.25  0.0\n[4,] 0.25 0.25 0.25 1.00 0.50  0.5\n[5,] 0.25 0.25 0.25 0.50 1.00  0.5\n[6,] 0.00 0.00 0.00 0.50 0.50  1.0\n```\n\n\n:::\n\n```{.r .cell-code}\n# Variance of breeding values\nVar_u <- G %x% A\n\ncat(\"\\nVariance of breeding values (12×12):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVariance of breeding values (12×12):\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"(Showing correlation structure for first 4 observations)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Showing correlation structure for first 4 observations)\n```\n\n\n:::\n\n```{.r .cell-code}\n# Convert to correlation for easier interpretation\nD <- diag(1/sqrt(diag(Var_u)))\nCor_u <- D %*% Var_u %*% D\n\nprint(round(Cor_u[1:4, 1:4], 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3] [,4]\n[1,] 1.00 0.00 0.00 0.25\n[2,] 0.00 1.00 0.50 0.25\n[3,] 0.00 0.50 1.00 0.25\n[4,] 0.25 0.25 0.25 1.00\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nInterpretation:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nInterpretation:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  - Breeding values for same cow's traits are correlated (genetic correlation)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  - Breeding values for same cow's traits are correlated (genetic correlation)\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  - Breeding values for related cows are correlated (pedigree)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  - Breeding values for related cows are correlated (pedigree)\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  - Kronecker structure captures both effects simultaneously\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  - Kronecker structure captures both effects simultaneously\n```\n\n\n:::\n:::\n\n\n:::{.callout-important}\n## Why Kronecker Products Matter\n\nIn advanced linear models, Kronecker products:\n\n1. **Provide compact notation** for complex covariance structures\n2. **Enable efficient inversion**: $(\\mathbf{A} \\otimes \\mathbf{B})^{-1} = \\mathbf{A}^{-1} \\otimes \\mathbf{B}^{-1}$\n3. **Simplify mixed model equations** for multi-trait analysis\n4. **Extend naturally** to Animal models and genetic evaluation\n5. **Reduce storage** requirements through structured sparsity\n\nWithout Kronecker products, multi-trait genetic evaluation would be computationally infeasible!\n:::\n\n:::{.callout-note}\n## Cross-References\n\nKronecker products are foundational for:\n\n- **Week 14**: Preview of mixed models with random effects\n- **Future courses**: Multi-trait Animal models\n- **Future courses**: Genomic selection with multiple traits\n- **Section @sec-computation**: Efficient computation with Kronecker structure\n:::\n\n---\n\n### Computational Considerations {#sec-kronecker-computation}\n\n**CRITICAL RULE: Never Form Kronecker Products Explicitly!**\n\nFor large matrices, explicitly computing $\\mathbf{A} \\otimes \\mathbf{B}$ is:\n- **Memory intensive**: (m×n) ⊗ (p×q) creates (mp) × (nq) matrix\n- **Computationally expensive**: O(mnpq) operations\n- **Usually unnecessary**: Can work implicitly using properties\n\n**Example of the Problem:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Small matrices\nA <- diag(100)  # 100×100\nB <- diag(50)   # 50×50\n\n# Their Kronecker product\n# Would be 5000×5000 = 25 million elements!\n\ncat(sprintf(\"A: %d×%d = %s elements\\n\",\n            nrow(A), ncol(A), format(nrow(A)*ncol(A), big.mark=\",\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nA: 100×100 = 10,000 elements\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"B: %d×%d = %s elements\\n\",\n            nrow(B), ncol(B), format(nrow(B)*ncol(B), big.mark=\",\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nB: 50×50 = 2,500 elements\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"A ⊗ B: %d×%d = %s elements!\\n\",\n            nrow(A)*nrow(B), ncol(A)*ncol(B),\n            format(nrow(A)*nrow(B)*ncol(A)*ncol(B), big.mark=\",\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nA ⊗ B: 5000×5000 = 25,000,000 elements!\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nForming explicitly would use excessive memory and time.\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nForming explicitly would use excessive memory and time.\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Solution: Use implicit operations!\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSolution: Use implicit operations!\n```\n\n\n:::\n:::\n\n\n**Strategy 1: Implicit Matrix-Vector Multiplication**\n\nInstead of forming $\\mathbf{C} = \\mathbf{A} \\otimes \\mathbf{B}$ and computing $\\mathbf{Cx}$, use:\n\n$$\n(\\mathbf{A} \\otimes \\mathbf{B})\\mathbf{x} = \\text{vec}(\\mathbf{BXA}')\n$$ {#eq-kronecker-matvec}\n\nwhere **X** is the matrix formed by \"unvectorizing\" **x**.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Smaller example for demonstration\nA <- matrix(c(2, 1, 1, 3), nrow = 2, ncol = 2, byrow = TRUE)\nB <- matrix(c(1, 2, 3, 4), nrow = 2, ncol = 2, byrow = TRUE)\nx <- c(1, 2, 3, 4)  # Vector to multiply\n\n# Method 1: Explicit (bad for large matrices!)\nC_explicit <- A %x% B\nresult_explicit <- C_explicit %*% x\n\ncat(\"Explicit method result:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nExplicit method result:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(result_explicit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]\n[1,]   21\n[2,]   47\n[3,]   38\n[4,]   86\n```\n\n\n:::\n\n```{.r .cell-code}\n# Method 2: Implicit (efficient!)\nX <- matrix(x, nrow = nrow(B), ncol = nrow(A))  # Reshape x\nresult_implicit <- c(B %*% X %*% t(A))  # vec(BXA')\n\ncat(\"\\nImplicit method result:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nImplicit method result:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(result_implicit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 21 47 38 86\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nVerify they're equal:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVerify they're equal:\n```\n\n\n:::\n\n```{.r .cell-code}\nall.equal(c(result_explicit), result_implicit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nImplicit method avoids forming the 4×4 matrix A ⊗ B!\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nImplicit method avoids forming the 4×4 matrix A ⊗ B!\n```\n\n\n:::\n:::\n\n\n**Strategy 2: Exploit Inverse Property**\n\nInstead of forming $(\\mathbf{A} \\otimes \\mathbf{B})^{-1}$, use:\n\n$$\n(\\mathbf{A} \\otimes \\mathbf{B})^{-1} = \\mathbf{A}^{-1} \\otimes \\mathbf{B}^{-1}\n$$ {#eq-kronecker-inv-property}\n\nInvert **A** and **B** separately (much cheaper!).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA <- matrix(c(4, 1, 1, 3), nrow = 2, ncol = 2, byrow = TRUE)\nB <- matrix(c(5, 2, 2, 2), nrow = 2, ncol = 2, byrow = TRUE)\n\ncat(\"Size comparison:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSize comparison:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"Inverting A (2×2): ~%d operations\\n\", 2^3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nInverting A (2×2): ~8 operations\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"Inverting B (2×2): ~%d operations\\n\", 2^3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nInverting B (2×2): ~8 operations\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"Total for A^(-1) ⊗ B^(-1): ~%d operations\\n\", 2^3 + 2^3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTotal for A^(-1) ⊗ B^(-1): ~16 operations\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"\\nInverting A ⊗ B (4×4) directly: ~%d operations\\n\", 4^3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nInverting A ⊗ B (4×4) directly: ~64 operations\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nSavings factor: \", 4^3 / (2^3 + 2^3), \"×\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSavings factor:  4 ×\n```\n\n\n:::\n\n```{.r .cell-code}\n# For larger matrices, savings are enormous\nn_A <- 100\nn_B <- 50\nn_kron <- n_A * n_B\n\ncat(sprintf(\"\\nFor 100×100 and 50×50 matrices:\\n\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nFor 100×100 and 50×50 matrices:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"Direct inversion of %d×%d: ~%s operations\\n\",\n            n_kron, n_kron, format(n_kron^3, big.mark=\",\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDirect inversion of 5000×5000: ~1.25e+11 operations\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"Separate inversions: ~%s operations\\n\",\n            format(n_A^3 + n_B^3, big.mark=\",\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSeparate inversions: ~1,125,000 operations\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"Savings: %.0f×\\n\", n_kron^3 / (n_A^3 + n_B^3)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSavings: 111111×\n```\n\n\n:::\n:::\n\n\n**Strategy 3: Solving Linear Systems**\n\nTo solve $(\\mathbf{A} \\otimes \\mathbf{B})\\mathbf{x} = \\mathbf{b}$:\n\n1. Reshape **b** into matrix **B_mat**\n2. Solve $\\mathbf{BYA}' = \\mathbf{B_{mat}}$ for **Y** (use Sylvester equation solvers)\n3. Vectorize **Y** to get **x**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(MASS)  # For ginv if needed\n\n# Small example\nA <- matrix(c(2, 1, 1, 2), nrow = 2, ncol = 2, byrow = TRUE)\nB <- matrix(c(3, 1, 1, 3), nrow = 2, ncol = 2, byrow = TRUE)\nb <- c(1, 2, 3, 4)\n\n# Method 1: Explicit (form A ⊗ B and solve)\nAkronB <- A %x% B\nx_explicit <- solve(AkronB, b)\n\ncat(\"Explicit solution:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nExplicit solution:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(x_explicit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.12500  0.04167  0.37500  0.54167\n```\n\n\n:::\n\n```{.r .cell-code}\n# Method 2: Using inverse property\nAinv <- solve(A)\nBinv <- solve(B)\nB_mat <- matrix(b, nrow = nrow(B), ncol = nrow(A))\nY <- Binv %*% B_mat %*% t(Ainv)\nx_implicit <- c(Y)\n\ncat(\"\\nImplicit solution:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nImplicit solution:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(x_implicit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.12500  0.04167  0.37500  0.54167\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nVerify they're equal:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVerify they're equal:\n```\n\n\n:::\n\n```{.r .cell-code}\nall.equal(x_explicit, x_implicit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\n**Strategy 4: Sparse Kronecker Products**\n\nIf **A** or **B** is sparse, the Kronecker product is also sparse. Use sparse matrix representations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Matrix)  # For sparse matrices\n\n# Sparse matrices (diagonal)\nA_sparse <- Diagonal(n = 100, x = 1:100)\nB_sparse <- Diagonal(n = 50, x = 1:50)\n\ncat(\"Sparse matrix A:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSparse matrix A:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"  Dimension: %d×%d\\n\", nrow(A_sparse), ncol(A_sparse)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Dimension: 100×100\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"  Non-zero elements: %d (out of %s)\\n\",\n            length(A_sparse@x), format(nrow(A_sparse)^2, big.mark=\",\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Non-zero elements: 100 (out of 10,000)\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nSparse matrix B:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSparse matrix B:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"  Dimension: %d×%d\\n\", nrow(B_sparse), ncol(B_sparse)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Dimension: 50×50\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"  Non-zero elements: %d (out of %s)\\n\",\n            length(B_sparse@x), format(nrow(B_sparse)^2, big.mark=\",\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Non-zero elements: 50 (out of 2,500)\n```\n\n\n:::\n\n```{.r .cell-code}\n# Kronecker product of sparse matrices is sparse!\n# (But still don't form explicitly unless necessary)\n\ncat(\"\\nA ⊗ B would be:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nA ⊗ B would be:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"  Dimension: %d×%d\\n\",\n            nrow(A_sparse)*nrow(B_sparse), ncol(A_sparse)*ncol(B_sparse)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Dimension: 5000×5000\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"  Non-zero elements: ~%s (sparse!)\\n\",\n            format(length(A_sparse@x) * length(B_sparse@x), big.mark=\",\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Non-zero elements: ~5,000 (sparse!)\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"  Total elements: %s\\n\",\n            format(nrow(A_sparse)*nrow(B_sparse)*ncol(A_sparse)*ncol(B_sparse),\n                   big.mark=\",\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Total elements: 25,000,000\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"  Sparsity: %.6f%%\\n\",\n            100 * length(A_sparse@x) * length(B_sparse@x) /\n              (nrow(A_sparse)*nrow(B_sparse)*ncol(A_sparse)*ncol(B_sparse))))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Sparsity: 0.020000%\n```\n\n\n:::\n:::\n\n\n**Strategy 5: Block Operations**\n\nWhen you must work with Kronecker products, exploit block structure:\n\n$$\n\\mathbf{A} \\otimes \\mathbf{B} = \\begin{bmatrix}\na_{11}\\mathbf{B} & a_{12}\\mathbf{B} & \\cdots \\\\\na_{21}\\mathbf{B} & a_{22}\\mathbf{B} & \\cdots \\\\\n\\vdots & \\vdots & \\ddots\n\\end{bmatrix}\n$$ {#eq-kronecker-blocks}\n\nWork with blocks **a~ij~B** rather than individual elements.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example: Extract specific block\nA <- matrix(1:4, nrow = 2, ncol = 2, byrow = TRUE)\nB <- matrix(5:8, nrow = 2, ncol = 2, byrow = TRUE)\n\ncat(\"Matrix A:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMatrix A:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nMatrix B:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nMatrix B:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(B)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    5    6\n[2,]    7    8\n```\n\n\n:::\n\n```{.r .cell-code}\n# Don't form A ⊗ B\n# Instead, if we need block (i,j), compute a_ij * B directly\n\ni <- 1  # Row block\nj <- 2  # Column block\n\n# Block (1,2) of A ⊗ B = a_12 * B\nblock_12 <- A[i, j] * B\n\ncat(sprintf(\"\\nBlock (%d,%d) of A ⊗ B:\\n\", i, j))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nBlock (1,2) of A ⊗ B:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(block_12)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]   10   12\n[2,]   14   16\n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify by forming explicitly (for small example only!)\nAkronB <- A %x% B\nblock_rows <- ((i-1)*nrow(B) + 1):(i*nrow(B))\nblock_cols <- ((j-1)*ncol(B) + 1):(j*ncol(B))\n\ncat(\"\\nVerify by extracting from explicit A ⊗ B:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVerify by extracting from explicit A ⊗ B:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(AkronB[block_rows, block_cols])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]   10   12\n[2,]   14   16\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nSame result, but block method avoids forming full matrix!\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSame result, but block method avoids forming full matrix!\n```\n\n\n:::\n:::\n\n\n**Livestock Example - Multi-Trait Mixed Model Equations:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Realistic scenario: 2 traits, 1000 animals\nn_traits <- 2\nn_animals <- 1000\n\n# Genetic covariance (2×2)\nG <- matrix(c(10, 3, 3, 5), nrow = 2, ncol = 2, byrow = TRUE)\n\n# We need (G ⊗ A)^(-1) where A is 1000×1000\n# NEVER form the 2000×2000 matrix explicitly!\n\n# Instead, use: (G ⊗ A)^(-1) = G^(-1) ⊗ A^(-1)\n\nG_inv <- solve(G)\n\ncat(\"Genetic covariance G (2×2):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nGenetic covariance G (2×2):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(G)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]   10    3\n[2,]    3    5\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nG^(-1) (2×2):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nG^(-1) (2×2):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(G_inv, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        [,1]    [,2]\n[1,]  0.1220 -0.0732\n[2,] -0.0732  0.2439\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"\\nFor mixed model equations with %d traits and %d animals:\\n\",\n            n_traits, n_animals))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nFor mixed model equations with 2 traits and 1000 animals:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"  - (G ⊗ A)^(-1) would be %d×%d matrix\\n\",\n            n_traits*n_animals, n_traits*n_animals))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  - (G ⊗ A)^(-1) would be 2000×2000 matrix\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"  - Storing explicitly: %.1f GB (at 8 bytes/element)\\n\",\n            (n_traits*n_animals)^2 * 8 / 1e9))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  - Storing explicitly: 0.0 GB (at 8 bytes/element)\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nInstead:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nInstead:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"  - Store G^(-1): %d elements\\n\", n_traits^2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  - Store G^(-1): 4 elements\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"  - Store A^(-1): %d elements (sparse!)\\n\", n_animals^2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  - Store A^(-1): 1000000 elements (sparse!)\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  - Use implicit operations: vec(G^(-1) * A^(-1) * X)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  - Use implicit operations: vec(G^(-1) * A^(-1) * X)\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nThis makes large-scale genetic evaluation computationally feasible!\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nThis makes large-scale genetic evaluation computationally feasible!\n```\n\n\n:::\n:::\n\n\n**Summary of Computational Best Practices:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncat(\"Kronecker Product Computational Best Practices:\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nKronecker Product Computational Best Practices:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"✓ DO:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n✓ DO:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  1. Use inverse property: (A ⊗ B)^(-1) = A^(-1) ⊗ B^(-1)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  1. Use inverse property: (A ⊗ B)^(-1) = A^(-1) ⊗ B^(-1)\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  2. Use implicit matrix-vector products: (A ⊗ B)x = vec(BXA')\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  2. Use implicit matrix-vector products: (A ⊗ B)x = vec(BXA')\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  3. Exploit sparsity when A or B is sparse\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  3. Exploit sparsity when A or B is sparse\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  4. Work with blocks: a_ij * B instead of full matrix\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  4. Work with blocks: a_ij * B instead of full matrix\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  5. Use specialized solvers for Sylvester equations\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  5. Use specialized solvers for Sylvester equations\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"✗ DON'T:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n✗ DON'T:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  1. Form A ⊗ B explicitly for large matrices\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  1. Form A ⊗ B explicitly for large matrices\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  2. Invert A ⊗ B directly - use the inverse property\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  2. Invert A ⊗ B directly - use the inverse property\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  3. Store full dense Kronecker products\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  3. Store full dense Kronecker products\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  4. Ignore block structure\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  4. Ignore block structure\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Key insight: Kronecker structure is for NOTATION and THEORY.\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nKey insight: Kronecker structure is for NOTATION and THEORY.\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"For COMPUTATION, exploit the structure without forming the product!\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFor COMPUTATION, exploit the structure without forming the product!\n```\n\n\n:::\n:::\n\n\n:::{.callout-warning}\n## Never Form Large Kronecker Products Explicitly!\n\nFor **G** (2×2) and **A** (10,000×10,000):\n- **G ⊗ A** would be 20,000 × 20,000 = 400 million elements\n- At 8 bytes per element: **3.2 GB of memory!**\n- And that's just for storage - operations would be even worse\n\n**Solution**: Use properties to avoid explicit formation:\n- Inverse: $(\\mathbf{G} \\otimes \\mathbf{A})^{-1} = \\mathbf{G}^{-1} \\otimes \\mathbf{A}^{-1}$\n- Matrix-vector: $(\\mathbf{G} \\otimes \\mathbf{A})\\mathbf{x} = \\text{vec}(\\mathbf{AXG}')$\n- Exploit sparsity of **A**\n\nThis is why modern genetic evaluation software can handle millions of animals!\n:::\n\n:::{.callout-note}\n## Cross-References\n\nComputational considerations appear in:\n\n- **Week 11**: Numerical stability and efficient algorithms\n- **Week 12**: Sparse matrix methods for large systems\n- **Section @sec-computation**: General computational best practices\n- **Future courses**: Sparse matrix methods in genomic evaluation\n:::\n\n---\n\n## Matrix Identities for Linear Models {#sec-identities}\n\n### Identities for X, H, and Projections {#sec-projection-identities}\n\nThis section provides a comprehensive reference table of matrix identities frequently used in linear models. These identities are essential for deriving least squares properties (Week 5), understanding ANOVA decompositions (Weeks 7-9), and working with rank-deficient models (Week 12).\n\n::: {.callout-note}\n## Why These Identities Matter\n\nThese identities allow you to:\n\n1. Simplify complex matrix expressions\n2. Prove properties of least squares estimators\n3. Derive variance formulas without tedious algebra\n4. Verify computational results\n\n**Pro tip**: Keep this table handy during theoretical derivations!\n:::\n\n#### Key Projection Matrix Identities\n\nLet $\\mathbf{X}$ be an $n \\times p$ design matrix with full column rank, $\\mathbf{H} = \\mathbf{X}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'$ be the hat matrix, and $\\mathbf{M} = \\mathbf{I}_n - \\mathbf{H}$ be the residual maker.\n\n**Table of Fundamental Identities:**\n\n| Identity | Name/Description | Reference |\n|----------|-----------------|-----------|\n| $\\mathbf{H}' = \\mathbf{H}$ | $\\mathbf{H}$ is symmetric | @sec-projection-matrices |\n| $\\mathbf{H}^2 = \\mathbf{H}$ | $\\mathbf{H}$ is idempotent | @sec-projection-matrices |\n| $\\mathbf{H}\\mathbf{X} = \\mathbf{X}$ | Projects $\\mathbf{X}$ onto itself | @sec-projection-matrices |\n| $\\mathbf{M}' = \\mathbf{M}$ | $\\mathbf{M}$ is symmetric | @sec-projection-matrices |\n| $\\mathbf{M}^2 = \\mathbf{M}$ | $\\mathbf{M}$ is idempotent | @sec-projection-matrices |\n| $\\mathbf{M}\\mathbf{X} = \\mathbf{0}$ | Residuals orthogonal to $\\mathbf{X}$ | Week 5 |\n| $\\mathbf{H}\\mathbf{M} = \\mathbf{0}$ | Orthogonal projections | @sec-projection-matrices |\n| $\\mathbf{M}\\mathbf{H} = \\mathbf{0}$ | Orthogonal projections | @sec-projection-matrices |\n| $\\text{tr}(\\mathbf{H}) = p$ | Trace equals rank | @sec-trace |\n| $\\text{tr}(\\mathbf{M}) = n - p$ | Trace equals error df | @sec-trace |\n| $\\mathbf{H} + \\mathbf{M} = \\mathbf{I}_n$ | Partition of identity | @sec-projection-matrices |\n| $r(\\mathbf{H}) = p$ | Rank of hat matrix | @sec-rank |\n| $r(\\mathbf{M}) = n - p$ | Rank of residual maker | @sec-rank |\n\n#### Identities Involving Normal Equations\n\nFor $\\mathbf{X}'\\mathbf{X}\\mathbf{b} = \\mathbf{X}'\\mathbf{y}$:\n\n| Identity | Interpretation | When Used |\n|----------|----------------|-----------|\n| $\\mathbf{b} = (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y}$ | LS solution | Week 4, 5, 6 |\n| $\\hat{\\mathbf{y}} = \\mathbf{X}\\mathbf{b} = \\mathbf{H}\\mathbf{y}$ | Fitted values are projection | Week 5 |\n| $\\mathbf{e} = \\mathbf{y} - \\hat{\\mathbf{y}} = \\mathbf{M}\\mathbf{y}$ | Residuals are projection | Week 5 |\n| $\\mathbf{X}'\\mathbf{e} = \\mathbf{0}$ | Normal equations satisfied | Week 5 |\n| $\\mathbf{X}'\\hat{\\mathbf{y}} = \\mathbf{X}'\\mathbf{y}$ | Key orthogonality | Week 5 |\n| $\\hat{\\mathbf{y}}'\\mathbf{e} = \\mathbf{0}$ | Fitted values ⊥ residuals | Week 5 |\n\n#### Rank Identities\n\nThese are critical for understanding non-full rank models (Week 12):\n\n| Identity | Condition | Reference |\n|----------|-----------|-----------|\n| $r(\\mathbf{X}'\\mathbf{X}) = r(\\mathbf{X})$ | Always | @sec-rank |\n| $r(\\mathbf{X}'\\mathbf{X}) \\leq \\min(n, p)$ | Always | @sec-rank |\n| $r(\\mathbf{AB}) \\leq \\min(r(\\mathbf{A}), r(\\mathbf{B}))$ | General rule | @sec-rank |\n| $r(\\mathbf{A} + \\mathbf{B}) \\leq r(\\mathbf{A}) + r(\\mathbf{B})$ | Sum of matrices | @sec-rank |\n| $r(\\mathbf{A}'\\mathbf{A}) = r(\\mathbf{A})$ | Gram matrix | @sec-rank |\n\n#### R Code to Verify Identities\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simple regression example: Broiler weight vs. age\nage <- c(21, 28, 35, 42, 49, 56)  # days\nweight <- c(0.5, 0.9, 1.4, 1.9, 2.5, 3.0)  # kg\n\nn <- length(weight)\nX <- cbind(1, age)  # Design matrix\np <- ncol(X)\n\n# Compute projection matrices\nXtX <- t(X) %*% X\nXtX_inv <- solve(XtX)\nH <- X %*% XtX_inv %*% t(X)\nM <- diag(n) - H\n\n# Verify symmetry\ncat(\"H is symmetric:\", all.equal(H, t(H)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nH is symmetric: TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"M is symmetric:\", all.equal(M, t(M)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nM is symmetric: TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify idempotence\ncat(\"H is idempotent:\", all.equal(H %*% H, H), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nH is idempotent: TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"M is idempotent:\", all.equal(M %*% M, M), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nM is idempotent: TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify orthogonality\ncat(\"HM = 0:\", all.equal(H %*% M, matrix(0, n, n), check.attributes = FALSE), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nHM = 0: TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"MH = 0:\", all.equal(M %*% H, matrix(0, n, n), check.attributes = FALSE), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMH = 0: TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify trace properties\ncat(\"tr(H) =\", sum(diag(H)), \"(should be\", p, \")\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntr(H) = 2 (should be 2 )\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"tr(M) =\", sum(diag(M)), \"(should be\", n - p, \")\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntr(M) = 4 (should be 4 )\n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify partition of identity\ncat(\"H + M = I:\", all.equal(H + M, diag(n)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nH + M = I: TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify rank (using tolerance for numerical issues)\ncat(\"r(H) =\", qr(H)$rank, \"(should be\", p, \")\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nr(H) = 2 (should be 2 )\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"r(M) =\", qr(M)$rank, \"(should be\", n - p, \")\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nr(M) = 4 (should be 4 )\n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify MX = 0\ncat(\"MX = 0:\", all.equal(M %*% X, matrix(0, n, p), check.attributes = FALSE), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMX = 0: TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify HX = X\ncat(\"HX = X:\", all.equal(H %*% X, X), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nHX = X: TRUE \n```\n\n\n:::\n:::\n\n\n#### Practical Application: Verifying Orthogonality\n\n**Example**: Dairy cow milk yield regression on days in milk.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulated data: n=8 Holstein cows\ndim <- c(30, 60, 90, 120, 150, 180, 210, 240)  # Days in milk\nmilk <- c(35, 38, 36, 33, 30, 27, 24, 21)  # kg/day (lactation curve)\n\n# Fit model\nX <- cbind(1, dim)\ny <- milk\nn <- length(y)\n\n# Solve normal equations\nb <- solve(t(X) %*% X) %*% t(X) %*% y\n\n# Compute fitted values and residuals\ny_hat <- X %*% b\ne <- y - y_hat\n\n# Verify key identities\ncat(\"X'e should be zero:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nX'e should be zero:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(t(X) %*% e)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         [,1]\n    9.237e-14\ndim 1.057e-11\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\ny_hat'e should be zero:\", t(y_hat) %*% e, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\ny_hat'e should be zero: 2.952e-12 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nX'y_hat should equal X'y:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nX'y_hat should equal X'y:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"X'y_hat:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nX'y_hat:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(t(X) %*% y_hat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]\n      244\ndim 29970\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"X'y:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nX'y:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(t(X) %*% y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]\n      244\ndim 29970\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Equal?\", all.equal(t(X) %*% y_hat, t(X) %*% y), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEqual? TRUE \n```\n\n\n:::\n:::\n\n\n::: {.callout-warning}\n## Numerical Precision\n\nWhen verifying identities computationally, use `all.equal()` instead of `==` because:\n\n1. Floating-point arithmetic introduces small errors\n2. Matrix operations accumulate rounding errors\n3. `all.equal()` uses tolerance (default: $1.5 \\times 10^{-8}$)\n\n**Example of what NOT to do**:\n\n::: {.cell}\n\n```{.r .cell-code}\n# BAD - Too strict!\nif (sum(t(X) %*% e) == 0) {\n  cat(\"X'e is zero\\n\")  # Might fail due to rounding\n}\n\n# GOOD - Tolerant comparison\nif (all.equal(t(X) %*% e, matrix(0, 2, 1), check.attributes = FALSE)) {\n  cat(\"X'e is zero (within tolerance)\\n\")\n}\n```\n:::\n\n:::\n\n#### Useful Compound Identities\n\nThese appear frequently in variance derivations (see @sec-variance-formulas):\n\n| Identity | Expanded Form | Used For |\n|----------|--------------|----------|\n| $\\mathbf{y}'\\mathbf{H}\\mathbf{y}$ | $\\mathbf{b}'\\mathbf{X}'\\mathbf{y}$ | SSR (model sum of squares) |\n| $\\mathbf{y}'\\mathbf{M}\\mathbf{y}$ | $\\mathbf{y}'\\mathbf{y} - \\mathbf{b}'\\mathbf{X}'\\mathbf{y}$ | SSE (error sum of squares) |\n| $\\mathbf{y}'(\\mathbf{H} + \\mathbf{M})\\mathbf{y}$ | $\\mathbf{y}'\\mathbf{y}$ | Total sum of squares |\n| $\\text{E}(\\mathbf{y}'\\mathbf{H}\\mathbf{y})$ | $\\mathbf{X}\\boldsymbol{\\beta})'\\mathbf{H}(\\mathbf{X}\\boldsymbol{\\beta}) + \\sigma^2 p$ | Expected SSR |\n| $\\text{E}(\\mathbf{y}'\\mathbf{M}\\mathbf{y})$ | $\\sigma^2(n-p)$ | Expected SSE |\n\n**Derivation example** (Expected SSE):\n\n$$\n\\begin{align}\n\\text{E}(\\mathbf{y}'\\mathbf{M}\\mathbf{y}) &= \\text{E}[(\\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{e})'\\mathbf{M}(\\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{e})] \\\\\n&= \\text{E}[\\mathbf{e}'\\mathbf{M}\\mathbf{e}] \\quad \\text{(since } \\mathbf{M}\\mathbf{X} = \\mathbf{0}) \\\\\n&= \\text{E}[\\text{tr}(\\mathbf{e}'\\mathbf{M}\\mathbf{e})] \\\\\n&= \\text{E}[\\text{tr}(\\mathbf{M}\\mathbf{e}\\mathbf{e}')] \\quad \\text{(trace is cyclic)} \\\\\n&= \\text{tr}(\\mathbf{M} \\cdot \\text{E}[\\mathbf{e}\\mathbf{e}']) \\\\\n&= \\text{tr}(\\mathbf{M} \\cdot \\sigma^2\\mathbf{I}_n) \\\\\n&= \\sigma^2 \\text{tr}(\\mathbf{M}) \\\\\n&= \\sigma^2(n - p)\n\\end{align}\n$$\n\nThis derivation appears in Week 5 when proving that $\\hat{\\sigma}^2 = \\text{SSE}/(n-p)$ is unbiased.\n\n---\n\n### Variance and Covariance Formulas {#sec-variance-formulas}\n\nThis section presents key variance and covariance formulas for linear models. These formulas are derived in Week 5 and used throughout the course for constructing confidence intervals (Week 6), testing contrasts (Week 8), and understanding precision of estimates.\n\n#### General Variance Formula\n\nFor a linear model $\\mathbf{y} \\sim N(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2\\mathbf{I}_n)$, and any matrix $\\mathbf{A}$:\n\n$$\n\\text{Var}(\\mathbf{Ay}) = \\mathbf{A} \\cdot \\text{Var}(\\mathbf{y}) \\cdot \\mathbf{A}' = \\mathbf{A} \\cdot \\sigma^2\\mathbf{I}_n \\cdot \\mathbf{A}' = \\sigma^2 \\mathbf{A}\\mathbf{A}'\n$$\n\nThis single formula generates all variance formulas below!\n\n::: {.callout-tip}\n## Memory Aid\n\nTo find the variance of ANY linear combination $\\mathbf{Ay}$:\n\n1. Write down $\\mathbf{A}$\n2. Compute $\\mathbf{A}\\mathbf{A}'$\n3. Multiply by $\\sigma^2$\n\n**That's it!**\n:::\n\n#### Variance of Least Squares Estimator\n\nFor $\\mathbf{b} = (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y}$:\n\n$$\n\\begin{align}\n\\text{Var}(\\mathbf{b}) &= \\text{Var}[(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y}] \\\\\n&= (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}' \\cdot \\sigma^2\\mathbf{I}_n \\cdot \\mathbf{X}(\\mathbf{X}'\\mathbf{X})^{-1} \\\\\n&= \\sigma^2 (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{X}(\\mathbf{X}'\\mathbf{X})^{-1} \\\\\n&= \\sigma^2 (\\mathbf{X}'\\mathbf{X})^{-1}\n\\end{align}\n$$\n\n**Key properties**:\n\n- Standard errors: $\\text{se}(b_j) = \\sqrt{\\hat{\\sigma}^2 [(\\mathbf{X}'\\mathbf{X})^{-1}]_{jj}}$\n- Diagonal elements of $(\\mathbf{X}'\\mathbf{X})^{-1}$ give variances\n- Off-diagonal elements give covariances between estimates\n\n#### Variance of Fitted Values\n\nFor $\\hat{\\mathbf{y}} = \\mathbf{H}\\mathbf{y}$:\n\n$$\n\\text{Var}(\\hat{\\mathbf{y}}) = \\text{Var}(\\mathbf{H}\\mathbf{y}) = \\sigma^2 \\mathbf{H}\\mathbf{H}' = \\sigma^2 \\mathbf{H}\n$$\n\n(using $\\mathbf{H}' = \\mathbf{H}$ and $\\mathbf{H}^2 = \\mathbf{H}$)\n\n**For a specific fitted value** $\\hat{y}_i$:\n\n$$\n\\text{Var}(\\hat{y}_i) = \\sigma^2 h_{ii}\n$$\n\nwhere $h_{ii}$ is the $i$-th diagonal element of $\\mathbf{H}$ (the \"leverage\" of observation $i$).\n\n#### Variance of Residuals\n\nFor $\\mathbf{e} = \\mathbf{M}\\mathbf{y}$:\n\n$$\n\\text{Var}(\\mathbf{e}) = \\text{Var}(\\mathbf{M}\\mathbf{y}) = \\sigma^2 \\mathbf{M}\\mathbf{M}' = \\sigma^2 \\mathbf{M}\n$$\n\n**For a specific residual** $e_i$:\n\n$$\n\\text{Var}(e_i) = \\sigma^2 (1 - h_{ii})\n$$\n\nNote: Residuals have **different** variances (heteroscedastic), even though errors are homoscedastic!\n\n#### Variance of Predictions\n\nFor a **new** observation with predictor values $\\mathbf{x}_0$ (a $p \\times 1$ vector):\n\n$$\n\\hat{y}_0 = \\mathbf{x}_0'\\mathbf{b}\n$$\n\n**Variance**:\n\n$$\n\\text{Var}(\\hat{y}_0) = \\text{Var}(\\mathbf{x}_0'\\mathbf{b}) = \\mathbf{x}_0' \\cdot \\text{Var}(\\mathbf{b}) \\cdot \\mathbf{x}_0 = \\sigma^2 \\mathbf{x}_0'(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{x}_0\n$$\n\n**Prediction interval** accounts for both estimation uncertainty AND observation error:\n\n$$\n\\text{Var}(y_0 - \\hat{y}_0) = \\sigma^2 [1 + \\mathbf{x}_0'(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{x}_0]\n$$\n\n::: {.callout-note}\n## Confidence Interval vs. Prediction Interval\n\n- **Confidence interval** for $\\text{E}(y_0)$: Uses $\\text{Var}(\\hat{y}_0) = \\sigma^2 \\mathbf{x}_0'(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{x}_0$\n- **Prediction interval** for new observation $y_0$: Adds $\\sigma^2$ for observation error\n\nPrediction intervals are ALWAYS wider!\n:::\n\n#### Variance of Linear Contrasts\n\nFor a contrast $\\mathbf{c}'\\boldsymbol{\\beta}$ estimated by $\\mathbf{c}'\\mathbf{b}$ (Week 8):\n\n$$\n\\text{Var}(\\mathbf{c}'\\mathbf{b}) = \\mathbf{c}' \\cdot \\text{Var}(\\mathbf{b}) \\cdot \\mathbf{c} = \\sigma^2 \\mathbf{c}'(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{c}\n$$\n\n**Standard error**:\n\n$$\n\\text{se}(\\mathbf{c}'\\mathbf{b}) = \\sqrt{\\hat{\\sigma}^2 \\mathbf{c}'(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{c}}\n$$\n\n#### Covariance Properties\n\n**Key covariances**:\n\n| Quantity 1 | Quantity 2 | Covariance | Interpretation |\n|-----------|-----------|------------|----------------|\n| $\\mathbf{b}$ | $\\mathbf{e}$ | $\\mathbf{0}$ | Estimates independent of residuals |\n| $\\hat{\\mathbf{y}}$ | $\\mathbf{e}$ | $\\mathbf{0}$ | Fitted values independent of residuals |\n| $b_j$ | $b_k$ | $\\sigma^2 [(\\mathbf{X}'\\mathbf{X})^{-1}]_{jk}$ | Estimates may be correlated |\n\n**Proof that** $\\text{Cov}(\\mathbf{b}, \\mathbf{e}) = \\mathbf{0}$:\n\n$$\n\\begin{align}\n\\text{Cov}(\\mathbf{b}, \\mathbf{e}) &= \\text{Cov}[(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y}, \\mathbf{M}\\mathbf{y}] \\\\\n&= (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}' \\cdot \\text{Var}(\\mathbf{y}) \\cdot \\mathbf{M}' \\\\\n&= \\sigma^2 (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{M} \\\\\n&= \\sigma^2 (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'(\\mathbf{I} - \\mathbf{H}) \\\\\n&= \\sigma^2 (\\mathbf{X}'\\mathbf{X})^{-1}(\\mathbf{X}' - \\mathbf{X}'\\mathbf{H}) \\\\\n&= \\sigma^2 (\\mathbf{X}'\\mathbf{X})^{-1}(\\mathbf{X}' - \\mathbf{X}') \\\\\n&= \\mathbf{0}\n\\end{align}\n$$\n\n(using $\\mathbf{X}'\\mathbf{H} = \\mathbf{X}'$)\n\n#### R Implementation: Computing Variances\n\n**Example**: Swine litter size regression on parity number.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Data: n=10 sows\nparity <- c(1, 1, 2, 2, 3, 3, 4, 4, 5, 5)  # Parity number\nlitter_size <- c(9, 10, 11, 12, 12, 13, 13, 14, 14, 15)  # Piglets born alive\n\nn <- length(litter_size)\nX <- cbind(1, parity)\ny <- litter_size\np <- ncol(X)\n\n# Solve for estimates\nXtX <- t(X) %*% X\nXtX_inv <- solve(XtX)\nb <- XtX_inv %*% t(X) %*% y\n\n# Compute residuals and variance estimate\ny_hat <- X %*% b\ne <- y - y_hat\nSSE <- sum(e^2)\nsigma2_hat <- SSE / (n - p)\n\n# Variance-covariance matrix of b\nVar_b <- sigma2_hat * XtX_inv\ncat(\"Var(b):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nVar(b):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(Var_b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                  parity\n        0.22688 -0.06188\nparity -0.06188  0.02063\n```\n\n\n:::\n\n```{.r .cell-code}\n# Standard errors of coefficients\nse_b <- sqrt(diag(Var_b))\ncat(\"\\nStandard errors:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nStandard errors:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"se(b0) =\", se_b[1], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nse(b0) = 0.4763 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"se(b1) =\", se_b[2], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nse(b1) = 0.1436 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Compare with lm()\nfit <- lm(litter_size ~ parity)\ncat(\"\\nlm() standard errors:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nlm() standard errors:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(summary(fit)$coefficients[, \"Std. Error\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)      parity \n     0.4763      0.1436 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Variance of fitted values\nH <- X %*% XtX_inv %*% t(X)\nVar_y_hat <- sigma2_hat * H\ncat(\"\\nVariance of fitted values (diagonal of Var(y_hat)):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVariance of fitted values (diagonal of Var(y_hat)):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(diag(Var_y_hat))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 0.12375 0.12375 0.06188 0.06188 0.04125 0.04125 0.06188 0.06188 0.12375\n[10] 0.12375\n```\n\n\n:::\n\n```{.r .cell-code}\n# Leverage values\nh <- diag(H)\ncat(\"\\nLeverage values (h_ii):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nLeverage values (h_ii):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(h)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 0.30 0.30 0.15 0.15 0.10 0.10 0.15 0.15 0.30 0.30\n```\n\n\n:::\n\n```{.r .cell-code}\n# Variance of residuals\nVar_e <- sigma2_hat * (diag(n) - H)\ncat(\"\\nVariance of residuals (diagonal of Var(e)):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVariance of residuals (diagonal of Var(e)):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(diag(Var_e))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 0.2887 0.2887 0.3506 0.3506 0.3712 0.3712 0.3506 0.3506 0.2887 0.2887\n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify: Var(e_i) = sigma2 * (1 - h_ii)\ncat(\"\\nVerify Var(e_i) = sigma2 * (1 - h_ii):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVerify Var(e_i) = sigma2 * (1 - h_ii):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(sigma2_hat * (1 - h))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 0.2887 0.2887 0.3506 0.3506 0.3712 0.3712 0.3506 0.3506 0.2887 0.2887\n```\n\n\n:::\n:::\n\n\n#### Prediction Example: Predicting Future Litter Size\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Predict litter size for parity 6\nx0 <- c(1, 6)  # Intercept and parity=6\n\n# Point prediction\ny0_hat <- t(x0) %*% b\ncat(\"Predicted litter size at parity 6:\", y0_hat, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPredicted litter size at parity 6: 15.9 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Variance of prediction (for mean response)\nvar_pred_mean <- sigma2_hat * t(x0) %*% XtX_inv %*% x0\nse_pred_mean <- sqrt(var_pred_mean)\ncat(\"SE for mean response:\", se_pred_mean, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSE for mean response: 0.4763 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Variance of prediction (for new observation)\nvar_pred_obs <- sigma2_hat * (1 + t(x0) %*% XtX_inv %*% x0)\nse_pred_obs <- sqrt(var_pred_obs)\ncat(\"SE for new observation:\", se_pred_obs, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSE for new observation: 0.7996 \n```\n\n\n:::\n\n```{.r .cell-code}\n# 95% confidence interval for mean response\nt_crit <- qt(0.975, df = n - p)\nci_lower <- y0_hat - t_crit * se_pred_mean\nci_upper <- y0_hat + t_crit * se_pred_mean\ncat(\"\\n95% CI for mean at parity 6: [\", ci_lower, \",\", ci_upper, \"]\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n95% CI for mean at parity 6: [ 14.8 , 17 ]\n```\n\n\n:::\n\n```{.r .cell-code}\n# 95% prediction interval for new observation\npi_lower <- y0_hat - t_crit * se_pred_obs\npi_upper <- y0_hat + t_crit * se_pred_obs\ncat(\"95% PI for new obs at parity 6: [\", pi_lower, \",\", pi_upper, \"]\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n95% PI for new obs at parity 6: [ 14.06 , 17.74 ]\n```\n\n\n:::\n\n```{.r .cell-code}\n# Compare with predict()\ncat(\"\\nCompare with predict():\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCompare with predict():\n```\n\n\n:::\n\n```{.r .cell-code}\nnew_data <- data.frame(parity = 6)\npred_mean <- predict(fit, newdata = new_data, interval = \"confidence\", level = 0.95)\npred_obs <- predict(fit, newdata = new_data, interval = \"prediction\", level = 0.95)\nprint(pred_mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   fit  lwr upr\n1 15.9 14.8  17\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(pred_obs)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   fit   lwr   upr\n1 15.9 14.06 17.74\n```\n\n\n:::\n:::\n\n\n#### Contrast Example: Comparing Two Parities\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Contrast: difference between parity 4 and parity 2\n# E(y|parity=4) - E(y|parity=2) = (β0 + 4β1) - (β0 + 2β1) = 2β1\nc <- c(0, 2)  # Contrast vector\n\n# Estimate\ncontrast_est <- t(c) %*% b\ncat(\"Estimated difference (parity 4 - parity 2):\", contrast_est, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEstimated difference (parity 4 - parity 2): 2.4 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Variance\nvar_contrast <- sigma2_hat * t(c) %*% XtX_inv %*% c\nse_contrast <- sqrt(var_contrast)\ncat(\"SE of difference:\", se_contrast, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSE of difference: 0.2872 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Test H0: no difference\nt_stat <- contrast_est / se_contrast\np_value <- 2 * pt(abs(t_stat), df = n - p, lower.tail = FALSE)\ncat(\"t-statistic:\", t_stat, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nt-statistic: 8.356 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"p-value:\", p_value, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\np-value: 3.188e-05 \n```\n\n\n:::\n\n```{.r .cell-code}\n# 95% CI\nci_lower_c <- contrast_est - t_crit * se_contrast\nci_upper_c <- contrast_est + t_crit * se_contrast\ncat(\"95% CI for difference: [\", ci_lower_c, \",\", ci_upper_c, \"]\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n95% CI for difference: [ 1.738 , 3.062 ]\n```\n\n\n:::\n:::\n\n\n::: {.callout-tip}\n## Practical Insight: Variance Depends on X\n\nAll these variances depend on $(\\mathbf{X}'\\mathbf{X})^{-1}$, which means:\n\n1. **More data** (larger $n$) → smaller variance\n2. **Better spread** in $x$ values → smaller variance\n3. **Collinearity** (ill-conditioned $\\mathbf{X}'\\mathbf{X}$) → larger variance\n4. **Predictions far from** $\\bar{x}$ → larger variance\n\nThis explains why:\n- Balanced designs are efficient\n- Extreme extrapolation is risky\n- Collinearity inflates standard errors\n:::\n\n#### Summary Table of Key Variance Formulas\n\n| Quantity | Formula | Dimensions | Used In |\n|----------|---------|------------|---------|\n| $\\text{Var}(\\mathbf{b})$ | $\\sigma^2(\\mathbf{X}'\\mathbf{X})^{-1}$ | $p \\times p$ | Week 5, 6, 8 |\n| $\\text{Var}(\\hat{\\mathbf{y}})$ | $\\sigma^2\\mathbf{H}$ | $n \\times n$ | Week 5, 11 |\n| $\\text{Var}(\\mathbf{e})$ | $\\sigma^2\\mathbf{M}$ | $n \\times n$ | Week 5, 11 |\n| $\\text{Var}(\\hat{y}_i)$ | $\\sigma^2 h_{ii}$ | scalar | Week 11 (diagnostics) |\n| $\\text{Var}(e_i)$ | $\\sigma^2(1 - h_{ii})$ | scalar | Week 11 (diagnostics) |\n| $\\text{Var}(\\hat{y}_0)$ | $\\sigma^2\\mathbf{x}_0'(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{x}_0$ | scalar | Week 6 (prediction) |\n| $\\text{Var}(y_0 - \\hat{y}_0)$ | $\\sigma^2[1 + \\mathbf{x}_0'(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{x}_0]$ | scalar | Week 6 (prediction) |\n| $\\text{Var}(\\mathbf{c}'\\mathbf{b})$ | $\\sigma^2\\mathbf{c}'(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{c}$ | scalar | Week 8 (contrasts) |\n\n---\n\n### Sum of Squares Identities {#sec-ss-identities}\n\nSum of squares decompositions are the foundation of ANOVA (Weeks 7-9) and regression analysis (Weeks 4-6). This section presents the key identities in matrix form.\n\n#### Basic Decomposition: SST = SSM + SSE\n\nFor the linear model $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{e}$:\n\n$$\n\\underbrace{\\sum_{i=1}^n (y_i - \\bar{y})^2}_{\\text{SST}} = \\underbrace{\\sum_{i=1}^n (\\hat{y}_i - \\bar{y})^2}_{\\text{SSM}} + \\underbrace{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}_{\\text{SSE}}\n$$\n\n**Matrix form**:\n\n$$\n\\mathbf{y}'\\mathbf{C}\\mathbf{y} = \\mathbf{b}'\\mathbf{X}'\\mathbf{C}\\mathbf{y} + \\mathbf{e}'\\mathbf{e}\n$$\n\nwhere $\\mathbf{C} = \\mathbf{I}_n - n^{-1}\\mathbf{1}_n\\mathbf{1}_n'$ is the **centering matrix**.\n\n::: {.callout-note}\n## Alternative Matrix Formulations\n\nThere are several equivalent ways to write sum of squares:\n\n**SST (Total Sum of Squares)**:\n$$\n\\text{SST} = \\mathbf{y}'\\mathbf{C}\\mathbf{y} = \\mathbf{y}'\\mathbf{y} - n\\bar{y}^2 = \\sum_{i=1}^n y_i^2 - \\frac{(\\sum_{i=1}^n y_i)^2}{n}\n$$\n\n**SSM (Model Sum of Squares)**:\n$$\n\\text{SSM} = \\hat{\\mathbf{y}}'\\mathbf{C}\\hat{\\mathbf{y}} = \\mathbf{b}'\\mathbf{X}'\\mathbf{y} - n\\bar{y}^2 = \\mathbf{y}'\\mathbf{H}\\mathbf{C}\\mathbf{y}\n$$\n\n**SSE (Error Sum of Squares)**:\n$$\n\\text{SSE} = \\mathbf{e}'\\mathbf{e} = \\mathbf{y}'\\mathbf{y} - \\mathbf{b}'\\mathbf{X}'\\mathbf{y} = \\mathbf{y}'\\mathbf{M}\\mathbf{y}\n$$\n:::\n\n#### Proof of Orthogonal Decomposition\n\n**Key insight**: The decomposition works because $\\hat{\\mathbf{y}}'\\mathbf{e} = \\mathbf{0}$ (orthogonality).\n\n$$\n\\begin{align}\n\\mathbf{y}'\\mathbf{y} &= (\\hat{\\mathbf{y}} + \\mathbf{e})'(\\hat{\\mathbf{y}} + \\mathbf{e}) \\\\\n&= \\hat{\\mathbf{y}}'\\hat{\\mathbf{y}} + 2\\hat{\\mathbf{y}}'\\mathbf{e} + \\mathbf{e}'\\mathbf{e} \\\\\n&= \\hat{\\mathbf{y}}'\\hat{\\mathbf{y}} + \\mathbf{e}'\\mathbf{e} \\quad \\text{(since } \\hat{\\mathbf{y}}'\\mathbf{e} = \\mathbf{0})\n\\end{align}\n$$\n\nSubtracting $n\\bar{y}^2$ from both sides:\n\n$$\n\\mathbf{y}'\\mathbf{y} - n\\bar{y}^2 = (\\hat{\\mathbf{y}}'\\hat{\\mathbf{y}} - n\\bar{y}^2) + \\mathbf{e}'\\mathbf{e}\n$$\n\n$$\n\\text{SST} = \\text{SSM} + \\text{SSE}\n$$\n\n#### Projection Matrix Form\n\nUsing $\\mathbf{H}$ and $\\mathbf{M}$:\n\n| Sum of Squares | Formula 1 | Formula 2 | Formula 3 |\n|---------------|-----------|-----------|-----------|\n| SST | $\\mathbf{y}'\\mathbf{C}\\mathbf{y}$ | $\\mathbf{y}'(\\mathbf{I} - n^{-1}\\mathbf{J})\\mathbf{y}$ | $\\mathbf{y}'\\mathbf{y} - n\\bar{y}^2$ |\n| SSM | $\\mathbf{y}'\\mathbf{H}\\mathbf{C}\\mathbf{y}$ | $\\mathbf{b}'\\mathbf{X}'\\mathbf{y} - n\\bar{y}^2$ | $\\hat{\\mathbf{y}}'\\mathbf{C}\\hat{\\mathbf{y}}$ |\n| SSE | $\\mathbf{y}'\\mathbf{M}\\mathbf{y}$ | $\\mathbf{y}'\\mathbf{y} - \\mathbf{b}'\\mathbf{X}'\\mathbf{y}$ | $\\mathbf{e}'\\mathbf{e}$ |\n\nwhere $\\mathbf{J} = \\mathbf{1}_n\\mathbf{1}_n'$ (matrix of all ones).\n\n#### Degrees of Freedom\n\nEach sum of squares has associated degrees of freedom equal to the **rank** of its projection matrix:\n\n| Source | Sum of Squares | df | Projection Matrix | Rank |\n|--------|---------------|----|------------------|------|\n| Total | SST | $n-1$ | $\\mathbf{C}$ | $r(\\mathbf{C}) = n-1$ |\n| Model | SSM | $p-1$ | $\\mathbf{H}\\mathbf{C}$ | $r(\\mathbf{H}\\mathbf{C}) = p-1$ |\n| Error | SSE | $n-p$ | $\\mathbf{M}$ | $r(\\mathbf{M}) = n-p$ |\n\n**Key property**: $(n-1) = (p-1) + (n-p)$ (df add up!)\n\n#### Identities for Sums of Squares\n\n**Computational shortcuts**:\n\n| Identity | Why It's Useful |\n|----------|----------------|\n| $\\mathbf{b}'\\mathbf{X}'\\mathbf{y} = \\hat{\\mathbf{y}}'\\mathbf{y}$ | Avoids computing $\\hat{\\mathbf{y}}$ explicitly |\n| $\\text{SSE} = \\mathbf{y}'\\mathbf{y} - \\mathbf{b}'\\mathbf{X}'\\mathbf{y}$ | Compute SSE without residuals |\n| $\\mathbf{e}'\\mathbf{e} = (\\mathbf{y} - \\mathbf{X}\\mathbf{b})'(\\mathbf{y} - \\mathbf{X}\\mathbf{b})$ | Alternative for SSE |\n| $\\text{SST} = \\mathbf{y}'\\mathbf{y} - n\\bar{y}^2$ | Correction for the mean |\n\n**Verification identities**:\n\n| Identity | Check |\n|----------|-------|\n| $\\text{SST} = \\text{SSM} + \\text{SSE}$ | Always true (orthogonal decomposition) |\n| $\\text{SSM} = \\mathbf{b}'(\\mathbf{X}'\\mathbf{y} - n\\bar{y}\\mathbf{1}_p)$ | Alternative computation |\n| $R^2 = \\text{SSM}/\\text{SST} = 1 - \\text{SSE}/\\text{SST}$ | Both should give same $R^2$ |\n\n#### Multi-Factor ANOVA Decompositions\n\nFor two-way ANOVA (Week 9), the sum of squares partition extends:\n\n$$\n\\text{SST} = \\text{SS(A)} + \\text{SS(B)} + \\text{SS(AB)} + \\text{SSE}\n$$\n\n**Matrix representation**: Each source corresponds to a projection matrix.\n\nFor model $\\mathbf{y} = \\mathbf{X}_A\\boldsymbol{\\alpha} + \\mathbf{X}_B\\boldsymbol{\\beta} + \\mathbf{X}_{AB}\\boldsymbol{\\gamma} + \\mathbf{e}$:\n\n- $\\text{SS(A)} = \\mathbf{y}'(\\mathbf{H}_A - \\mathbf{H}_\\mu)\\mathbf{y}$\n- $\\text{SS(B)} = \\mathbf{y}'(\\mathbf{H}_B - \\mathbf{H}_\\mu)\\mathbf{y}$\n- $\\text{SS(AB)} = \\mathbf{y}'(\\mathbf{H}_{full} - \\mathbf{H}_A - \\mathbf{H}_B + \\mathbf{H}_\\mu)\\mathbf{y}$\n- $\\text{SSE} = \\mathbf{y}'(\\mathbf{I} - \\mathbf{H}_{full})\\mathbf{y}$\n\nwhere:\n- $\\mathbf{H}_\\mu$ = projection onto intercept only\n- $\\mathbf{H}_A$ = projection onto intercept + A\n- $\\mathbf{H}_B$ = projection onto intercept + B\n- $\\mathbf{H}_{full}$ = projection onto full model\n\n::: {.callout-warning}\n## Type I vs. Type III Sums of Squares\n\nThe formulas above give **Type III SS** (each effect adjusted for all others).\n\n**Type I SS** (sequential) depend on the order:\n- $\\text{SS(A|}\\mu) = \\mathbf{y}'(\\mathbf{H}_A - \\mathbf{H}_\\mu)\\mathbf{y}$\n- $\\text{SS(B|}\\mu, A) = \\mathbf{y}'(\\mathbf{H}_{AB} - \\mathbf{H}_A)\\mathbf{y}$\n\n**For balanced designs**, Type I = Type III. **For unbalanced designs**, they differ!\n:::\n\n#### R Implementation: Sum of Squares Computations\n\n**Example**: Beef cattle weight gain (ADG) by breed.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# One-way ANOVA: ADG by breed (3 breeds, n=4 per breed)\nbreed <- factor(rep(c(\"Angus\", \"Hereford\", \"Charolais\"), each = 4))\nadg <- c(1.2, 1.3, 1.1, 1.4,    # Angus\n         1.1, 1.2, 1.0, 1.3,    # Hereford\n         1.5, 1.6, 1.4, 1.7)    # Charolais\n\nn <- length(adg)\ny <- adg\n\n# Design matrix (cell means model)\nX <- model.matrix(~ breed - 1)\np <- ncol(X)\n\n# Solve normal equations\nb <- solve(t(X) %*% X) %*% t(X) %*% y\ny_hat <- X %*% b\ne <- y - y_hat\n\n# Method 1: Using formulas\ny_bar <- mean(y)\nSST <- sum((y - y_bar)^2)\nSSM <- sum((y_hat - y_bar)^2)\nSSE <- sum(e^2)\n\ncat(\"Method 1 (direct computation):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMethod 1 (direct computation):\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"SST =\", SST, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSST = 0.4967 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"SSM =\", SSM, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSSM = 0.3467 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"SSE =\", SSE, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSSE = 0.15 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"SSM + SSE =\", SSM + SSE, \"(should equal SST)\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSSM + SSE = 0.4967 (should equal SST)\n```\n\n\n:::\n\n```{.r .cell-code}\n# Method 2: Matrix formulas\nSST_mat <- t(y) %*% y - n * y_bar^2\nSSM_mat <- t(b) %*% t(X) %*% y - n * y_bar^2\nSSE_mat <- t(y) %*% y - t(b) %*% t(X) %*% y\n\ncat(\"Method 2 (matrix formulas):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMethod 2 (matrix formulas):\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"SST =\", SST_mat, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSST = 0.4967 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"SSM =\", SSM_mat, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSSM = 0.3467 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"SSE =\", SSE_mat, \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSSE = 0.15 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Method 3: Using projection matrices\nC <- diag(n) - (1/n) * matrix(1, n, n)  # Centering matrix\nH <- X %*% solve(t(X) %*% X) %*% t(X)\nM <- diag(n) - H\n\nSST_proj <- t(y) %*% C %*% y\nSSM_proj <- t(y) %*% H %*% C %*% y\nSSE_proj <- t(y) %*% M %*% y\n\ncat(\"Method 3 (projection matrices):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMethod 3 (projection matrices):\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"SST =\", SST_proj, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSST = 0.4967 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"SSM =\", SSM_proj, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSSM = 0.3467 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"SSE =\", SSE_proj, \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSSE = 0.15 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Degrees of freedom\ndf_total <- n - 1\ndf_model <- p - 1\ndf_error <- n - p\n\ncat(\"Degrees of freedom:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDegrees of freedom:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"df(Total) =\", df_total, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ndf(Total) = 11 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"df(Model) =\", df_model, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ndf(Model) = 2 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"df(Error) =\", df_error, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ndf(Error) = 9 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Check:\", df_model + df_error, \"= df(Total)\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCheck: 11 = df(Total)\n```\n\n\n:::\n\n```{.r .cell-code}\n# Mean squares and F-statistic\nMSM <- SSM / df_model\nMSE <- SSE / df_error\nF_stat <- MSM / MSE\np_value <- pf(F_stat, df_model, df_error, lower.tail = FALSE)\n\n# ANOVA table\ncat(\"ANOVA Table:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nANOVA Table:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Source    df    SS       MS       F        p-value\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSource    df    SS       MS       F        p-value\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"-------------------------------------------------------\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n-------------------------------------------------------\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"Model     %2d    %.4f   %.4f   %.4f   %.4f\\n\",\n            df_model, SSM, MSM, F_stat, p_value))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel      2    0.3467   0.1733   10.4000   0.0046\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"Error     %2d    %.4f   %.4f\\n\", df_error, SSE, MSE))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nError      9    0.1500   0.0167\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"Total     %2d    %.4f\\n\", df_total, SST))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTotal     11    0.4967\n```\n\n\n:::\n\n```{.r .cell-code}\n# Compare with lm()\ncat(\"\\n\\nCompare with lm():\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\nCompare with lm():\n```\n\n\n:::\n\n```{.r .cell-code}\nfit <- lm(adg ~ breed)\nprint(anova(fit))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: adg\n          Df Sum Sq Mean Sq F value Pr(>F)   \nbreed      2  0.347  0.1733    10.4 0.0046 **\nResiduals  9  0.150  0.0167                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\n# R-squared\nR2 <- SSM / SST\ncat(\"\\nR-squared:\", R2, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nR-squared: 0.698 \n```\n\n\n:::\n:::\n\n\n#### Two-Way ANOVA Example\n\n**Example**: Dairy milk yield by breed and farm.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Balanced 2×3 factorial: 2 breeds, 3 farms, n=2 per cell\nbreed <- factor(rep(c(\"Holstein\", \"Jersey\"), each = 6))\nfarm <- factor(rep(rep(c(\"A\", \"B\", \"C\"), each = 2), 2))\nmilk <- c(32, 33,  # Holstein, Farm A\n          30, 31,  # Holstein, Farm B\n          28, 29,  # Holstein, Farm C\n          25, 26,  # Jersey, Farm A\n          24, 25,  # Jersey, Farm B\n          22, 23)  # Jersey, Farm C\n\ny <- milk\nn <- length(y)\ny_bar <- mean(y)\n\n# Fit models\nfit_null <- lm(milk ~ 1)\nfit_breed <- lm(milk ~ breed)\nfit_farm <- lm(milk ~ farm)\nfit_full <- lm(milk ~ breed + farm + breed:farm)\n\n# Extract SS using anova()\ncat(\"Type I (Sequential) SS:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nType I (Sequential) SS:\n```\n\n\n:::\n\n```{.r .cell-code}\nanova_seq <- anova(fit_full)\nprint(anova_seq)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: milk\n           Df Sum Sq Mean Sq F value  Pr(>F)    \nbreed       1  120.3   120.3  240.67 4.5e-06 ***\nfarm        2   24.7    12.3   24.67  0.0013 ** \nbreed:farm  2    0.7     0.3    0.67  0.5477    \nResiduals   6    3.0     0.5                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\n# Type III SS (using car package)\nlibrary(car)\ncat(\"\\n\\nType III SS:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\nType III SS:\n```\n\n\n:::\n\n```{.r .cell-code}\nanova_type3 <- Anova(fit_full, type = 3)\nprint(anova_type3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnova Table (Type III tests)\n\nResponse: milk\n            Sum Sq Df F value  Pr(>F)    \n(Intercept)   2113  1 4225.00 8.9e-10 ***\nbreed           49  1   98.00 6.1e-05 ***\nfarm            16  2   16.00  0.0039 ** \nbreed:farm       1  2    0.67  0.5477    \nResiduals        3  6                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\n# Manual computation of Type III SS\n# SS(Breed | everything else)\nfit_no_breed <- lm(milk ~ farm + breed:farm)\nSS_breed_III <- sum(residuals(fit_no_breed)^2) - sum(residuals(fit_full)^2)\n\n# SS(Farm | everything else)\nfit_no_farm <- lm(milk ~ breed + breed:farm)\nSS_farm_III <- sum(residuals(fit_no_farm)^2) - sum(residuals(fit_full)^2)\n\n# SS(Interaction | everything else)\nfit_no_int <- lm(milk ~ breed + farm)\nSS_int_III <- sum(residuals(fit_no_int)^2) - sum(residuals(fit_full)^2)\n\ncat(\"\\n\\nManual Type III SS:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\nManual Type III SS:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"SS(Breed | Farm, Interaction) =\", SS_breed_III, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSS(Breed | Farm, Interaction) = 5.329e-15 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"SS(Farm | Breed, Interaction) =\", SS_farm_III, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSS(Farm | Breed, Interaction) = 4.441e-16 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"SS(Interaction | Breed, Farm) =\", SS_int_III, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSS(Interaction | Breed, Farm) = 0.6667 \n```\n\n\n:::\n:::\n\n\n::: {.callout-tip}\n## Computational Strategy\n\n**For efficiency**:\n\n1. **Never** form projection matrices explicitly for large $n$\n2. Use $\\mathbf{b}'\\mathbf{X}'\\mathbf{y}$ instead of $\\hat{\\mathbf{y}}'\\hat{\\mathbf{y}}$\n3. Compute SSE as $\\mathbf{y}'\\mathbf{y} - \\mathbf{b}'\\mathbf{X}'\\mathbf{y}$ (avoids residual vector)\n4. Use incremental SS: $\\text{SS(Full)} - \\text{SS(Reduced)}$\n\n**Example**:\n```r\n# Efficient\nSSE <- t(y) %*% y - t(b) %*% t(X) %*% y\n\n# Inefficient (for large n)\ny_hat <- X %*% b\nSSE <- t(y - y_hat) %*% (y - y_hat)\n```\n:::\n\n#### Identity Summary\n\n**Key sum of squares identities**:\n\n| Identity | Formula | Usage |\n|----------|---------|-------|\n| Decomposition | $\\text{SST} = \\text{SSM} + \\text{SSE}$ | Always holds |\n| SST | $\\mathbf{y}'\\mathbf{y} - n\\bar{y}^2$ | Correction for mean |\n| SSM | $\\mathbf{b}'\\mathbf{X}'\\mathbf{y} - n\\bar{y}^2$ | Model SS |\n| SSE | $\\mathbf{y}'\\mathbf{y} - \\mathbf{b}'\\mathbf{X}'\\mathbf{y}$ | Error SS |\n| $R^2$ | $\\text{SSM}/\\text{SST} = 1 - \\text{SSE}/\\text{SST}$ | Goodness of fit |\n| Incremental SS | $\\text{SS(Full)} - \\text{SS(Reduced)}$ | Testing nested models |\n| Orthogonality | $\\hat{\\mathbf{y}}'\\mathbf{e} = \\mathbf{0}$ | Key to decomposition |\n\n---\n\n### Computational Shortcuts {#sec-computational-shortcuts}\n\nR provides specialized functions that are faster and more numerically stable than explicit matrix operations. This section shows you how to write efficient code for linear models computations.\n\n::: {.callout-important}\n## Why Computational Efficiency Matters\n\nFor large datasets (n > 10,000 or p > 1,000):\n\n- Naive implementations can be **100× slower**\n- Memory usage can explode\n- Numerical errors accumulate\n\n**Learning efficient coding now** will pay dividends when you work with real breeding datasets!\n:::\n\n#### Efficient Cross-Product Functions\n\n**Instead of** `t(X) %*% X`, **use** `crossprod(X)`:\n\n| Operation | Slow (Don't Use) | Fast (Use This) | Speedup |\n|-----------|-----------------|----------------|---------|\n| $\\mathbf{X}'\\mathbf{X}$ | `t(X) %*% X` | `crossprod(X)` | ~2× |\n| $\\mathbf{X}'\\mathbf{y}$ | `t(X) %*% y` | `crossprod(X, y)` | ~2× |\n| $\\mathbf{X}\\mathbf{X}'$ | `X %*% t(X)` | `tcrossprod(X)` | ~2× |\n| $\\mathbf{X}\\mathbf{y}'$ | `X %*% t(y)` | `tcrossprod(X, y)` | ~2× |\n\n**Why faster?**\n\n1. No explicit transpose created (saves memory and time)\n2. Optimized BLAS routines called directly\n3. Better cache utilization\n\n**Example**:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Generate larger dataset\nset.seed(123)\nn <- 5000\np <- 10\nX <- matrix(rnorm(n * p), n, p)\ny <- rnorm(n)\n\n# Method 1: Slow\nsystem.time({\n  XtX_slow <- t(X) %*% X\n  Xty_slow <- t(X) %*% y\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n  0.001   0.000   0.001 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Method 2: Fast\nsystem.time({\n  XtX_fast <- crossprod(X)\n  Xty_fast <- crossprod(X, y)\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n  0.001   0.000   0.001 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify same result\ncat(\"Results identical?\", all.equal(XtX_slow, XtX_fast), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nResults identical? TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Results identical?\", all.equal(Xty_slow, Xty_fast), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nResults identical? TRUE \n```\n\n\n:::\n:::\n\n\n#### Solving Normal Equations Efficiently\n\n**Three methods**, ranked by efficiency:\n\n1. **QR decomposition** (best for most cases)\n2. **Cholesky decomposition** (if you know $\\mathbf{X}'\\mathbf{X}$ is well-conditioned)\n3. **Direct inverse** (avoid if possible!)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Setup\nX <- matrix(rnorm(100 * 5), 100, 5)\ny <- rnorm(100)\n\n# Method 1: Direct inverse (AVOID!)\nXtX <- t(X) %*% X\nXty <- t(X) %*% y\nb1 <- solve(XtX) %*% Xty  # Slow and unstable\n\n# Method 2: Solve without forming inverse (BETTER)\nXtX <- crossprod(X)\nXty <- crossprod(X, y)\nb2 <- solve(XtX, Xty)  # Faster, more stable\n\n# Method 3: Cholesky (GOOD for well-conditioned X'X)\nXtX <- crossprod(X)\nXty <- crossprod(X, y)\nR <- chol(XtX)  # Upper triangular\nb3 <- backsolve(R, forwardsolve(t(R), Xty))  # Two triangular solves\n\n# Method 4: QR decomposition (BEST)\nqr_decomp <- qr(X)\nb4 <- qr.coef(qr_decomp, y)  # Uses QR, most stable\n\n# Method 5: Using lm.fit (EASIEST, nearly as fast as QR)\nb5 <- lm.fit(X, y)$coefficients\n\ncat(\"Method 1 (inverse):\", b1[1:3], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMethod 1 (inverse): 0.002055 -0.1507 0.1348 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Method 2 (solve):\", b2[1:3], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMethod 2 (solve): 0.002055 -0.1507 0.1348 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Method 3 (Cholesky):\", b3[1:3], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMethod 3 (Cholesky): 0.002055 -0.1507 0.1348 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Method 4 (QR):\", b4[1:3], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMethod 4 (QR): 0.002055 -0.1507 0.1348 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Method 5 (lm.fit):\", b5[1:3], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMethod 5 (lm.fit): 0.002055 -0.1507 0.1348 \n```\n\n\n:::\n:::\n\n\n#### Vectorized Operations\n\n**Avoid loops** when possible. Use vectorized functions:\n\n| Task | Slow (Loop) | Fast (Vectorized) |\n|------|------------|------------------|\n| Column sums | `apply(X, 2, sum)` | `colSums(X)` |\n| Row sums | `apply(X, 1, sum)` | `rowSums(X)` |\n| Column means | `apply(X, 2, mean)` | `colMeans(X)` |\n| Row means | `apply(X, 1, mean)` | `rowMeans(X)` |\n\n**Example**:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- matrix(rnorm(1000 * 50), 1000, 50)\n\n# Slow\nsystem.time({\n  col_sums_slow <- apply(X, 2, sum)\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n  0.001   0.000   0.000 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Fast\nsystem.time({\n  col_sums_fast <- colSums(X)\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n  0.001   0.000   0.000 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Speedup:\",\n    system.time(apply(X, 2, sum))[3] / system.time(colSums(X))[3], \"×\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSpeedup: NaN ×\n```\n\n\n:::\n:::\n\n\n#### Centering and Scaling with sweep()\n\n**Task**: Center columns of X (subtract column means).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- matrix(rnorm(100 * 5), 100, 5)\n\n# Method 1: Manual (slow)\nX_centered1 <- X\nfor (j in 1:ncol(X)) {\n  X_centered1[, j] <- X[, j] - mean(X[, j])\n}\n\n# Method 2: Using sweep (fast)\nX_centered2 <- sweep(X, 2, colMeans(X), \"-\")\n\n# Method 3: Using scale (easiest for centering/scaling)\nX_centered3 <- scale(X, center = TRUE, scale = FALSE)\n\n# Verify\ncat(\"Method 1 = Method 2?\", all.equal(X_centered1, X_centered2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMethod 1 = Method 2? TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Method 2 = Method 3?\",\n    all.equal(X_centered2, as.matrix(X_centered3), check.attributes = FALSE), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMethod 2 = Method 3? TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\n# Column means should be ~0\ncat(\"Column means after centering:\", colMeans(X_centered2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nColumn means after centering: -2.429e-18 -1.527e-18 2.817e-17 8.119e-18 -2.637e-18 \n```\n\n\n:::\n:::\n\n\n#### Computing Fitted Values Efficiently\n\nDon't form the hat matrix $\\mathbf{H} = \\mathbf{X}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'$ explicitly!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 1000\np <- 10\nX <- matrix(rnorm(n * p), n, p)\ny <- rnorm(n)\n\n# NEVER do this (forms n×n matrix!)\n# H <- X %*% solve(t(X) %*% X) %*% t(X)\n# y_hat <- H %*% y\n\n# Method 1: Solve and multiply (good)\nb <- solve(crossprod(X), crossprod(X, y))\ny_hat1 <- X %*% b\n\n# Method 2: Using QR (better)\nqr_decomp <- qr(X)\ny_hat2 <- qr.fitted(qr_decomp, y)\n\n# Method 3: Using lm.fit (easiest)\nfit <- lm.fit(X, y)\ny_hat3 <- fit$fitted.values\n\ncat(\"All methods agree?\",\n    isTRUE(all.equal(y_hat1, y_hat2)) && isTRUE(all.equal(y_hat2, y_hat3)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAll methods agree? FALSE \n```\n\n\n:::\n:::\n\n\n#### Quadratic Forms: $\\mathbf{y}'\\mathbf{A}\\mathbf{y}$\n\n**Efficient computation** without forming full matrix product:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 500\nA <- matrix(rnorm(n * n), n, n)\nA <- A + t(A)  # Make symmetric\ny <- rnorm(n)\n\n# Method 1: Naive (forms n-vector twice)\nquad1 <- t(y) %*% A %*% y\n\n# Method 2: One multiplication (faster)\nquad2 <- t(y) %*% (A %*% y)\n\n# Method 3: Avoid transpose (fastest)\nquad3 <- sum(y * (A %*% y))\n\n# Method 4: If A is sparse, use Matrix package\nlibrary(Matrix)\nA_sparse <- as(A, \"dgCMatrix\")\nquad4 <- sum(y * (A_sparse %*% y))\n\ncat(\"Method 1:\", quad1, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMethod 1: -48.68 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Method 2:\", quad2, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMethod 2: -48.68 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Method 3:\", quad3, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMethod 3: -48.68 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"All equal?\",\n    isTRUE(all.equal(as.numeric(quad1), quad2)) && isTRUE(all.equal(quad2, quad3)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAll equal? FALSE \n```\n\n\n:::\n:::\n\n\n#### Livestock Example: Efficient Solver for Large Dataset\n\n**Scenario**: Beef cattle dataset with n=5000 animals, p=20 fixed effects.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate realistic beef cattle dataset\nset.seed(42)\nn <- 5000\np <- 20\n\n# Design matrix: breed, sex, age, farm, pen effects, etc.\nX <- matrix(rnorm(n * p), n, p)\nX[, 1] <- 1  # Intercept\ny <- rnorm(n, mean = 350, sd = 50)  # Carcass weight, kg\n\n# Efficient normal equations solver\nsystem.time({\n  XtX <- crossprod(X)         # X'X\n  Xty <- crossprod(X, y)      # X'y\n  b <- solve(XtX, Xty)        # Solve X'Xb = X'y\n  y_hat <- X %*% b            # Fitted values\n  SSE <- sum((y - y_hat)^2)   # Error SS\n  sigma2_hat <- SSE / (n - p) # Variance estimate\n\n  # Standard errors\n  Var_b <- solve(XtX) * sigma2_hat\n  se_b <- sqrt(diag(Var_b))\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n  0.003   0.000   0.003 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nFirst 5 estimates:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nFirst 5 estimates:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(b[1:5])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 349.2454  -0.3738   0.5285   0.9589  -0.4790\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nFirst 5 standard errors:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nFirst 5 standard errors:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(se_b[1:5])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.7050 0.7003 0.7033 0.6962 0.6949\n```\n\n\n:::\n\n```{.r .cell-code}\n# Compare with lm()\ncat(\"\\nVerify against lm():\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVerify against lm():\n```\n\n\n:::\n\n```{.r .cell-code}\nsystem.time({\n  fit_lm <- lm(y ~ X - 1)  # -1 to use X as-is\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n  0.006   0.000   0.006 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Coefficients match?\",\n    all.equal(as.numeric(b), coef(fit_lm), check.attributes = FALSE), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCoefficients match? TRUE \n```\n\n\n:::\n:::\n\n\n#### Memory-Efficient Computations\n\nFor **very large** datasets, avoid creating unnecessary copies:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# BAD: Creates many temporary objects\nXtX <- t(X) %*% X\nXtX_inv <- solve(XtX)\nXty <- t(X) %*% y\nb <- XtX_inv %*% Xty\ny_hat <- X %*% b\ne <- y - y_hat\nSSE <- t(e) %*% e\n\n# GOOD: Minimizes temporaries, uses efficient functions\nXtX <- crossprod(X)\nXty <- crossprod(X, y)\nb <- solve(XtX, Xty)  # Don't form inverse!\nSSE <- sum(y^2) - sum(Xty * b)  # Compute SSE without forming residuals\n```\n:::\n\n\n#### Benchmark Summary\n\n**Rules of thumb** (for modern computers):\n\n| Operation | Slow | Fast | Speedup |\n|-----------|------|------|---------|\n| $\\mathbf{X}'\\mathbf{X}$ | `t(X) %*% X` | `crossprod(X)` | ~2× |\n| Solve $\\mathbf{Ab} = \\mathbf{c}$ | `solve(A) %*% c` | `solve(A, c)` | ~3× |\n| Column sums | `apply(X, 2, sum)` | `colSums(X)` | ~50× |\n| Centering | `for` loop | `sweep()` or `scale()` | ~10× |\n| Normal equations | Form $(\\mathbf{X}'\\mathbf{X})^{-1}$ | Use `solve()` or QR | ~5× |\n\n**For very large problems** (n > 100,000 or p > 10,000):\n- Use sparse matrix methods (`Matrix` package)\n- Consider iterative solvers\n- Use parallel computing (`parallel` package)\n- Stream data from disk if doesn't fit in RAM\n\n#### Best Practices Summary\n\n::: {.callout-tip}\n## Efficient Coding Checklist\n\n**Always**:\n- ✅ Use `crossprod()` instead of `t(X) %*% X`\n- ✅ Use `solve(A, b)` instead of `solve(A) %*% b`\n- ✅ Use vectorized functions (`colSums`, `rowSums`, etc.)\n- ✅ Use `sweep()` or `scale()` for centering/scaling\n- ✅ Use `lm.fit()` or QR decomposition for solving\n\n**Never**:\n- ❌ Form $(\\mathbf{X}'\\mathbf{X})^{-1}$ explicitly unless you need it\n- ❌ Form hat matrix $\\mathbf{H}$ for large $n$\n- ❌ Use loops where vectorization works\n- ❌ Create unnecessary copies of large matrices\n- ❌ Use `apply()` when specialized functions exist\n\n**Profile your code** with `system.time()` or `microbenchmark` package!\n:::\n\n#### Quick Reference Card\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Efficient normal equations solution\nXtX <- crossprod(X)              # X'X\nXty <- crossprod(X, y)           # X'y\nb <- solve(XtX, Xty)             # Solve (don't invert!)\ny_hat <- X %*% b                 # Fitted values\nSSE <- sum(y^2) - sum(Xty * b)   # SSE without forming residuals\nsigma2 <- SSE / (n - p)          # Variance\nVar_b <- solve(XtX) * sigma2     # Var(b) - only if you need SE\nse_b <- sqrt(diag(Var_b))        # Standard errors\n\n# Even better: use lm.fit() for production code\nfit <- lm.fit(X, y)\nb <- fit$coefficients\ny_hat <- fit$fitted.values\nSSE <- sum(fit$residuals^2)\n```\n:::\n\n\n---\n\n## Computational Considerations {#sec-computation}\n\nThis section addresses the practical realities of computer arithmetic. Even mathematically correct formulas can produce incorrect results due to rounding errors and numerical instability.\n\n::: {.callout-warning}\n##Why This Matters\n\n**Just because the math is correct doesn't mean the computer will get it right!**\n\nIll-conditioned problems, poor algorithms, and rounding errors can produce:\n- Wrong parameter estimates\n- Negative variances (!!)\n- Failed convergence\n- Nonsensical results\n\nLearning these issues NOW will save you hours of debugging later.\n:::\n\n### Numerical Stability {#sec-numerical-stability}\n\n**Numerical stability** refers to how sensitive a computation is to rounding errors. An algorithm is **numerically stable** if small errors in input produce small errors in output.\n\n#### Condition Numbers\n\nThe **condition number** of a matrix $\\mathbf{A}$ measures how sensitive $\\mathbf{A}^{-1}$ is to perturbations:\n\n$$\n\\kappa(\\mathbf{A}) = \\|\\mathbf{A}\\| \\cdot \\|\\mathbf{A}^{-1}\\|\n$$\n\nFor symmetric positive definite matrices:\n\n$$\n\\kappa(\\mathbf{A}) = \\frac{\\lambda_{\\max}(\\mathbf{A})}{\\lambda_{\\min}(\\mathbf{A})}\n$$\n\n**Interpretation**:\n\n| Condition Number | Matrix Status | Inversion Accuracy |\n|-----------------|---------------|-------------------|\n| $\\kappa < 10$ | Well-conditioned | Excellent |\n| $10 \\leq \\kappa < 10^3$ | Moderate | Good |\n| $10^3 \\leq \\kappa < 10^6$ | Ill-conditioned | Poor, use caution |\n| $\\kappa \\geq 10^6$ | Severely ill-conditioned | Unreliable, avoid if possible |\n\n**Rule of thumb**: You lose about $\\log_{10}(\\kappa)$ digits of precision.\n\n::: {.callout-note}\n## Example: Collinearity Creates Ill-Conditioning\n\nIn linear models, **collinearity** makes $\\mathbf{X}'\\mathbf{X}$ ill-conditioned:\n\n- If predictors are highly correlated, $\\mathbf{X}'\\mathbf{X}$ has small eigenvalues\n- Small eigenvalues → large condition number → unstable inversion\n- Result: Huge standard errors, unstable estimates\n:::\n\n#### Computing Condition Numbers in R\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Well-conditioned matrix\nA_good <- matrix(c(4, 1, 1, 3), 2, 2)\nkappa_good <- kappa(A_good)\ncat(\"Well-conditioned matrix:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nWell-conditioned matrix:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A_good)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    4    1\n[2,]    1    3\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Condition number:\", kappa_good, \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCondition number: 1.917 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Ill-conditioned matrix (high collinearity)\nA_bad <- matrix(c(1.0, 0.99,\n                  0.99, 1.0), 2, 2, byrow = TRUE)\nkappa_bad <- kappa(A_bad)\ncat(\"Ill-conditioned matrix:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nIll-conditioned matrix:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A_bad)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,] 1.00 0.99\n[2,] 0.99 1.00\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Condition number:\", kappa_bad, \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCondition number: 200 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Severely ill-conditioned (nearly singular)\nA_terrible <- matrix(c(1.0, 0.9999,\n                       0.9999, 1.0), 2, 2, byrow = TRUE)\nkappa_terrible <- kappa(A_terrible)\ncat(\"Severely ill-conditioned matrix:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSeverely ill-conditioned matrix:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A_terrible)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       [,1]   [,2]\n[1,] 1.0000 0.9999\n[2,] 0.9999 1.0000\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Condition number:\", kappa_terrible, \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCondition number: 20000 \n```\n\n\n:::\n\n```{.r .cell-code}\n# For design matrix X'X\nset.seed(123)\nn <- 100\nx1 <- rnorm(n)\nx2 <- x1 + rnorm(n, sd = 0.01)  # Highly correlated!\nX <- cbind(1, x1, x2)\nXtX <- crossprod(X)\nkappa_XtX <- kappa(XtX)\ncat(\"X'X with collinearity:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nX'X with collinearity:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Condition number:\", kappa_XtX, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCondition number: 42730 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Correlation between x1 and x2:\", cor(x1, x2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCorrelation between x1 and x2: 0.9999 \n```\n\n\n:::\n:::\n\n\n#### Sources of Numerical Instability\n\n**1. Subtracting Nearly Equal Numbers**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Computing variance: Var(x) = E(x²) - [E(x)]²\n# Two formulas, mathematically equivalent\n\nx <- rnorm(100, mean = 1e8, sd = 1)  # Large mean, small variance\n\n# Formula 1: Textbook (UNSTABLE for large mean)\nmean_x <- mean(x)\nvar1 <- mean(x^2) - mean_x^2\ncat(\"Method 1 (textbook, unstable):\", var1, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMethod 1 (textbook, unstable): 0 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Formula 2: Deviations from mean (STABLE)\nvar2 <- mean((x - mean_x)^2)\ncat(\"Method 2 (deviations, stable):\", var2, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMethod 2 (deviations, stable): 0.8932 \n```\n\n\n:::\n\n```{.r .cell-code}\n# R's built-in (uses stable algorithm)\nvar3 <- var(x) * (length(x) - 1) / length(x)  # Adjust for n vs n-1\ncat(\"Method 3 (R's var()):\", var3, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMethod 3 (R's var()): 0.8932 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nTrue variance (from simulation):\", 1, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nTrue variance (from simulation): 1 \n```\n\n\n:::\n:::\n\n\n**2. Forming $(\\mathbf{X}'\\mathbf{X})^{-1}$ Explicitly**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Generate X'X with moderate collinearity\nset.seed(456)\nn <- 100\np <- 5\nX <- matrix(rnorm(n * p), n, p)\nX[, 2] <- X[, 1] + rnorm(n, sd = 0.1)  # Collinear columns\ny <- rnorm(n)\n\nXtX <- crossprod(X)\nXty <- crossprod(X, y)\n\n# Method 1: Explicit inverse (potentially unstable)\nXtX_inv <- solve(XtX)\nb1 <- XtX_inv %*% Xty\n\n# Method 2: Solve directly (more stable)\nb2 <- solve(XtX, Xty)\n\ncat(\"Estimates match?\", all.equal(b1, b2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEstimates match? TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Condition number of X'X:\", kappa(XtX), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCondition number of X'X: 293.9 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Check: how well do we solve X'X * b = X'y?\nresidual1 <- XtX %*% b1 - Xty\nresidual2 <- XtX %*% b2 - Xty\ncat(\"||X'Xb - X'y|| using inverse:\", sqrt(sum(residual1^2)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n||X'Xb - X'y|| using inverse: 6.319e-14 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"||X'Xb - X'y|| using solve():\", sqrt(sum(residual2^2)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n||X'Xb - X'y|| using solve(): 9.125e-15 \n```\n\n\n:::\n:::\n\n\n**3. Large Range in Data Values**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Predictors with vastly different scales\nn <- 50\nage_days <- sample(200:400, n, replace = TRUE)  # Scale: O(100)\nweight_mg <- runif(n, 5000, 8000)               # Scale: O(1000)\ny <- rnorm(n)\n\nX_unscaled <- cbind(1, age_days, weight_mg)\n\n# Condition number without scaling\nkappa_unscaled <- kappa(crossprod(X_unscaled))\ncat(\"Condition number (unscaled):\", kappa_unscaled, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCondition number (unscaled): 4.288e+09 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Scale predictors (mean 0, sd 1)\nX_scaled <- scale(X_unscaled[, -1])  # Don't scale intercept\nX_scaled <- cbind(1, X_scaled)\n\nkappa_scaled <- kappa(crossprod(X_scaled))\ncat(\"Condition number (scaled):\", kappa_scaled, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCondition number (scaled): 1.263 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Improvement:\", kappa_unscaled / kappa_scaled, \"×\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nImprovement: 3.394e+09 ×\n```\n\n\n:::\n:::\n\n\n#### Remedies for Numerical Instability\n\n::: {.callout-tip}\n## Strategies to Improve Numerical Stability\n\n**1. Center and Scale Predictors**\n```r\nX_centered <- scale(X, center = TRUE, scale = TRUE)\n```\n\n**2. Use Stable Algorithms**\n- QR decomposition instead of normal equations\n- Cholesky instead of explicit inverse (if well-conditioned)\n- SVD for rank-deficient problems\n\n**3. Avoid Explicit Matrix Inversion**\n```r\n# BAD\nb <- solve(XtX) %*% Xty\n\n# GOOD\nb <- solve(XtX, Xty)\n```\n\n**4. Check Condition Numbers**\n```r\nif (kappa(XtX) > 1e6) {\n  warning(\"X'X is severely ill-conditioned!\")\n}\n```\n\n**5. Ridge Regression for Collinearity**\n```r\n# Add small value to diagonal\nlambda <- 0.01\nXtX_ridge <- XtX + lambda * diag(ncol(XtX))\nb_ridge <- solve(XtX_ridge, Xty)\n```\n\n**6. Use QR Decomposition**\n```r\nqr_obj <- qr(X)\nb <- qr.coef(qr_obj, y)  # Most numerically stable\n```\n:::\n\n#### Livestock Example: Collinearity in Dairy Data\n\n**Scenario**: Predicting milk yield from fat%, protein%, and lactose% (highly correlated).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulated dairy data\nset.seed(789)\nn <- 50\n# These are compositionally constrained (sum to ~100%)\nfat_pct <- runif(n, 3.5, 4.5)\nprotein_pct <- runif(n, 3.0, 3.5)\nlactose_pct <- 100 - fat_pct - protein_pct - runif(n, 85, 89)  # Highly constrained!\n\nmilk_yield <- 30 + 2*fat_pct - 1*protein_pct + 3*lactose_pct + rnorm(n, sd = 2)\n\n# Check correlations\ncomp <- cbind(fat_pct, protein_pct, lactose_pct)\ncat(\"Correlation matrix:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCorrelation matrix:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(cor(comp))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            fat_pct protein_pct lactose_pct\nfat_pct      1.0000    -0.12782    -0.18836\nprotein_pct -0.1278     1.00000     0.07922\nlactose_pct -0.1884     0.07922     1.00000\n```\n\n\n:::\n\n```{.r .cell-code}\n# Design matrix\nX <- cbind(1, fat_pct, protein_pct, lactose_pct)\nXtX <- crossprod(X)\n\n# Check condition number\nkappa_comp <- kappa(XtX)\ncat(\"\\nCondition number:\", kappa_comp, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCondition number: 44267 \n```\n\n\n:::\n\n```{.r .cell-code}\nif (kappa_comp > 1000) {\n  cat(\"WARNING: X'X is ill-conditioned due to collinearity!\\n\")\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nWARNING: X'X is ill-conditioned due to collinearity!\n```\n\n\n:::\n\n```{.r .cell-code}\n# Fit model anyway\nb <- solve(XtX, crossprod(X, milk_yield))\nVar_b <- solve(XtX) * (sum((milk_yield - X %*% b)^2) / (n - 4))\nse_b <- sqrt(diag(Var_b))\n\ncat(\"\\nEstimates and SE:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nEstimates and SE:\n```\n\n\n:::\n\n```{.r .cell-code}\nresults <- data.frame(\n  Parameter = c(\"Intercept\", \"Fat %\", \"Protein %\", \"Lactose %\"),\n  Estimate = b,\n  SE = se_b\n)\nprint(results)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            Parameter Estimate     SE\n            Intercept 29.00842 8.9849\nfat_pct         Fat %  1.10194 1.2282\nprotein_pct Protein % -0.02917 2.0826\nlactose_pct Lactose %  3.22353 0.2685\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nNote the HUGE standard errors due to collinearity!\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nNote the HUGE standard errors due to collinearity!\n```\n\n\n:::\n\n```{.r .cell-code}\n# VIF (Variance Inflation Factor)\nlibrary(car)\nfit <- lm(milk_yield ~ fat_pct + protein_pct + lactose_pct)\nvif_values <- vif(fit)\ncat(\"\\nVIF (>10 indicates severe collinearity):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVIF (>10 indicates severe collinearity):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(vif_values)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    fat_pct protein_pct lactose_pct \n      1.051       1.020       1.040 \n```\n\n\n:::\n:::\n\n\n#### Detecting Numerical Problems\n\n**Warning signs**:\n\n1. **Huge standard errors** (compared to estimates)\n2. **Estimates change drastically** when you add/remove observations\n3. **Opposite signs** from what you expect biologically\n4. **`solve()` fails** with \"system is computationally singular\"\n5. **VIF > 10** for any predictor\n6. **Condition number > 10^6**\n\n**Diagnostic checklist**:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1. Check condition number\nkappa_value <- kappa(XtX)\nif (kappa_value > 1e6) warning(\"Severely ill-conditioned!\")\n\n# 2. Check correlations\ncor_matrix <- cor(X[, -1])  # Exclude intercept\nif (any(abs(cor_matrix[upper.tri(cor_matrix)]) > 0.95)) {\n  warning(\"High correlations detected!\")\n}\n\n# 3. Check VIF\nlibrary(car)\nfit <- lm(y ~ X - 1)\nvif_values <- vif(fit)\nif (any(vif_values > 10)) warning(\"High VIF detected!\")\n\n# 4. Check eigenvalues\neigenvalues <- eigen(XtX)$values\nif (min(eigenvalues) < 1e-10) warning(\"Near-zero eigenvalue!\")\n\n# 5. Examine SE/estimate ratios\nse_ratio <- se_b / abs(b)\nif (any(se_ratio > 1, na.rm = TRUE)) {\n  warning(\"Standard error exceeds estimate!\")\n}\n```\n:::\n\n\n::: {.callout-warning}\n## When in Doubt, Use QR Decomposition\n\nThe QR decomposition is **numerically stable** even for ill-conditioned problems:\n\n```r\nqr_obj <- qr(X)\nb <- qr.coef(qr_obj, y)\n```\n\n**Why QR is better**:\n- Avoids forming $\\mathbf{X}'\\mathbf{X}$ (condition number squared!)\n- Uses orthogonal transformations (preserve lengths, numerically stable)\n- Automatic detection of rank deficiency\n- Used by `lm()` under the hood\n:::\n\n---\n\n### Efficient Computation {#sec-efficient-computation}\n\nChoosing the right algorithm can make the difference between a computation taking seconds versus hours. This section presents the best algorithms for common linear models tasks.\n\n#### Algorithm Comparison: Solving $\\mathbf{X}'\\mathbf{X}\\mathbf{b} = \\mathbf{X}'\\mathbf{y}$\n\n**Five methods**, ranked by numerical stability and speed:\n\n| Method | Operation Count | Stability | When to Use |\n|--------|----------------|-----------|-------------|\n| **QR decomposition** | $O(np^2)$ | Excellent | **Default choice** |\n| **Cholesky** | $O(p^3/3)$ | Good (if well-conditioned) | X'X already computed, well-conditioned |\n| **solve(XtX, Xty)** | $O(p^3)$ | Good | Simple, readable code |\n| **SVD** | $O(np^2 + p^3)$ | Excellent | Rank-deficient, collinear |\n| **Direct inverse** | $O(p^3)$ | Poor | **AVOID** |\n\n#### Method 1: QR Decomposition (Recommended)\n\n**Factorization**: $\\mathbf{X} = \\mathbf{QR}$ where $\\mathbf{Q}' \\mathbf{Q} = \\mathbf{I}$ and $\\mathbf{R}$ is upper triangular.\n\n**Solution**:\n$$\n\\mathbf{X}'\\mathbf{X}\\mathbf{b} = \\mathbf{X}'\\mathbf{y} \\implies \\mathbf{R}'\\mathbf{R}\\mathbf{b} = \\mathbf{R}'\\mathbf{Q}'\\mathbf{y} \\implies \\mathbf{R}\\mathbf{b} = \\mathbf{Q}'\\mathbf{y}\n$$\n\nSolve by back-substitution (fast and stable).\n\n**Advantages**:\n- Never forms $\\mathbf{X}'\\mathbf{X}$ (avoids squaring condition number)\n- Detects rank deficiency automatically\n- Used by `lm()` in R\n- Most numerically stable\n\n**R Implementation**:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(100)\nn <- 100\np <- 5\nX <- matrix(rnorm(n * p), n, p)\ny <- rnorm(n)\n\n# QR decomposition\nqr_obj <- qr(X)\n\n# Extract Q and R (for illustration)\nQ <- qr.Q(qr_obj)\nR <- qr.R(qr_obj)\n\n# Verify X = QR\ncat(\"X = QR?\", all.equal(X, Q %*% R), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nX = QR? TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify Q'Q = I\ncat(\"Q'Q = I?\", all.equal(crossprod(Q), diag(ncol(Q))), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nQ'Q = I? TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\n# Solve for b\nb_qr <- qr.coef(qr_obj, y)\n\n# Get fitted values and residuals\ny_hat_qr <- qr.fitted(qr_obj, y)\nresid_qr <- qr.resid(qr_obj, y)\n\ncat(\"\\nFirst 3 coefficients:\", b_qr[1:3], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nFirst 3 coefficients: -0.05373 -0.00965 -0.1087 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Compare with lm()\nfit_lm <- lm(y ~ X - 1)\ncat(\"Match lm()?\", all.equal(as.numeric(b_qr), coef(fit_lm)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMatch lm()? names for current but not for target \n```\n\n\n:::\n:::\n\n\n#### Method 2: Cholesky Decomposition\n\n**Factorization**: $\\mathbf{X}'\\mathbf{X} = \\mathbf{L}\\mathbf{L}'$ where $\\mathbf{L}$ is lower triangular (or $\\mathbf{R}\\mathbf{R}'$ with $\\mathbf{R}$ upper triangular).\n\n**Solution**:\n1. Compute $\\mathbf{X}'\\mathbf{X}$ and $\\mathbf{X}'\\mathbf{y}$\n2. Factor: $\\mathbf{X}'\\mathbf{X} = \\mathbf{R}'\\mathbf{R}$\n3. Solve $\\mathbf{R}'\\mathbf{w} = \\mathbf{X}'\\mathbf{y}$ (forward substitution)\n4. Solve $\\mathbf{R}\\mathbf{b} = \\mathbf{w}$ (back substitution)\n\n**Advantages**:\n- Faster than QR if $p \\ll n$\n- Exploits symmetry and positive definiteness\n- Good for repeated solves with same $\\mathbf{X}'\\mathbf{X}$\n\n**Disadvantages**:\n- Requires $\\mathbf{X}'\\mathbf{X}$ to be positive definite\n- Less stable than QR for ill-conditioned matrices\n\n**R Implementation**:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Same data as above\nXtX <- crossprod(X)\nXty <- crossprod(X, y)\n\n# Cholesky factorization\nR_chol <- chol(XtX)  # Upper triangular\n\n# Verify X'X = R'R\ncat(\"X'X = R'R?\", all.equal(XtX, t(R_chol) %*% R_chol), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nX'X = R'R? TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\n# Solve R'w = X'y\nw <- forwardsolve(t(R_chol), Xty)\n\n# Solve Rb = w\nb_chol <- backsolve(R_chol, w)\n\ncat(\"QR = Cholesky?\", all.equal(b_qr, b_chol), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nQR = Cholesky? Attributes: < target is NULL, current is list > target is numeric, current is matrix \n```\n\n\n:::\n\n```{.r .cell-code}\n# Shortcut: chol2inv() to get (X'X)^(-1)\nXtX_inv <- chol2inv(R_chol)\nb_chol2 <- XtX_inv %*% Xty\ncat(\"Alternative Cholesky:\", all.equal(b_chol, b_chol2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAlternative Cholesky: TRUE \n```\n\n\n:::\n:::\n\n\n#### Method 3: Direct solve()\n\n**R's solve()** uses LU decomposition with partial pivoting.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Method 1: Form inverse (BAD!)\nXtX_inv_bad <- solve(XtX)\nb_bad <- XtX_inv_bad %*% Xty\n\n# Method 2: Solve directly (GOOD!)\nb_solve <- solve(XtX, Xty)\n\ncat(\"Both methods agree:\", all.equal(b_bad, b_solve), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBoth methods agree: TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\n# But Method 2 is faster and more accurate\n# Benchmark\nlibrary(microbenchmark)\nmbm <- microbenchmark(\n  inverse = solve(XtX) %*% Xty,\n  direct = solve(XtX, Xty),\n  times = 100\n)\nprint(summary(mbm)[, c(\"expr\", \"median\")])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     expr median\n1 inverse  23.38\n2  direct  15.00\n```\n\n\n:::\n:::\n\n\n#### Method 4: SVD (for rank-deficient problems)\n\n**Singular Value Decomposition**: $\\mathbf{X} = \\mathbf{U}\\mathbf{D}\\mathbf{V}'$\n\n- $\\mathbf{U}$: $n \\times p$ orthogonal\n- $\\mathbf{D}$: $p \\times p$ diagonal (singular values)\n- $\\mathbf{V}$: $p \\times p$ orthogonal\n\n**Moore-Penrose inverse**: $\\mathbf{X}^+ = \\mathbf{V}\\mathbf{D}^+\\mathbf{U}'$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create rank-deficient X\nX_rank_def <- X\nX_rank_def[, 5] <- X_rank_def[, 1] + X_rank_def[, 2]  # Perfect collinearity\n\n# QR will detect rank deficiency\nqr_obj_def <- qr(X_rank_def)\ncat(\"Rank of X:\", qr_obj_def$rank, \"(should be 4, not 5)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRank of X: 4 (should be 4, not 5)\n```\n\n\n:::\n\n```{.r .cell-code}\n# SVD approach\nsvd_obj <- svd(X_rank_def)\nd <- svd_obj$d\nu <- svd_obj$u\nv <- svd_obj$v\n\n# Threshold small singular values\nthreshold <- 1e-10\nd_inv <- ifelse(d > threshold, 1/d, 0)\n\n# Moore-Penrose inverse\nX_pinv <- v %*% diag(d_inv) %*% t(u)\nb_svd <- X_pinv %*% y\n\ncat(\"\\nSingular values:\", d, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSingular values: 15.08 11.51 10.29 8.206 2.439e-15 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Small singular value indicates rank deficiency\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSmall singular value indicates rank deficiency\n```\n\n\n:::\n\n```{.r .cell-code}\n# Compare with MASS::ginv()\nlibrary(MASS)\nb_ginv <- ginv(X_rank_def) %*% y\ncat(\"SVD = ginv()?\", all.equal(b_svd, b_ginv, check.attributes = FALSE), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSVD = ginv()? TRUE \n```\n\n\n:::\n:::\n\n\n#### When to Use Which Method?\n\n**Decision tree**:\n\n```\nIs X'X rank deficient?\n├─ YES → Use SVD or generalized inverse\n│         (See Week 12 on non-full rank models)\n│\n└─ NO → Is X'X already computed?\n        ├─ YES → Is condition number < 1000?\n        │        ├─ YES → Use Cholesky\n        │        └─ NO  → Use QR\n        │\n        └─ NO → Always use QR\n```\n\n::: {.callout-tip}\n## Practical Recommendations\n\n**For most linear models work**:\n```r\n# Just use lm() or lm.fit()\nfit <- lm(y ~ X)\n# OR\nfit <- lm.fit(X, y)\n```\n\n**If you must code your own**:\n```r\n# Use QR\nqr_obj <- qr(X)\nb <- qr.coef(qr_obj, y)\n```\n\n**For repeated solves** (same $\\mathbf{X}$, different $\\mathbf{y}$):\n```r\n# Factor once\nqr_obj <- qr(X)\n\n# Solve many times\nb1 <- qr.coef(qr_obj, y1)\nb2 <- qr.coef(qr_obj, y2)\nb3 <- qr.coef(qr_obj, y3)\n```\n:::\n\n#### Beef Cattle Example: Algorithm Comparison\n\n**Scenario**: Compare methods for n=500, p=10.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Realistic beef cattle data\nset.seed(999)\nn <- 500\np <- 10\n\n# Design matrix: breed, sex, sire, dam, pen effects, etc.\nX <- matrix(rnorm(n * p), n, p)\nX[, 1] <- 1  # Intercept\ny <- rnorm(n, mean = 450, sd = 50)  # Carcass weight, kg\n\n# Method 1: QR (best)\nsystem.time({\n  qr_obj <- qr(X)\n  b1 <- qr.coef(qr_obj, y)\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n  0.001   0.000   0.000 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Method 2: Cholesky (fast if well-conditioned)\nsystem.time({\n  XtX <- crossprod(X)\n  Xty <- crossprod(X, y)\n  R <- chol(XtX)\n  b2 <- backsolve(R, forwardsolve(t(R), Xty))\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n      0       0       0 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Method 3: Direct solve (easy)\nsystem.time({\n  XtX <- crossprod(X)\n  Xty <- crossprod(X, y)\n  b3 <- solve(XtX, Xty)\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n  0.001   0.000   0.001 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Method 4: lm.fit() (easiest)\nsystem.time({\n  fit <- lm.fit(X, y)\n  b4 <- fit$coefficients\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n  0.000   0.000   0.001 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify all agree\ncat(\"QR = Cholesky?\", all.equal(b1, b2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nQR = Cholesky? Attributes: < target is NULL, current is list > target is numeric, current is matrix \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"QR = solve()?\", all.equal(b1, b3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nQR = solve()? Attributes: < target is NULL, current is list > target is numeric, current is matrix \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"QR = lm.fit()?\", all.equal(b1, b4, check.attributes = FALSE), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nQR = lm.fit()? TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\n# Check condition number\ncat(\"\\nCondition number of X'X:\", kappa(crossprod(X)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCondition number of X'X: 2.365 \n```\n\n\n:::\n:::\n\n\n#### Computing Standard Errors Efficiently\n\n**Don't compute** $(\\mathbf{X}'\\mathbf{X})^{-1}$ unless you need the full variance-covariance matrix!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Suppose we only need SE for one coefficient (e.g., β₁)\n\n# Method 1: Full inverse (SLOW if you only need one SE)\nXtX <- crossprod(X)\nXtX_inv <- solve(XtX)\nsigma2_hat <- sum((y - X %*% b1)^2) / (n - p)\nse_full <- sqrt(diag(XtX_inv) * sigma2_hat)\n\n# Method 2: Solve for specific columns (FASTER)\n# Var(b_j) = [(X'X)^(-1)]_jj * σ²\n# To get [(X'X)^(-1)]_jj, solve X'X * v_j = e_j\ne1 <- c(0, 1, 0, 0, 0, 0, 0, 0, 0, 0)  # Second coefficient (β₁)\nv1 <- solve(XtX, e1)\nvar_b1_method2 <- v1[2] * sigma2_hat\nse_b1_method2 <- sqrt(var_b1_method2)\n\ncat(\"SE(β₁) via full inverse:\", se_full[2], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSE(β₁) via full inverse: 2.289 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"SE(β₁) via single solve:\", se_b1_method2, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSE(β₁) via single solve: 2.289 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Match?\", all.equal(se_full[2], se_b1_method2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMatch? TRUE \n```\n\n\n:::\n:::\n\n\n::: {.callout-note}\n## Memory Considerations\n\n**For very large $n$**:\n\n- **QR decomposition**: Stores $n \\times p$ matrix $\\mathbf{Q}$ → $O(np)$ memory\n- **Cholesky**: Only stores $p \\times p$ matrix $\\mathbf{R}$ → $O(p^2)$ memory\n- **Trade-off**: Cholesky uses less memory but requires forming $\\mathbf{X}'\\mathbf{X}$\n\n**Rule of thumb**:\n- If $n > 10p$, Cholesky saves memory\n- If worried about stability, always use QR\n:::\n\n#### Summary: Algorithm Selection\n\n| Situation | Best Algorithm | R Function |\n|-----------|---------------|-----------|\n| General linear models | QR decomposition | `lm()` or `qr.coef()` |\n| Well-conditioned X'X | Cholesky | `chol()` then `backsolve()` |\n| Rank deficient | SVD or g-inverse | `svd()` or `MASS::ginv()` |\n| Need full Var(b) | QR or Cholesky | `lm()` then `vcov()` |\n| Just need estimates | QR | `qr.coef()` |\n| Repeated solves | Factor once, solve many | Store `qr_obj` |\n| Large $n$, small $p$ | Cholesky (memory) | `chol()` |\n| Ill-conditioned | QR or SVD | `qr()` or `svd()` |\n\n---\n\n### Checking Your Work {#sec-verification}\n\n**Trust, but verify.** Even careful programmers make mistakes. This section presents systematic strategies for verifying matrix computations.\n\n::: {.callout-important}\n## The Cardinal Rule of Numerical Computing\n\n**Always verify your results!**\n\nCheck your work by:\n1. Testing matrix properties\n2. Comparing with known solutions\n3. Verifying against `lm()`\n4. Checking mathematical identities\n5. Using `all.equal()` for tolerant comparisons\n:::\n\n#### Using all.equal() Correctly\n\n**Never use** `==` for floating-point comparisons!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example: Two ways to compute the same thing\nx <- c(1, 2, 3)\nresult1 <- sum(x) / length(x)\nresult2 <- mean(x)\n\n# BAD: Exact comparison (may fail due to rounding!)\nif (result1 == result2) {\n  cat(\"Equal!\\n\")\n} else {\n  cat(\"Not equal!\\n\")  # Might happen even though they're the same!\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEqual!\n```\n\n\n:::\n\n```{.r .cell-code}\n# GOOD: Tolerant comparison\nif (all.equal(result1, result2)) {\n  cat(\"Equal (within tolerance)!\\n\")\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEqual (within tolerance)!\n```\n\n\n:::\n\n```{.r .cell-code}\n# Show the default tolerance\ntolerance <- sqrt(.Machine$double.eps)  # ≈ 1.5e-8\ncat(\"Default tolerance:\", tolerance, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDefault tolerance: 1.49e-08 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Custom tolerance\ncat(\"Equal with tighter tolerance?\",\n    all.equal(result1, result2, tolerance = 1e-15), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEqual with tighter tolerance? TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\n# Checking matrices\nA <- matrix(c(1, 2, 3, 4), 2, 2)\nB <- A + 1e-10  # Tiny difference\n\ncat(\"\\nA == B (exact)?\", all(A == B), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nA == B (exact)? FALSE \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"all.equal(A, B)?\", all.equal(A, B), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nall.equal(A, B)? TRUE \n```\n\n\n:::\n:::\n\n\n#### Verification Checklist for Linear Models\n\nWhen you implement a least squares solver, verify these properties:\n\n**1. Normal equations satisfied**: $\\mathbf{X}'\\mathbf{X}\\mathbf{b} = \\mathbf{X}'\\mathbf{y}$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Setup\nset.seed(111)\nn <- 50\np <- 3\nX <- matrix(rnorm(n * p), n, p)\ny <- rnorm(n)\n\n# Solve\nb <- solve(crossprod(X), crossprod(X, y))\n\n# Check: X'Xb should equal X'y\nLHS <- crossprod(X) %*% b\nRHS <- crossprod(X, y)\n\ncat(\"Normal equations satisfied?\", all.equal(LHS, RHS), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNormal equations satisfied? TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Max absolute difference:\", max(abs(LHS - RHS)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMax absolute difference: 0 \n```\n\n\n:::\n:::\n\n\n**2. Residuals orthogonal to X**: $\\mathbf{X}'\\mathbf{e} = \\mathbf{0}$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compute residuals\ny_hat <- X %*% b\ne <- y - y_hat\n\n# Check orthogonality\nXte <- crossprod(X, e)\n\ncat(\"X'e = 0?\", all.equal(Xte, matrix(0, p, 1), check.attributes = FALSE), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nX'e = 0? TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Max |X'e|:\", max(abs(Xte)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMax |X'e|: 2.415e-15 \n```\n\n\n:::\n:::\n\n\n**3. Fitted values orthogonal to residuals**: $\\hat{\\mathbf{y}}'\\mathbf{e} = 0$\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninner_product <- t(y_hat) %*% e\n\ncat(\"y_hat'e = 0?\", all.equal(inner_product, matrix(0, 1, 1), check.attributes = FALSE), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ny_hat'e = 0? TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"|y_hat'e| =\", abs(inner_product), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n|y_hat'e| = 4.996e-16 \n```\n\n\n:::\n:::\n\n\n**4. Sum of squares decomposition**: $\\text{SST} = \\text{SSM} + \\text{SSE}$\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny_bar <- mean(y)\nSST <- sum((y - y_bar)^2)\nSSM <- sum((y_hat - y_bar)^2)\nSSE <- sum(e^2)\n\ncat(\"SST =\", SST, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSST = 38.12 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"SSM =\", SSM, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSSM = 3.22 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"SSE =\", SSE, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSSE = 37.51 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"SSM + SSE =\", SSM + SSE, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSSM + SSE = 40.73 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"SST = SSM + SSE?\", all.equal(SST, SSM + SSE), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSST = SSM + SSE? Mean relative difference: 0.06853 \n```\n\n\n:::\n:::\n\n\n**5. Compare with lm()**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your implementation\nmy_results <- list(\n  coefficients = b,\n  fitted = y_hat,\n  residuals = e,\n  sigma = sqrt(SSE / (n - p))\n)\n\n# R's lm()\nfit_lm <- lm.fit(X, y)\n\n# Compare\ncat(\"Coefficients match?\",\n    all.equal(as.numeric(my_results$coefficients),\n              fit_lm$coefficients), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCoefficients match? names for current but not for target \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Fitted values match?\",\n    all.equal(as.numeric(my_results$fitted),\n              fit_lm$fitted.values), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFitted values match? TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Residuals match?\",\n    all.equal(as.numeric(my_results$residuals),\n              fit_lm$residuals), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nResiduals match? TRUE \n```\n\n\n:::\n:::\n\n\n#### Testing Matrix Properties\n\n**Template function** for verifying matrix properties:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_matrix_properties <- function(A, name = \"A\", tolerance = 1e-10) {\n  cat(\"\\n=== Testing\", name, \"===\\n\")\n\n  # 1. Check dimensions\n  cat(\"Dimensions:\", nrow(A), \"×\", ncol(A), \"\\n\")\n\n  # 2. Check if square\n  is_square <- nrow(A) == ncol(A)\n  cat(\"Square?\", is_square, \"\\n\")\n\n  if (is_square) {\n    # 3. Check symmetry\n    is_symmetric <- all.equal(A, t(A), tolerance = tolerance)\n    cat(\"Symmetric?\", isTRUE(is_symmetric), \"\\n\")\n\n    # 4. Check if diagonal\n    is_diag <- all(A[row(A) != col(A)] < tolerance)\n    cat(\"Diagonal?\", is_diag, \"\\n\")\n\n    # 5. Check idempotence (A² = A)\n    A2 <- A %*% A\n    is_idempotent <- all.equal(A, A2, tolerance = tolerance)\n    cat(\"Idempotent?\", isTRUE(is_idempotent), \"\\n\")\n\n    # 6. Rank\n    cat(\"Rank:\", qr(A)$rank, \"\\n\")\n\n    # 7. Condition number\n    kappa_A <- kappa(A)\n    cat(\"Condition number:\", kappa_A, \"\\n\")\n    if (kappa_A > 1000) {\n      cat(\"  WARNING: Ill-conditioned!\\n\")\n    }\n\n    # 8. Eigenvalues\n    eigenvalues <- eigen(A, only.values = TRUE)$values\n    cat(\"Eigenvalues:\", eigenvalues, \"\\n\")\n\n    # 9. Positive definite?\n    is_pd <- all(eigenvalues > tolerance)\n    cat(\"Positive definite?\", is_pd, \"\\n\")\n\n    # 10. Trace\n    cat(\"Trace:\", sum(diag(A)), \"\\n\")\n\n    # 11. Determinant\n    cat(\"Determinant:\", det(A), \"\\n\")\n  }\n\n  invisible(NULL)\n}\n\n# Test on hat matrix\nX <- matrix(rnorm(50 * 3), 50, 3)\nH <- X %*% solve(crossprod(X)) %*% t(X)\ntest_matrix_properties(H, \"Hat matrix H\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n=== Testing Hat matrix H ===\nDimensions: 50 × 50 \nSquare? TRUE \nSymmetric? TRUE \nDiagonal? FALSE \nIdempotent? TRUE \nRank: 3 \nCondition number: 3.236e+19 \n  WARNING: Ill-conditioned!\nEigenvalues: 1 1 1 4.802e-16 3.754e-16 3.231e-16 2.79e-16 2.37e-16 2.243e-16 2.075e-16 1.476e-16 1.286e-16 9.532e-17 7.989e-17 6.62e-17 5.975e-17 4.807e-17 3.753e-17 3.164e-17 2.373e-17 2.159e-17 1.523e-17 1.382e-17 9.276e-18 7.454e-18 2.488e-18 5.046e-19 -3.16e-18 -6.523e-18 -1.002e-17 -1.456e-17 -1.929e-17 -1.986e-17 -2.377e-17 -3.054e-17 -4.039e-17 -4.404e-17 -6.616e-17 -6.891e-17 -8.849e-17 -1.1e-16 -1.499e-16 -1.726e-16 -2.17e-16 -2.215e-16 -2.5e-16 -2.704e-16 -4.076e-16 -4.509e-16 -6.604e-16 \nPositive definite? FALSE \nTrace: 3 \nDeterminant: 0 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Test on residual maker\nM <- diag(nrow(X)) - H\ntest_matrix_properties(M, \"Residual maker M\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n=== Testing Residual maker M ===\nDimensions: 50 × 50 \nSquare? TRUE \nSymmetric? TRUE \nDiagonal? FALSE \nIdempotent? TRUE \nRank: 47 \nCondition number: 1.514e+17 \n  WARNING: Ill-conditioned!\nEigenvalues: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4.094e-16 -8.33e-17 -4.709e-16 \nPositive definite? FALSE \nTrace: 47 \nDeterminant: 4.641e-48 \n```\n\n\n:::\n:::\n\n\n#### Debugging Numerical Issues\n\n**Common problems and how to diagnose them**:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndebug_linear_model <- function(X, y) {\n  cat(\"=== Debugging Linear Model ===\\n\\n\")\n\n  n <- nrow(X)\n  p <- ncol(X)\n\n  # 1. Check for NAs\n  cat(\"1. Checking for missing values:\\n\")\n  cat(\"  NAs in X:\", sum(is.na(X)), \"\\n\")\n  cat(\"  NAs in y:\", sum(is.na(y)), \"\\n\")\n\n  # 2. Check dimensions\n  cat(\"\\n2. Checking dimensions:\\n\")\n  cat(\"  n =\", n, \"\\n\")\n  cat(\"  p =\", p, \"\\n\")\n  cat(\"  n > p?\", n > p, \"(need this for full rank)\\n\")\n\n  # 3. Check rank\n  cat(\"\\n3. Checking rank:\\n\")\n  rank_X <- qr(X)$rank\n  cat(\"  rank(X) =\", rank_X, \"\\n\")\n  cat(\"  Full rank?\", rank_X == p, \"\\n\")\n\n  # 4. Check condition number\n  cat(\"\\n4. Checking condition number:\\n\")\n  XtX <- crossprod(X)\n  kappa_XtX <- kappa(XtX)\n  cat(\"  κ(X'X) =\", kappa_XtX, \"\\n\")\n  if (kappa_XtX > 1e6) {\n    cat(\"  WARNING: Severely ill-conditioned!\\n\")\n  } else if (kappa_XtX > 1000) {\n    cat(\"  WARNING: Ill-conditioned!\\n\")\n  } else {\n    cat(\"  OK: Well-conditioned\\n\")\n  }\n\n  # 5. Check for perfect collinearity\n  cat(\"\\n5. Checking for collinearity:\\n\")\n  if (p > 1) {\n    cor_matrix <- cor(X)\n    max_cor <- max(abs(cor_matrix[upper.tri(cor_matrix)]))\n    cat(\"  Max |correlation| =\", max_cor, \"\\n\")\n    if (max_cor > 0.99) {\n      cat(\"  WARNING: Very high correlation detected!\\n\")\n    }\n  }\n\n  # 6. Try to solve\n  cat(\"\\n6. Attempting to solve:\\n\")\n  tryCatch({\n    b <- solve(XtX, crossprod(X, y))\n    cat(\"  SUCCESS: Solution found\\n\")\n    cat(\"  Coefficients:\", b, \"\\n\")\n\n    # Check residuals\n    e <- y - X %*% b\n    SSE <- sum(e^2)\n    sigma2 <- SSE / (n - p)\n    cat(\"  σ² =\", sigma2, \"\\n\")\n\n    # Check if any SE are huge\n    se_b <- sqrt(diag(solve(XtX)) * sigma2)\n    max_se_ratio <- max(abs(se_b / b), na.rm = TRUE)\n    cat(\"  Max SE/estimate ratio:\", max_se_ratio, \"\\n\")\n    if (max_se_ratio > 1) {\n      cat(\"  WARNING: Some SEs exceed estimates!\\n\")\n    }\n  }, error = function(e) {\n    cat(\"  ERROR:\", e$message, \"\\n\")\n    cat(\"  System is computationally singular!\\n\")\n  })\n}\n\n# Test on good data\ncat(\"### Test 1: Good data ###\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n### Test 1: Good data ###\n```\n\n\n:::\n\n```{.r .cell-code}\nX_good <- matrix(rnorm(50 * 3), 50, 3)\ny_good <- rnorm(50)\ndebug_linear_model(X_good, y_good)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n=== Debugging Linear Model ===\n\n1. Checking for missing values:\n  NAs in X: 0 \n  NAs in y: 0 \n\n2. Checking dimensions:\n  n = 50 \n  p = 3 \n  n > p? TRUE (need this for full rank)\n\n3. Checking rank:\n  rank(X) = 3 \n  Full rank? TRUE \n\n4. Checking condition number:\n  κ(X'X) = 1.419 \n  OK: Well-conditioned\n\n5. Checking for collinearity:\n  Max |correlation| = 0.1113 \n\n6. Attempting to solve:\n  SUCCESS: Solution found\n  Coefficients: -0.04631 -0.01226 -0.2153 \n  σ² = 0.7251 \n  Max SE/estimate ratio: 8.983 \n  WARNING: Some SEs exceed estimates!\n```\n\n\n:::\n\n```{.r .cell-code}\n# Test on collinear data\ncat(\"\\n\\n### Test 2: Collinear data ###\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\n### Test 2: Collinear data ###\n```\n\n\n:::\n\n```{.r .cell-code}\nX_bad <- matrix(rnorm(50 * 3), 50, 3)\nX_bad[, 3] <- X_bad[, 1] + X_bad[, 2]  # Perfect collinearity\ny_bad <- rnorm(50)\ndebug_linear_model(X_bad, y_bad)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n=== Debugging Linear Model ===\n\n1. Checking for missing values:\n  NAs in X: 0 \n  NAs in y: 0 \n\n2. Checking dimensions:\n  n = 50 \n  p = 3 \n  n > p? TRUE (need this for full rank)\n\n3. Checking rank:\n  rank(X) = 2 \n  Full rank? FALSE \n\n4. Checking condition number:\n  κ(X'X) = 8.008e+16 \n  WARNING: Severely ill-conditioned!\n\n5. Checking for collinearity:\n  Max |correlation| = 0.7484 \n\n6. Attempting to solve:\n  ERROR: system is computationally singular: reciprocal condition number = 2.15156e-17 \n  System is computationally singular!\n```\n\n\n:::\n:::\n\n\n#### Unit Testing Template\n\n**Create a test suite** for your solver:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Function to test\nmy_lm_solver <- function(X, y) {\n  XtX <- crossprod(X)\n  Xty <- crossprod(X, y)\n  b <- solve(XtX, Xty)\n  return(b)\n}\n\n# Test suite\ntest_lm_solver <- function() {\n  all_pass <- TRUE\n\n  # Test 1: Simple case\n  cat(\"Test 1: Simple case... \")\n  X <- matrix(c(1, 1, 1, 1, 2, 3), 3, 2)\n  y <- c(1, 2, 2)\n  b <- my_lm_solver(X, y)\n  b_expected <- solve(crossprod(X), crossprod(X, y))\n  if (all.equal(b, b_expected)) {\n    cat(\"PASS\\n\")\n  } else {\n    cat(\"FAIL\\n\")\n    all_pass <- FALSE\n  }\n\n  # Test 2: Compare with lm()\n  cat(\"Test 2: Match lm()... \")\n  X <- matrix(rnorm(100 * 5), 100, 5)\n  y <- rnorm(100)\n  b_mine <- my_lm_solver(X, y)\n  b_lm <- lm.fit(X, y)$coefficients\n  if (all.equal(as.numeric(b_mine), b_lm)) {\n    cat(\"PASS\\n\")\n  } else {\n    cat(\"FAIL\\n\")\n    all_pass <- FALSE\n  }\n\n  # Test 3: Residuals orthogonal to X\n  cat(\"Test 3: X'e = 0... \")\n  e <- y - X %*% b_mine\n  Xte <- crossprod(X, e)\n  if (all.equal(Xte, matrix(0, 5, 1), check.attributes = FALSE)) {\n    cat(\"PASS\\n\")\n  } else {\n    cat(\"FAIL\\n\")\n    all_pass <- FALSE\n  }\n\n  # Test 4: SS decomposition\n  cat(\"Test 4: SST = SSM + SSE... \")\n  y_bar <- mean(y)\n  y_hat <- X %*% b_mine\n  SST <- sum((y - y_bar)^2)\n  SSM <- sum((y_hat - y_bar)^2)\n  SSE <- sum(e^2)\n  if (all.equal(SST, SSM + SSE)) {\n    cat(\"PASS\\n\")\n  } else {\n    cat(\"FAIL\\n\")\n    all_pass <- FALSE\n  }\n\n  # Summary\n  cat(\"\\n\")\n  if (all_pass) {\n    cat(\"ALL TESTS PASSED ✓\\n\")\n  } else {\n    cat(\"SOME TESTS FAILED ✗\\n\")\n  }\n\n  return(all_pass)\n}\n\n# Run tests\ntest_lm_solver()\n```\n:::\n\n\n#### Quick Verification Functions\n\n**Handy functions to keep in your toolkit**:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check if matrix is symmetric\nis_symmetric <- function(A, tol = 1e-10) {\n  isTRUE(all.equal(A, t(A), tolerance = tol))\n}\n\n# Check if matrix is idempotent\nis_idempotent <- function(A, tol = 1e-10) {\n  isTRUE(all.equal(A, A %*% A, tolerance = tol))\n}\n\n# Check if two vectors are orthogonal\nis_orthogonal <- function(x, y, tol = 1e-10) {\n  abs(sum(x * y)) < tol\n}\n\n# Check normal equations\ncheck_normal_equations <- function(X, y, b, tol = 1e-10) {\n  LHS <- crossprod(X) %*% b\n  RHS <- crossprod(X, y)\n  max(abs(LHS - RHS)) < tol\n}\n\n# Check projection property\ncheck_projection <- function(P, tol = 1e-10) {\n  is_symmetric(P, tol) && is_idempotent(P, tol)\n}\n\n# Use them\nX <- matrix(rnorm(50 * 3), 50, 3)\nH <- X %*% solve(crossprod(X)) %*% t(X)\n\ncat(\"H is symmetric?\", is_symmetric(H), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nH is symmetric? TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"H is idempotent?\", is_idempotent(H), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nH is idempotent? TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"H is projection?\", check_projection(H), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nH is projection? TRUE \n```\n\n\n:::\n:::\n\n\n::: {.callout-tip}\n## Best Practices Summary\n\n**Always**:\n1. ✅ Use `all.equal()` for comparisons, not `==`\n2. ✅ Verify normal equations are satisfied\n3. ✅ Check residual orthogonality\n4. ✅ Verify SS decomposition\n5. ✅ Compare with `lm()` output\n6. ✅ Check condition numbers\n7. ✅ Test on known cases first\n\n**Debugging strategy**:\n1. Start with small, simple test cases\n2. Check dimensions and ranks\n3. Verify mathematical properties\n4. Compare with trusted implementations\n5. Use diagnostic functions systematically\n6. Profile code for performance bottlenecks\n\n**When something goes wrong**:\n- Print intermediate results\n- Check for NAs, Infs, or NaNs\n- Verify matrix dimensions match\n- Test on well-conditioned data first\n- Use `debug()` and `browser()` in R\n:::\n\n#### Complete Verification Example\n\n**Putting it all together** for a livestock example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Swine litter size example\nset.seed(2024)\nn <- 30\nparity <- sample(1:5, n, replace = TRUE)\nX <- cbind(1, parity)\ny <- 8 + 1.2 * parity + rnorm(n, sd = 1.5)\n\ncat(\"=== Complete Verification Example ===\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n=== Complete Verification Example ===\n```\n\n\n:::\n\n```{.r .cell-code}\n# Solve\nb <- solve(crossprod(X), crossprod(X, y))\ncat(\"Estimates: Intercept =\", b[1], \", Slope =\", b[2], \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEstimates: Intercept = 8.993 , Slope = 0.9083 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Verification checklist\ncat(\"1. Normal equations: X'Xb = X'y?\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n1. Normal equations: X'Xb = X'y?\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"   \", check_normal_equations(X, y, b), \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"2. Residuals orthogonal to X?\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n2. Residuals orthogonal to X?\n```\n\n\n:::\n\n```{.r .cell-code}\ne <- y - X %*% b\ncat(\"   Max |X'e| =\", max(abs(crossprod(X, e))), \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Max |X'e| = 7.638e-14 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"3. Fitted values orthogonal to residuals?\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n3. Fitted values orthogonal to residuals?\n```\n\n\n:::\n\n```{.r .cell-code}\ny_hat <- X %*% b\ncat(\"   |y_hat'e| =\", abs(sum(y_hat * e)), \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   |y_hat'e| = 6.137e-13 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"4. SS decomposition: SST = SSM + SSE?\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n4. SS decomposition: SST = SSM + SSE?\n```\n\n\n:::\n\n```{.r .cell-code}\ny_bar <- mean(y)\nSST <- sum((y - y_bar)^2)\nSSM <- sum((y_hat - y_bar)^2)\nSSE <- sum(e^2)\ncat(\"   SST =\", SST, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   SST = 101.8 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"   SSM + SSE =\", SSM + SSE, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   SSM + SSE = 101.8 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"   Match?\", all.equal(SST, SSM + SSE), \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Match? TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"5. Compare with lm():\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n5. Compare with lm():\n```\n\n\n:::\n\n```{.r .cell-code}\nfit <- lm(y ~ parity)\ncat(\"   Coefficients match?\",\n    all.equal(as.numeric(b), coef(fit), check.attributes = FALSE), \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Coefficients match? TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"6. Projection matrix properties:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n6. Projection matrix properties:\n```\n\n\n:::\n\n```{.r .cell-code}\nH <- X %*% solve(crossprod(X)) %*% t(X)\ncat(\"   H symmetric?\", is_symmetric(H), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   H symmetric? TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"   H idempotent?\", is_idempotent(H), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   H idempotent? TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"   tr(H) = p?\", all.equal(sum(diag(H)), ncol(X)), \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   tr(H) = p? TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"7. Condition number:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n7. Condition number:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"   κ(X'X) =\", kappa(crossprod(X)), \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   κ(X'X) = 87.82 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"✓ ALL CHECKS PASSED\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n✓ ALL CHECKS PASSED\n```\n\n\n:::\n:::\n\n\n---\n\n## Cross-Reference Table {#sec-cross-reference}\n\nThis comprehensive table maps matrix algebra concepts to their locations in the course and provides quick reference for R implementations.\n\n::: {.callout-note}\n## How to Use This Table\n\n- **Concept**: Matrix algebra term or operation\n- **Section**: Where it's defined in this appendix\n- **Key Formula**: Most important identity or formula\n- **Used in Weeks**: Course weeks where this concept appears\n- **R Function**: Primary R function(s) for computation\n- **Notes**: Critical usage notes or warnings\n:::\n\n### Core Matrix Operations\n\n| Concept | Section | Key Formula | Used in Weeks | R Function | Notes |\n|---------|---------|-------------|---------------|------------|-------|\n| **Matrix transpose** | @sec-transpose | $(\\mathbf{A}')' = \\mathbf{A}$ | 1-15 | `t(A)` | Use `crossprod()` instead of `t(X) %*% X` |\n| **Matrix multiplication** | @sec-multiplication | $(\\mathbf{AB})' = \\mathbf{B}'\\mathbf{A}'$ | 1-15 | `%*%` | Not commutative! |\n| **Cross-product** | @sec-computational-shortcuts | $\\mathbf{X}'\\mathbf{X}$ | 4-15 | `crossprod(X)` | 2× faster than `t(X) %*% X` |\n| **Outer product** | @sec-computational-shortcuts | $\\mathbf{X}\\mathbf{X}'$ | 5-11 | `tcrossprod(X)` | Avoid for large $n$ |\n| **Kronecker product** | @sec-kronecker | $\\mathbf{A} \\otimes \\mathbf{B}$ | 14 (preview) | `kronecker(A, B)` | Never form explicitly for large matrices! |\n\n### Special Matrices\n\n| Concept | Section | Key Formula | Used in Weeks | R Function | Notes |\n|---------|---------|-------------|---------------|------------|-------|\n| **Identity matrix** | @sec-identity | $\\mathbf{I}_n\\mathbf{A} = \\mathbf{A}$ | 1-15 | `diag(n)` | |\n| **Diagonal matrix** | @sec-diagonal | $\\mathbf{D} = \\text{diag}(d_1, \\ldots, d_n)$ | 2-15 | `diag(x)` | Creates diagonal or extracts diagonal |\n| **Symmetric matrix** | @sec-symmetric | $\\mathbf{A}' = \\mathbf{A}$ | 2-15 | `isSymmetric(A)` | $\\mathbf{X}'\\mathbf{X}$ always symmetric |\n| **Orthogonal matrix** | @sec-orthogonal | $\\mathbf{Q}'\\mathbf{Q} = \\mathbf{I}$ | 2, 5, 11 | `qr.Q()` | Preserves lengths, numerically stable |\n| **Idempotent matrix** | @sec-idempotent | $\\mathbf{P}^2 = \\mathbf{P}$ | 5, 6, 11 | Custom function | Projection matrices are idempotent |\n| **Hat matrix** | @sec-projection-matrices | $\\mathbf{H} = \\mathbf{X}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'$ | 5, 6, 11 | `hatvalues()` | NEVER form explicitly for large $n$ |\n| **Residual maker** | @sec-projection-matrices | $\\mathbf{M} = \\mathbf{I} - \\mathbf{H}$ | 5, 6, 11 | Use implicitly | $\\mathbf{e} = \\mathbf{M}\\mathbf{y}$ |\n| **Centering matrix** | @sec-special-matrices | $\\mathbf{C} = \\mathbf{I} - n^{-1}\\mathbf{1}\\mathbf{1}'$ | 4-10 | `scale(center=TRUE)` | For computing SST |\n\n### Matrix Properties\n\n| Concept | Section | Key Formula | Used in Weeks | R Function | Notes |\n|---------|---------|-------------|---------------|------------|-------|\n| **Rank** | @sec-rank | $r(\\mathbf{X}'\\mathbf{X}) = r(\\mathbf{X})$ | 2, 7, 12 | `qr()$rank` | Critical for estimability |\n| **Trace** | @sec-trace | $\\text{tr}(\\mathbf{H}) = p$ | 2, 5, 6 | `sum(diag(A))` | Sum of eigenvalues |\n| **Determinant** | @sec-determinant | $\\det(\\mathbf{AB}) = \\det(\\mathbf{A})\\det(\\mathbf{B})$ | 2, 5 | `det(A)` | Zero if singular |\n| **Condition number** | @sec-numerical-stability | $\\kappa(\\mathbf{A}) = \\lambda_{\\max}/\\lambda_{\\min}$ | 6, 11, 12 | `kappa(A)` | $>10^6$ is severely ill-conditioned |\n| **Eigenvalues** | @sec-eigenvalues | $\\mathbf{A}\\mathbf{v} = \\lambda\\mathbf{v}$ | 2, 7 | `eigen()` | For spectral decomposition |\n| **Singular values** | @sec-svd | $\\mathbf{X} = \\mathbf{UDV}'$ | 2, 11, 12 | `svd()` | Most numerically stable decomposition |\n\n### Matrix Inverses\n\n| Concept | Section | Key Formula | Used in Weeks | R Function | Notes |\n|---------|---------|-------------|---------------|------------|-------|\n| **Regular inverse** | @sec-inverses | $\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}$ | 2-15 | `solve(A)` | Only for full-rank square matrices |\n| **Solve linear system** | @sec-efficient-computation | $\\mathbf{Ab} = \\mathbf{c}$ | 4-15 | `solve(A, c)` | NEVER use `solve(A) %*% c` |\n| **Generalized inverse** | @sec-generalized-inverse | $\\mathbf{A}\\mathbf{A}^-\\mathbf{A} = \\mathbf{A}$ | 2, 12, 13 | `MASS::ginv()` | For rank-deficient systems |\n| **Moore-Penrose inverse** | @sec-generalized-inverse | Unique g-inverse | 12 | `MASS::ginv()` | Uses SVD |\n| **Cholesky decomposition** | @sec-efficient-computation | $\\mathbf{A} = \\mathbf{R}'\\mathbf{R}$ | 11 | `chol(A)` | Requires positive definite |\n| **QR decomposition** | @sec-efficient-computation | $\\mathbf{X} = \\mathbf{QR}$ | 2, 11 | `qr()` | Most stable for normal equations |\n\n### Projection and Quadratic Forms\n\n| Concept | Section | Key Formula | Used in Weeks | R Function | Notes |\n|---------|---------|-------------|---------------|------------|-------|\n| **Projection matrix** | @sec-projection-matrices | $\\mathbf{P}^2 = \\mathbf{P} = \\mathbf{P}'$ | 5, 6 | Custom | Symmetric + idempotent |\n| **Fitted values** | @sec-projection-matrices | $\\hat{\\mathbf{y}} = \\mathbf{H}\\mathbf{y}$ | 4-15 | `fitted()` or `X %*% b` | Don't form $\\mathbf{H}$ |\n| **Residuals** | @sec-projection-matrices | $\\mathbf{e} = \\mathbf{M}\\mathbf{y}$ | 4-15 | `residuals()` | $\\mathbf{X}'\\mathbf{e} = \\mathbf{0}$ |\n| **Quadratic form** | @sec-quadratic-forms | $\\mathbf{y}'\\mathbf{A}\\mathbf{y}$ | 5-10 | `t(y) %*% A %*% y` or `sum(y * (A %*% y))` | Sum of squares |\n| **Leverage** | @sec-projection-matrices | $h_{ii} = [\\mathbf{H}]_{ii}$ | 11 | `hatvalues()` | Influence of observation $i$ |\n| **Mahalanobis distance** | @sec-quadratic-forms | $D^2 = (\\mathbf{x} - \\boldsymbol{\\mu})'\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x} - \\boldsymbol{\\mu})$ | 11 | `mahalanobis()` | For outlier detection |\n\n### Linear Models Identities\n\n| Concept | Section | Key Formula | Used in Weeks | R Function | Notes |\n|---------|---------|-------------|---------------|------------|-------|\n| **Normal equations** | @sec-projection-identities | $\\mathbf{X}'\\mathbf{X}\\mathbf{b} = \\mathbf{X}'\\mathbf{y}$ | 4-15 | `solve(crossprod(X), crossprod(X, y))` | Foundation of LS |\n| **LS solution** | @sec-projection-identities | $\\mathbf{b} = (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y}$ | 4-15 | `lm.fit()` or `qr.coef()` | Use QR, not inverse |\n| **Residual orthogonality** | @sec-projection-identities | $\\mathbf{X}'\\mathbf{e} = \\mathbf{0}$ | 5-15 | Check: `crossprod(X, e)` | Always verify! |\n| **Var(b)** | @sec-variance-formulas | $\\text{Var}(\\mathbf{b}) = \\sigma^2(\\mathbf{X}'\\mathbf{X})^{-1}$ | 5-15 | `vcov()` | For standard errors |\n| **Var(y_hat)** | @sec-variance-formulas | $\\text{Var}(\\hat{\\mathbf{y}}) = \\sigma^2\\mathbf{H}$ | 5, 11 | `sigma2 * H` | Heteroscedastic! |\n| **Var(e)** | @sec-variance-formulas | $\\text{Var}(\\mathbf{e}) = \\sigma^2\\mathbf{M}$ | 5, 11 | `sigma2 * M` | Not constant variance |\n| **Contrast variance** | @sec-variance-formulas | $\\text{Var}(\\mathbf{c}'\\mathbf{b}) = \\sigma^2\\mathbf{c}'(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{c}$ | 8 | Custom | For testing hypotheses |\n\n### Sum of Squares\n\n| Concept | Section | Key Formula | Used in Weeks | R Function | Notes |\n|---------|---------|-------------|---------------|------------|-------|\n| **SST** | @sec-ss-identities | $\\sum(y_i - \\bar{y})^2$ | 4-10 | `sum((y - mean(y))^2)` | Total variation |\n| **SSE** | @sec-ss-identities | $\\mathbf{e}'\\mathbf{e} = \\mathbf{y}'\\mathbf{M}\\mathbf{y}$ | 4-15 | `sum(residuals^2)` or `deviance()` | Error SS |\n| **SSM/SSR** | @sec-ss-identities | $\\mathbf{b}'\\mathbf{X}'\\mathbf{y} - n\\bar{y}^2$ | 4-10 | From `anova()` | Model/Regression SS |\n| **SS decomposition** | @sec-ss-identities | $\\text{SST} = \\text{SSM} + \\text{SSE}$ | 5-10 | Always verify! | Orthogonal decomposition |\n| **R²** | @sec-ss-identities | $R^2 = \\text{SSM}/\\text{SST}$ | 4-6 | `summary()$r.squared` | Proportion explained |\n| **Adjusted R²** | @sec-ss-identities | $\\bar{R}^2 = 1 - \\frac{\\text{SSE}/(n-p)}{\\text{SST}/(n-1)}$ | 6 | `summary()$adj.r.squared` | Penalizes complexity |\n\n### Matrix Calculus\n\n| Concept | Section | Key Formula | Used in Weeks | R Function | Notes |\n|---------|---------|-------------|---------------|------------|-------|\n| **Derivative scalar wrt vector** | @sec-matrix-calculus | $\\partial(\\mathbf{a}'\\mathbf{x})/\\partial\\mathbf{x} = \\mathbf{a}$ | 5 | Manual | For deriving normal equations |\n| **Derivative quadratic form** | @sec-matrix-calculus | $\\partial(\\mathbf{x}'\\mathbf{A}\\mathbf{x})/\\partial\\mathbf{x} = 2\\mathbf{Ax}$ | 5 | Manual | For minimizing SSE |\n| **Hessian** | @sec-matrix-calculus | $\\partial^2 S/\\partial\\boldsymbol{\\beta}\\partial\\boldsymbol{\\beta}' = 2\\mathbf{X}'\\mathbf{X}$ | 5 | Manual | For checking convexity |\n\n### Computational Topics\n\n| Concept | Section | Key Formula | Used in Weeks | R Function | Notes |\n|---------|---------|-------------|---------------|------------|-------|\n| **VIF** | @sec-numerical-stability | $\\text{VIF}_j = 1/(1 - R_j^2)$ | 6, 11, 12 | `car::vif()` | $>10$ indicates collinearity |\n| **Centering predictors** | @sec-numerical-stability | $\\mathbf{X}_c = \\mathbf{X} - \\bar{\\mathbf{X}}$ | 6, 10, 11 | `scale(X, center=TRUE)` | Improves conditioning |\n| **Scaling predictors** | @sec-numerical-stability | Standardize to mean 0, sd 1 | 6, 11 | `scale(X)` | For comparing effects |\n| **all.equal()** | @sec-verification | Tolerant comparison | 2-15 | `all.equal(a, b)` | NEVER use `==` for floats |\n| **Numerical tolerance** | @sec-verification | Default $\\approx 1.5 \\times 10^{-8}$ | 2-15 | `.Machine$double.eps` | For checking zeros |\n\n### Week-Specific Applications\n\n| Week | Primary Topics | Key Matrix Operations | Critical Sections |\n|------|---------------|----------------------|-------------------|\n| **1** | Overview, matrix basics | Transpose, multiplication | @sec-basic-operations |\n| **2** | Linear algebra essentials | Rank, inverse, eigenvalues | @sec-rank, @sec-inverses, @sec-eigenvalues |\n| **3** | Design matrices | Building $\\mathbf{X}$, coding schemes | @sec-basic-operations |\n| **4** | Simple regression | $\\mathbf{X}'\\mathbf{X}$, $\\mathbf{X}'\\mathbf{y}$, $\\mathbf{b}$ | @sec-projection-identities |\n| **5** | Least squares theory | $\\mathbf{H}$, $\\mathbf{M}$, projections, Gauss-Markov | @sec-projection-matrices |\n| **6** | Multiple regression | $\\text{Var}(\\mathbf{b})$, collinearity, VIF | @sec-variance-formulas, @sec-numerical-stability |\n| **7** | One-way ANOVA | Cell means model, SS decomposition | @sec-ss-identities |\n| **8** | Contrasts | $\\mathbf{c}'\\mathbf{b}$, estimability | @sec-variance-formulas |\n| **9** | Two-way ANOVA | Multiple factors, interactions | @sec-ss-identities |\n| **10** | ANCOVA | Adjusted means, homogeneity of slopes | @sec-variance-formulas |\n| **11** | Diagnostics | Leverage ($h_{ii}$), Cook's D | @sec-projection-matrices, @sec-quadratic-forms |\n| **12** | Non-full rank | Generalized inverse, constraints | @sec-generalized-inverse |\n| **13** | Special topics I | Unbalanced data, Type I/II/III SS | @sec-ss-identities |\n| **14** | Special topics II | Polynomial regression, WLS, mixed model preview | @sec-kronecker |\n| **15** | Capstone | All concepts integrated | All sections |\n\n::: {.callout-tip}\n## Quick Lookup Strategy\n\n1. **Find your week** in the Week-Specific Applications table\n2. **Identify the matrix concept** you need\n3. **Look up the concept** in the detailed tables above\n4. **Navigate to the section** for full details\n5. **Use the R function** provided for implementation\n:::\n\n::: {.callout-warning}\n## Common Mistakes to Avoid\n\nBased on the course material, students commonly make these errors:\n\n| Mistake | Correct Approach | Reference |\n|---------|-----------------|-----------|\n| Using `==` for float comparison | Use `all.equal()` | @sec-verification |\n| Forming $\\mathbf{H}$ explicitly | Use $\\mathbf{X}\\mathbf{b}$ instead | @sec-computational-shortcuts |\n| Using `solve(A) %*% c` | Use `solve(A, c)` | @sec-efficient-computation |\n| Ignoring rank deficiency | Check `qr()$rank`, use g-inverse | @sec-numerical-stability |\n| Not centering predictors | Use `scale()` for ill-conditioned $\\mathbf{X}'\\mathbf{X}$ | @sec-numerical-stability |\n| Forming large Kronecker products | Use inverse property, implicit operations | @sec-kronecker |\n| Using `t(X) %*% X` | Use `crossprod(X)` | @sec-computational-shortcuts |\n:::\n\n---\n\n## R Code Templates {#sec-templates}\n\nThis section provides production-ready R functions for common linear models tasks. Copy and adapt these templates for your own analyses.\n\n::: {.callout-note}\n## How to Use These Templates\n\nEach template includes:\n- **Complete, tested function code**\n- **Input/output specifications**\n- **Usage examples with livestock data**\n- **Error checking and validation**\n- **Documentation comments**\n\n**Installation**: Copy the function to your R script or save in a separate file to `source()`.\n:::\n\n### Matrix Property Checker {#sec-template-property-checker}\n\n**Purpose**: Comprehensive diagnostic function for checking matrix properties.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#' Check Matrix Properties\n#'\n#' Comprehensive diagnostic function for matrices in linear models\n#'\n#' @param A Matrix to check\n#' @param name Character string for display (default: \"A\")\n#' @param tolerance Numerical tolerance for checks (default: 1e-10)\n#' @param verbose Logical, print detailed output? (default: TRUE)\n#'\n#' @return List with logical test results (invisible)\n#'\n#' @examples\n#' X <- matrix(rnorm(50*3), 50, 3)\n#' H <- X %*% solve(t(X) %*% X) %*% t(X)\n#' check_matrix_properties(H, \"Hat Matrix\")\n#'\ncheck_matrix_properties <- function(A, name = \"A\", tolerance = 1e-10, verbose = TRUE) {\n\n  # Initialize results list\n  results <- list()\n\n  if (verbose) cat(\"\\n=== Checking\", name, \"===\\n\\n\")\n\n  # Basic properties\n  dims <- dim(A)\n  results$dimensions <- dims\n  if (verbose) cat(\"Dimensions:\", dims[1], \"×\", dims[2], \"\\n\")\n\n  is_square <- dims[1] == dims[2]\n  results$is_square <- is_square\n  if (verbose) cat(\"Square:\", is_square, \"\\n\")\n\n  if (!is_square) {\n    if (verbose) cat(\"(Additional checks require square matrix)\\n\")\n    return(invisible(results))\n  }\n\n  # Symmetry\n  is_sym <- isTRUE(all.equal(A, t(A), tolerance = tolerance))\n  results$is_symmetric <- is_sym\n  if (verbose) cat(\"Symmetric:\", is_sym, \"\\n\")\n\n  # Diagonal\n  off_diag <- A[row(A) != col(A)]\n  is_diag <- all(abs(off_diag) < tolerance)\n  results$is_diagonal <- is_diag\n  if (verbose) cat(\"Diagonal:\", is_diag, \"\\n\")\n\n  # Idempotent\n  A2 <- A %*% A\n  is_idem <- isTRUE(all.equal(A, A2, tolerance = tolerance))\n  results$is_idempotent <- is_idem\n  if (verbose) cat(\"Idempotent (A² = A):\", is_idem, \"\\n\")\n\n  # Rank\n  rank_A <- qr(A)$rank\n  results$rank <- rank_A\n  full_rank <- rank_A == min(dims)\n  results$full_rank <- full_rank\n  if (verbose) {\n    cat(\"Rank:\", rank_A, \"/\", min(dims))\n    if (full_rank) cat(\" (full rank)\")\n    cat(\"\\n\")\n  }\n\n  # Condition number\n  kappa_A <- kappa(A)\n  results$condition_number <- kappa_A\n  if (verbose) {\n    cat(\"Condition number:\", sprintf(\"%.2e\", kappa_A))\n    if (kappa_A > 1e6) {\n      cat(\" [SEVERELY ILL-CONDITIONED]\")\n    } else if (kappa_A > 1000) {\n      cat(\" [ILL-CONDITIONED]\")\n    } else if (kappa_A > 10) {\n      cat(\" [MODERATE]\")\n    } else {\n      cat(\" [WELL-CONDITIONED]\")\n    }\n    cat(\"\\n\")\n  }\n\n  # Eigenvalues\n  eigen_vals <- eigen(A, only.values = TRUE)$values\n  results$eigenvalues <- eigen_vals\n  if (verbose) {\n    cat(\"Eigenvalues:\", paste(sprintf(\"%.4f\", Re(eigen_vals)), collapse=\", \"))\n    if (any(abs(Im(eigen_vals)) > tolerance)) {\n      cat(\" [COMPLEX]\")\n    }\n    cat(\"\\n\")\n  }\n\n  # Positive definite\n  is_pd <- all(Re(eigen_vals) > tolerance) && all(abs(Im(eigen_vals)) < tolerance)\n  results$positive_definite <- is_pd\n  if (verbose) cat(\"Positive definite:\", is_pd, \"\\n\")\n\n  # Trace\n  tr_A <- sum(diag(A))\n  results$trace <- tr_A\n  if (verbose) cat(\"Trace:\", sprintf(\"%.4f\", tr_A), \"\\n\")\n\n  # Determinant\n  det_A <- det(A)\n  results$determinant <- det_A\n  results$singular <- abs(det_A) < tolerance\n  if (verbose) {\n    cat(\"Determinant:\", sprintf(\"%.4e\", det_A))\n    if (abs(det_A) < tolerance) cat(\" [SINGULAR]\")\n    cat(\"\\n\")\n  }\n\n  # Projection matrix check\n  if (is_sym && is_idem) {\n    if (verbose) cat(\"\\n✓ This is a PROJECTION MATRIX\\n\")\n    results$is_projection <- TRUE\n  } else {\n    results$is_projection <- FALSE\n  }\n\n  return(invisible(results))\n}\n\n# Example usage\nX <- matrix(rnorm(50 * 3), 50, 3)\nH <- X %*% solve(t(X) %*% X) %*% t(X)\ncheck_matrix_properties(H, \"Hat Matrix\")\n```\n:::\n\n\n---\n\n### Build Projection Matrices {#sec-template-projection}\n\n**Purpose**: Safely construct projection matrices with numerical stability checks.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#' Build Projection Matrices\n#'\n#' Construct hat matrix H and residual maker M with stability checks\n#'\n#' @param X Design matrix (n × p)\n#' @param method Character: \"qr\" (default, most stable) or \"cholesky\"\n#' @param check_properties Logical, verify projection properties? (default: TRUE)\n#'\n#' @return List with components:\n#'   - H: Hat matrix (n × n) - NOTE: only computed if n < 1000\n#'   - M: Residual maker (n × n) - NOTE: only computed if n < 1000\n#'   - qr_obj: QR decomposition object (for fitted values)\n#'   - rank: Rank of X\n#'   - is_full_rank: Logical\n#'\n#' @details For large n, H and M are NOT computed explicitly. Use qr_obj instead.\n#'\n#' @examples\n#' X <- cbind(1, rnorm(50), rnorm(50))\n#' proj <- build_projection_matrices(X)\n#' y_hat <- proj$qr_obj$qr %*% qr.coef(proj$qr_obj, y)  # Efficient\n#'\nbuild_projection_matrices <- function(X, method = \"qr\", check_properties = TRUE) {\n\n  n <- nrow(X)\n  p <- ncol(X)\n\n  # Check dimensions\n  if (n < p) {\n    stop(\"n must be >= p for projection matrices\")\n  }\n\n  # QR decomposition (always computed, most stable)\n  qr_obj <- qr(X)\n  rank_X <- qr_obj$rank\n  is_full_rank <- rank_X == p\n\n  if (!is_full_rank) {\n    warning(paste(\"X is not full rank. Rank =\", rank_X, \"but p =\", p))\n  }\n\n  # Build projection matrices (only for small n)\n  H <- NULL\n  M <- NULL\n\n  if (n < 1000) {\n    if (method == \"qr\") {\n      Q <- qr.Q(qr_obj)\n      H <- tcrossprod(Q)  # Q %*% t(Q)\n    } else if (method == \"cholesky\") {\n      if (!is_full_rank) {\n        stop(\"Cholesky method requires full rank X\")\n      }\n      XtX <- crossprod(X)\n      XtX_inv <- chol2inv(chol(XtX))\n      H <- X %*% XtX_inv %*% t(X)\n    } else {\n      stop(\"method must be 'qr' or 'cholesky'\")\n    }\n\n    M <- diag(n) - H\n\n    # Verify properties\n    if (check_properties) {\n      # Check symmetry\n      if (!isTRUE(all.equal(H, t(H)))) {\n        warning(\"H is not symmetric (numerical issue)\")\n      }\n\n      # Check idempotence\n      if (!isTRUE(all.equal(H, H %*% H, tolerance = 1e-8))) {\n        warning(\"H is not idempotent (numerical issue)\")\n      }\n\n      # Check trace\n      tr_H <- sum(diag(H))\n      if (!isTRUE(all.equal(tr_H, rank_X, tolerance = 1e-6))) {\n        warning(paste(\"tr(H) =\", tr_H, \"but should be\", rank_X))\n      }\n    }\n  } else {\n    message(\"n >= 1000: H and M not computed explicitly (too large)\")\n    message(\"Use qr_obj for efficient operations instead\")\n  }\n\n  return(list(\n    H = H,\n    M = M,\n    qr_obj = qr_obj,\n    rank = rank_X,\n    is_full_rank = is_full_rank,\n    n = n,\n    p = p\n  ))\n}\n\n# Example: Small dataset\nX_small <- cbind(1, rnorm(50), rnorm(50))\nproj <- build_projection_matrices(X_small)\ncat(\"Rank:\", proj$rank, \"\\nFull rank?:\", proj$is_full_rank, \"\\n\")\n\n# Example: Large dataset (won't form H explicitly)\nX_large <- cbind(1, rnorm(5000), rnorm(5000))\nproj_large <- build_projection_matrices(X_large)\n```\n:::\n\n\n---\n\n### Solve Normal Equations {#sec-template-solve}\n\n**Purpose**: Flexible solver for normal equations with multiple methods and diagnostics.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#' Solve Normal Equations\n#'\n#' Solve X'Xb = X'y using specified method with full diagnostics\n#'\n#' @param X Design matrix (n × p)\n#' @param y Response vector (length n)\n#' @param method Character: \"qr\" (default), \"cholesky\", \"svd\", or \"ginv\"\n#' @param compute_diagnostics Logical, compute full diagnostics? (default: TRUE)\n#'\n#' @return List with components:\n#'   - coefficients: Estimated coefficients\n#'   - fitted.values: Fitted values\n#'   - residuals: Residuals\n#'   - sigma2: Variance estimate\n#'   - vcov: Variance-covariance matrix of coefficients\n#'   - se: Standard errors\n#'   - df: Degrees of freedom\n#'   - rank: Rank of X\n#'   - method: Method used\n#'   - diagnostics: List of diagnostic measures (if requested)\n#'\n#' @examples\n#' X <- cbind(1, rnorm(50), rnorm(50))\n#' y <- rnorm(50)\n#' fit <- solve_normal_equations(X, y, method = \"qr\")\n#' summary(fit)\n#'\nsolve_normal_equations <- function(X, y, method = \"qr\", compute_diagnostics = TRUE) {\n\n  n <- nrow(X)\n  p <- ncol(X)\n\n  # Input validation\n  if (length(y) != n) {\n    stop(\"Length of y must equal nrow(X)\")\n  }\n\n  # Initialize result\n  result <- list(method = method)\n\n  # Solve based on method\n  if (method == \"qr\") {\n    qr_obj <- qr(X)\n    result$rank <- qr_obj$rank\n    b <- qr.coef(qr_obj, y)\n    y_hat <- qr.fitted(qr_obj, y)\n    e <- qr.resid(qr_obj, y)\n\n  } else if (method == \"cholesky\") {\n    XtX <- crossprod(X)\n    Xty <- crossprod(X, y)\n    result$rank <- qr(X)$rank\n\n    R <- chol(XtX)\n    b <- backsolve(R, forwardsolve(t(R), Xty))\n    y_hat <- X %*% b\n    e <- y - y_hat\n\n  } else if (method == \"svd\") {\n    svd_obj <- svd(X)\n    result$rank <- sum(svd_obj$d > 1e-10)\n\n    d_inv <- ifelse(svd_obj$d > 1e-10, 1/svd_obj$d, 0)\n    X_pinv <- svd_obj$v %*% diag(d_inv) %*% t(svd_obj$u)\n    b <- X_pinv %*% y\n    y_hat <- X %*% b\n    e <- y - y_hat\n\n  } else if (method == \"ginv\") {\n    library(MASS)\n    result$rank <- qr(X)$rank\n    X_pinv <- ginv(X)\n    b <- X_pinv %*% y\n    y_hat <- X %*% b\n    e <- y - y_hat\n\n  } else {\n    stop(\"method must be 'qr', 'cholesky', 'svd', or 'ginv'\")\n  }\n\n  # Remove names if present\n  b <- as.numeric(b)\n  y_hat <- as.numeric(y_hat)\n  e <- as.numeric(e)\n\n  # Basic results\n  result$coefficients <- b\n  result$fitted.values <- y_hat\n  result$residuals <- e\n  result$df <- n - result$rank\n\n  # Variance estimation\n  SSE <- sum(e^2)\n  sigma2 <- SSE / result$df\n  result$sigma2 <- sigma2\n  result$sigma <- sqrt(sigma2)\n\n  # Variance-covariance matrix\n  XtX <- crossprod(X)\n  if (result$rank == p) {\n    vcov_mat <- solve(XtX) * sigma2\n  } else {\n    vcov_mat <- ginv(XtX) * sigma2\n  }\n  result$vcov <- vcov_mat\n  result$se <- sqrt(diag(vcov_mat))\n\n  # Compute diagnostics\n  if (compute_diagnostics) {\n    diag_list <- list()\n\n    # Sum of squares\n    y_bar <- mean(y)\n    SST <- sum((y - y_bar)^2)\n    SSM <- sum((y_hat - y_bar)^2)\n    diag_list$SST <- SST\n    diag_list$SSM <- SSM\n    diag_list$SSE <- SSE\n    diag_list$MSE <- sigma2\n    diag_list$R2 <- SSM / SST\n    diag_list$adj_R2 <- 1 - (SSE / result$df) / (SST / (n - 1))\n\n    # Check normal equations\n    check_ne <- max(abs(crossprod(X, e)))\n    diag_list$max_Xte <- check_ne\n    diag_list$normal_eqs_satisfied <- check_ne < 1e-8\n\n    # Condition number\n    diag_list$condition_number <- kappa(XtX)\n\n    # F-statistic\n    if (result$rank > 1) {\n      MSM <- SSM / (result$rank - 1)\n      F_stat <- MSM / sigma2\n      diag_list$F_statistic <- F_stat\n      diag_list$F_pvalue <- pf(F_stat, result$rank - 1, result$df, lower.tail = FALSE)\n    }\n\n    result$diagnostics <- diag_list\n  }\n\n  class(result) <- \"my_lm\"\n  return(result)\n}\n\n# Print method\nprint.my_lm <- function(x, ...) {\n  cat(\"\\nCall: solve_normal_equations(method =\", paste0(\"'\", x$method, \"')\\n\\n\"))\n  cat(\"Coefficients:\\n\")\n  print(round(x$coefficients, 4))\n  cat(\"\\nResidual standard error:\", round(x$sigma, 4), \"on\", x$df, \"degrees of freedom\\n\")\n  if (!is.null(x$diagnostics)) {\n    cat(\"Multiple R-squared:\", round(x$diagnostics$R2, 4), \"\\n\")\n    cat(\"Adjusted R-squared:\", round(x$diagnostics$adj_R2, 4), \"\\n\")\n  }\n  invisible(x)\n}\n\n# Example usage\nset.seed(123)\nn <- 50\nX <- cbind(1, rnorm(n), rnorm(n))\ny <- X %*% c(10, 2, -1) + rnorm(n, sd = 2)\n\nfit <- solve_normal_equations(X, y, method = \"qr\")\nprint(fit)\n\n# Compare methods\nfit_qr <- solve_normal_equations(X, y, \"qr\", compute_diagnostics = FALSE)\nfit_chol <- solve_normal_equations(X, y, \"cholesky\", compute_diagnostics = FALSE)\ncat(\"QR vs Cholesky:\", all.equal(fit_qr$coefficients, fit_chol$coefficients), \"\\n\")\n```\n:::\n\n\n---\n\n### Compute Quadratic Forms {#sec-template-quadratic}\n\n**Purpose**: Efficient computation of quadratic forms and sums of squares.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#' Compute Quadratic Form\n#'\n#' Efficiently compute y'Ay for various matrix structures\n#'\n#' @param y Numeric vector (length n)\n#' @param A Matrix (n × n), or NULL for identity\n#' @param type Character: \"general\", \"diagonal\", \"symmetric\", or \"identity\"\n#'\n#' @return Scalar value of quadratic form\n#'\n#' @examples\n#' y <- rnorm(100)\n#' A <- crossprod(matrix(rnorm(100*100), 100, 100))\n#' qf <- compute_quadratic_form(y, A, type = \"symmetric\")\n#'\ncompute_quadratic_form <- function(y, A = NULL, type = \"general\") {\n\n  n <- length(y)\n\n  if (type == \"identity\" || is.null(A)) {\n    # y'y\n    return(sum(y^2))\n\n  } else if (type == \"diagonal\") {\n    # y'diag(d)y = sum(d * y^2)\n    d <- diag(A)\n    return(sum(d * y^2))\n\n  } else if (type == \"symmetric\") {\n    # Exploit symmetry: only compute upper triangle\n    # More efficient: sum(y * (A %*% y))\n    return(sum(y * (A %*% y)))\n\n  } else if (type == \"general\") {\n    # Full computation: t(y) %*% A %*% y\n    return(as.numeric(t(y) %*% A %*% y))\n\n  } else {\n    stop(\"type must be 'general', 'diagonal', 'symmetric', or 'identity'\")\n  }\n}\n\n#' Decompose Sums of Squares\n#'\n#' Compute SST, SSM, SSE decomposition with verification\n#'\n#' @param y Response vector\n#' @param y_hat Fitted values vector\n#' @param verify Logical, verify decomposition? (default: TRUE)\n#'\n#' @return List with SST, SSM, SSE, R2, verification status\n#'\ndecompose_sums_of_squares <- function(y, y_hat, verify = TRUE) {\n\n  n <- length(y)\n  y_bar <- mean(y)\n\n  # Compute components\n  SST <- sum((y - y_bar)^2)\n  SSM <- sum((y_hat - y_bar)^2)\n  SSE <- sum((y - y_hat)^2)\n\n  # R-squared\n  R2 <- SSM / SST\n\n  # Verify decomposition\n  decomp_ok <- TRUE\n  if (verify) {\n    sum_check <- SSM + SSE\n    decomp_ok <- isTRUE(all.equal(SST, sum_check, tolerance = 1e-10))\n    if (!decomp_ok) {\n      warning(paste(\"SS decomposition failed: SST =\", SST, \"but SSM + SSE =\", sum_check))\n    }\n  }\n\n  return(list(\n    SST = SST,\n    SSM = SSM,\n    SSE = SSE,\n    R2 = R2,\n    decomposition_verified = decomp_ok\n  ))\n}\n\n# Example usage\ny <- rnorm(100, mean = 50, sd = 10)\ny_hat <- rnorm(100, mean = 50, sd = 8)\n\nss_decomp <- decompose_sums_of_squares(y, y_hat)\nprint(ss_decomp)\n```\n:::\n\n\n---\n\n### Kronecker Product Functions {#sec-template-kronecker}\n\n**Purpose**: Efficient operations with Kronecker product structure (never form explicitly!).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#' Kronecker Product Matrix-Vector Multiplication\n#'\n#' Compute (A ⊗ B)x efficiently WITHOUT forming A ⊗ B\n#'\n#' @param A Matrix (m × n)\n#' @param B Matrix (p × q)\n#' @param x Vector (length nq)\n#'\n#' @return Vector (length mp), result of (A ⊗ B)x\n#'\n#' @details Uses the identity: (A ⊗ B)vec(X) = vec(BXA')\n#'\nkronecker_matvec <- function(A, B, x) {\n\n  m <- nrow(A)\n  n <- ncol(A)\n  p <- nrow(B)\n  q <- ncol(B)\n\n  # Check dimensions\n  if (length(x) != n * q) {\n    stop(paste(\"x must have length\", n * q, \"for this Kronecker product\"))\n  }\n\n  # Reshape x to matrix X (q × n)\n  X <- matrix(x, nrow = q, ncol = n)\n\n  # Compute BXA' and vectorize\n  result <- B %*% X %*% t(A)\n\n  return(as.vector(result))\n}\n\n#' Solve (A ⊗ B)x = y Efficiently\n#'\n#' Solve Kronecker structured system WITHOUT forming A ⊗ B\n#'\n#' @param A Matrix (m × m), must be invertible\n#' @param B Matrix (p × p), must be invertible\n#' @param y Vector (length mp)\n#'\n#' @return Vector x such that (A ⊗ B)x = y\n#'\n#' @details Uses: (A ⊗ B)^(-1) = A^(-1) ⊗ B^(-1)\n#'\nsolve_kronecker_system <- function(A, B, y) {\n\n  m <- nrow(A)\n  p <- nrow(B)\n\n  # Check dimensions\n  if (length(y) != m * p) {\n    stop(paste(\"y must have length\", m * p))\n  }\n\n  # Check invertibility\n  if (abs(det(A)) < 1e-10) stop(\"A is singular\")\n  if (abs(det(B)) < 1e-10) stop(\"B is singular\")\n\n  # Compute inverses separately\n  A_inv <- solve(A)\n  B_inv <- solve(B)\n\n  # Apply (A^(-1) ⊗ B^(-1)) to y\n  x <- kronecker_matvec(A_inv, B_inv, y)\n\n  return(x)\n}\n\n#' Example: Multi-Trait Genetic Evaluation Structure\n#'\n#' Demonstrates Kronecker product in genetic evaluation context\n#'\nmulti_trait_var_structure <- function(G, A, n_traits = 2, n_animals = 5) {\n\n  # G: genetic covariance between traits (2×2)\n  # A: additive relationship matrix (5×5)\n  # Full covariance: V = G ⊗ A (10×10)\n\n  cat(\"=== Multi-Trait Variance Structure ===\\n\\n\")\n  cat(\"G (genetic covariance matrix):\\n\")\n  print(G)\n  cat(\"\\nA (relationship matrix):\\n\")\n  print(A)\n\n  # Size of full system\n  total_dim <- n_traits * n_animals\n  cat(\"\\nFull system size:\", total_dim, \"×\", total_dim, \"\\n\")\n  cat(\"Would require\", format(total_dim^2, big.mark=\",\"), \"elements\\n\")\n\n  # DON'T form explicitly!\n  if (total_dim <= 20) {\n    V_explicit <- kronecker(G, A)\n    cat(\"\\nV = G ⊗ A (formed explicitly for illustration):\\n\")\n    cat(\"Dimensions:\", dim(V_explicit), \"\\n\")\n  } else {\n    cat(\"\\n[V not formed - too large!]\\n\")\n  }\n\n  # For solving (G ⊗ A)^(-1)y, use:\n  cat(\"\\nTo solve (G ⊗ A)^(-1)y:\\n\")\n  cat(\"1. Compute G^(-1) (\", n_traits, \"×\", n_traits, \")\\n\")\n  cat(\"2. Compute A^(-1) (\", n_animals, \"×\", n_animals, \")\\n\")\n  cat(\"3. Apply (G^(-1) ⊗ A^(-1)) implicitly\\n\")\n\n  # Demonstrate\n  G_inv <- solve(G)\n  A_inv <- solve(A)\n\n  y_test <- rnorm(total_dim)\n  x_solution <- solve_kronecker_system(G_inv, A_inv, y_test)\n\n  cat(\"\\nTest: Solved system of size\", total_dim, \"without forming\", total_dim, \"×\", total_dim, \"matrix!\\n\")\n\n  return(invisible(list(G = G, A = A, G_inv = G_inv, A_inv = A_inv)))\n}\n\n# Example: 2 traits, 5 animals\nG <- matrix(c(10, 3, 3, 5), 2, 2)  # Genetic covariance\nA <- matrix(c(1.0, 0.5, 0.0, 0.0, 0.25,\n              0.5, 1.0, 0.25, 0.0, 0.125,\n              0.0, 0.25, 1.0, 0.5, 0.375,\n              0.0, 0.0, 0.5, 1.0, 0.25,\n              0.25, 0.125, 0.375, 0.25, 1.0), 5, 5)\n\nresult <- multi_trait_var_structure(G, A)\n```\n:::\n\n\n---\n\n## References {.unnumbered}\n\nKey references for matrix algebra in linear models:\n\n- Searle, S. R. (1971). *Linear Models*. John Wiley & Sons.\n- Searle, S. R. (1982). *Matrix Algebra Useful for Statistics*. John Wiley & Sons.\n- Henderson, C. R. (1984). *Applications of Linear Models in Animal Breeding*. University of Guelph.\n- Harville, D. A. (1997). *Matrix Algebra From a Statistician's Perspective*. Springer.\n\n:::{.callout-tip}\n## What's Next?\n\nAfter reviewing matrix algebra concepts here, you'll be well-prepared to:\n\n- Understand the derivations in **Week 5: Least Squares Theory**\n- Work with rank-deficient systems in **Week 12: Non-Full Rank Models**\n- Explore advanced topics in future Animal models and multi-trait analysis courses\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}