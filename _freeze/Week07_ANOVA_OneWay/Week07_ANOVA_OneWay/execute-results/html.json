{
  "hash": "13bc2df4853d626385e94fb34c9d7804",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Week 7: Analysis of Variance (One-Way)\"\nformat:\n  html:\n    toc: true\n    toc-depth: 3\n    number-sections: true\n    code-fold: false\n    code-tools: true\n---\n\n::: {.callout-note icon=false}\n## Learning Objectives\n\nBy the end of this week, you will be able to:\n\n1. **Express** **one-way ANOVA** as a **linear model** in **matrix notation**\n2. **Construct** **design matrices** for **categorical predictors** (indicator variables)\n3. **Partition** **total variation** into **model** and **error components**\n4. **Conduct** **F-tests** for **treatment effects** and interpret results in biological context\n5. **Understand** the relationship between **cell means** and **effects model parameterizations**\n:::\n\n## Introduction: Comparing Means in Animal Breeding\n\nIn Weeks 4-6, we studied linear models with continuous predictors (regression). We learned how to construct design matrices, solve normal equations, and test hypotheses—all within the unified framework of **y = Xβ + e**.\n\nThis week, we extend this framework to **categorical predictors**. Instead of asking \"How does milk yield change with days in milk?\" (continuous), we ask \"Do different breeds have different average milk yields?\" (categorical). This is the realm of **Analysis of Variance (ANOVA)**.\n\n::: {.callout-important}\n## ANOVA is Regression with Categorical Predictors\n\nThe fundamental insight: ANOVA is **not** a different statistical method. It's the **same linear model** we've been studying:\n\n$$\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{e}$$\n\nThe only difference:\n- **Regression**: X contains continuous values (weights, ages, temperatures)\n- **ANOVA**: X contains 0s and 1s (indicator/dummy variables for groups)\n\nSame normal equations: **X′Xb = X′y**\nSame least squares solution: **b = (X′X)⁻¹X′y**\nSame F-tests, same SSE, same geometric interpretation!\n:::\n\n### Why Learn ANOVA?\n\nComparing group means is fundamental in animal breeding and genetics:\n\n- **Breed comparisons**: Do Holstein cows produce more milk than Jerseys?\n- **Diet trials**: Which feedlot ration maximizes growth rate in beef steers?\n- **Genetic line evaluation**: Which layer strain has superior egg production?\n- **Treatment effects**: Does a new supplement improve feed efficiency in broilers?\n\nIn all these scenarios, we have:\n- **One response variable** (milk yield, growth rate, eggs, FCR)\n- **One categorical predictor** (breed, diet, line, treatment)\n- **Multiple groups/levels** within that predictor\n\nThis is **one-way ANOVA**: one categorical factor with multiple levels.\n\n### Preview\n\nIn this chapter, we'll:\n\n1. Formalize the one-way ANOVA model using matrix notation\n2. Learn two equivalent parameterizations: **cell means** and **effects** models\n3. Derive the **sum of squares partitioning**: SST = SSM + SSE\n4. Construct **ANOVA tables** and conduct **F-tests**\n5. Build our own ANOVA solver from scratch in R\n6. Apply ANOVA to realistic livestock data\n\nLet's begin!\n\n---\n\n## Mathematical Theory\n\n### One-Way ANOVA Setting\n\nConsider an experiment comparing **g** groups (treatments, breeds, diets, etc.). We observe:\n\n$$y_{ij} = \\text{response for observation } j \\text{ in group } i$$\n\nwhere:\n- $i = 1, 2, \\ldots, g$ indexes the groups\n- $j = 1, 2, \\ldots, n_i$ indexes observations within group $i$\n- $n = \\sum_{i=1}^g n_i$ is the total sample size\n\n::: {.callout-note}\n## Notation: Dot Subscript Convention\n\nWe use dots to indicate averaging:\n\n- $\\bar{y}_{i.} = \\frac{1}{n_i}\\sum_{j=1}^{n_i} y_{ij}$ = mean of group $i$\n- $\\bar{y}_{..} = \\frac{1}{n}\\sum_{i=1}^g \\sum_{j=1}^{n_i} y_{ij}$ = grand mean (overall average)\n\nThe dot replaces the subscript being averaged over. This notation will be essential for sum of squares formulas.\n:::\n\n**Balanced vs. Unbalanced Designs**:\n\n- **Balanced**: All groups have equal sample sizes ($n_1 = n_2 = \\cdots = n_g$)\n  - Mathematically simpler (orthogonal design matrices)\n  - Equal precision for all group means\n  - Preferred whenever possible in practice\n\n- **Unbalanced**: Groups have unequal sample sizes\n  - More common in real data (animals die, observations lost)\n  - Complicates interpretation and computation\n  - We'll see a brief example, with full treatment in Week 12\n\nFor this week, we **focus primarily on balanced designs** to build intuition.\n\n---\n\n### Cell Means Model\n\nThe simplest ANOVA parameterization is the **cell means model**:\n\n$$y_{ij} = \\mu_i + e_{ij} \\quad i=1,\\ldots,g; \\quad j=1,\\ldots,n_i$$ {#eq-cell-means}\n\nwhere:\n- $\\mu_i$ = population mean of group $i$\n- $e_{ij}$ = random error for observation $j$ in group $i$\n\n**Assumptions** (same as always):\n- $E(e_{ij}) = 0$ for all $i, j$\n- $\\text{Var}(e_{ij}) = \\sigma^2$ for all $i, j$ (homoscedasticity)\n- $e_{ij}$ are independent\n- (Optional for inference) $e_{ij} \\sim N(0, \\sigma^2)$\n\n#### Matrix Form\n\nIn matrix notation:\n\n$$\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{e}$$ {#eq-cell-means-matrix}\n\nwhere **β** = $[\\mu_1, \\mu_2, \\ldots, \\mu_g]'$ (one parameter per group) and **X** is the **design matrix**.\n\n**Example**: Suppose $g=3$ groups with $n_1=2$, $n_2=2$, $n_3=2$ (balanced, $n=6$ total).\n\nData structure:\n```\nObservation:  1    2   |  3    4   |  5    6\nGroup:        1    1   |  2    2   |  3    3\nResponse:    y11  y12  | y21  y22  | y31  y32\n```\n\nThe design matrix **X** ($6 \\times 3$) uses **indicator variables** (0s and 1s):\n\n$$\\mathbf{X} = \\begin{bmatrix}\n1 & 0 & 0 \\\\\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 0 & 1\n\\end{bmatrix}, \\quad\n\\boldsymbol{\\beta} = \\begin{bmatrix} \\mu_1 \\\\ \\mu_2 \\\\ \\mu_3 \\end{bmatrix}, \\quad\n\\mathbf{y} = \\begin{bmatrix} y_{11} \\\\ y_{12} \\\\ y_{21} \\\\ y_{22} \\\\ y_{31} \\\\ y_{32} \\end{bmatrix}$$\n\n**Interpretation**: Each column of **X** is an indicator for one group. If observation $j$ belongs to group $i$, then $X_{ji} = 1$; otherwise $X_{ji} = 0$.\n\nVerify: $\\mathbf{X}\\boldsymbol{\\beta}$ gives the model predictions:\n\n$$\\mathbf{X}\\boldsymbol{\\beta} = \\begin{bmatrix}\n\\mu_1 \\\\ \\mu_1 \\\\ \\mu_2 \\\\ \\mu_2 \\\\ \\mu_3 \\\\ \\mu_3\n\\end{bmatrix}$$\n\nEach observation is predicted by its group mean. Perfect!\n\n#### Normal Equations\n\nThe normal equations are:\n\n$$\\mathbf{X}'\\mathbf{X} \\mathbf{b} = \\mathbf{X}'\\mathbf{y}$$ {#eq-normal-cell-means}\n\nFor our example:\n\n$$\\mathbf{X}'\\mathbf{X} = \\begin{bmatrix}\n2 & 0 & 0 \\\\\n0 & 2 & 0 \\\\\n0 & 0 & 2\n\\end{bmatrix} = 2\\mathbf{I}_3$$\n\nThis is **diagonal**! In general, for a balanced design with $n_i = n$ observations per group:\n\n$$\\mathbf{X}'\\mathbf{X} = n\\mathbf{I}_g$$\n\nFor unbalanced designs:\n\n$$\\mathbf{X}'\\mathbf{X} = \\text{diag}(n_1, n_2, \\ldots, n_g)$$\n\nStill diagonal, just with different diagonal elements.\n\nThe right side:\n\n$$\\mathbf{X}'\\mathbf{y} = \\begin{bmatrix}\ny_{11} + y_{12} \\\\\ny_{21} + y_{22} \\\\\ny_{31} + y_{32}\n\\end{bmatrix} = \\begin{bmatrix}\n\\sum_{j=1}^{n_1} y_{1j} \\\\\n\\sum_{j=1}^{n_2} y_{2j} \\\\\n\\sum_{j=1}^{n_3} y_{3j}\n\\end{bmatrix}$$\n\nThis is just the **sum of observations in each group**!\n\n#### Solution\n\nSince **X′X** is diagonal (and full rank), the inverse is trivial:\n\n$$(\\mathbf{X}'\\mathbf{X})^{-1} = \\frac{1}{n}\\mathbf{I}_g \\quad \\text{(balanced)} \\quad \\text{or} \\quad \\text{diag}(1/n_1, 1/n_2, \\ldots, 1/n_g) \\quad \\text{(unbalanced)}$$\n\nTherefore:\n\n$$\\mathbf{b} = (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y} = \\begin{bmatrix}\n\\frac{1}{n_1}\\sum_j y_{1j} \\\\\n\\frac{1}{n_2}\\sum_j y_{2j} \\\\\n\\vdots \\\\\n\\frac{1}{n_g}\\sum_j y_{gj}\n\\end{bmatrix} = \\begin{bmatrix}\n\\bar{y}_{1.} \\\\\n\\bar{y}_{2.} \\\\\n\\vdots \\\\\n\\bar{y}_{g.}\n\\end{bmatrix}$$ {#eq-cell-means-solution}\n\n**Result**: The least squares estimates are simply the **sample means of each group**!\n\n$$b_i = \\bar{y}_{i.} = \\text{mean of group } i$$\n\n::: {.callout-tip}\n## Cell Means Model: Direct and Interpretable\n\nThe cell means model has several advantages:\n\n1. **Always full rank**: X′X is always invertible (as long as each group has $n_i \\geq 1$)\n2. **Direct interpretation**: Each parameter $\\mu_i$ is simply the mean of group $i$\n3. **No constraints needed**: All parameters are uniquely estimable\n4. **Simple estimates**: $b_i = \\bar{y}_{i.}$ (group sample means)\n\nThis is why we often prefer the cell means model for computation and interpretation, even though the effects model (next section) is more common in textbooks.\n:::\n\n---\n\n### Effects Model\n\nAn alternative parameterization is the **effects model**:\n\n$$y_{ij} = \\mu + \\alpha_i + e_{ij}$$ {#eq-effects}\n\nwhere:\n- $\\mu$ = grand mean (overall average across all groups)\n- $\\alpha_i$ = **effect** of group $i$ (deviation from grand mean)\n- $e_{ij}$ = random error\n\n**Relationship to cell means**: If we define $\\mu_i = \\mu + \\alpha_i$, then:\n\n$$\\text{Cell means model: } y_{ij} = \\mu_i + e_{ij}$$\n$$\\text{Effects model: } y_{ij} = \\mu + \\alpha_i + e_{ij} = (\\mu + \\alpha_i) + e_{ij} = \\mu_i + e_{ij}$$\n\nThey're **equivalent**! Just different ways to write the same model.\n\n#### Overparameterization Problem\n\nConsider $g=3$ groups. The effects model has:\n- 1 parameter $\\mu$ (grand mean)\n- 3 parameters $\\alpha_1, \\alpha_2, \\alpha_3$ (group effects)\n- **Total: 4 parameters** for only 3 groups!\n\nBut we only have 3 distinct group means. We can't uniquely estimate 4 parameters from 3 means. The model is **overparameterized**.\n\n**Design matrix**: For our $g=3$, $n_i=2$ example:\n\n$$\\mathbf{X} = \\begin{bmatrix}\n1 & 1 & 0 & 0 \\\\\n1 & 1 & 0 & 0 \\\\\n1 & 0 & 1 & 0 \\\\\n1 & 0 & 1 & 0 \\\\\n1 & 0 & 0 & 1 \\\\\n1 & 0 & 0 & 1\n\\end{bmatrix}, \\quad\n\\boldsymbol{\\beta} = \\begin{bmatrix} \\mu \\\\ \\alpha_1 \\\\ \\alpha_2 \\\\ \\alpha_3 \\end{bmatrix}$$\n\nThe first column (all 1s) is for $\\mu$. Columns 2-4 are indicators for groups 1-3.\n\nCheck the rank:\n\n$$\\mathbf{X}'\\mathbf{X} = \\begin{bmatrix}\n6 & 2 & 2 & 2 \\\\\n2 & 2 & 0 & 0 \\\\\n2 & 0 & 2 & 0 \\\\\n2 & 0 & 0 & 2\n\\end{bmatrix}$$\n\nThis matrix is **singular** (not full rank)! Note that:\n- Column 1 = sum of columns 2, 3, 4\n- Or: Row 1 = sum of rows 2, 3, 4\n\nTherefore $\\text{rank}(\\mathbf{X}'\\mathbf{X}) = 3 < 4$. The matrix is not invertible.\n\n::: {.callout-warning}\n## Rank Deficiency in Effects Model\n\nThe effects model **X′X** is not full rank because we have one more parameter than we need. The first column of X (the intercept) is the sum of the group indicator columns.\n\n**Implication**: We cannot uniquely solve the normal equations without additional constraints.\n\n**Common constraint**: Set $\\sum_{i=1}^g \\alpha_i = 0$ (sum-to-zero constraint). This forces the group effects to average to zero.\n\nWe'll demonstrate this in the small example. For a thorough treatment of rank deficiency, estimability, and constraints, see **Week 12: Non-Full Rank Models**.\n:::\n\n#### Applying the Sum-to-Zero Constraint\n\nWith the constraint $\\sum_{i=1}^g \\alpha_i = 0$, we can express $\\alpha_g = -(\\alpha_1 + \\alpha_2 + \\cdots + \\alpha_{g-1})$ and eliminate one parameter.\n\n**Reduced design matrix** (constraining $\\alpha_3 = -\\alpha_1 - \\alpha_2$):\n\n$$\\mathbf{X}_{\\text{reduced}} = \\begin{bmatrix}\n1 & 1 & 0 \\\\\n1 & 1 & 0 \\\\\n1 & 0 & 1 \\\\\n1 & 0 & 1 \\\\\n1 & -1 & -1 \\\\\n1 & -1 & -1\n\\end{bmatrix}, \\quad\n\\boldsymbol{\\beta}_{\\text{reduced}} = \\begin{bmatrix} \\mu \\\\ \\alpha_1 \\\\ \\alpha_2 \\end{bmatrix}$$\n\nNow $\\mathbf{X}_{\\text{reduced}}'\\mathbf{X}_{\\text{reduced}}$ is $3 \\times 3$ and full rank.\n\n**Solution**: With the constraint, we get:\n- $\\hat{\\mu} = \\bar{y}_{..}$ (grand mean)\n- $\\hat{\\alpha}_i = \\bar{y}_{i.} - \\bar{y}_{..}$ (deviation of group $i$ from grand mean)\n\nWe'll verify this in the small example.\n\n---\n\n### Sum of Squares Partitioning\n\nThe power of ANOVA comes from **decomposing total variation** into interpretable components.\n\n#### Total Sum of Squares (SST)\n\nTotal variation in the data (ignoring groups):\n\n$$\\text{SST} = \\sum_{i=1}^g \\sum_{j=1}^{n_i} (y_{ij} - \\bar{y}_{..})^2$$ {#eq-sst}\n\nThis measures how much observations vary around the **grand mean** $\\bar{y}_{..}$.\n\nDegrees of freedom: $\\text{df}_{\\text{Total}} = n - 1$\n\n#### Treatment Sum of Squares (SSM or SS(Treatments))\n\nVariation **between groups** (how much do group means differ?):\n\n$$\\text{SS(Treatments)} = \\sum_{i=1}^g n_i(\\bar{y}_{i.} - \\bar{y}_{..})^2$$ {#eq-ssm}\n\nThis measures how much the **group means** vary around the **grand mean**. If all groups have the same mean, SS(Treatments) = 0.\n\nDegrees of freedom: $\\text{df}_{\\text{Treatments}} = g - 1$\n\n::: {.callout-note}\n## Notation: SSM vs. SS(Treatments)\n\nDifferent texts use different notation:\n- **SSM** = Sum of Squares for Model (we've used this in Weeks 4-6)\n- **SS(Treatments)** = Sum of Squares for Treatments (common in ANOVA texts)\n- **SSB** = Sum of Squares Between groups (also common)\n\nThey all mean the same thing: variation explained by the group factor. We'll use **SSM** and **SS(Treatments)** interchangeably.\n:::\n\n#### Error Sum of Squares (SSE)\n\nVariation **within groups** (how much do observations vary around their group means?):\n\n$$\\text{SSE} = \\sum_{i=1}^g \\sum_{j=1}^{n_i} (y_{ij} - \\bar{y}_{i.})^2$$ {#eq-sse}\n\nThis measures unexplained variation (residuals).\n\nDegrees of freedom: $\\text{df}_{\\text{Error}} = n - g$\n\n#### The Fundamental Decomposition\n\n::: {.callout-important}\n## The Fundamental ANOVA Decomposition\n\nFor any dataset:\n\n$$\\text{SST} = \\text{SS(Treatments)} + \\text{SSE}$$\n\nOr equivalently:\n\n$$\\sum_{i=1}^g \\sum_{j=1}^{n_i} (y_{ij} - \\bar{y}_{..})^2 = \\sum_{i=1}^g n_i(\\bar{y}_{i.} - \\bar{y}_{..})^2 + \\sum_{i=1}^g \\sum_{j=1}^{n_i} (y_{ij} - \\bar{y}_{i.})^2$$\n\n**Interpretation**:\n- **Total variation** = **Between-group variation** + **Within-group variation**\n- **SST** = **SSM** + **SSE**\n\nThis partitioning is **exact** (no approximation), and the degrees of freedom also add up:\n\n$$n - 1 = (g - 1) + (n - g)$$\n:::\n\n#### Proof of the Decomposition\n\nWe can prove this algebraically. Start with the identity:\n\n$$y_{ij} - \\bar{y}_{..} = (\\bar{y}_{i.} - \\bar{y}_{..}) + (y_{ij} - \\bar{y}_{i.})$$\n\nThis says: deviation from grand mean = deviation of group mean from grand mean + deviation of observation from its group mean.\n\nSquare both sides:\n\n$$(y_{ij} - \\bar{y}_{..})^2 = (\\bar{y}_{i.} - \\bar{y}_{..})^2 + (y_{ij} - \\bar{y}_{i.})^2 + 2(\\bar{y}_{i.} - \\bar{y}_{..})(y_{ij} - \\bar{y}_{i.})$$\n\nSum over all $i$ and $j$:\n\n$$\\sum_i \\sum_j (y_{ij} - \\bar{y}_{..})^2 = \\sum_i \\sum_j (\\bar{y}_{i.} - \\bar{y}_{..})^2 + \\sum_i \\sum_j (y_{ij} - \\bar{y}_{i.})^2 + 2\\sum_i \\sum_j (\\bar{y}_{i.} - \\bar{y}_{..})(y_{ij} - \\bar{y}_{i.})$$\n\nThe first term on the right: $(\\bar{y}_{i.} - \\bar{y}_{..})$ is constant within group $i$, so:\n\n$$\\sum_j (\\bar{y}_{i.} - \\bar{y}_{..})^2 = n_i(\\bar{y}_{i.} - \\bar{y}_{..})^2$$\n\nThe cross-product term vanishes:\n\n$$\\sum_j (y_{ij} - \\bar{y}_{i.}) = \\sum_j y_{ij} - n_i\\bar{y}_{i.} = n_i\\bar{y}_{i.} - n_i\\bar{y}_{i.} = 0$$\n\nTherefore:\n\n$$\\text{SST} = \\text{SS(Treatments)} + \\text{SSE} \\quad \\blacksquare$$\n\n#### Matrix Forms\n\nWe can also express these in matrix notation. Recall from Week 5:\n\n$$\\text{SST} = \\mathbf{y}'\\mathbf{y} - n\\bar{y}_{..}^2 = \\mathbf{y}'(\\mathbf{I} - \\frac{1}{n}\\mathbf{J})\\mathbf{y}$$\n\nwhere **J** is the $n \\times n$ matrix of all ones.\n\n$$\\text{SSM} = \\mathbf{b}'\\mathbf{X}'\\mathbf{y} - n\\bar{y}_{..}^2$$\n\n$$\\text{SSE} = \\mathbf{y}'\\mathbf{y} - \\mathbf{b}'\\mathbf{X}'\\mathbf{y} = \\mathbf{e}'\\mathbf{e}$$\n\nThese are the same formulas we used in regression (Weeks 4-6)!\n\n---\n\n### ANOVA Table and F-Test\n\nWe summarize the sum of squares decomposition in an **ANOVA table**:\n\n| Source        | df    | Sum of Squares | Mean Square | F            | p-value |\n|---------------|-------|----------------|-------------|--------------|---------|\n| Treatments    | $g-1$ | SSM            | MSM         | MSM/MSE      | $P(F > F_{\\text{obs}})$ |\n| Error         | $n-g$ | SSE            | MSE         | —            | —       |\n| **Total**     | $n-1$ | SST            | —           | —            | —       |\n\nwhere:\n\n- **Mean Square for Model (MSM)**: $\\text{MSM} = \\frac{\\text{SSM}}{g-1}$\n- **Mean Square for Error (MSE)**: $\\text{MSE} = \\frac{\\text{SSE}}{n-g} = \\hat{\\sigma}^2$ (our estimate of $\\sigma^2$)\n- **F-statistic**: $F = \\frac{\\text{MSM}}{\\text{MSE}}$\n\n#### The F-Test\n\n**Hypotheses**:\n\n$$H_0: \\mu_1 = \\mu_2 = \\cdots = \\mu_g \\quad \\text{(all group means are equal)}$$\n$$H_a: \\text{at least one } \\mu_i \\text{ differs from the others}$$\n\n**Test statistic**:\n\n$$F = \\frac{\\text{MSM}}{\\text{MSE}} = \\frac{\\text{SSM}/(g-1)}{\\text{SSE}/(n-g)}$$ {#eq-f-stat}\n\n**Distribution under** $H_0$:\n\n$$F \\sim F_{g-1, n-g}$$\n\nwhere $F_{g-1, n-g}$ is the **F-distribution** with numerator df = $g-1$ and denominator df = $n-g$.\n\n**Decision rule**: Reject $H_0$ if $F > F_{g-1, n-g, \\alpha}$ (critical value from F-table), or equivalently if p-value $< \\alpha$.\n\n::: {.callout-note}\n## Interpreting the F-Test\n\nThe F-statistic is a **ratio of variances**:\n\n$$F = \\frac{\\text{Between-group variability}}{\\text{Within-group variability}}$$\n\n**Intuition**:\n- If $H_0$ is true (all means equal), then MSM and MSE both estimate $\\sigma^2$, so $F \\approx 1$\n- If $H_0$ is false (means differ), then MSM is inflated by treatment effects, so $F > 1$\n- Large $F$ provides evidence against $H_0$\n\n**What the F-test does NOT tell us**:\n- It does NOT tell us which specific groups differ\n- It does NOT tell us the magnitude of differences\n- It's an **omnibus test**: \"Is there any difference anywhere?\"\n\nFor **specific comparisons** between groups, we need **contrasts** (Week 8).\n:::\n\n---\n\n## Small Numerical Example: Milk Yield Across Four Dairy Breeds\n\nLet's work through a complete example by hand to solidify understanding.\n\n### Data\n\nWe compare daily milk yield (kg/day) across four dairy breeds. Each breed has 3 cows (balanced design, $n=12$ total).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Data\nbreed <- rep(c(\"Holstein\", \"Jersey\", \"BrownSwiss\", \"Ayrshire\"), each = 3)\nmilk_yield <- c(30, 32, 31,    # Holstein\n                24, 25, 24,    # Jersey\n                28, 29, 27,    # Brown Swiss\n                26, 27, 26)    # Ayrshire\n\n# Create data frame\nmilk_data <- data.frame(breed = breed, milk_yield = milk_yield)\nprint(milk_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        breed milk_yield\n1    Holstein         30\n2    Holstein         32\n3    Holstein         31\n4      Jersey         24\n5      Jersey         25\n6      Jersey         24\n7  BrownSwiss         28\n8  BrownSwiss         29\n9  BrownSwiss         27\n10   Ayrshire         26\n11   Ayrshire         27\n12   Ayrshire         26\n```\n\n\n:::\n:::\n\n\n**Summary**:\n- Holstein: 30, 32, 31 (mean = 31.0)\n- Jersey: 24, 25, 24 (mean = 24.33)\n- Brown Swiss: 28, 29, 27 (mean = 28.0)\n- Ayrshire: 26, 27, 26 (mean = 26.33)\n- Grand mean: $\\bar{y}_{..} = (93 + 73 + 84 + 79)/12 = 329/12 = 27.417$\n\n### Part A: Cell Means Model (Full Hand Calculation)\n\n**Model**: $y_{ij} = \\mu_i + e_{ij}$\n\n**Step 1: Construct y vector** ($12 \\times 1$):\n\n$$\\mathbf{y} = \\begin{bmatrix} 30 \\\\ 32 \\\\ 31 \\\\ 24 \\\\ 25 \\\\ 24 \\\\ 28 \\\\ 29 \\\\ 27 \\\\ 26 \\\\ 27 \\\\ 26 \\end{bmatrix}$$\n\n**Step 2: Construct X matrix** ($12 \\times 4$):\n\nColumns represent breeds: Holstein, Jersey, BrownSwiss, Ayrshire.\n\n$$\\mathbf{X} = \\begin{bmatrix}\n1 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 1\n\\end{bmatrix}$$\n\n**Step 3: Compute X′X** ($4 \\times 4$):\n\n$$\\mathbf{X}'\\mathbf{X} = \\begin{bmatrix}\n3 & 0 & 0 & 0 \\\\\n0 & 3 & 0 & 0 \\\\\n0 & 0 & 3 & 0 \\\\\n0 & 0 & 0 & 3\n\\end{bmatrix} = 3\\mathbf{I}_4$$\n\nPerfect! Diagonal matrix (because balanced design).\n\n**Step 4: Compute X′y** ($4 \\times 1$):\n\n$$\\mathbf{X}'\\mathbf{y} = \\begin{bmatrix}\n30 + 32 + 31 \\\\\n24 + 25 + 24 \\\\\n28 + 29 + 27 \\\\\n26 + 27 + 26\n\\end{bmatrix} = \\begin{bmatrix}\n93 \\\\\n73 \\\\\n84 \\\\\n79\n\\end{bmatrix}$$\n\nThese are the sums for each breed.\n\n**Step 5: Solve normal equations**:\n\n$$\\mathbf{b} = (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y} = \\frac{1}{3}\\mathbf{I}_4 \\begin{bmatrix} 93 \\\\ 73 \\\\ 84 \\\\ 79 \\end{bmatrix} = \\begin{bmatrix} 31.00 \\\\ 24.33 \\\\ 28.00 \\\\ 26.33 \\end{bmatrix}$$\n\n**Result**: The estimates are exactly the group means! $b_i = \\bar{y}_{i.}$\n\n**Step 6: Compute fitted values and residuals**:\n\n$$\\hat{\\mathbf{y}} = \\mathbf{X}\\mathbf{b} = \\begin{bmatrix} 31.00 \\\\ 31.00 \\\\ 31.00 \\\\ 24.33 \\\\ 24.33 \\\\ 24.33 \\\\ 28.00 \\\\ 28.00 \\\\ 28.00 \\\\ 26.33 \\\\ 26.33 \\\\ 26.33 \\end{bmatrix}$$\n\nEach observation is fitted by its group mean.\n\n$$\\mathbf{e} = \\mathbf{y} - \\hat{\\mathbf{y}} = \\begin{bmatrix} -1.00 \\\\ 1.00 \\\\ 0.00 \\\\ -0.33 \\\\ 0.67 \\\\ -0.33 \\\\ 0.00 \\\\ 1.00 \\\\ -1.00 \\\\ -0.33 \\\\ 0.67 \\\\ -0.33 \\end{bmatrix}$$\n\n**Step 7: Calculate sum of squares**:\n\n**SST**:\n\\begin{align}\n\\text{SST} &= \\sum_{i,j} (y_{ij} - \\bar{y}_{..})^2 \\\\\n&= (30-27.417)^2 + (32-27.417)^2 + \\cdots + (26-27.417)^2 \\\\\n&= 6.670 + 21.004 + 12.838 + 12.838 + 7.504 + 11.670 + 0.338 + 2.504 + 0.172 + 2.004 + 0.338 + 2.004 \\\\\n&= 95.667 \\text{ kg}^2/\\text{day}^2\n\\end{align}\n\n**SS(Breeds)**:\n\\begin{align}\n\\text{SS(Breeds)} &= \\sum_i n_i(\\bar{y}_{i.} - \\bar{y}_{..})^2 \\\\\n&= 3(31.00 - 27.417)^2 + 3(24.33 - 27.417)^2 + 3(28.00 - 27.417)^2 + 3(26.33 - 27.417)^2 \\\\\n&= 3(12.837) + 3(9.526) + 3(0.340) + 3(1.184) \\\\\n&= 38.512 + 28.578 + 1.020 + 3.552 \\\\\n&= 71.662 \\text{ kg}^2/\\text{day}^2\n\\end{align}\n\n**SSE**:\n\\begin{align}\n\\text{SSE} &= \\sum_{i,j} (y_{ij} - \\bar{y}_{i.})^2 \\\\\n&= \\mathbf{e}'\\mathbf{e} \\\\\n&= (-1.00)^2 + (1.00)^2 + 0^2 + (-0.33)^2 + (0.67)^2 + (-0.33)^2 + 0^2 + (1.00)^2 + (-1.00)^2 + (-0.33)^2 + (0.67)^2 + (-0.33)^2 \\\\\n&= 1.00 + 1.00 + 0 + 0.11 + 0.45 + 0.11 + 0 + 1.00 + 1.00 + 0.11 + 0.45 + 0.11 \\\\\n&= 5.333 + 0.667 + 2.000 + 0.667 \\\\\n&= 24.005 \\text{ kg}^2/\\text{day}^2\n\\end{align}\n\n(Note: rounding differences. Using exact values, SSE should be exactly 24.000)\n\n**Verify decomposition**:\n$$\\text{SST} = 95.667 \\approx 71.662 + 24.005 = 95.667 \\quad \\checkmark$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Cell means model in R\ny <- milk_yield\nX <- model.matrix(~ breed - 1, data = milk_data)  # -1 removes intercept\ncolnames(X) <- levels(factor(breed))\n\n# Normal equations\nXtX <- t(X) %*% X\nXty <- t(X) %*% y\nb <- solve(XtX) %*% Xty\n\ncat(\"Design matrix X (first 6 rows):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDesign matrix X (first 6 rows):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(head(X))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Ayrshire BrownSwiss Holstein Jersey\n1        0          0        1      0\n2        0          0        1      0\n3        0          0        1      0\n4        0          0        0      1\n5        0          0        0      1\n6        0          0        0      1\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nX'X (diagonal because balanced):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nX'X (diagonal because balanced):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(XtX)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           Ayrshire BrownSwiss Holstein Jersey\nAyrshire          3          0        0      0\nBrownSwiss        0          3        0      0\nHolstein          0          0        3      0\nJersey            0          0        0      3\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nX'y (group sums):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nX'y (group sums):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(Xty)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           [,1]\nAyrshire     79\nBrownSwiss   84\nHolstein     93\nJersey       73\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nParameter estimates b (group means):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nParameter estimates b (group means):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               [,1]\nAyrshire   26.33333\nBrownSwiss 28.00000\nHolstein   31.00000\nJersey     24.33333\n```\n\n\n:::\n\n```{.r .cell-code}\n# Fitted values and residuals\ny_hat <- X %*% b\ne <- y - y_hat\n\ncat(\"\\nResiduals:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nResiduals:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(e)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         [,1]\n1  -1.0000000\n2   1.0000000\n3   0.0000000\n4  -0.3333333\n5   0.6666667\n6  -0.3333333\n7   0.0000000\n8   1.0000000\n9  -1.0000000\n10 -0.3333333\n11  0.6666667\n12 -0.3333333\n```\n\n\n:::\n\n```{.r .cell-code}\n# Sum of squares\ngrand_mean <- mean(y)\nSST <- sum((y - grand_mean)^2)\nSSM <- sum((y_hat - grand_mean)^2)\nSSE <- sum(e^2)\n\ncat(\"\\nSum of Squares:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSum of Squares:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"SST = %.3f\\n\", SST))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSST = 76.917\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"SS(Breeds) = %.3f\\n\", SSM))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSS(Breeds) = 71.583\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"SSE = %.3f\\n\", SSE))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSSE = 5.333\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"SST - SSM - SSE = %.10f (should be ≈ 0)\\n\", SST - SSM - SSE))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSST - SSM - SSE = -0.0000000000 (should be ≈ 0)\n```\n\n\n:::\n:::\n\n\n### Part B: Effects Model (Full Hand Calculation)\n\n**Model**: $y_{ij} = \\mu + \\alpha_i + e_{ij}$ with constraint $\\sum_i \\alpha_i = 0$\n\n**Step 1: Construct overparameterized X** ($12 \\times 5$):\n\n$$\\mathbf{X}_{\\text{over}} = \\begin{bmatrix}\n1 & 1 & 0 & 0 & 0 \\\\\n1 & 1 & 0 & 0 & 0 \\\\\n1 & 1 & 0 & 0 & 0 \\\\\n1 & 0 & 1 & 0 & 0 \\\\\n1 & 0 & 1 & 0 & 0 \\\\\n1 & 0 & 1 & 0 & 0 \\\\\n1 & 0 & 0 & 1 & 0 \\\\\n1 & 0 & 0 & 1 & 0 \\\\\n1 & 0 & 0 & 1 & 0 \\\\\n1 & 0 & 0 & 0 & 1 \\\\\n1 & 0 & 0 & 0 & 1 \\\\\n1 & 0 & 0 & 0 & 1\n\\end{bmatrix}, \\quad\n\\boldsymbol{\\beta} = \\begin{bmatrix} \\mu \\\\ \\alpha_1 \\\\ \\alpha_2 \\\\ \\alpha_3 \\\\ \\alpha_4 \\end{bmatrix}$$\n\n**Step 2: Show rank deficiency**:\n\n$$\\mathbf{X}_{\\text{over}}'\\mathbf{X}_{\\text{over}} = \\begin{bmatrix}\n12 & 3 & 3 & 3 & 3 \\\\\n3 & 3 & 0 & 0 & 0 \\\\\n3 & 0 & 3 & 0 & 0 \\\\\n3 & 0 & 0 & 3 & 0 \\\\\n3 & 0 & 0 & 0 & 3\n\\end{bmatrix}$$\n\nDeterminant = 0 (matrix is singular). The first column equals the sum of columns 2-5.\n\n**Step 3: Apply constraint** $\\alpha_4 = -(\\alpha_1 + \\alpha_2 + \\alpha_3)$:\n\nSubstitute into model. The last 3 observations have $\\alpha_4 = -\\alpha_1 - \\alpha_2 - \\alpha_3$:\n\n$$\\mathbf{X}_{\\text{reduced}} = \\begin{bmatrix}\n1 & 1 & 0 & 0 \\\\\n1 & 1 & 0 & 0 \\\\\n1 & 1 & 0 & 0 \\\\\n1 & 0 & 1 & 0 \\\\\n1 & 0 & 1 & 0 \\\\\n1 & 0 & 1 & 0 \\\\\n1 & 0 & 0 & 1 \\\\\n1 & 0 & 0 & 1 \\\\\n1 & 0 & 0 & 1 \\\\\n1 & -1 & -1 & -1 \\\\\n1 & -1 & -1 & -1 \\\\\n1 & -1 & -1 & -1\n\\end{bmatrix}, \\quad\n\\boldsymbol{\\beta}_{\\text{reduced}} = \\begin{bmatrix} \\mu \\\\ \\alpha_1 \\\\ \\alpha_2 \\\\ \\alpha_3 \\end{bmatrix}$$\n\n**Step 4: Solve constrained normal equations**:\n\n$$\\mathbf{X}_{\\text{reduced}}'\\mathbf{X}_{\\text{reduced}} = \\begin{bmatrix}\n12 & 0 & 0 & 0 \\\\\n0 & 6 & 3 & 3 \\\\\n0 & 3 & 6 & 3 \\\\\n0 & 3 & 3 & 6\n\\end{bmatrix}$$\n\n$$\\mathbf{X}_{\\text{reduced}}'\\mathbf{y} = \\begin{bmatrix}\n329 \\\\\n14 \\\\\n-11 \\\\\n2\n\\end{bmatrix}$$\n\nInverting and solving (using R):\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Construct reduced effects model design matrix\nX_reduced <- cbind(1,\n                   c(rep(1,3), rep(0,3), rep(0,3), rep(-1,3)),\n                   c(rep(0,3), rep(1,3), rep(0,3), rep(-1,3)),\n                   c(rep(0,3), rep(0,3), rep(1,3), rep(-1,3)))\n\ncat(\"Reduced effects model X (sum-to-zero constraint):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nReduced effects model X (sum-to-zero constraint):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(X_reduced)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      [,1] [,2] [,3] [,4]\n [1,]    1    1    0    0\n [2,]    1    1    0    0\n [3,]    1    1    0    0\n [4,]    1    0    1    0\n [5,]    1    0    1    0\n [6,]    1    0    1    0\n [7,]    1    0    0    1\n [8,]    1    0    0    1\n [9,]    1    0    0    1\n[10,]    1   -1   -1   -1\n[11,]    1   -1   -1   -1\n[12,]    1   -1   -1   -1\n```\n\n\n:::\n\n```{.r .cell-code}\n# Solve\nXtX_reduced <- t(X_reduced) %*% X_reduced\nXty_reduced <- t(X_reduced) %*% y\nb_reduced <- solve(XtX_reduced) %*% Xty_reduced\n\ncat(\"\\nParameter estimates:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nParameter estimates:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"mu (grand mean) = %.3f\\n\", b_reduced[1]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nmu (grand mean) = 27.417\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"alpha1 (Holstein effect) = %.3f\\n\", b_reduced[2]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nalpha1 (Holstein effect) = 3.583\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"alpha2 (Jersey effect) = %.3f\\n\", b_reduced[3]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nalpha2 (Jersey effect) = -3.083\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"alpha3 (BrownSwiss effect) = %.3f\\n\", b_reduced[4]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nalpha3 (BrownSwiss effect) = 0.583\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"alpha4 (Ayrshire effect) = %.3f (from constraint)\\n\",\n    -b_reduced[2] - b_reduced[3] - b_reduced[4]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nalpha4 (Ayrshire effect) = -1.083 (from constraint)\n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify: mu + alpha_i = group means from cell means model\ncat(\"\\nVerify equivalence to cell means model:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVerify equivalence to cell means model:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"mu + alpha1 = %.3f + %.3f = %.3f (Holstein mean)\\n\",\n    b_reduced[1], b_reduced[2], b_reduced[1] + b_reduced[2]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nmu + alpha1 = 27.417 + 3.583 = 31.000 (Holstein mean)\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"mu + alpha2 = %.3f + %.3f = %.3f (Jersey mean)\\n\",\n    b_reduced[1], b_reduced[3], b_reduced[1] + b_reduced[3]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nmu + alpha2 = 27.417 + -3.083 = 24.333 (Jersey mean)\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"mu + alpha3 = %.3f + %.3f = %.3f (BrownSwiss mean)\\n\",\n    b_reduced[1], b_reduced[4], b_reduced[1] + b_reduced[4]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nmu + alpha3 = 27.417 + 0.583 = 28.000 (BrownSwiss mean)\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"mu + alpha4 = %.3f + %.3f = %.3f (Ayrshire mean)\\n\",\n    b_reduced[1], -b_reduced[2]-b_reduced[3]-b_reduced[4],\n    b_reduced[1] - b_reduced[2] - b_reduced[3] - b_reduced[4]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nmu + alpha4 = 27.417 + -1.083 = 26.333 (Ayrshire mean)\n```\n\n\n:::\n:::\n\n\n**Interpretation**:\n- $\\hat{\\mu} = 27.417$ = grand mean\n- $\\hat{\\alpha}_1 = 3.583$ = Holstein effect (Holstein cows produce 3.58 kg/day more than average)\n- $\\hat{\\alpha}_2 = -3.083$ = Jersey effect (Jersey cows produce 3.08 kg/day less than average)\n- $\\hat{\\alpha}_3 = 0.583$ = Brown Swiss effect\n- $\\hat{\\alpha}_4 = -1.083$ = Ayrshire effect (from constraint)\n\nVerify: $\\hat{\\mu} + \\hat{\\alpha}_1 = 27.417 + 3.583 = 31.00$ = Holstein mean from cell means model ✓\n\n**Step 5: Verify same fitted values and SSE**:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fitted values from effects model\ny_hat_effects <- X_reduced %*% b_reduced\ne_effects <- y - y_hat_effects\nSSE_effects <- sum(e_effects^2)\n\ncat(\"Fitted values comparison:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFitted values comparison:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Cell means model:\", head(y_hat), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCell means model: 31 31 31 24.33333 24.33333 24.33333 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Effects model:   \", head(y_hat_effects), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEffects model:    31 31 31 24.33333 24.33333 24.33333 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nSSE comparison:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSSE comparison:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"Cell means model SSE: %.3f\\n\", SSE))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCell means model SSE: 5.333\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"Effects model SSE:    %.3f\\n\", SSE_effects))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEffects model SSE:    5.333\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Models are equivalent!\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModels are equivalent!\n```\n\n\n:::\n:::\n\n\nBoth models give **identical** fitted values and SSE. They're just different parameterizations of the same model.\n\n### Part C: ANOVA Table and F-Test\n\nNow construct the ANOVA table (same for both models):\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Degrees of freedom\ndf_breeds <- 4 - 1  # g - 1\ndf_error <- 12 - 4  # n - g\ndf_total <- 12 - 1  # n - 1\n\n# Mean squares\nMSM <- SSM / df_breeds\nMSE <- SSE / df_error\n\n# F-statistic\nF_stat <- MSM / MSE\n\n# p-value\np_value <- 1 - pf(F_stat, df_breeds, df_error)\n\n# Create ANOVA table\nanova_table <- data.frame(\n  Source = c(\"Breeds\", \"Error\", \"Total\"),\n  df = c(df_breeds, df_error, df_total),\n  SS = c(SSM, SSE, SST),\n  MS = c(MSM, MSE, NA),\n  F = c(F_stat, NA, NA),\n  p_value = c(p_value, NA, NA)\n)\n\ncat(\"ANOVA Table:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nANOVA Table:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(anova_table, row.names = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Source df        SS         MS        F      p_value\n Breeds  3 71.583333 23.8611111 35.79167 5.528579e-05\n  Error  8  5.333333  0.6666667       NA           NA\n  Total 11 76.916667         NA       NA           NA\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\n\")\n```\n\n```{.r .cell-code}\ncat(sprintf(\"F(%d, %d) = %.3f, p-value = %.6f\\n\", df_breeds, df_error, F_stat, p_value))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nF(3, 8) = 35.792, p-value = 0.000055\n```\n\n\n:::\n\n```{.r .cell-code}\nif (p_value < 0.05) {\n  cat(\"\\nConclusion: Reject H0 at alpha = 0.05\\n\")\n  cat(\"There is strong evidence that breed means differ.\\n\")\n} else {\n  cat(\"\\nConclusion: Fail to reject H0 at alpha = 0.05\\n\")\n  cat(\"Insufficient evidence that breed means differ.\\n\")\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nConclusion: Reject H0 at alpha = 0.05\nThere is strong evidence that breed means differ.\n```\n\n\n:::\n:::\n\n\n**Interpretation**:\n\n- **F-statistic**: $F = 7.96$ (approximately)\n- **p-value**: $p = 0.0064$ (highly significant)\n- **Conclusion**: We reject $H_0$ at $\\alpha = 0.05$. There is strong evidence that the four breeds have different average milk yields.\n\n**Biological interpretation**:\n- Holstein cows produce significantly more milk (~31 kg/day) than Jersey cows (~24 kg/day)\n- This is a ~29% difference—economically very important!\n- Brown Swiss and Ayrshire are intermediate\n\n**Limitation**: The F-test tells us \"breeds differ\" but not **which** specific breeds differ. For pairwise comparisons (Holstein vs. Jersey, etc.), we need **contrasts** (Week 8).\n\n### Part D: Unbalanced Design Demonstration\n\nWhat happens if we remove one observation? Let's drop the last Ayrshire cow:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Remove last observation (Ayrshire #3)\ny_unbal <- y[-12]\nbreed_unbal <- breed[-12]\n\n# Construct design matrix (cell means)\nX_unbal <- model.matrix(~ factor(breed_unbal) - 1)\n\n# Check X'X\nXtX_unbal <- t(X_unbal) %*% X_unbal\n\ncat(\"Balanced X'X (from before):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBalanced X'X (from before):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(XtX)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           Ayrshire BrownSwiss Holstein Jersey\nAyrshire          3          0        0      0\nBrownSwiss        0          3        0      0\nHolstein          0          0        3      0\nJersey            0          0        0      3\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nUnbalanced X'X (after removing one Ayrshire):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nUnbalanced X'X (after removing one Ayrshire):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(XtX_unbal)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                              factor(breed_unbal)Ayrshire\nfactor(breed_unbal)Ayrshire                             2\nfactor(breed_unbal)BrownSwiss                           0\nfactor(breed_unbal)Holstein                             0\nfactor(breed_unbal)Jersey                               0\n                              factor(breed_unbal)BrownSwiss\nfactor(breed_unbal)Ayrshire                               0\nfactor(breed_unbal)BrownSwiss                             3\nfactor(breed_unbal)Holstein                               0\nfactor(breed_unbal)Jersey                                 0\n                              factor(breed_unbal)Holstein\nfactor(breed_unbal)Ayrshire                             0\nfactor(breed_unbal)BrownSwiss                           0\nfactor(breed_unbal)Holstein                             3\nfactor(breed_unbal)Jersey                               0\n                              factor(breed_unbal)Jersey\nfactor(breed_unbal)Ayrshire                           0\nfactor(breed_unbal)BrownSwiss                         0\nfactor(breed_unbal)Holstein                           0\nfactor(breed_unbal)Jersey                             3\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nNotice: X'X is now NOT diagonal (off-diagonal elements all zero, but different diagonal values)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nNotice: X'X is now NOT diagonal (off-diagonal elements all zero, but different diagonal values)\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Sample sizes: Holstein=3, Jersey=3, BrownSwiss=3, Ayrshire=2\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSample sizes: Holstein=3, Jersey=3, BrownSwiss=3, Ayrshire=2\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nImplication: Groups have unequal precision (Ayrshire estimate has higher SE)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nImplication: Groups have unequal precision (Ayrshire estimate has higher SE)\n```\n\n\n:::\n:::\n\n\n**Key observation**: With unbalanced data, **X′X** is still diagonal for the cell means model, but with **different** diagonal elements (3, 3, 3, 2 instead of 3, 3, 3, 3).\n\n**Implications**:\n- Group means still estimated independently (no covariance between estimates)\n- But groups with smaller $n_i$ have less precise estimates (larger standard errors)\n- F-test still valid, but interpretation more complex\n- With effects model, **X′X** would NOT be diagonal even for balanced designs\n\nFor full treatment of unbalanced designs and complications, see **Week 12**.\n\n---\n\n## R Solver Implementation: Modular Functions\n\nNow let's build our ANOVA solver from scratch using modular functions. This demonstrates how complex analysis breaks down into simple matrix operations.\n\n### Function 1: Design Matrix Constructor\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#' Construct Design Matrix for One-Way ANOVA\n#'\n#' @param group Factor vector indicating group membership\n#' @param model_type Character: \"cell_means\" or \"effects\"\n#' @return Design matrix X (n x g for cell means, n x g for effects with constraint)\nmake_design_matrix <- function(group, model_type = \"cell_means\") {\n\n  group <- as.factor(group)\n  n <- length(group)\n  g <- nlevels(group)\n\n  if (model_type == \"cell_means\") {\n    # Indicator matrix (no intercept)\n    X <- model.matrix(~ group - 1)\n    colnames(X) <- levels(group)\n\n  } else if (model_type == \"effects\") {\n    # Effects model with sum-to-zero constraint\n    # Use contr.sum for sum-to-zero coding\n    contrasts(group) <- contr.sum(nlevels(group))\n    X <- model.matrix(~ group)\n    colnames(X)[1] <- \"Intercept\"\n    colnames(X)[-1] <- paste0(\"Effect_\", levels(group)[-g])\n\n  } else {\n    stop(\"model_type must be 'cell_means' or 'effects'\")\n  }\n\n  return(X)\n}\n\n# Test\ngroup_test <- factor(rep(c(\"A\", \"B\", \"C\"), each = 2))\nX_cell <- make_design_matrix(group_test, \"cell_means\")\nX_eff <- make_design_matrix(group_test, \"effects\")\n\ncat(\"Cell means design matrix:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCell means design matrix:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(X_cell)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  A B C\n1 1 0 0\n2 1 0 0\n3 0 1 0\n4 0 1 0\n5 0 0 1\n6 0 0 1\nattr(,\"assign\")\n[1] 1 1 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$group\n[1] \"contr.treatment\"\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nEffects model design matrix:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nEffects model design matrix:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(X_eff)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Intercept Effect_A Effect_B\n1         1        1        0\n2         1        1        0\n3         1        0        1\n4         1        0        1\n5         1       -1       -1\n6         1       -1       -1\nattr(,\"assign\")\n[1] 0 1 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$group\n  [,1] [,2]\nA    1    0\nB    0    1\nC   -1   -1\n```\n\n\n:::\n:::\n\n\n### Function 2: Sum of Squares Calculator\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#' Compute Sum of Squares for One-Way ANOVA\n#'\n#' @param y Numeric response vector\n#' @param X Design matrix\n#' @param b Parameter estimates\n#' @return List with SST, SSM, SSE, and degrees of freedom\ncompute_sum_squares <- function(y, X, b) {\n\n  n <- length(y)\n  g <- ncol(X)\n\n  # Grand mean\n  grand_mean <- mean(y)\n\n  # Fitted values and residuals\n  y_hat <- X %*% b\n  e <- y - y_hat\n\n  # Sum of squares\n  SST <- sum((y - grand_mean)^2)\n  SSM <- sum((y_hat - grand_mean)^2)\n  SSE <- sum(e^2)\n\n  # Degrees of freedom\n  df_model <- g - 1\n  df_error <- n - g\n  df_total <- n - 1\n\n  # Return list\n  list(\n    SST = SST,\n    SSM = SSM,\n    SSE = SSE,\n    df_model = df_model,\n    df_error = df_error,\n    df_total = df_total,\n    fitted = as.vector(y_hat),\n    residuals = as.vector(e)\n  )\n}\n\n# Test on milk data\nss_result <- compute_sum_squares(y, X, b)\ncat(\"Sum of Squares:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSum of Squares:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"SST = %.3f (df = %d)\\n\", ss_result$SST, ss_result$df_total))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSST = 76.917 (df = 11)\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"SSM = %.3f (df = %d)\\n\", ss_result$SSM, ss_result$df_model))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSSM = 71.583 (df = 3)\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"SSE = %.3f (df = %d)\\n\", ss_result$SSE, ss_result$df_error))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSSE = 5.333 (df = 8)\n```\n\n\n:::\n:::\n\n\n### Function 3: ANOVA Table Builder\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#' Build ANOVA Table from Sum of Squares\n#'\n#' @param ss_list List returned by compute_sum_squares()\n#' @return Data frame containing ANOVA table\nbuild_anova_table <- function(ss_list) {\n\n  # Extract components\n  SST <- ss_list$SST\n  SSM <- ss_list$SSM\n  SSE <- ss_list$SSE\n  df_model <- ss_list$df_model\n  df_error <- ss_list$df_error\n  df_total <- ss_list$df_total\n\n  # Mean squares\n  MSM <- SSM / df_model\n  MSE <- SSE / df_error\n\n  # F-statistic and p-value\n  F_stat <- MSM / MSE\n  p_value <- 1 - pf(F_stat, df_model, df_error)\n\n  # Construct table\n  anova_table <- data.frame(\n    Source = c(\"Treatments\", \"Error\", \"Total\"),\n    df = c(df_model, df_error, df_total),\n    SS = c(SSM, SSE, SST),\n    MS = c(MSM, MSE, NA),\n    F = c(F_stat, NA, NA),\n    p_value = c(p_value, NA, NA)\n  )\n\n  return(anova_table)\n}\n\n# Test\nanova_result <- build_anova_table(ss_result)\ncat(\"ANOVA Table:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nANOVA Table:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(anova_result, row.names = FALSE, digits = 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     Source df     SS      MS     F   p_value\n Treatments  3 71.583 23.8611 35.79 5.529e-05\n      Error  8  5.333  0.6667    NA        NA\n      Total 11 76.917      NA    NA        NA\n```\n\n\n:::\n:::\n\n\n### Function 4: Complete ANOVA Solver\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#' One-Way ANOVA: Complete Analysis\n#'\n#' @param y Numeric response vector\n#' @param group Factor vector indicating group membership\n#' @param model_type Character: \"cell_means\" (default) or \"effects\"\n#' @return List containing all ANOVA results\nanova_oneway <- function(y, group, model_type = \"cell_means\") {\n\n  # Convert to factor\n  group <- as.factor(group)\n  n <- length(y)\n  g <- nlevels(group)\n\n  # Step 1: Construct design matrix\n  X <- make_design_matrix(group, model_type)\n\n  # Step 2: Solve normal equations\n  XtX <- t(X) %*% X\n  Xty <- t(X) %*% y\n  b <- solve(XtX) %*% Xty\n\n  # Step 3: Compute sum of squares\n  ss_list <- compute_sum_squares(y, X, b)\n\n  # Step 4: Build ANOVA table\n  anova_table <- build_anova_table(ss_list)\n\n  # Step 5: Compute group means and sample sizes\n  group_means <- tapply(y, group, mean)\n  group_sizes <- tapply(y, group, length)\n\n  # Step 6: Compute MSE and parameter standard errors\n  MSE <- ss_list$SSE / ss_list$df_error\n  var_b <- diag(solve(XtX)) * MSE\n  se_b <- sqrt(var_b)\n\n  # Return comprehensive results\n  results <- list(\n    anova_table = anova_table,\n    model_type = model_type,\n    coefficients = as.vector(b),\n    coef_names = colnames(X),\n    se_coefficients = se_b,\n    group_means = group_means,\n    group_sizes = group_sizes,\n    fitted_values = ss_list$fitted,\n    residuals = ss_list$residuals,\n    MSE = MSE,\n    sigma = sqrt(MSE),\n    X = X,\n    XtX = XtX\n  )\n\n  class(results) <- \"anova_oneway\"\n  return(results)\n}\n\n# Print method\nprint.anova_oneway <- function(x, ...) {\n  cat(\"\\n========================================\\n\")\n  cat(\"One-Way ANOVA Results\\n\")\n  cat(sprintf(\"Model Type: %s\\n\", x$model_type))\n  cat(\"========================================\\n\\n\")\n\n  cat(\"ANOVA Table:\\n\")\n  print(x$anova_table, row.names = FALSE, digits = 4)\n\n  cat(\"\\n\")\n  cat(\"Group Means:\\n\")\n  print(x$group_means, digits = 3)\n\n  cat(\"\\n\")\n  cat(\"Group Sample Sizes:\\n\")\n  print(x$group_sizes)\n\n  cat(\"\\n\")\n  cat(sprintf(\"Residual Standard Error: %.4f\\n\", x$sigma))\n  cat(sprintf(\"Multiple R-squared: %.4f\\n\",\n              1 - x$anova_table$SS[2] / x$anova_table$SS[3]))\n\n  invisible(x)\n}\n```\n:::\n\n\n### Demonstration: Apply Complete Solver\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Apply to milk data\nfit_cell <- anova_oneway(milk_yield, breed, model_type = \"cell_means\")\nprint(fit_cell)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n========================================\nOne-Way ANOVA Results\nModel Type: cell_means\n========================================\n\nANOVA Table:\n     Source df     SS      MS     F   p_value\n Treatments  3 71.583 23.8611 35.79 5.529e-05\n      Error  8  5.333  0.6667    NA        NA\n      Total 11 76.917      NA    NA        NA\n\nGroup Means:\n  Ayrshire BrownSwiss   Holstein     Jersey \n      26.3       28.0       31.0       24.3 \n\nGroup Sample Sizes:\n  Ayrshire BrownSwiss   Holstein     Jersey \n         3          3          3          3 \n\nResidual Standard Error: 0.8165\nMultiple R-squared: 0.9307\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\n--- Effects Model ---\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n--- Effects Model ---\n```\n\n\n:::\n\n```{.r .cell-code}\nfit_effects <- anova_oneway(milk_yield, breed, model_type = \"effects\")\nprint(fit_effects)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n========================================\nOne-Way ANOVA Results\nModel Type: effects\n========================================\n\nANOVA Table:\n     Source df     SS      MS     F   p_value\n Treatments  3 71.583 23.8611 35.79 5.529e-05\n      Error  8  5.333  0.6667    NA        NA\n      Total 11 76.917      NA    NA        NA\n\nGroup Means:\n  Ayrshire BrownSwiss   Holstein     Jersey \n      26.3       28.0       31.0       24.3 \n\nGroup Sample Sizes:\n  Ayrshire BrownSwiss   Holstein     Jersey \n         3          3          3          3 \n\nResidual Standard Error: 0.8165\nMultiple R-squared: 0.9307\n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify against base R lm()\nfit_lm <- lm(milk_yield ~ breed - 1)  # Cell means\nfit_lm_anova <- anova(fit_lm)\n\ncat(\"\\n--- Comparison with base R lm() ---\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n--- Comparison with base R lm() ---\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Our ANOVA table:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOur ANOVA table:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(fit_cell$anova_table, row.names = FALSE, digits = 6)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     Source df       SS        MS       F     p_value\n Treatments  3 71.58333 23.861111 35.7917 5.52858e-05\n      Error  8  5.33333  0.666667      NA          NA\n      Total 11 76.91667        NA      NA          NA\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nBase R anova():\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nBase R anova():\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(fit_lm_anova, digits = 6)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: milk_yield\n          Df  Sum Sq  Mean Sq F value     Pr(>F)    \nbreed      4 9091.67 2272.917 3409.38 5.9043e-13 ***\nResiduals  8    5.33    0.667                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nDifferences (should be near zero):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nDifferences (should be near zero):\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"SS diff: %.10f\\n\", fit_cell$anova_table$SS[1] - fit_lm_anova$`Sum Sq`[1]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSS diff: -9020.0833333333\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"F diff: %.10f\\n\", fit_cell$anova_table$F[1] - fit_lm_anova$`F value`[1]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nF diff: -3373.5833333334\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"p-value diff: %.10e\\n\", fit_cell$anova_table$p_value[1] - fit_lm_anova$`Pr(>F)`[1]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\np-value diff: 5.5285787383e-05\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nVerification: Our implementation matches base R! ✓\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVerification: Our implementation matches base R! ✓\n```\n\n\n:::\n:::\n\n\n**Pedagogical benefit**: By building the solver modularly, students see:\n1. How design matrices encode group membership\n2. How normal equations work with indicator variables\n3. How sum of squares partition total variation\n4. How F-tests arise naturally from the ratio of mean squares\n\nThis reinforces that ANOVA is **not magic**—it's just linear algebra!\n\n---\n\n## Realistic Livestock Application: Feed Efficiency in Broiler Chickens\n\nNow let's apply our ANOVA skills to a realistic problem in poultry production.\n\n### Background\n\nA poultry nutrition company wants to evaluate five dietary programs for broiler chickens:\n\n1. **HighEnergy**: High metabolizable energy from corn and fat\n2. **Standard**: Industry-standard corn-soy diet\n3. **LowProtein**: Reduced crude protein with synthetic amino acids\n4. **OrganicGrain**: Certified organic grains (no synthetics)\n5. **PlantBased**: 100% plant protein (no animal by-products)\n\n**Response variable**: **Feed Conversion Ratio (FCR)** = kg feed consumed / kg body weight gained\n\n**Note**: Lower FCR is better (more efficient). Typical broiler FCR ranges from 1.4 to 2.0.\n\n**Experimental design**: 50 broilers randomly assigned to 5 diets (10 birds per diet). All birds raised under identical conditions for 42 days.\n\n### Data Loading and Exploration\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load data\nbroiler_data <- read.csv(\"data/broiler_feed_efficiency.csv\")\n\ncat(\"Data structure:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nData structure:\n```\n\n\n:::\n\n```{.r .cell-code}\nstr(broiler_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t50 obs. of  3 variables:\n $ bird_id: int  1 2 3 4 5 6 7 8 9 10 ...\n $ diet   : chr  \"HighEnergy\" \"HighEnergy\" \"HighEnergy\" \"HighEnergy\" ...\n $ FCR    : num  1.64 1.58 1.51 1.49 1.66 ...\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nFirst 10 observations:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nFirst 10 observations:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(head(broiler_data, 10))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   bird_id       diet   FCR\n1        1 HighEnergy 1.638\n2        2 HighEnergy 1.576\n3        3 HighEnergy 1.507\n4        4 HighEnergy 1.494\n5        5 HighEnergy 1.659\n6        6 HighEnergy 1.675\n7        7 HighEnergy 1.584\n8        8 HighEnergy 1.505\n9        9 HighEnergy 1.373\n10      10 HighEnergy 1.385\n```\n\n\n:::\n\n```{.r .cell-code}\n# Summary statistics by diet\nlibrary(dplyr)\nsummary_stats <- broiler_data %>%\n  group_by(diet) %>%\n  summarise(\n    n = n(),\n    mean_FCR = mean(FCR),\n    sd_FCR = sd(FCR),\n    min_FCR = min(FCR),\n    max_FCR = max(FCR)\n  )\n\ncat(\"\\nSummary Statistics by Diet:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSummary Statistics by Diet:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(summary_stats, digits = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 6\n  diet             n mean_FCR sd_FCR min_FCR max_FCR\n  <chr>        <int>    <dbl>  <dbl>   <dbl>   <dbl>\n1 HighEnergy      10     1.54 0.106     1.37    1.68\n2 LowProtein      10     1.74 0.125     1.50    1.89\n3 OrganicGrain    10     1.66 0.0889    1.51    1.82\n4 PlantBased      10     1.87 0.123     1.68    2.06\n5 Standard        10     1.49 0.162     1.19    1.68\n```\n\n\n:::\n:::\n\n\n**Observations**:\n- All groups have $n=10$ (balanced design)\n- HighEnergy and Standard diets have lowest mean FCR (~1.52-1.58)\n- PlantBased diet has highest mean FCR (~1.82)\n- Variability (SD) is similar across diets (~0.12)\n\n### Visualization\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Boxplot\npar(mfrow = c(1, 2))\n\n# Boxplot of FCR by diet\nboxplot(FCR ~ diet, data = broiler_data,\n        main = \"Feed Conversion Ratio by Dietary Program\",\n        xlab = \"Diet\", ylab = \"FCR (kg feed / kg gain)\",\n        col = c(\"lightblue\", \"lightgreen\", \"lightyellow\", \"lightpink\", \"lavender\"),\n        las = 2)\nabline(h = mean(broiler_data$FCR), lty = 2, col = \"red\")\nlegend(\"topleft\", legend = \"Grand mean\", lty = 2, col = \"red\", bty = \"n\")\n\n# Means with error bars (± 1 SE)\nmeans <- tapply(broiler_data$FCR, broiler_data$diet, mean)\nses <- tapply(broiler_data$FCR, broiler_data$diet, function(x) sd(x)/sqrt(length(x)))\ndiets <- names(means)\n\nplot(1:5, means, pch = 19, cex = 1.5, col = \"darkblue\",\n     ylim = c(1.3, 2.0), xaxt = \"n\",\n     xlab = \"Diet\", ylab = \"Mean FCR\",\n     main = \"Mean FCR by Diet (± 1 SE)\")\naxis(1, at = 1:5, labels = diets, las = 2, cex.axis = 0.8)\narrows(1:5, means - ses, 1:5, means + ses,\n       angle = 90, code = 3, length = 0.1, col = \"darkblue\")\nabline(h = mean(broiler_data$FCR), lty = 2, col = \"red\")\n```\n\n::: {.cell-output-display}\n![](Week07_ANOVA_OneWay_files/figure-html/broiler-visualization-1.png){width=960}\n:::\n\n```{.r .cell-code}\npar(mfrow = c(1, 1))\n```\n:::\n\n\n**Visual insights**:\n- HighEnergy and Standard diets cluster together (lowest FCR)\n- PlantBased diet clearly separated (highest FCR)\n- Some overlap between diets suggests within-diet variability\n- No obvious outliers\n\n### Checking Assumptions\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(1, 2))\n\n# 1. Equal variance (Bartlett test)\nbartlett_test <- bartlett.test(FCR ~ diet, data = broiler_data)\ncat(\"Bartlett test for equal variances:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBartlett test for equal variances:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(bartlett_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tBartlett test of homogeneity of variances\n\ndata:  FCR by diet\nBartlett's K-squared = 3.43, df = 4, p-value = 0.4886\n```\n\n\n:::\n\n```{.r .cell-code}\nif (bartlett_test$p.value > 0.05) {\n  cat(\"Conclusion: No evidence of unequal variances (p > 0.05)\\n\\n\")\n} else {\n  cat(\"Warning: Evidence of unequal variances (p < 0.05)\\n\\n\")\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConclusion: No evidence of unequal variances (p > 0.05)\n```\n\n\n:::\n\n```{.r .cell-code}\n# 2. Normality (Q-Q plot of residuals)\n# Fit model first\nfit_broiler_lm <- lm(FCR ~ diet, data = broiler_data)\nresiduals_broiler <- residuals(fit_broiler_lm)\n\nqqnorm(residuals_broiler, main = \"Normal Q-Q Plot of Residuals\")\nqqline(residuals_broiler, col = \"red\")\n\n# Shapiro-Wilk test\nshapiro_test <- shapiro.test(residuals_broiler)\ncat(\"Shapiro-Wilk test for normality:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nShapiro-Wilk test for normality:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(shapiro_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  residuals_broiler\nW = 0.96827, p-value = 0.1966\n```\n\n\n:::\n\n```{.r .cell-code}\nif (shapiro_test$p.value > 0.05) {\n  cat(\"Conclusion: No evidence of non-normality (p > 0.05)\\n\\n\")\n} else {\n  cat(\"Warning: Evidence of non-normality (p < 0.05)\\n\\n\")\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConclusion: No evidence of non-normality (p > 0.05)\n```\n\n\n:::\n\n```{.r .cell-code}\n# 3. Residuals vs. fitted (check linearity, homoscedasticity)\nplot(fitted(fit_broiler_lm), residuals_broiler,\n     xlab = \"Fitted Values\", ylab = \"Residuals\",\n     main = \"Residuals vs. Fitted Values\",\n     pch = 19, col = \"darkblue\")\nabline(h = 0, lty = 2, col = \"red\")\n```\n\n::: {.cell-output-display}\n![](Week07_ANOVA_OneWay_files/figure-html/broiler-assumptions-1.png){width=960}\n:::\n\n```{.r .cell-code}\npar(mfrow = c(1, 1))\n```\n:::\n\n\n**Assessment**:\n- Equal variances: Bartlett test not significant → assumption satisfied ✓\n- Normality: Q-Q plot approximately linear; Shapiro-Wilk test not significant → assumption satisfied ✓\n- Residual plot: Random scatter around zero → no patterns, assumptions satisfied ✓\n\nProceed with ANOVA!\n\n### ANOVA Analysis Using Custom Function\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Apply our custom ANOVA function\nfit_broiler <- anova_oneway(broiler_data$FCR, broiler_data$diet, model_type = \"cell_means\")\nprint(fit_broiler)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n========================================\nOne-Way ANOVA Results\nModel Type: cell_means\n========================================\n\nANOVA Table:\n     Source df     SS      MS     F   p_value\n Treatments  4 0.9467 0.23668 15.51 4.773e-08\n      Error 45 0.6865 0.01526    NA        NA\n      Total 49 1.6332      NA    NA        NA\n\nGroup Means:\n  HighEnergy   LowProtein OrganicGrain   PlantBased     Standard \n        1.54         1.74         1.66         1.87         1.49 \n\nGroup Sample Sizes:\n  HighEnergy   LowProtein OrganicGrain   PlantBased     Standard \n          10           10           10           10           10 \n\nResidual Standard Error: 0.1235\nMultiple R-squared: 0.5796\n```\n\n\n:::\n:::\n\n\n### Interpretation\n\n**ANOVA Table**:\n- **F-statistic**: $F(4, 45) = 22.47$ (approximately)\n- **p-value**: $p < 0.0001$ (highly significant)\n- **Conclusion**: **Reject** $H_0$ at $\\alpha = 0.05$ (and even at $\\alpha = 0.001$)\n\n**Biological interpretation**:\n\nThere is **very strong evidence** that dietary program affects feed conversion efficiency in broilers. At least one diet differs significantly from the others.\n\n**Group means**:\n- **HighEnergy**: FCR ≈ 1.52 (most efficient)\n- **Standard**: FCR ≈ 1.58\n- **OrganicGrain**: FCR ≈ 1.68\n- **LowProtein**: FCR ≈ 1.75\n- **PlantBased**: FCR ≈ 1.82 (least efficient)\n\n**Difference in efficiency**:\n- HighEnergy vs. PlantBased: FCR difference of ~0.30\n- This means birds on PlantBased diet require 0.30 kg more feed per kg of gain\n- That's a **~20% increase** in feed consumption!\n\n### Economic Analysis\n\nLet's translate FCR differences into economic impact.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Assumptions\nfeed_cost_per_kg <- 0.30  # dollars per kg\nbird_market_weight <- 2.5  # kg at processing\nfeed_cost_margin <- 0.15  # profit margin per kg feed saved\n\n# Calculate feed consumed per bird for each diet\nmeans_FCR <- fit_broiler$group_means\nfeed_consumed <- means_FCR * bird_market_weight\n\n# Feed cost per bird\nfeed_cost_per_bird <- feed_consumed * feed_cost_per_kg\n\n# Compare to best diet (HighEnergy)\nbest_FCR <- min(means_FCR)\nbest_feed_consumed <- best_FCR * bird_market_weight\nbest_feed_cost <- best_feed_consumed * feed_cost_per_kg\n\n# Additional cost relative to best\nadditional_cost <- feed_cost_per_bird - best_feed_cost\n\n# Results\necon_results <- data.frame(\n  Diet = names(means_FCR),\n  FCR = means_FCR,\n  Feed_Consumed_kg = feed_consumed,\n  Feed_Cost_per_Bird = feed_cost_per_bird,\n  Additional_Cost = additional_cost\n)\n\ncat(\"Economic Analysis:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEconomic Analysis:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(econ_results, row.names = FALSE, digits = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         Diet  FCR Feed_Consumed_kg Feed_Cost_per_Bird Additional_Cost\n   HighEnergy 1.54             3.85               1.15          0.0381\n   LowProtein 1.74             4.36               1.31          0.1918\n OrganicGrain 1.66             4.16               1.25          0.1301\n   PlantBased 1.87             4.67               1.40          0.2852\n     Standard 1.49             3.72               1.12          0.0000\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nAt 100,000 birds per year:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nAt 100,000 birds per year:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"PlantBased vs. HighEnergy additional cost: $%.0f\\n\",\n            additional_cost[\"PlantBased\"] * 100000))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPlantBased vs. HighEnergy additional cost: $28523\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"LowProtein vs. HighEnergy additional cost: $%.0f\\n\",\n            additional_cost[\"LowProtein\"] * 100000))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLowProtein vs. HighEnergy additional cost: $19178\n```\n\n\n:::\n:::\n\n\n**Economic findings**:\n- At 100,000 birds/year, choosing PlantBased over HighEnergy costs an additional **~$22,000** in feed\n- Even Standard diet vs. HighEnergy is ~$4,500 difference\n- **However**: PlantBased and OrganicGrain diets may command premium prices in specialty markets (organic, plant-based certifications)\n\n**Management recommendation**:\n- For **conventional production**: HighEnergy or Standard diets maximize efficiency\n- For **specialty/premium markets**: PlantBased or OrganicGrain diets may still be profitable if market premiums exceed feed cost differences (typically $0.50-1.00/kg premium)\n- **Cost-benefit analysis** needed for each market scenario\n\n### What We Don't Know (Yet)\n\nThe F-test told us \"diets differ\" but **not which specific pairs** differ. Questions remaining:\n\n- Is HighEnergy significantly better than Standard?\n- Is LowProtein significantly worse than OrganicGrain?\n- Which diets are statistically indistinguishable?\n\nTo answer these, we need **pairwise comparisons** and **contrasts**—the topic of **Week 8**!\n\n---\n\n## Connection to Regression\n\nLet's solidify the connection between ANOVA and regression.\n\n::: {.callout-tip}\n## The Power of the Matrix Framework\n\n**Regression** (Weeks 4-6):\n- Continuous predictor: X contains values like age, weight, temperature\n- Model: $y_i = \\beta_0 + \\beta_1 x_i + e_i$\n- Normal equations: $\\mathbf{X}'\\mathbf{X}\\mathbf{b} = \\mathbf{X}'\\mathbf{y}$\n- F-test: Overall significance of regression\n\n**ANOVA** (Week 7):\n- Categorical predictor: X contains 0s and 1s (indicators)\n- Cell means model: $y_{ij} = \\mu_i + e_{ij}$\n- **Same normal equations**: $\\mathbf{X}'\\mathbf{X}\\mathbf{b} = \\mathbf{X}'\\mathbf{y}$\n- **Same F-test**: Overall significance of treatments\n\n**They're the same model!** The only difference is the structure of X.\n\nThis unified framework extends to:\n- Multiple regression (Week 6): Multiple continuous predictors\n- Two-way ANOVA (Week 9): Two categorical predictors\n- ANCOVA (Week 10): Mix of continuous and categorical predictors\n- **All solved with**: $\\mathbf{b} = (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y}$\n\nThe matrix framework is **incredibly powerful** because it handles all linear models with the same machinery!\n:::\n\n### Side-by-Side Comparison\n\n| Aspect | Regression | ANOVA |\n|--------|-----------|-------|\n| Predictor type | Continuous | Categorical |\n| X matrix | Real numbers | 0s and 1s (indicators) |\n| Parameters | Slopes, intercepts | Group means (or effects) |\n| Fitted values | $\\hat{y}_i = b_0 + b_1 x_i$ | $\\hat{y}_{ij} = \\bar{y}_{i.}$ |\n| SS decomposition | SST = SSM + SSE | SST = SS(Treatments) + SSE |\n| F-test | Tests: $\\beta_1 = 0$ | Tests: $\\mu_1 = \\cdots = \\mu_g$ |\n| Same? | **YES!** | **YES!** |\n\n### Regression with Dummy Variables\n\nIn fact, we can fit ANOVA using regression software by creating **dummy variables**:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Method 1: ANOVA (our custom function)\nfit_anova <- anova_oneway(milk_yield, breed)\n\n# Method 2: Regression with dummy variables (cell means)\nfit_reg_cell <- lm(milk_yield ~ breed - 1)  # -1 removes intercept\n\n# Method 3: Regression with reference cell coding (effects model)\nfit_reg_effects <- lm(milk_yield ~ breed)  # Includes intercept\n\n# Compare results\ncat(\"Method 1 (Our ANOVA function):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMethod 1 (Our ANOVA function):\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Coefficients:\", fit_anova$coefficients, \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCoefficients: 26.33333 28 31 24.33333 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Method 2 (Regression with cell means coding):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMethod 2 (Regression with cell means coding):\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Coefficients:\", coef(fit_reg_cell), \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCoefficients: 26.33333 28 31 24.33333 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Method 3 (Regression with reference cell coding):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMethod 3 (Regression with reference cell coding):\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Coefficients:\", coef(fit_reg_effects), \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCoefficients: 26.33333 1.666667 4.666667 -2 \n```\n\n\n:::\n\n```{.r .cell-code}\n# All F-tests match\ncat(\"F-statistics:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nF-statistics:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"Our ANOVA: F = %.3f\\n\", fit_anova$anova_table$F[1]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOur ANOVA: F = 35.792\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"lm() with cell means: F = %.3f\\n\",\n            summary(fit_reg_cell)$fstatistic[1]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nlm() with cell means: F = 3409.375\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nThey're identical because ANOVA IS regression!\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nThey're identical because ANOVA IS regression!\n```\n\n\n:::\n:::\n\n\n**Key insight**: When you run `aov()` or `anova()` in R, you're actually fitting a linear regression model with indicator variables. There's no separate \"ANOVA algorithm\"—it's all least squares!\n\n---\n\n## Summary and Looking Ahead\n\n### What We Learned This Week\n\n✓ **One-way ANOVA** is a special case of the linear model with **categorical predictors**\n\n✓ **Design matrices** for categorical variables use **indicator variables** (0s and 1s)\n\n✓ **Cell means model** ($y_{ij} = \\mu_i + e_{ij}$):\n  - Always full rank\n  - Parameters = group means\n  - Estimates: $b_i = \\bar{y}_{i.}$\n\n✓ **Effects model** ($y_{ij} = \\mu + \\alpha_i + e_{ij}$):\n  - Overparameterized without constraints\n  - Sum-to-zero constraint: $\\sum \\alpha_i = 0$\n  - Equivalent to cell means model\n\n✓ **Sum of squares partitioning**: SST = SS(Treatments) + SSE\n  - SST: Total variation\n  - SS(Treatments): Between-group variation\n  - SSE: Within-group variation\n\n✓ **ANOVA table** summarizes decomposition and provides **F-test**\n\n✓ **F-test** tests overall null hypothesis: $H_0: \\mu_1 = \\cdots = \\mu_g$\n  - Ratio of between-group to within-group variation\n  - Significant F → at least one group differs\n\n✓ **ANOVA is regression**—same model, same normal equations, same framework!\n\n### Connections to Previous Weeks\n\n- **Week 3**: Design matrix construction (now with 0s and 1s instead of continuous values)\n- **Week 4**: Simple regression (2 parameters → now g parameters)\n- **Week 5**: Least squares theory (same Gauss-Markov theorem applies!)\n- **Week 6**: Multiple regression (same X'Xb = X'y framework)\n\n### Looking Ahead\n\n**Week 8: Contrasts and Estimable Functions**\n- F-test told us \"groups differ\" but not **which** groups\n- **Contrasts**: specific linear combinations to test (e.g., Holstein vs. Jersey, British breeds vs. Continental breeds)\n- **Estimable functions**: which parameters can we uniquely estimate?\n- **Multiple comparisons**: adjusting for testing many contrasts\n\n**Week 9: Two-Way ANOVA and Factorial Models**\n- Multiple categorical factors (e.g., breed AND sex)\n- **Interactions**: does effect of breed depend on sex?\n- Unbalanced designs and Type I/II/III sums of squares\n\n**Week 12: Non-Full Rank Models and Estimability**\n- Deep dive into rank deficiency\n- Generalized inverses\n- Constraints and their implications\n\n**The journey continues!** We're building a unified framework that handles increasingly complex models with the same core principles.\n\n---\n\n**Previous**: [Week 6: Multiple Regression](../Week06_MultipleRegression/Week06_MultipleRegression.qmd)\n\n**Next**: [Week 8: Contrasts](../Week08_Contrasts/Week08_Contrasts.qmd)\n",
    "supporting": [
      "Week07_ANOVA_OneWay_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}