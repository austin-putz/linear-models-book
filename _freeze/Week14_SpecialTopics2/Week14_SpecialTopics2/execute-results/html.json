{
  "hash": "ecc39320d22aedd28977a83963c4f478",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Week 14: Special Topics II\"\nformat:\n  html:\n    toc: true\n    toc-depth: 3\n    number-sections: true\n    code-fold: false\n    code-tools: true\n---\n\n::: {.callout-note icon=false}\n## Learning Objectives\n\nAfter completing this week's material, you will be able to:\n\n1. **Fit** **polynomial regression models** to capture **nonlinear relationships**, such as **animal growth curves**\n2. **Apply** **weighted least squares** when **error variances** are **heterogeneous** or known to differ\n3. **Understand** when and how to use **regression through the origin** (no-intercept models)\n4. **Recognize** the distinction between **fixed** and **random effects**\n5. **Preview** **mixed model equations** and understand how they extend the least squares framework to accommodate **random effects** and **genetic evaluation (BLUP)**\n:::\n\n---\n\n## Conceptual Introduction\n\nThroughout this course, we have built a solid foundation in linear models with fixed effects. We've mastered simple and multiple regression, ANOVA, contrasts, and diagnostics. This week, we explore four specialized extensions that push the boundaries of what we can accomplish with the linear model framework.\n\n**Polynomial Regression** allows us to model nonlinear relationships—such as growth curves or lactation curves—while staying within the linear model framework. By including powers of predictors ($x$, $x^2$, $x^3$, etc.), we can capture curves, peaks, and inflection points that are essential in animal science.\n\n**Weighted Least Squares (WLS)** addresses a violation of the classical Gauss-Markov assumptions: when observations have different variances (heteroscedasticity). This is common when working with pen averages, grouped data, or measurements with known reliability differences. WLS gives less weight to noisier observations and more weight to precise ones.\n\n**Regression Through the Origin** (no-intercept models) applies when theory dictates that the response must be zero when the predictor is zero. While this seems straightforward, it requires careful consideration and comes with important caveats about interpretation.\n\n**Mixed Models** represent the bridge between this course and advanced genetic evaluation methods. By introducing **random effects**—parameters treated as random variables rather than fixed unknowns—we can model animal breeding structures, account for family relationships, and preview **Best Linear Unbiased Prediction (BLUP)**. While a full treatment of mixed models requires a separate course, understanding their structure and connection to what we've learned is crucial for animal breeding applications.\n\nThese four topics share a common thread: they extend the least squares framework to handle real-world complexities in animal science data. By the end of this week, you'll have a powerful toolkit for addressing a wide range of practical problems in livestock genetics and management.\n\n---\n\n## Polynomial Regression: Modeling Growth Curves {#sec-polynomial}\n\n### Introduction to Polynomial Regression\n\nMany biological processes in animal science are inherently nonlinear. Animals don't grow linearly—they follow sigmoid curves. Lactation doesn't increase linearly—milk yield rises to a peak and then declines. Feed efficiency changes with age in complex ways.\n\nPolynomial regression provides a flexible way to model these curved relationships while staying firmly within the linear model framework. The key insight: although the relationship between $y$ and $x$ is nonlinear, the model is still **linear in the parameters** $\\beta_0, \\beta_1, \\beta_2, \\ldots$\n\n### Mathematical Theory\n\nThe **polynomial regression model** of degree $k$ is:\n\n$$\ny_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 x_i^3 + \\cdots + \\beta_k x_i^k + e_i\n$$\n\nwhere:\n\n- $y_i$: response variable (e.g., weight, milk yield)\n- $x_i$: predictor variable (e.g., age, days in milk)\n- $\\beta_0, \\beta_1, \\ldots, \\beta_k$: parameters to be estimated\n- $k$: degree of the polynomial\n- $e_i$: random error, $e_i \\sim N(0, \\sigma^2)$\n\n::: {.callout-note}\n## Why This is Still a \"Linear\" Model\n\nAlthough we have $x^2$, $x^3$, etc., the model is **linear in the parameters** $\\boldsymbol{\\beta}$. We can write it in matrix form:\n\n$$\n\\boldsymbol{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{e}\n$$\n\nwhere $\\mathbf{X}$ includes columns for $1, x, x^2, x^3, \\ldots, x^k$. All our least squares theory applies!\n:::\n\n#### Matrix Form\n\nFor a quadratic model ($k=2$), the design matrix is:\n\n$$\n\\mathbf{X} = \\begin{bmatrix}\n1 & x_1 & x_1^2 \\\\\n1 & x_2 & x_2^2 \\\\\n\\vdots & \\vdots & \\vdots \\\\\n1 & x_n & x_n^2\n\\end{bmatrix}_{n \\times 3}\n$$\n\nThe normal equations are:\n\n$$\n\\mathbf{X}'\\mathbf{X}\\boldsymbol{b} = \\mathbf{X}'\\boldsymbol{y}\n$$\n\nAnd the solution is:\n\n$$\n\\boldsymbol{b} = (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\boldsymbol{y}\n$$\n\n#### Choosing the Polynomial Degree\n\nHow do we decide whether to use $k=1$ (linear), $k=2$ (quadratic), $k=3$ (cubic), etc.?\n\n**Method 1: Sequential F-tests**\n\nFit models of increasing degree and test whether adding the next term significantly reduces SSE:\n\n- Fit $k=1$: Compute $SSE_1$\n- Fit $k=2$: Compute $SSE_2$\n- Test: $H_0: \\beta_2 = 0$ using $F = \\frac{(SSE_1 - SSE_2)/1}{SSE_2/(n-3)}$\n\n**Method 2: Adjusted R²**\n\nChoose the model that maximizes:\n\n$$\n\\bar{R}^2 = 1 - \\frac{SSE/(n-p)}{SST/(n-1)}\n$$\n\nAdjusted $R^2$ penalizes adding parameters that don't substantially improve fit.\n\n**Method 3: Biological Plausibility**\n\nStatistical significance isn't everything! A cubic model might fit better statistically, but does it make biological sense? For growth curves, we typically expect smooth, sigmoid curves—not wild oscillations.\n\n::: {.callout-warning}\n## The Collinearity Problem\n\nHigh-degree polynomials create severe collinearity. The columns $x$, $x^2$, $x^3$ are highly correlated, leading to:\n\n- Large standard errors for parameter estimates\n- Unstable estimates (small data changes → large estimate changes)\n- Ill-conditioned $\\mathbf{X}'\\mathbf{X}$ matrices\n\n**Solution**: Use **orthogonal polynomials** (next section).\n:::\n\n#### Orthogonal Polynomials\n\nOrthogonal polynomials are constructed so that the columns of $\\mathbf{X}$ are uncorrelated. In R, the `poly()` function creates these automatically.\n\n**Benefits**:\n\n- Reduced collinearity (VIF near 1.0)\n- More stable estimates\n- Tests for individual terms are independent\n- Numerical stability\n\n**Trade-off**: Coefficients are harder to interpret directly (they're in a transformed space).\n\n### Small Numerical Example: Broiler Weight vs. Age\n\nLet's analyze broiler growth using a small dataset where we can see all the matrix operations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load data\nbroiler <- read.csv(\"data/broiler_growth.csv\")\nbroiler\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| age_days| weight_g|\n|--------:|--------:|\n|        7|      150|\n|       14|      350|\n|       21|      650|\n|       28|     1050|\n|       35|     1500|\n|       42|     1950|\n\n</div>\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Visualize the data\nlibrary(ggplot2)\nggplot(broiler, aes(x = age_days, y = weight_g)) +\n  geom_point(size = 3) +\n  labs(title = \"Broiler Weight vs. Age\",\n       x = \"Age (days)\",\n       y = \"Weight (g)\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](Week14_SpecialTopics2_files/figure-html/unnamed-chunk-2-1.png){width=768}\n:::\n:::\n\n\nThe relationship is clearly nonlinear! Let's fit linear, quadratic, and cubic models.\n\n#### Fitting Linear Model (k=1)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Linear model\nfit1 <- lm(weight_g ~ age_days, data = broiler)\nsummary(fit1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = weight_g ~ age_days, data = broiler)\n\nResiduals:\n       1        2        3        4        5        6 \n 126.190  -40.952 -108.095  -75.238    7.619   90.476 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -343.333     96.736  -3.549 0.023816 *  \nage_days      52.449      3.549  14.781 0.000122 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 103.9 on 4 degrees of freedom\nMultiple R-squared:  0.982,\tAdjusted R-squared:  0.9775 \nF-statistic: 218.5 on 1 and 4 DF,  p-value: 0.000122\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Manual calculation to verify\nX1 <- cbind(1, broiler$age_days)\ny <- broiler$weight_g\n\n# Normal equations: X'Xb = X'y\nXtX1 <- t(X1) %*% X1\nXty1 <- t(X1) %*% y\nb1 <- solve(XtX1) %*% Xty1\n\ncat(\"Manual estimates:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nManual estimates:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(b1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           [,1]\n[1,] -343.33333\n[2,]   52.44898\n```\n\n\n:::\n\n```{.r .cell-code}\n# SSE for linear model\ny_hat1 <- X1 %*% b1\ne1 <- y - y_hat1\nSSE1 <- sum(e1^2)\ncat(\"\\nSSE (linear):\", SSE1, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSSE (linear): 43190.48 \n```\n\n\n:::\n:::\n\n\n#### Fitting Quadratic Model (k=2)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Quadratic model\nfit2 <- lm(weight_g ~ age_days + I(age_days^2), data = broiler)\nsummary(fit2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = weight_g ~ age_days + I(age_days^2), data = broiler)\n\nResiduals:\n     1      2      3      4      5      6 \n 16.07 -18.93 -20.00  12.86  29.64 -19.64 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)   \n(Intercept)   -35.00000   51.08350  -0.685  0.54244   \nage_days       19.41327    4.77432   4.066  0.02683 * \nI(age_days^2)   0.67420    0.09538   7.068  0.00582 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 28.56 on 3 degrees of freedom\nMultiple R-squared:  0.999,\tAdjusted R-squared:  0.9983 \nF-statistic:  1471 on 2 and 3 DF,  p-value: 3.25e-05\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Manual calculation\nX2 <- cbind(1, broiler$age_days, broiler$age_days^2)\nXtX2 <- t(X2) %*% X2\nXty2 <- t(X2) %*% y\nb2 <- solve(XtX2) %*% Xty2\n\ncat(\"Manual estimates (quadratic):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nManual estimates (quadratic):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(b2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            [,1]\n[1,] -35.0000000\n[2,]  19.4132653\n[3,]   0.6741983\n```\n\n\n:::\n\n```{.r .cell-code}\n# SSE for quadratic model\ny_hat2 <- X2 %*% b2\ne2 <- y - y_hat2\nSSE2 <- sum(e2^2)\ncat(\"\\nSSE (quadratic):\", SSE2, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSSE (quadratic): 2446.429 \n```\n\n\n:::\n:::\n\n\n#### Fitting Cubic Model (k=3)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Cubic model\nfit3 <- lm(weight_g ~ age_days + I(age_days^2) + I(age_days^3), data = broiler)\nsummary(fit3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = weight_g ~ age_days + I(age_days^2) + I(age_days^3), \n    data = broiler)\n\nResiduals:\n     1      2      3      4      5      6 \n-1.984  6.349 -5.556 -1.587  4.365 -1.587 \n\nCoefficients:\n                Estimate Std. Error t value Pr(>|t|)   \n(Intercept)   116.666667  25.393725   4.594  0.04425 * \nage_days       -7.842026   4.133395  -1.897  0.19824   \nI(age_days^2)   1.963881   0.188952  10.394  0.00913 **\nI(age_days^3)  -0.017547   0.002551  -6.879  0.02049 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.043 on 2 degrees of freedom\nMultiple R-squared:      1,\tAdjusted R-squared:  0.9999 \nF-statistic: 1.614e+04 on 3 and 2 DF,  p-value: 6.195e-05\n```\n\n\n:::\n\n```{.r .cell-code}\n# SSE\nSSE3 <- sum(residuals(fit3)^2)\ncat(\"SSE (cubic):\", SSE3, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSSE (cubic): 99.20635 \n```\n\n\n:::\n:::\n\n\n#### Model Comparison\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compare models with ANOVA\nanova(fit1, fit2, fit3)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| Res.Df|         RSS| Df| Sum of Sq|      F|    Pr(>F)|\n|------:|-----------:|--:|---------:|------:|---------:|\n|      4| 43190.47619| NA|        NA|     NA|        NA|\n|      3|  2446.42857|  1| 40744.048| 821.40| 0.0012152|\n|      2|    99.20635|  1|  2347.222|  47.32| 0.0204856|\n\n</div>\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compare adjusted R²\ncat(\"Adjusted R²:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAdjusted R²:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  Linear:   \", summary(fit1)$adj.r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Linear:    0.9775245 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  Quadratic:\", summary(fit2)$adj.r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Quadratic: 0.9983026 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  Cubic:    \", summary(fit3)$adj.r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Cubic:     0.9998967 \n```\n\n\n:::\n:::\n\n\n**Interpretation**: The quadratic model provides a substantial improvement over linear ($F$ test highly significant), but adding the cubic term provides minimal additional improvement. The quadratic model is preferred based on parsimony and biological plausibility.\n\n#### Visualizing the Fits\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create prediction data\nage_pred <- seq(7, 42, length.out = 100)\npred_data <- data.frame(age_days = age_pred)\n\n# Predictions from each model\npred_data$linear <- predict(fit1, newdata = pred_data)\npred_data$quadratic <- predict(fit2, newdata = pred_data)\npred_data$cubic <- predict(fit3, newdata = pred_data)\n\n# Plot\nggplot(broiler, aes(x = age_days, y = weight_g)) +\n  geom_point(size = 3, color = \"black\") +\n  geom_line(data = pred_data, aes(y = linear, color = \"Linear\"), linewidth = 1) +\n  geom_line(data = pred_data, aes(y = quadratic, color = \"Quadratic\"), linewidth = 1) +\n  geom_line(data = pred_data, aes(y = cubic, color = \"Cubic\"), linewidth = 1) +\n  scale_color_manual(values = c(\"Linear\" = \"blue\",\n                                 \"Quadratic\" = \"red\",\n                                 \"Cubic\" = \"green\")) +\n  labs(title = \"Comparing Polynomial Models\",\n       subtitle = \"Broiler Growth Curve\",\n       x = \"Age (days)\",\n       y = \"Weight (g)\",\n       color = \"Model\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![](Week14_SpecialTopics2_files/figure-html/unnamed-chunk-10-1.png){width=768}\n:::\n:::\n\n\n#### Making Predictions\n\nLet's predict weight at day 49 using the quadratic model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Predict at day 49\nnew_data <- data.frame(age_days = 49)\npred_49 <- predict(fit2, newdata = new_data, interval = \"prediction\")\ncat(\"Predicted weight at day 49:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPredicted weight at day 49:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(pred_49)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   fit      lwr      upr\n1 2535 2348.752 2721.248\n```\n\n\n:::\n:::\n\n\n::: {.callout-tip}\n## Interpretation of Quadratic Coefficients\n\nFor the quadratic model: $y = \\beta_0 + \\beta_1 x + \\beta_2 x^2$\n\n- $\\beta_0$: Intercept (weight at day 0, often not biologically meaningful)\n- $\\beta_1$: Linear component of growth rate\n- $\\beta_2$: Curvature term\n  - If $\\beta_2 > 0$: growth rate is increasing (accelerating growth)\n  - If $\\beta_2 < 0$: growth rate is decreasing (decelerating growth)\n\nIn our example, $\\beta_2 > 0$, indicating accelerating growth typical of young broilers.\n:::\n\n### Realistic Livestock Application: Dairy Lactation Curve\n\nDairy lactation curves are a classic application of polynomial regression. Milk yield typically:\n\n1. Rises rapidly after calving\n2. Reaches a peak around 60-90 days\n3. Gradually declines through the rest of lactation\n\nLet's analyze a larger dataset (n=60) of milk yield measurements at various days in milk.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load lactation data\nlactation <- read.csv(\"data/lactation_curve.csv\")\n\n# Quick look at the data\nhead(lactation, 10)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| cow_id| days_in_milk| milk_yield_kg|\n|------:|------------:|-------------:|\n|      1|            5|          40.1|\n|      2|           10|          41.3|\n|      3|           15|          47.2|\n|      4|           20|          50.4|\n|      5|           25|          51.6|\n|      6|           30|          51.6|\n|      7|           35|          56.6|\n|      8|           40|          53.3|\n|      9|           45|          59.1|\n|     10|           50|          54.3|\n\n</div>\n:::\n\n```{.r .cell-code}\ncat(\"\\nDataset dimensions:\", nrow(lactation), \"observations\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nDataset dimensions: 60 observations\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Visualize lactation curve\nggplot(lactation, aes(x = days_in_milk, y = milk_yield_kg)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"gray50\", linetype = \"dashed\") +\n  labs(title = \"Dairy Lactation Curve\",\n       subtitle = \"Milk Yield vs. Days in Milk (n=60)\",\n       x = \"Days in Milk (DIM)\",\n       y = \"Milk Yield (kg/day)\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](Week14_SpecialTopics2_files/figure-html/unnamed-chunk-13-1.png){width=768}\n:::\n:::\n\n\n#### Fit Multiple Polynomial Degrees\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit 2nd and 3rd degree polynomials\nfit_lac2 <- lm(milk_yield_kg ~ days_in_milk + I(days_in_milk^2), data = lactation)\nfit_lac3 <- lm(milk_yield_kg ~ days_in_milk + I(days_in_milk^2) + I(days_in_milk^3),\n               data = lactation)\n\n# Compare models\nanova(fit_lac2, fit_lac3)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| Res.Df|      RSS| Df| Sum of Sq|        F|   Pr(>F)|\n|------:|--------:|--:|---------:|--------:|--------:|\n|     57| 863.0734| NA|        NA|       NA|       NA|\n|     56| 626.7906|  1|  236.2827| 21.11045| 2.51e-05|\n\n</div>\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Model summaries\nsummary(fit_lac2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = milk_yield_kg ~ days_in_milk + I(days_in_milk^2), \n    data = lactation)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.7499  -2.5532   0.4242   2.3426   8.5839 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        5.064e+01  1.559e+00  32.491  < 2e-16 ***\ndays_in_milk       4.305e-02  2.358e-02   1.826   0.0732 .  \nI(days_in_milk^2) -3.920e-04  7.494e-05  -5.231 2.51e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.891 on 57 degrees of freedom\nMultiple R-squared:  0.7793,\tAdjusted R-squared:  0.7716 \nF-statistic: 100.7 on 2 and 57 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check for collinearity\nlibrary(car)\nvif(fit_lac2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     days_in_milk I(days_in_milk^2) \n         16.52141          16.52141 \n```\n\n\n:::\n:::\n\n\n::: {.callout-warning}\n## High VIF Values Detected\n\nVIF values above 10 indicate problematic collinearity. The raw polynomial terms `days_in_milk` and `days_in_milk^2` are highly correlated, leading to unstable estimates.\n\n**Solution**: Use orthogonal polynomials.\n:::\n\n#### Using Orthogonal Polynomials\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit with orthogonal polynomials\nfit_lac2_orth <- lm(milk_yield_kg ~ poly(days_in_milk, 2), data = lactation)\nfit_lac3_orth <- lm(milk_yield_kg ~ poly(days_in_milk, 3), data = lactation)\n\ncat(\"Orthogonal polynomials are constructed to be uncorrelated by design.\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOrthogonal polynomials are constructed to be uncorrelated by design.\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"This eliminates collinearity problems!\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nThis eliminates collinearity problems!\n```\n\n\n:::\n:::\n\n\nMuch better! Orthogonal polynomials have VIF near 1.0 by construction—no collinearity problems.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compare orthogonal models\nanova(fit_lac2_orth, fit_lac3_orth)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| Res.Df|      RSS| Df| Sum of Sq|        F|   Pr(>F)|\n|------:|--------:|--:|---------:|--------:|--------:|\n|     57| 863.0734| NA|        NA|       NA|       NA|\n|     56| 626.7906|  1|  236.2827| 21.11045| 2.51e-05|\n\n</div>\n:::\n:::\n\n\nThe 3rd degree term is not significant. The quadratic (2nd degree) model is sufficient.\n\n#### Visualize Fitted Lactation Curve\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create prediction data\ndim_pred <- seq(5, 300, length.out = 100)\npred_lac <- data.frame(days_in_milk = dim_pred)\n\n# Predictions\npred_lac$quadratic <- predict(fit_lac2, newdata = pred_lac)\npred_lac$quadratic_orth <- predict(fit_lac2_orth, newdata = pred_lac)\n\n# Plot\nggplot(lactation, aes(x = days_in_milk, y = milk_yield_kg)) +\n  geom_point(alpha = 0.4) +\n  geom_line(data = pred_lac, aes(y = quadratic),\n            color = \"blue\", linewidth = 1.2) +\n  labs(title = \"Fitted Lactation Curve (Quadratic Model)\",\n       subtitle = \"Blue line: y = β₀ + β₁x + β₂x²\",\n       x = \"Days in Milk (DIM)\",\n       y = \"Milk Yield (kg/day)\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](Week14_SpecialTopics2_files/figure-html/unnamed-chunk-19-1.png){width=768}\n:::\n:::\n\n\n#### Finding Peak Lactation\n\nFor a quadratic model $y = \\beta_0 + \\beta_1 x + \\beta_2 x^2$, the peak occurs at:\n\n$$\nx_{peak} = -\\frac{\\beta_1}{2\\beta_2}\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract coefficients\nb0 <- coef(fit_lac2)[1]\nb1 <- coef(fit_lac2)[2]\nb2 <- coef(fit_lac2)[3]\n\n# Calculate peak day\npeak_day <- -b1 / (2 * b2)\npeak_yield <- b0 + b1 * peak_day + b2 * peak_day^2\n\ncat(\"Peak lactation occurs at day:\", round(peak_day, 1), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPeak lactation occurs at day: 54.9 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Peak milk yield:\", round(peak_yield, 1), \"kg/day\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPeak milk yield: 51.8 kg/day\n```\n\n\n:::\n:::\n\n\n::: {.callout-note}\n## Biological Interpretation\n\nThe peak lactation at ~60 days in milk is consistent with dairy cattle physiology. After calving, milk production rises rapidly as the cow enters peak lactation, then gradually declines due to:\n\n- Declining mammary cell numbers\n- Reduced feed intake relative to energy demands\n- Pregnancy (if the cow is bred back)\n\nUnderstanding lactation curves is essential for:\n\n- Feeding management (peak energy demands)\n- Breeding decisions (when to rebreed)\n- Culling decisions (persistency of lactation)\n- Genetic evaluation (lactation curve shape is heritable)\n:::\n\n### Summary: Polynomial Regression\n\n**When to use**:\n\n- Nonlinear relationships (growth, lactation, feed efficiency curves)\n- Curved response patterns with clear peaks or inflections\n- Biological processes that follow polynomial-like trajectories\n\n**Key considerations**:\n\n- Start with low degrees (k=2 or k=3); higher degrees rarely justified\n- Use sequential F-tests or adjusted R² to select degree\n- Check biological plausibility—don't overfit!\n- Use orthogonal polynomials to avoid collinearity\n- Remember: polynomial approximations work well locally but may behave poorly outside the data range (extrapolation risky)\n\n---\n\n## Weighted Least Squares: Handling Heterogeneous Variance {#sec-wls}\n\n### Introduction to Weighted Least Squares\n\nThe Gauss-Markov assumptions include **homoscedasticity**: constant error variance, $Var(e_i) = \\sigma^2$ for all $i$. But this assumption is often violated in animal science:\n\n- **Pen averages**: Pens with more animals have less variable means\n- **Grouped data**: Groups with different sample sizes have different precision\n- **Repeated measures**: Variance may increase over time\n- **Measurement error**: Some measurements are more reliable than others\n\nWhen variances differ across observations (**heteroscedasticity**), ordinary least squares (OLS) is still unbiased but **no longer efficient**. Weighted least squares (WLS) provides **better** estimates by giving less weight to noisy observations and more weight to precise ones.\n\n### Mathematical Theory\n\n#### The Heteroscedastic Model\n\nAssume:\n\n$$\nVar(e_i) = \\sigma_i^2\n$$\n\nThe error variances differ across observations. In matrix form:\n\n$$\nVar(\\boldsymbol{e}) = \\mathbf{V} = \\begin{bmatrix}\n\\sigma_1^2 & 0 & \\cdots & 0 \\\\\n0 & \\sigma_2^2 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & \\sigma_n^2\n\\end{bmatrix}\n$$\n\n(Note: We assume errors are still independent, just with different variances.)\n\n#### Weighted Least Squares Criterion\n\nInstead of minimizing $\\sum e_i^2$, we minimize a **weighted** sum:\n\n$$\nS(\\boldsymbol{b}) = \\sum w_i e_i^2 = \\sum w_i (y_i - \\mathbf{x}_i'\\boldsymbol{b})^2\n$$\n\nwhere $w_i$ is the weight for observation $i$.\n\n**How to choose weights?**\n\nIf we know the variances $\\sigma_i^2$, the optimal weights are:\n\n$$\nw_i = \\frac{1}{\\sigma_i^2}\n$$\n\nGive more weight to observations with smaller variance (more precise), and less weight to noisy observations.\n\n#### WLS in Matrix Form\n\nDefine the **weight matrix** $\\mathbf{W}$:\n\n$$\n\\mathbf{W} = \\begin{bmatrix}\nw_1 & 0 & \\cdots & 0 \\\\\n0 & w_2 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & w_n\n\\end{bmatrix}\n$$\n\nThe weighted sum of squares is:\n\n$$\nS(\\boldsymbol{b}) = (\\boldsymbol{y} - \\mathbf{X}\\boldsymbol{b})'\\mathbf{W}(\\boldsymbol{y} - \\mathbf{X}\\boldsymbol{b})\n$$\n\nTaking the derivative and setting to zero gives the **weighted normal equations**:\n\n$$\n\\mathbf{X}'\\mathbf{W}\\mathbf{X}\\boldsymbol{b} = \\mathbf{X}'\\mathbf{W}\\boldsymbol{y}\n$$\n\n::: {.callout-important}\n## WLS Solution\n\nThe weighted least squares estimator is:\n\n$$\n\\boldsymbol{b}_{WLS} = (\\mathbf{X}'\\mathbf{W}\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{W}\\boldsymbol{y}\n$$\n\nAnd its variance is:\n\n$$\nVar(\\boldsymbol{b}_{WLS}) = (\\mathbf{X}'\\mathbf{W}\\mathbf{X})^{-1}\n$$\n\n(No $\\sigma^2$ multiplier because weights already account for variance differences!)\n:::\n\n#### Properties of WLS Estimators\n\nUnder the assumptions $E(\\boldsymbol{e}) = \\mathbf{0}$ and $Var(\\boldsymbol{e}) = \\mathbf{V}$:\n\n1. **Unbiased**: $E(\\boldsymbol{b}_{WLS}) = \\boldsymbol{\\beta}$\n2. **Efficient**: WLS has smaller variance than OLS when weights are correct\n3. **BLUE**: WLS is the Best Linear Unbiased Estimator under heteroscedasticity\n\n#### When Weights are Known vs. Estimated\n\n**Case 1: Weights Known**\n\n- Example: Pen averages where $Var(\\bar{y}_i) = \\sigma^2 / n_i$, so $w_i = n_i$\n- Example: Measurements with known measurement error variances\n\n**Case 2: Weights Estimated**\n\n- Estimate variance function from data (e.g., $\\sigma_i^2 = \\sigma^2 x_i$)\n- Use residuals from OLS fit to estimate weights\n- Iteratively reweighted least squares (IRLS)\n\n### Small Numerical Example: Pen Average ADG\n\nSuppose we have 5 pens of pigs, and we've measured the average daily gain (ADG) for each pen. Pens have different numbers of pigs, so the pen averages have different variances.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load pen data\npen_data <- read.csv(\"data/pen_avg_adg.csv\")\npen_data\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| pen| adg_kg_day| pen_size|\n|---:|----------:|--------:|\n|   1|       0.85|        8|\n|   2|       0.90|       12|\n|   3|       0.88|        6|\n|   4|       0.87|       10|\n|   5|       0.92|        9|\n\n</div>\n:::\n:::\n\n\nSince these are pen **averages**, the variance of $\\bar{y}_i$ is:\n\n$$\nVar(\\bar{y}_i) = \\frac{\\sigma^2}{n_i}\n$$\n\nwhere $n_i$ is the number of pigs in pen $i$. Therefore, the appropriate weight is:\n\n$$\nw_i = n_i\n$$\n\nLet's fit both OLS and WLS models to compare.\n\n#### Ordinary Least Squares (Unweighted)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# OLS regression (ignoring pen size)\nfit_ols <- lm(adg_kg_day ~ 1, data = pen_data)\nsummary(fit_ols)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = adg_kg_day ~ 1, data = pen_data)\n\nResiduals:\n     1      2      3      4      5 \n-0.034  0.016 -0.004 -0.014  0.036 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.88400    0.01208   73.16 2.09e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02702 on 4 degrees of freedom\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Manual calculation\nX <- matrix(1, nrow = 5, ncol = 1)  # Just intercept\ny <- pen_data$adg_kg_day\n\n# OLS: b = (X'X)^{-1}X'y\nb_ols <- solve(t(X) %*% X) %*% t(X) %*% y\ncat(\"OLS estimate (simple mean):\", b_ols, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOLS estimate (simple mean): 0.884 \n```\n\n\n:::\n:::\n\n\n#### Weighted Least Squares\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# WLS regression (weighted by pen size)\nfit_wls <- lm(adg_kg_day ~ 1, data = pen_data, weights = pen_size)\nsummary(fit_wls)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = adg_kg_day ~ 1, data = pen_data, weights = pen_size)\n\nWeighted Residuals:\n       1        2        3        4        5 \n-0.10119  0.04927 -0.01415 -0.04989  0.10267 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.88578    0.01199   73.85 2.02e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.08046 on 4 degrees of freedom\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Manual WLS calculation\nW <- diag(pen_data$pen_size)  # Weight matrix\n\n# WLS: b = (X'WX)^{-1}X'Wy\nXtWX <- t(X) %*% W %*% X\nXtWy <- t(X) %*% W %*% y\nb_wls <- solve(XtWX) %*% XtWy\n\ncat(\"WLS estimate (weighted mean):\", b_wls, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nWLS estimate (weighted mean): 0.8857778 \n```\n\n\n:::\n\n```{.r .cell-code}\n# This is equivalent to the weighted mean\nweighted_mean <- sum(pen_data$adg_kg_day * pen_data$pen_size) / sum(pen_data$pen_size)\ncat(\"Verification (weighted mean):\", weighted_mean, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nVerification (weighted mean): 0.8857778 \n```\n\n\n:::\n:::\n\n\n#### Comparison\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compare estimates\ncat(\"OLS estimate:\", coef(fit_ols), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOLS estimate: 0.884 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"WLS estimate:\", coef(fit_wls), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nWLS estimate: 0.8857778 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Difference:  \", coef(fit_wls) - coef(fit_ols), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDifference:   0.001777778 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Compare standard errors\ncat(\"\\nOLS SE:\", summary(fit_ols)$coefficients[1, 2], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nOLS SE: 0.01208305 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"WLS SE:\", summary(fit_wls)$coefficients[1, 2], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nWLS SE: 0.01199485 \n```\n\n\n:::\n:::\n\n\n**Interpretation**: The WLS estimate gives more weight to pens with more pigs (more reliable averages). The standard error is also smaller for WLS, reflecting the more efficient use of information.\n\n### Realistic Livestock Application: Feed Efficiency by Diet\n\nWe have feed conversion ratio (FCR) data from 30 pens across 3 diets. Pen sizes vary from 5 to 15 pigs. We expect variance to be inversely proportional to pen size.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load feed efficiency data\nfeed_data <- read.csv(\"data/feed_efficiency_pens.csv\")\nhead(feed_data, 10)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| pen_id|diet | pen_size|  fcr|\n|------:|:----|--------:|----:|\n|      1|A    |        7| 1.67|\n|      2|A    |        7| 1.76|\n|      3|A    |       14| 1.76|\n|      4|A    |        6| 1.59|\n|      5|A    |       10| 1.79|\n|      6|A    |       15| 1.93|\n|      7|A    |        9| 1.78|\n|      8|A    |        8| 1.60|\n|      9|A    |       10| 1.68|\n|     10|A    |       13| 1.89|\n\n</div>\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Summary by diet\nlibrary(dplyr)\nfeed_summary <- feed_data %>%\n  group_by(diet) %>%\n  summarise(\n    n_pens = n(),\n    mean_fcr = mean(fcr),\n    sd_fcr = sd(fcr),\n    mean_pen_size = mean(pen_size)\n  )\nfeed_summary\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|diet | n_pens| mean_fcr|    sd_fcr| mean_pen_size|\n|:----|------:|--------:|---------:|-------------:|\n|A    |     10|    1.745| 0.1124722|           9.9|\n|B    |     10|    1.830| 0.1484737|          11.8|\n|C    |     10|    1.984| 0.1405703|          10.7|\n\n</div>\n:::\n:::\n\n\n#### Check for Heteroscedasticity\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit OLS model\nfit_feed_ols <- lm(fcr ~ diet, data = feed_data)\n\n# Residual plot\nfeed_data$residuals <- residuals(fit_feed_ols)\nfeed_data$fitted <- fitted(fit_feed_ols)\n\nggplot(feed_data, aes(x = pen_size, y = abs(residuals))) +\n  geom_point(size = 2) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Checking for Heteroscedasticity\",\n       subtitle = \"Are residuals related to pen size?\",\n       x = \"Pen Size\",\n       y = \"|Residuals|\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](Week14_SpecialTopics2_files/figure-html/unnamed-chunk-29-1.png){width=768}\n:::\n:::\n\n\nThere's a slight negative relationship: larger pens tend to have smaller residuals (more stable averages). This suggests WLS is appropriate.\n\n#### Fit Weighted Least Squares Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# WLS model weighted by pen size\nfit_feed_wls <- lm(fcr ~ diet, data = feed_data, weights = pen_size)\n\n# Compare models\ncat(\"OLS estimates:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOLS estimates:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(coef(fit_feed_ols))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)       dietB       dietC \n      1.745       0.085       0.239 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nWLS estimates:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nWLS estimates:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(coef(fit_feed_wls))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)       dietB       dietC \n 1.77000000  0.06254237  0.20934579 \n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compare standard errors\ncat(\"\\nOLS standard errors:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nOLS standard errors:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(summary(fit_feed_ols)$coefficients[, 2])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)       dietB       dietC \n 0.04260456  0.06025194  0.06025194 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nWLS standard errors:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nWLS standard errors:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(summary(fit_feed_wls)$coefficients[, 2])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)       dietB       dietC \n 0.04497616  0.06099177  0.06240568 \n```\n\n\n:::\n:::\n\n\nThe WLS standard errors are smaller, reflecting more efficient estimation.\n\n#### ANOVA Comparison\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Test for diet effect\ncat(\"OLS ANOVA:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOLS ANOVA:\n```\n\n\n:::\n\n```{.r .cell-code}\nanova(fit_feed_ols)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|          | Df|  Sum Sq|   Mean Sq|  F value|   Pr(>F)|\n|:---------|--:|-------:|---------:|--------:|--------:|\n|diet      |  2| 0.29354| 0.1467700| 8.085841| 0.001771|\n|Residuals | 27| 0.49009| 0.0181515|       NA|       NA|\n\n</div>\n:::\n\n```{.r .cell-code}\ncat(\"\\nWLS ANOVA:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nWLS ANOVA:\n```\n\n\n:::\n\n```{.r .cell-code}\nanova(fit_feed_wls)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|          | Df|   Sum Sq|   Mean Sq|  F value|    Pr(>F)|\n|:---------|--:|--------:|---------:|--------:|---------:|\n|diet      |  2| 2.413722| 1.2068610| 6.026391| 0.0068566|\n|Residuals | 27| 5.407091| 0.2002626|       NA|        NA|\n\n</div>\n:::\n:::\n\n\nBoth models detect a significant diet effect, but the WLS F-statistic is larger (more power) due to more efficient use of information.\n\n#### Visualize Results\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate diet means (OLS vs WLS)\ndiet_means <- feed_data %>%\n  group_by(diet) %>%\n  summarise(\n    ols_mean = mean(fcr),\n    wls_mean = weighted.mean(fcr, pen_size)\n  )\n\n# Plot\nlibrary(tidyr)\ndiet_means_long <- diet_means %>%\n  pivot_longer(cols = c(ols_mean, wls_mean),\n               names_to = \"method\",\n               values_to = \"fcr\")\n\nggplot(diet_means_long, aes(x = diet, y = fcr, fill = method)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(values = c(\"ols_mean\" = \"lightblue\", \"wls_mean\" = \"darkblue\"),\n                    labels = c(\"OLS (Unweighted)\", \"WLS (Weighted)\")) +\n  labs(title = \"Feed Conversion Ratio by Diet\",\n       subtitle = \"Comparing OLS and WLS Estimates\",\n       x = \"Diet\",\n       y = \"FCR (Feed:Gain)\",\n       fill = \"Method\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](Week14_SpecialTopics2_files/figure-html/unnamed-chunk-33-1.png){width=768}\n:::\n:::\n\n\n::: {.callout-note}\n## When to Use WLS in Animal Science\n\n**Common scenarios**:\n\n1. **Pen/group averages**: Weight by group size ($w_i = n_i$)\n2. **Progeny means**: Sires with different numbers of offspring\n3. **Repeated measures**: Weight by number of measurements\n4. **Known measurement error**: Weight by inverse of measurement variance\n5. **Heterogeneous environments**: Different farms, years, or conditions\n\n**Diagnostic**: Always check residual plots! Plot residuals vs. fitted values and vs. potential variance predictors.\n:::\n\n### Summary: Weighted Least Squares\n\n**When to use**:\n\n- Error variances differ across observations (heteroscedasticity)\n- Working with averages/summaries from groups of different sizes\n- Measurements with known or estimable precision differences\n\n**Key points**:\n\n- Weights should be $w_i = 1/\\sigma_i^2$ (inverse variance)\n- WLS is BLUE under heteroscedasticity (OLS is not)\n- Always verify weights are appropriate (diagnostic plots)\n- In R: `lm(y ~ x, weights = w)`\n\n**Benefits**:\n\n- More efficient estimates (smaller standard errors)\n- Correct inference (valid hypothesis tests)\n- Properly accounts for information quality\n\n---\n\n## Regression Through the Origin: No-Intercept Models {#sec-no-intercept}\n\n### Introduction\n\nIn standard linear regression, we fit:\n\n$$\ny_i = \\beta_0 + \\beta_1 x_i + e_i\n$$\n\nThe intercept $\\beta_0$ represents the expected value of $y$ when $x=0$.\n\nBut sometimes, **theory dictates** that $y$ must be zero when $x$ is zero. For example:\n\n- Zero feed intake → zero milk production\n- Zero dose → zero response\n- Zero time → zero growth\n\nIn these cases, forcing the line through the origin makes biological sense. The model becomes:\n\n$$\ny_i = \\beta_1 x_i + e_i\n$$\n\nThis is called **regression through the origin** or a **no-intercept model**.\n\n::: {.callout-warning}\n## Use With Extreme Caution!\n\nWhile appealing in theory, no-intercept models should be used **rarely** and only when:\n\n1. There's strong theoretical justification\n2. The intercept in a standard model is very close to zero\n3. You understand the consequences for $R^2$ and residuals\n\nForcing a regression through the origin when inappropriate can badly distort the fit!\n:::\n\n### Mathematical Theory\n\n#### Model and Matrix Form\n\nThe no-intercept model is:\n\n$$\ny_i = \\beta_1 x_i + e_i, \\quad i = 1, \\ldots, n\n$$\n\nIn matrix form:\n\n$$\n\\boldsymbol{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{e}\n$$\n\nwhere:\n\n$$\n\\mathbf{X} = \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix}_{n \\times 1}, \\quad\n\\boldsymbol{\\beta} = \\begin{bmatrix}\n\\beta_1\n\\end{bmatrix}_{1 \\times 1}\n$$\n\nNote: $\\mathbf{X}$ has **no column of ones**!\n\n#### Normal Equations\n\n$$\n\\mathbf{X}'\\mathbf{X}\\boldsymbol{b} = \\mathbf{X}'\\boldsymbol{y}\n$$\n\nExpanding:\n\n$$\n\\left(\\sum_{i=1}^n x_i^2\\right) b_1 = \\sum_{i=1}^n x_i y_i\n$$\n\n::: {.callout-important}\n## Solution for No-Intercept Model\n\n$$\nb_1 = \\frac{\\sum_{i=1}^n x_i y_i}{\\sum_{i=1}^n x_i^2}\n$$\n\nCompare with the standard formula:\n\n$$\nb_1 = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2}\n$$\n\nThese are **different** unless $\\bar{x} = \\bar{y} = 0$!\n:::\n\n#### Critical Differences from Standard Regression\n\n1. **Residuals don't sum to zero**: $\\sum e_i \\neq 0$ in general\n2. **$R^2$ definition changes**: Standard $R^2 = 1 - SSE/SST$ can be negative or misleading\n3. **Different degrees of freedom**: $df = n - 1$ (only 1 parameter estimated)\n4. **Fitted line forced through (0,0)**: Can cause poor fit if intercept should be nonzero\n\n### Small Numerical Example: Milk Production vs. Feed Intake\n\nBiological constraint: If a cow eats zero feed, she produces zero milk (ignoring body tissue mobilization).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load milk-feed data\nmilk_feed <- read.csv(\"data/milk_feed.csv\")\nmilk_feed\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| feed_intake_kg| milk_production_kg|\n|--------------:|------------------:|\n|             15|                 22|\n|             20|                 29|\n|             25|                 38|\n|             30|                 44|\n|             35|                 51|\n\n</div>\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Visualize\nggplot(milk_feed, aes(x = feed_intake_kg, y = milk_production_kg)) +\n  geom_point(size = 3) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\", linewidth = 1) +\n  geom_smooth(method = \"lm\", formula = y ~ x - 1, se = FALSE,\n              color = \"red\", linewidth = 1, linetype = \"dashed\") +\n  geom_abline(intercept = 0, slope = 0, linetype = \"dotted\") +\n  expand_limits(x = 0, y = 0) +\n  labs(title = \"Milk Production vs. Feed Intake\",\n       subtitle = \"Blue: standard model | Red: no-intercept model\",\n       x = \"Feed Intake (kg/day)\",\n       y = \"Milk Production (kg/day)\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](Week14_SpecialTopics2_files/figure-html/unnamed-chunk-35-1.png){width=768}\n:::\n:::\n\n\n#### Standard Model (With Intercept)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Standard linear regression\nfit_with_int <- lm(milk_production_kg ~ feed_intake_kg, data = milk_feed)\nsummary(fit_with_int)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = milk_production_kg ~ feed_intake_kg, data = milk_feed)\n\nResiduals:\n   1    2    3    4    5 \n-0.2 -0.5  1.2 -0.1 -0.4 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     0.30000    1.30767   0.229    0.833    \nfeed_intake_kg  1.46000    0.05033  29.007    9e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7958 on 3 degrees of freedom\nMultiple R-squared:  0.9964,\tAdjusted R-squared:  0.9953 \nF-statistic: 841.4 on 1 and 3 DF,  p-value: 8.997e-05\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Manual calculation\nX_int <- cbind(1, milk_feed$feed_intake_kg)\ny <- milk_feed$milk_production_kg\n\nb_int <- solve(t(X_int) %*% X_int) %*% t(X_int) %*% y\ncat(\"Manual estimates (with intercept):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nManual estimates (with intercept):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(b_int)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]\n[1,] 0.30\n[2,] 1.46\n```\n\n\n:::\n:::\n\n\n**Interpretation**: Intercept is 1.71 kg/day. This suggests there's some \"baseline\" milk production even before accounting for feed intake (biologically questionable).\n\n#### No-Intercept Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# No-intercept regression\nfit_no_int <- lm(milk_production_kg ~ feed_intake_kg - 1, data = milk_feed)\n# or equivalently: lm(milk_production_kg ~ 0 + feed_intake_kg, data = milk_feed)\nsummary(fit_no_int)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = milk_production_kg ~ feed_intake_kg - 1, data = milk_feed)\n\nResiduals:\n       1        2        3        4        5 \n-0.06667 -0.42222  1.22222 -0.13333 -0.48889 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \nfeed_intake_kg  1.47111    0.01197   122.9 2.63e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6952 on 4 degrees of freedom\nMultiple R-squared:  0.9997,\tAdjusted R-squared:  0.9997 \nF-statistic: 1.511e+04 on 1 and 4 DF,  p-value: 2.626e-08\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Manual calculation\nX_no_int <- matrix(milk_feed$feed_intake_kg, ncol = 1)\n\nb_no_int <- solve(t(X_no_int) %*% X_no_int) %*% t(X_no_int) %*% y\ncat(\"Manual estimate (no intercept):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nManual estimate (no intercept):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(b_no_int)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         [,1]\n[1,] 1.471111\n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify formula\nb1_formula <- sum(milk_feed$feed_intake_kg * milk_feed$milk_production_kg) /\n               sum(milk_feed$feed_intake_kg^2)\ncat(\"Using formula: b₁ = Σ(xᵢyᵢ) / Σ(xᵢ²) =\", b1_formula, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nUsing formula: b₁ = Σ(xᵢyᵢ) / Σ(xᵢ²) = 1.471111 \n```\n\n\n:::\n:::\n\n\n**Interpretation**: For each kg of feed intake, milk production increases by 1.46 kg/day. The line is forced through the origin.\n\n#### Compare Residuals\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Residuals from both models\nresiduals_with <- residuals(fit_with_int)\nresiduals_no <- residuals(fit_no_int)\n\ncat(\"Residuals WITH intercept:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nResiduals WITH intercept:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(residuals_with, 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   1    2    3    4    5 \n-0.2 -0.5  1.2 -0.1 -0.4 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Sum:\", sum(residuals_with), \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSum: 0 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Residuals NO intercept:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nResiduals NO intercept:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(residuals_no, 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    1     2     3     4     5 \n-0.07 -0.42  1.22 -0.13 -0.49 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Sum:\", sum(residuals_no), \"(NOT zero!)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSum: 0.1111111 (NOT zero!)\n```\n\n\n:::\n:::\n\n\n::: {.callout-note}\n## Key Observation\n\nWith a standard intercept model, residuals always sum to zero: $\\sum e_i = 0$.\n\nFor no-intercept models, this is **not** guaranteed! Residuals can have a systematic bias.\n:::\n\n#### Compare SSE and $R^2$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sum of squared errors\nSSE_with <- sum(residuals_with^2)\nSSE_no <- sum(residuals_no^2)\n\ncat(\"SSE (with intercept):\", SSE_with, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSSE (with intercept): 1.9 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"SSE (no intercept): \", SSE_no, \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSSE (no intercept):  1.933333 \n```\n\n\n:::\n\n```{.r .cell-code}\n# R²\nR2_with <- summary(fit_with_int)$r.squared\nR2_no <- summary(fit_no_int)$r.squared\n\ncat(\"R² (with intercept):\", R2_with, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR² (with intercept): 0.9964473 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"R² (no intercept): \", R2_no, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR² (no intercept):  0.9997354 \n```\n\n\n:::\n:::\n\n\n::: {.callout-warning}\n## $R^2$ is NOT Comparable!\n\nFor no-intercept models, $R^2$ is calculated differently and can exceed 1.0 or be misleading. **Do not compare** $R^2$ values between models with and without intercept!\n\nThe definition changes because SST (total sum of squares) changes:\n\n- With intercept: $SST = \\sum(y_i - \\bar{y})^2$\n- No intercept: $SST = \\sum y_i^2$ (no mean correction)\n:::\n\n### Realistic Livestock Application: Broiler Gain vs. Feed\n\nWe have weight gain and feed intake data for 40 broilers. Does it make sense to force through the origin?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load broiler gain-feed data\nbroiler_feed <- read.csv(\"data/broiler_gain_feed.csv\")\nhead(broiler_feed, 10)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| bird_id| feed_intake_kg| weight_gain_kg|\n|-------:|--------------:|--------------:|\n|       1|           1.68|           1.02|\n|       2|           1.92|           0.99|\n|       3|           2.97|           1.71|\n|       4|           3.20|           2.11|\n|       5|           3.08|           1.99|\n|       6|           2.16|           1.58|\n|       7|           1.66|           1.01|\n|       8|           2.07|           1.30|\n|       9|           1.98|           1.51|\n|      10|           2.27|           1.31|\n\n</div>\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Visualize with both models\nggplot(broiler_feed, aes(x = feed_intake_kg, y = weight_gain_kg)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"blue\", fill = \"lightblue\") +\n  geom_smooth(method = \"lm\", formula = y ~ x - 1, se = TRUE,\n              color = \"red\", fill = \"pink\", linetype = \"dashed\") +\n  expand_limits(x = 0, y = 0) +\n  labs(title = \"Broiler Weight Gain vs. Feed Intake (n=40)\",\n       subtitle = \"Blue: standard model | Red: no-intercept model\",\n       x = \"Feed Intake (kg)\",\n       y = \"Weight Gain (kg)\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](Week14_SpecialTopics2_files/figure-html/unnamed-chunk-43-1.png){width=768}\n:::\n:::\n\n\n#### Fit Both Models\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Standard model\nfit_broiler_std <- lm(weight_gain_kg ~ feed_intake_kg, data = broiler_feed)\n\n# No-intercept model\nfit_broiler_no <- lm(weight_gain_kg ~ feed_intake_kg - 1, data = broiler_feed)\n\n# Compare coefficients\ncat(\"Standard model:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStandard model:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(coef(fit_broiler_std))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   (Intercept) feed_intake_kg \n    -0.1270924      0.7005248 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nNo-intercept model:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nNo-intercept model:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(coef(fit_broiler_no))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nfeed_intake_kg \n     0.6548412 \n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Is the intercept significantly different from zero?\ncat(\"Standard model summary:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStandard model summary:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(summary(fit_broiler_std)$coefficients)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                 Estimate Std. Error   t value     Pr(>|t|)\n(Intercept)    -0.1270924 0.09810467 -1.295478 2.029702e-01\nfeed_intake_kg  0.7005248 0.03610774 19.400958 2.593917e-21\n```\n\n\n:::\n:::\n\n\n**Interpretation**: The intercept is -0.127 with SE = 0.098. The t-test ($p$ = 0.203) shows the intercept is **not significantly different from zero**.\n\nThis suggests the no-intercept model might be reasonable here.\n\n#### Diagnostic Comparison\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Residual plots side by side\npar(mfrow = c(1, 2))\n\n# Standard model\nplot(fitted(fit_broiler_std), residuals(fit_broiler_std),\n     main = \"Standard Model Residuals\",\n     xlab = \"Fitted Values\", ylab = \"Residuals\")\nabline(h = 0, col = \"red\", lty = 2)\n\n# No-intercept model\nplot(fitted(fit_broiler_no), residuals(fit_broiler_no),\n     main = \"No-Intercept Model Residuals\",\n     xlab = \"Fitted Values\", ylab = \"Residuals\")\nabline(h = 0, col = \"red\", lty = 2)\n```\n\n::: {.cell-output-display}\n![](Week14_SpecialTopics2_files/figure-html/unnamed-chunk-46-1.png){width=960}\n:::\n:::\n\n\nBoth residual plots look reasonable—no strong patterns or heteroscedasticity.\n\n::: {.callout-tip}\n## Decision Framework: Should You Use No-Intercept?\n\nAsk these questions:\n\n1. **Theory**: Does $y=0$ when $x=0$ make biological sense?\n   - Yes: Feed → production relationships (with caveats)\n   - No: Most growth curves, many dose-response curves\n\n2. **Data**: Is the intercept close to zero in a standard model?\n   - If intercept ≈ 0 and not significant → no-intercept may be OK\n   - If intercept significantly different from 0 → keep it!\n\n3. **Fit**: Do diagnostic plots look similar for both models?\n   - Check residual patterns\n   - Check influential observations\n\n4. **Context**: What's the consequence of being wrong?\n   - If forcing through origin distorts predictions → don't do it\n   - If it's theoretically necessary → justify carefully\n\n**General advice**: Start with a standard model. Only remove the intercept if there's strong justification.\n:::\n\n### Summary: Regression Through the Origin\n\n**When to consider**:\n\n- Strong theoretical reason that $y=0$ when $x=0$\n- Intercept in standard model is near zero and not significant\n- Used in specific applications (calibration, certain physical models)\n\n**Critical warnings**:\n\n- $R^2$ values not comparable between models\n- Residuals don't sum to zero\n- Can badly distort fit if intercept should be nonzero\n- **Use sparingly and justify carefully!**\n\n**R syntax**:\n\n- `lm(y ~ x - 1)` or `lm(y ~ 0 + x)`\n\n---\n\n## Preview of Mixed Models: The Bridge to BLUP {#sec-mixed-models}\n\n### Introduction: Fixed Effects vs. Random Effects\n\nThroughout this entire course, we've been fitting **fixed effects models**. The parameters $\\beta_0, \\beta_1, \\ldots, \\beta_p$ are treated as **fixed unknown constants** that we estimate from data.\n\nBut in animal breeding and genetics, we often encounter **random effects**:\n\n- **Animal genetic effects**: Breeding values for thousands of animals\n- **Sire/dam effects**: Parental contributions to offspring performance\n- **Herd/year effects**: Environmental factors sampled from a population\n- **Litter/pen effects**: Group-level variation\n\nRandom effects are **parameters treated as random variables** with a distribution. Instead of estimating a fixed value for each animal, we estimate:\n\n1. The **variance** of breeding values in the population\n2. **Predictions** of individual breeding values (BLUPs) that \"borrow strength\" from relatives\n\nThis is the foundation of **genetic evaluation**, **Estimated Progeny Differences (EPDs)**, and **Genomic Selection**.\n\n::: {.callout-note}\n## Why This Matters for Animal Breeding\n\nIn dairy cattle genetic evaluation:\n\n- Fixed effects: herd, year-season, age, stage of lactation\n- Random effects: animal breeding values (10,000s of animals!)\n\nTreating animal effects as fixed would:\n\n- Require estimating 10,000+ parameters\n- Give poor estimates for animals with little data\n- Not account for family relationships (pedigree)\n\nTreating them as random:\n\n- Estimates 2 parameters: additive genetic variance and residual variance\n- Provides BLUPs that \"shrink\" toward the mean for animals with little data\n- Incorporates pedigree relationships (relatedness matrix)\n\nThis is why **BLUP (Best Linear Unbiased Prediction)** revolutionized animal breeding in the 1970s-1980s.\n:::\n\n### The Mixed Model Framework\n\nThe general **linear mixed model** is:\n\n$$\n\\boldsymbol{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{Z}\\boldsymbol{u} + \\boldsymbol{e}\n$$\n\nwhere:\n\n- $\\boldsymbol{y}$: vector of observations (n × 1)\n- $\\mathbf{X}$: design matrix for **fixed effects** (n × p)\n- $\\boldsymbol{\\beta}$: vector of **fixed effects** parameters (p × 1)\n- $\\mathbf{Z}$: incidence matrix for **random effects** (n × q)\n- $\\boldsymbol{u}$: vector of **random effects** (q × 1), with $\\boldsymbol{u} \\sim N(\\mathbf{0}, \\mathbf{G})$\n- $\\boldsymbol{e}$: vector of residual errors (n × 1), with $\\boldsymbol{e} \\sim N(\\mathbf{0}, \\mathbf{R})$\n\n**Key assumptions**:\n\n- $E(\\boldsymbol{u}) = \\mathbf{0}$ (random effects are zero on average)\n- $Var(\\boldsymbol{u}) = \\mathbf{G}$ (variance-covariance matrix for random effects)\n- $Var(\\boldsymbol{e}) = \\mathbf{R}$ (variance-covariance matrix for residuals)\n- $\\boldsymbol{u}$ and $\\boldsymbol{e}$ are independent\n\n::: {.callout-important icon=true}\n## NOTATION EXTENSION\n\nThis is the first time in the course we introduce **random effects notation**:\n\n- **$\\boldsymbol{u}$**: vector of random effects (lowercase bold)\n- **$\\mathbf{Z}$**: incidence matrix for random effects (uppercase bold)\n- **$\\mathbf{G}$**: variance-covariance matrix for random effects (uppercase bold)\n- **$\\mathbf{R}$**: variance-covariance matrix for residuals (uppercase bold)\n\nCompare with what we've used all course:\n\n- **$\\boldsymbol{\\beta}$**: vector of **fixed** effects (known to exist, unknown value)\n- **$\\mathbf{X}$**: design matrix for **fixed** effects\n- $\\sigma^2\\mathbf{I}$: simple residual variance structure (now generalized to $\\mathbf{R}$)\n:::\n\n### Henderson's Mixed Model Equations (MME)\n\nIn 1949, Charles Henderson derived the equations that simultaneously solve for fixed effects ($\\boldsymbol{\\beta}$) and predict random effects ($\\boldsymbol{u}$).\n\nFor the general case:\n\n$$\n\\begin{bmatrix}\n\\mathbf{X}'\\mathbf{R}^{-1}\\mathbf{X} & \\mathbf{X}'\\mathbf{R}^{-1}\\mathbf{Z} \\\\\n\\mathbf{Z}'\\mathbf{R}^{-1}\\mathbf{X} & \\mathbf{Z}'\\mathbf{R}^{-1}\\mathbf{Z} + \\mathbf{G}^{-1}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\hat{\\boldsymbol{\\beta}} \\\\\n\\hat{\\boldsymbol{u}}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\mathbf{X}'\\mathbf{R}^{-1}\\boldsymbol{y} \\\\\n\\mathbf{Z}'\\mathbf{R}^{-1}\\boldsymbol{y}\n\\end{bmatrix}\n$$\n\nThis looks intimidating! But notice the similarity to our familiar normal equations:\n\n$$\n\\mathbf{X}'\\mathbf{X}\\boldsymbol{b} = \\mathbf{X}'\\boldsymbol{y}\n$$\n\nThe MME are an **augmented** version that includes:\n\n- Equations for fixed effects (top row)\n- Equations for random effects (bottom row)\n- A \"penalty\" term $\\mathbf{G}^{-1}$ that shrinks random effects toward zero\n\n**Simplified Case**: When $\\mathbf{R} = \\sigma^2\\mathbf{I}$ and $\\mathbf{G} = \\sigma_u^2\\mathbf{I}$, the MME simplify to:\n\n$$\n\\begin{bmatrix}\n\\mathbf{X}'\\mathbf{X} & \\mathbf{X}'\\mathbf{Z} \\\\\n\\mathbf{Z}'\\mathbf{X} & \\mathbf{Z}'\\mathbf{Z} + \\lambda\\mathbf{I}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\hat{\\boldsymbol{\\beta}} \\\\\n\\hat{\\boldsymbol{u}}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\mathbf{X}'\\boldsymbol{y} \\\\\n\\mathbf{Z}'\\boldsymbol{y}\n\\end{bmatrix}\n$$\n\nwhere $\\lambda = \\sigma^2 / \\sigma_u^2$ is the **variance ratio**.\n\n::: {.callout-note}\n## Interpreting $\\lambda$\n\nThe variance ratio $\\lambda = \\sigma^2 / \\sigma_u^2$ controls the amount of shrinkage:\n\n- **Large $\\lambda$** (residual variance >> random effect variance): Heavy shrinkage toward zero\n- **Small $\\lambda$** (random effect variance large): Little shrinkage, random effects estimates closer to fixed effects estimates\n\nIn animal breeding: $\\lambda$ relates to **heritability**!\n\n$$\nh^2 = \\frac{\\sigma_u^2}{\\sigma_u^2 + \\sigma^2}\n$$\n\nHigh heritability → small $\\lambda$ → less shrinkage (genetics matters more than noise).\n:::\n\n### Properties of Henderson's Solutions\n\nSolving the MME gives:\n\n1. **$\\hat{\\boldsymbol{\\beta}}$**: BLUE (Best Linear Unbiased Estimator) of fixed effects\n2. **$\\hat{\\boldsymbol{u}}$**: BLUP (Best Linear Unbiased Predictor) of random effects\n\n**BLUP properties**:\n\n- **Best**: Minimum mean squared error (MSE) among linear predictors\n- **Linear**: Linear function of observations $\\boldsymbol{y}$\n- **Unbiased**: $E(\\hat{\\boldsymbol{u}} - \\boldsymbol{u}) = \\mathbf{0}$\n- **Predictor** (not \"estimator\"): We're predicting realized values of random variables\n\n::: {.callout-important}\n## Connection to This Course\n\nEverything you've learned applies to mixed models:\n\n- The $\\mathbf{X}'\\mathbf{X}$ and $\\mathbf{X}'\\boldsymbol{y}$ terms are exactly what we've been computing!\n- Matrix algebra: transpose, multiplication, inversion—all the same\n- Rank deficiency, generalized inverses, estimable functions—all relevant\n- Residuals, SSE, diagnostics—all extend naturally\n\n**The only addition**: An extra set of equations for random effects with a shrinkage penalty $\\mathbf{G}^{-1}$.\n\nThis course has given you the foundation to understand and implement mixed models!\n:::\n\n### Small Numerical Example: Simple Sire Model\n\nLet's fit a simple sire model where we compare treating sire as fixed vs. random.\n\n**Data**: 5 sires with 3-5 daughters each (n=20 total cows).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load sire data\nsire_data <- read.csv(\"data/simple_sire_model.csv\")\nsire_data\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| cow_id|sire  | milk_yield|\n|------:|:-----|----------:|\n|      1|Sire1 |       84.1|\n|      2|Sire1 |       83.2|\n|      3|Sire1 |       85.7|\n|      4|Sire1 |       81.5|\n|      5|Sire1 |       90.3|\n|      6|Sire2 |       76.2|\n|      7|Sire2 |       73.2|\n|      8|Sire2 |       77.3|\n|      9|Sire2 |       75.8|\n|     10|Sire3 |       89.6|\n|     11|Sire3 |       81.1|\n|     12|Sire3 |       87.1|\n|     13|Sire4 |       82.5|\n|     14|Sire4 |       86.6|\n|     15|Sire4 |       81.7|\n|     16|Sire4 |       82.0|\n|     17|Sire5 |       87.3|\n|     18|Sire5 |       79.6|\n|     19|Sire5 |       85.8|\n|     20|Sire5 |       76.8|\n\n</div>\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Summary by sire\nsire_summary <- sire_data %>%\n  group_by(sire) %>%\n  summarise(\n    n_daughters = n(),\n    mean_yield = mean(milk_yield),\n    sd_yield = sd(milk_yield)\n  )\nsire_summary\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|sire  | n_daughters| mean_yield| sd_yield|\n|:-----|-----------:|----------:|--------:|\n|Sire1 |           5|   84.96000| 3.349328|\n|Sire2 |           4|   75.62500| 1.736616|\n|Sire3 |           3|   85.93333| 4.368448|\n|Sire4 |           4|   83.20000| 2.290560|\n|Sire5 |           4|   82.37500| 4.992244|\n\n</div>\n:::\n:::\n\n\n#### Fixed Effects Model (What We've Been Doing)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Treat sire as fixed effect\nfit_sire_fixed <- lm(milk_yield ~ sire, data = sire_data)\nsummary(fit_sire_fixed)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = milk_yield ~ sire, data = sire_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.5750 -1.9263 -0.2625  2.1062  5.3400 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  84.9600     1.5603  54.451  < 2e-16 ***\nsireSire2    -9.3350     2.3405  -3.989  0.00119 ** \nsireSire3     0.9733     2.5480   0.382  0.70782    \nsireSire4    -1.7600     2.3405  -0.752  0.46370    \nsireSire5    -2.5850     2.3405  -1.104  0.28679    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.489 on 15 degrees of freedom\nMultiple R-squared:  0.584,\tAdjusted R-squared:  0.4731 \nF-statistic: 5.265 on 4 and 15 DF,  p-value: 0.007477\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get all sire estimates (using cell means model for clarity)\nfit_sire_fixed_means <- lm(milk_yield ~ sire - 1, data = sire_data)\nsire_fixed_estimates <- coef(fit_sire_fixed_means)\nnames(sire_fixed_estimates) <- levels(factor(sire_data$sire))\nsire_fixed_estimates\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Sire1    Sire2    Sire3    Sire4    Sire5 \n84.96000 75.62500 85.93333 83.20000 82.37500 \n```\n\n\n:::\n:::\n\n\nThese are the **daughter averages** for each sire—no shrinkage, no borrowing of strength.\n\n#### Random Effects Model (Mixed Model)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Treat sire as random effect\nlibrary(lme4)\nfit_sire_random <- lmer(milk_yield ~ 1 + (1|sire), data = sire_data)\nsummary(fit_sire_random)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear mixed model fit by REML ['lmerMod']\nFormula: milk_yield ~ 1 + (1 | sire)\n   Data: sire_data\n\nREML criterion at convergence: 111.1\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.5986 -0.4914 -0.1783  0.6746  1.6445 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n sire     (Intercept) 13.19    3.632   \n Residual             12.18    3.490   \nNumber of obs: 20, groups:  sire, 5\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   82.397      1.806   45.63\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Variance components\nvc <- as.data.frame(VarCorr(fit_sire_random))\nsigma_u <- vc$sdcor[1]  # SD of sire effects\nsigma_e <- vc$sdcor[2]  # Residual SD\n\ncat(\"Random effect SD (sire):\", sigma_u, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRandom effect SD (sire): 3.632048 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Residual SD:            \", sigma_e, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nResidual SD:             3.490063 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Variance ratio (λ):     \", sigma_e^2 / sigma_u^2, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nVariance ratio (λ):      0.9233437 \n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# BLUPs (Best Linear Unbiased Predictions)\nsire_blups <- ranef(fit_sire_random)$sire[,1]\noverall_mean <- fixef(fit_sire_random)\nsire_random_predictions <- overall_mean + sire_blups\n\nnames(sire_random_predictions) <- rownames(ranef(fit_sire_random)$sire)\nsire_random_predictions\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Sire1    Sire2    Sire3    Sire4    Sire5 \n84.56048 76.89505 85.10108 83.04941 82.37913 \n```\n\n\n:::\n:::\n\n\n#### Compare Fixed vs. Random Estimates\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create comparison data frame\nsire_comparison <- data.frame(\n  sire = names(sire_fixed_estimates),\n  n_daughters = sire_summary$n_daughters,\n  fixed_effect = as.numeric(sire_fixed_estimates),\n  random_effect = as.numeric(sire_random_predictions)\n)\n\n# Plot comparison\nggplot(sire_comparison, aes(x = sire)) +\n  geom_point(aes(y = fixed_effect, color = \"Fixed Effects\"), size = 3) +\n  geom_point(aes(y = random_effect, color = \"Random Effects (BLUP)\"), size = 3) +\n  geom_hline(yintercept = overall_mean, linetype = \"dashed\", color = \"gray50\") +\n  geom_text(aes(x = 1.5, y = overall_mean + 1.5,\n                label = paste(\"Overall mean =\", round(overall_mean, 1))),\n            color = \"gray30\") +\n  scale_color_manual(values = c(\"Fixed Effects\" = \"blue\",\n                                  \"Random Effects (BLUP)\" = \"red\")) +\n  labs(title = \"Comparing Fixed Effects vs. Random Effects (BLUP)\",\n       subtitle = \"Notice shrinkage toward the population mean\",\n       x = \"Sire\",\n       y = \"Predicted Milk Yield (kg/day)\",\n       color = \"Method\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![](Week14_SpecialTopics2_files/figure-html/unnamed-chunk-54-1.png){width=768}\n:::\n:::\n\n\n::: {.callout-note}\n## Observing Shrinkage\n\nNotice how the **BLUP estimates (red) are pulled toward the population mean** compared to fixed effects (blue). This shrinkage is:\n\n- **Larger** for sires with **fewer daughters** (less information → more shrinkage)\n- **Smaller** for sires with **more daughters** (more information → less shrinkage)\n\nThis is **\"borrowing strength\"**: Using information about the population distribution to improve predictions for individuals with sparse data.\n\nThis is exactly what happens in genetic evaluation: young bulls with few daughters get more shrinkage than proven bulls with many daughters.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Quantify shrinkage\nsire_comparison$shrinkage <- sire_comparison$fixed_effect - sire_comparison$random_effect\nsire_comparison$shrinkage_pct <- (sire_comparison$shrinkage /\n                                   (sire_comparison$fixed_effect - overall_mean)) * 100\n\ncat(\"Shrinkage by sire:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nShrinkage by sire:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(sire_comparison[, c(\"sire\", \"n_daughters\", \"shrinkage\", \"shrinkage_pct\")])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   sire n_daughters    shrinkage shrinkage_pct\n1 Sire1           5  0.399521335      15.58822\n2 Sire2           4 -1.270053742      18.75440\n3 Sire3           3  0.832255245      23.53461\n4 Sire4           4  0.150592229      18.75440\n5 Sire5           4 -0.004131589      18.75440\n```\n\n\n:::\n:::\n\n\nSires with fewer daughters experience more shrinkage—exactly as BLUP theory predicts!\n\n### Realistic Livestock Application: Dairy Sire Evaluation\n\nNow let's examine a more realistic scenario: 10 sires with varying numbers of daughters (3 to 25 per sire, n=100 total).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load dairy sire evaluation data\ndairy_sire <- read.csv(\"data/dairy_sire_eval.csv\")\n\n# Summary by sire\ndairy_summary <- dairy_sire %>%\n  group_by(sire) %>%\n  summarise(\n    n_daughters = n(),\n    mean_yield = mean(milk_yield),\n    sd_yield = sd(milk_yield)\n  ) %>%\n  arrange(n_daughters)\n\ndairy_summary\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|sire   | n_daughters| mean_yield| sd_yield|\n|:------|-----------:|----------:|--------:|\n|Sire01 |           3|  100.83333| 4.244212|\n|Sire02 |           5|   88.78000| 8.473901|\n|Sire10 |           6|   87.60000| 4.616059|\n|Sire05 |           7|   89.25714| 4.340836|\n|Sire03 |           8|   89.71250| 4.508544|\n|Sire09 |           9|   93.10000| 6.859847|\n|Sire08 |          10|   86.75000| 4.714811|\n|Sire04 |          12|   87.81667| 7.718317|\n|Sire07 |          15|   93.82667| 4.175005|\n|Sire06 |          25|   91.17600| 6.043749|\n\n</div>\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fixed effects model\nfit_dairy_fixed <- lm(milk_yield ~ sire - 1, data = dairy_sire)\ndairy_fixed_est <- coef(fit_dairy_fixed)\n\n# Random effects model\nfit_dairy_random <- lmer(milk_yield ~ 1 + (1|sire), data = dairy_sire)\n\n# Extract estimates\ndairy_overall_mean <- fixef(fit_dairy_random)\ndairy_blups <- ranef(fit_dairy_random)$sire[,1]\ndairy_random_pred <- dairy_overall_mean + dairy_blups\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Variance components\ndairy_vc <- as.data.frame(VarCorr(fit_dairy_random))\ncat(\"Sire variance (σ²ᵤ):\", round(dairy_vc$vcov[1], 2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSire variance (σ²ᵤ): 6.83 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Residual variance (σ²):\", round(dairy_vc$vcov[2], 2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nResidual variance (σ²): 34.48 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Variance ratio (λ):\", round(dairy_vc$vcov[2] / dairy_vc$vcov[1], 2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nVariance ratio (λ): 5.05 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Approximate heritability (sire model gives 1/4 h²)\nh2_approx <- 4 * dairy_vc$vcov[1] / (dairy_vc$vcov[1] + dairy_vc$vcov[2])\ncat(\"Approximate heritability:\", round(h2_approx, 2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nApproximate heritability: 0.66 \n```\n\n\n:::\n:::\n\n\n#### Visualize Shrinkage by Information Level\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Comparison dataframe\ndairy_comp <- data.frame(\n  sire = levels(factor(dairy_sire$sire)),\n  n_daughters = dairy_summary$n_daughters,\n  fixed = as.numeric(dairy_fixed_est),\n  blup = as.numeric(dairy_random_pred)\n)\n\ndairy_comp$shrinkage <- abs(dairy_comp$fixed - dairy_comp$blup)\n\n# Plot: shrinkage vs. information\nggplot(dairy_comp, aes(x = n_daughters, y = shrinkage)) +\n  geom_point(size = 3, color = \"darkred\") +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"blue\") +\n  labs(title = \"Shrinkage Increases with Less Information\",\n       subtitle = \"Sires with fewer daughters experience more shrinkage toward population mean\",\n       x = \"Number of Daughters\",\n       y = \"Absolute Shrinkage (Fixed - BLUP)\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](Week14_SpecialTopics2_files/figure-html/unnamed-chunk-59-1.png){width=960}\n:::\n:::\n\n\nPerfect! As predicted by theory, shrinkage is **inversely related** to information: sires with fewer daughters get pulled more strongly toward the population mean.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot fixed vs. BLUP estimates\ndairy_comp_long <- dairy_comp %>%\n  select(sire, n_daughters, fixed, blup) %>%\n  pivot_longer(cols = c(fixed, blup), names_to = \"method\", values_to = \"estimate\")\n\nggplot(dairy_comp_long, aes(x = reorder(sire, n_daughters), y = estimate,\n                             color = method, group = method)) +\n  geom_point(size = 3) +\n  geom_line(aes(group = sire), color = \"gray70\") +\n  geom_hline(yintercept = dairy_overall_mean, linetype = \"dashed\", color = \"black\") +\n  scale_color_manual(values = c(\"fixed\" = \"blue\", \"blup\" = \"red\"),\n                     labels = c(\"Fixed Effects\", \"BLUP\")) +\n  labs(title = \"Fixed Effects vs. BLUP for 10 Dairy Sires\",\n       subtitle = \"Ordered by number of daughters (left = few, right = many)\",\n       x = \"Sire (ordered by information)\",\n       y = \"Predicted Milk Yield (kg/day)\",\n       color = \"Method\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![](Week14_SpecialTopics2_files/figure-html/unnamed-chunk-60-1.png){width=960}\n:::\n:::\n\n\n::: {.callout-important}\n## Why BLUP Matters in Animal Breeding\n\nThis demonstrates the power of BLUP for genetic evaluation:\n\n1. **Young sires** (few daughters): Fixed effects unreliable, BLUP provides \"regularized\" estimates\n2. **Proven sires** (many daughters): Fixed and BLUP estimates converge (data speaks for itself)\n3. **Variance components**: Estimate 2 parameters (σ²ᵤ, σ²) instead of 10 sire effects\n4. **Heritability**: Can estimate from variance components\n5. **Selection decisions**: BLUP provides more accurate sire rankings\n\n**In practice**:\n\n- Add pedigree: $\\mathbf{G} = \\mathbf{A}\\sigma_u^2$ (relationship matrix)\n- Add genomics: $\\mathbf{G} = \\mathbf{G}_{genomic}$ (marker-based relationships)\n- Multiple traits, correlated effects, heterogeneous variances...\n\nAll build on the foundation you've learned in this course!\n:::\n\n### Connection to Estimated Progeny Differences (EPDs)\n\nIn livestock breeding, **EPDs (Estimated Progeny Differences)** are essentially BLUPs:\n\n- EPD for a sire = 2 × (sire BLUP)\n- Represents expected difference in offspring performance\n- Accounts for number of progeny (reliability)\n- Incorporates pedigree relationships\n- Regresses toward breed average for unproven sires\n\nThe EPDs you see in bull catalogs, sow selection indexes, and chicken breeding programs all come from mixed model equations—direct descendants of what we've studied this week!\n\n### Summary: Mixed Models Preview\n\n**Key concepts**:\n\n- **Fixed effects**: Parameters with fixed unknown values (this entire course)\n- **Random effects**: Parameters as random variables with a distribution\n- **Mixed models**: Combine fixed and random effects in one framework\n- **Henderson's MME**: Augmented normal equations that solve for both\n- **BLUP**: Best Linear Unbiased Prediction—optimal predictor of random effects\n- **Shrinkage**: Random effects pulled toward population mean (more shrinkage = less information)\n\n**Why it matters**:\n\n- Foundation of modern genetic evaluation\n- Handles large numbers of effects efficiently\n- Accounts for relationships (pedigree, genomics)\n- Provides accuracy measures (reliability, repeatability)\n\n**What you're ready for**:\n\n- Advanced mixed models course\n- Quantitative genetics\n- Genomic selection methods\n- Variance component estimation\n- BLUP implementation\n\nThis course has prepared you with the matrix algebra, least squares theory, and linear model framework to understand and apply these advanced methods!\n\n---\n\n## Exercises {#sec-exercises}\n\n### Exercise 1: Polynomial Regression (Hand Calculation)\n\nYou have the following data on pig weight (kg) vs. age (weeks):\n\n| Age (weeks) | Weight (kg) |\n|-------------|-------------|\n| 4           | 8           |\n| 8           | 22          |\n| 12          | 45          |\n\n**(a)** Fit a quadratic model $y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + e$ by hand. Construct the $\\mathbf{X}$ matrix, compute $\\mathbf{X}'\\mathbf{X}$ and $\\mathbf{X}'\\boldsymbol{y}$, and solve for $\\boldsymbol{b}$.\n\n**(b)** Predict the weight at age 16 weeks.\n\n**(c)** Does the quadratic term ($\\beta_2$) appear necessary based on the data pattern? Explain.\n\n---\n\n### Exercise 2: Weighted Least Squares (Pen Averages)\n\nYou have average daily gain (ADG, kg/day) data from 4 pens:\n\n| Pen | ADG  | Pen Size |\n|-----|------|----------|\n| 1   | 0.90 | 10       |\n| 2   | 0.88 | 5        |\n| 3   | 0.93 | 15       |\n| 4   | 0.85 | 8        |\n\n**(a)** Calculate the unweighted mean ADG (OLS estimate).\n\n**(b)** Calculate the weighted mean ADG (WLS estimate) using weights $w_i = n_i$ (pen size).\n\n**(c)** Which estimate is more reliable and why?\n\n---\n\n### Exercise 3: Regression Through the Origin (Theoretical)\n\nConsider the no-intercept model $y_i = \\beta_1 x_i + e_i$.\n\n**(a)** Prove that the residuals do not necessarily sum to zero: $\\sum e_i \\neq 0$ in general.\n\n**(b)** Show that the fitted values $\\hat{y}_i = b_1 x_i$ and observed values $y_i$ do not necessarily have the same mean: $\\bar{\\hat{y}} \\neq \\bar{y}$ in general.\n\n**(c)** Explain why this property makes no-intercept models problematic if the true intercept is nonzero.\n\n---\n\n### Exercise 4: Polynomial Model Selection (Applied)\n\nUsing the lactation curve dataset (`lactation_curve.csv`):\n\n**(a)** Fit linear, quadratic, and cubic models for milk yield vs. days in milk.\n\n**(b)** Use `anova()` to test whether the quadratic term is needed. Report the F-statistic and p-value.\n\n**(c)** Use `anova()` to test whether the cubic term (given quadratic is included) is needed.\n\n**(d)** Compare adjusted R² for all three models. Which model would you recommend and why?\n\n**(e)** Calculate VIF for the quadratic model with raw polynomials vs. orthogonal polynomials. Comment on collinearity.\n\n---\n\n### Exercise 5: Weighted Least Squares (Applied)\n\nUsing the feed efficiency dataset (`feed_efficiency_pens.csv`):\n\n**(a)** Fit an OLS model: `fcr ~ diet` (unweighted).\n\n**(b)** Fit a WLS model: `fcr ~ diet` weighted by `pen_size`.\n\n**(c)** Create a residual plot for the OLS model: plot |residuals| vs. pen size. Is there evidence of heteroscedasticity?\n\n**(d)** Compare standard errors for diet effects between OLS and WLS. Which are smaller and why?\n\n**(e)** Test the hypothesis $H_0$: no diet effect using both models. Do your conclusions differ?\n\n---\n\n### Exercise 6: No-Intercept Model (Applied)\n\nUsing the broiler gain-feed dataset (`broiler_gain_feed.csv`):\n\n**(a)** Fit a standard linear model: `weight_gain_kg ~ feed_intake_kg`.\n\n**(b)** Test whether the intercept is significantly different from zero.\n\n**(c)** Fit a no-intercept model: `weight_gain_kg ~ feed_intake_kg - 1`.\n\n**(d)** Compare predictions at feed intake = 2.5 kg for both models.\n\n**(e)** Plot both fitted lines on the same graph (extending to the origin). Which model seems more biologically plausible?\n\n---\n\n### Exercise 7: Mixed Models (Conceptual)\n\nConsider a sheep breeding program with 20 rams, each with 5-15 offspring tested for weaning weight.\n\n**(a)** Write out the mixed model equation for this scenario. Define all components ($\\boldsymbol{y}$, $\\mathbf{X}$, $\\boldsymbol{\\beta}$, $\\mathbf{Z}$, $\\boldsymbol{u}$, $\\boldsymbol{e}$).\n\n**(b)** What are the fixed effects and what are the random effects?\n\n**(c)** Explain conceptually why treating ram as a random effect is preferable to treating it as a fixed effect.\n\n**(d)** If a ram has only 5 offspring while most have 15, how will the BLUP for this ram differ from the fixed effect estimate? Explain the shrinkage concept.\n\n**(e)** If the estimated variance components are $\\sigma_u^2 = 4$ kg² and $\\sigma^2 = 16$ kg², calculate the variance ratio $\\lambda$ and approximate heritability $h^2$.\n\n---\n\n## Summary and Looking Ahead {#sec-summary}\n\nThis week, we explored four powerful extensions of the linear model framework:\n\n**Polynomial Regression** enables us to model curved relationships common in animal science—growth curves, lactation curves, and response surfaces—while maintaining the linear model structure. Key takeaways: start with low degrees, use orthogonal polynomials to combat collinearity, and always prioritize biological plausibility over statistical fit.\n\n**Weighted Least Squares** addresses heteroscedasticity by optimally weighting observations according to their precision. This is essential when working with grouped data, pen averages, or measurements with varying reliability. WLS provides more efficient estimates and valid inference when variances differ across observations.\n\n**Regression Through the Origin** applies in special cases where theory dictates $y=0$ when $x=0$. While appealing, it requires careful justification and comes with important caveats about residual properties and $R^2$ interpretation. Use sparingly!\n\n**Mixed Models** bridge the gap between this course and advanced genetic evaluation methods. By introducing random effects, we can efficiently handle large numbers of effects, account for population structure, and preview BLUP—the foundation of modern animal breeding. Understanding the connection between the normal equations and Henderson's MME shows how everything we've learned naturally extends to this powerful framework.\n\n### Skills You've Gained\n\nAfter completing this week, you can:\n\n- Fit and interpret polynomial models for nonlinear biological relationships\n- Apply weighted least squares when error variances are heterogeneous\n- Understand when (and when not) to use no-intercept models\n- Explain the distinction between fixed and random effects\n- Recognize the structure of mixed model equations and their connection to least squares\n- Understand conceptually how BLUP works and why it matters for breeding\n\n### Connection to Week 15\n\nNext week, the **Capstone Project** integrates all concepts from Weeks 1-14. You'll:\n\n- Analyze a comprehensive multi-factor livestock dataset\n- Build design matrices for complex models\n- Handle rank deficiency and unbalanced data\n- Conduct hypothesis tests with contrasts\n- Perform full diagnostic analysis\n- Build your own least squares solver package\n- (Optional) Preview BLUP application with pedigree\n\nEverything comes together: matrix algebra, least squares theory, ANOVA, regression, diagnostics, and now these special topics. You're ready!\n\n---\n\n**Previous**: [Week 13: Special Topics I](../Week13_SpecialTopics1/Week13_SpecialTopics1.qmd)\n\n**Next**: [Week 15: Capstone Project](../Week15_Capstone/Week15_Capstone.qmd)\n",
    "supporting": [
      "Week14_SpecialTopics2_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}