{
  "hash": "6cebea858b87ba234ceca42fc1e79856",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Week 4: Simple Linear Regression\"\nformat:\n  html:\n    toc: true\n    toc-depth: 3\n    number-sections: true\n    code-fold: false\n    code-tools: true\n---\n\n::: {.callout-note icon=false}\n## Learning Objectives\n\nBy the end of this week, you will be able to:\n\n1. **Derive** **least squares estimates** for **simple linear regression** from first principles\n2. **Interpret** **slope** and **intercept parameters** in biological context\n3. **Make predictions** and compute **residuals** for new observations\n4. **Understand** the geometry of **least squares** and the concept of **\"best fit\"**\n:::\n\n## Why Simple Linear Regression Matters\n\nSimple linear regression is one of the most fundamental and widely used statistical methods in animal breeding and genetics. It allows us to quantify relationships between two continuous variables and make predictions.\n\n**Common applications in livestock science:**\n\n- **Growth curves**: Predicting animal weight from age (broilers, pigs, beef cattle)\n- **Feed efficiency**: Relating feed intake to weight gain\n- **Lactation curves**: Modeling milk yield over days in milk (dairy cattle)\n- **Carcass traits**: Predicting carcass weight from live weight\n- **Economic traits**: Relating input costs to productivity measures\n\n::: {.callout-note}\n## Connection to Previous Weeks\n\nIn [Week 3](#sec-week03), we learned how to build design matrices for different types of predictors. Simple linear regression uses a design matrix with just two columns: one for the intercept (all 1s) and one for our predictor variable. This week, we'll learn how to estimate the parameters in this model and interpret the results.\n:::\n\n::: {.callout-tip}\n## Why Learn Regression from First Principles?\n\nUnderstanding how to derive regression estimates manually gives you:\n\n- Deep insight into what statistical software is actually doing\n- Ability to build custom solvers for specialized problems\n- Foundation for understanding more complex models (multiple regression, ANOVA, mixed models)\n- Confidence to troubleshoot when results seem unexpected\n:::\n\n## The Simple Linear Regression Model\n\n### Model Specification\n\nIn simple linear regression, we model the relationship between a **response variable** $y$ and a **predictor variable** $x$ using a straight line:\n\n$$\ny_i = \\beta_0 + \\beta_1 x_i + e_i, \\quad i = 1, 2, \\ldots, n\n$$ {#eq-simple-regression}\n\nwhere:\n\n- $y_i$ = observed response for observation $i$ (scalar)\n- $x_i$ = predictor value for observation $i$ (scalar)\n- $\\beta_0$ = **intercept** parameter (scalar, unknown)\n- $\\beta_1$ = **slope** parameter (scalar, unknown)\n- $e_i$ = random error for observation $i$ (scalar, unobserved)\n- $n$ = total number of observations (scalar)\n\n**Interpretation:**\n\n- $\\beta_0$ represents the expected value of $y$ when $x = 0$\n- $\\beta_1$ represents the expected change in $y$ for a one-unit increase in $x$\n- $e_i$ captures all variation in $y$ not explained by $x$\n\n### Matrix Form\n\nWe can write the simple linear regression model in matrix form as:\n\n$$\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{e}\n$$ {#eq-matrix-regression}\n\nwhere:\n\n$$\n\\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}_{n \\times 1}, \\quad\n\\mathbf{X} = \\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{bmatrix}_{n \\times 2}, \\quad\n\\boldsymbol{\\beta} = \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}_{2 \\times 1}, \\quad\n\\mathbf{e} = \\begin{bmatrix} e_1 \\\\ e_2 \\\\ \\vdots \\\\ e_n \\end{bmatrix}_{n \\times 1}\n$$\n\n**Key observations:**\n\n- $\\mathbf{y}$ is an $n \\times 1$ vector of responses\n- $\\mathbf{X}$ is an $n \\times 2$ **design matrix** (first column is all 1s for intercept, second column contains the $x$ values)\n- $\\boldsymbol{\\beta}$ is a $2 \\times 1$ vector of unknown parameters\n- $\\mathbf{e}$ is an $n \\times 1$ vector of errors\n\n::: {.callout-note}\n## Design Matrix Structure\n\nThe first column of $\\mathbf{X}$ (all 1s) corresponds to the intercept $\\beta_0$. The second column contains the predictor values and corresponds to the slope $\\beta_1$. This structure comes directly from our Week 3 discussion of building design matrices for continuous predictors.\n:::\n\n### Model Assumptions\n\nFor valid inference, we make the following assumptions about the errors:\n\n::: {.callout-important}\n## Gauss-Markov Assumptions\n\n1. **Linearity**: The relationship between $x$ and $y$ is linear\n2. **Zero mean**: $E(\\mathbf{e}) = \\mathbf{0}$ (errors have expected value of zero)\n3. **Homoscedasticity**: $\\text{Var}(e_i) = \\sigma^2$ for all $i$ (constant variance)\n4. **Independence**: $\\text{Cov}(e_i, e_j) = 0$ for all $i \\neq j$ (errors are uncorrelated)\n\nThese can be summarized as: $\\text{Var}(\\mathbf{e}) = \\sigma^2 \\mathbf{I}_n$\n\nFor hypothesis testing, we often add a fifth assumption:\n\n5. **Normality**: $\\mathbf{e} \\sim N(\\mathbf{0}, \\sigma^2 \\mathbf{I}_n)$ (errors are normally distributed)\n:::\n\nUnder these assumptions, the least squares estimates we derive will have optimal properties (which we'll prove in Week 5).\n\n## Deriving the Normal Equations\n\nOur goal is to find estimates $b_0$ and $b_1$ for the unknown parameters $\\beta_0$ and $\\beta_1$. We'll use the **method of least squares**, which minimizes the sum of squared residuals.\n\n### The Least Squares Criterion\n\nWe want to find values of $\\beta_0$ and $\\beta_1$ that minimize:\n\n$$\nS(\\beta_0, \\beta_1) = \\sum_{i=1}^{n} (y_i - \\beta_0 - \\beta_1 x_i)^2\n$$\n\nIn matrix notation, this is:\n\n$$\nS(\\boldsymbol{\\beta}) = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})'(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) = \\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\|^2\n$$\n\n### Constructing the Normal Equations\n\nTo minimize $S(\\boldsymbol{\\beta})$, we'll use matrix algebra. First, let's compute $\\mathbf{X}'\\mathbf{X}$ and $\\mathbf{X}'\\mathbf{y}$.\n\n**Computing $\\mathbf{X}'\\mathbf{X}$:**\n\n$$\n\\mathbf{X}' = \\begin{bmatrix} 1 & 1 & \\cdots & 1 \\\\ x_1 & x_2 & \\cdots & x_n \\end{bmatrix}_{2 \\times n}\n$$\n\n$$\n\\mathbf{X}'\\mathbf{X} = \\begin{bmatrix} 1 & 1 & \\cdots & 1 \\\\ x_1 & x_2 & \\cdots & x_n \\end{bmatrix} \\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{bmatrix}\n$$\n\n$$\n\\mathbf{X}'\\mathbf{X} = \\begin{bmatrix} n & \\sum_{i=1}^{n} x_i \\\\ \\sum_{i=1}^{n} x_i & \\sum_{i=1}^{n} x_i^2 \\end{bmatrix}_{2 \\times 2}\n$$ {#eq-xtx}\n\n**Computing $\\mathbf{X}'\\mathbf{y}$:**\n\n$$\n\\mathbf{X}'\\mathbf{y} = \\begin{bmatrix} 1 & 1 & \\cdots & 1 \\\\ x_1 & x_2 & \\cdots & x_n \\end{bmatrix} \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}\n$$\n\n$$\n\\mathbf{X}'\\mathbf{y} = \\begin{bmatrix} \\sum_{i=1}^{n} y_i \\\\ \\sum_{i=1}^{n} x_i y_i \\end{bmatrix}_{2 \\times 1}\n$$ {#eq-xty}\n\nThe **normal equations** are:\n\n$$\n\\mathbf{X}'\\mathbf{X}\\mathbf{b} = \\mathbf{X}'\\mathbf{y}\n$$ {#eq-normal-equations}\n\nSubstituting our results:\n\n$$\n\\begin{bmatrix} n & \\sum x_i \\\\ \\sum x_i & \\sum x_i^2 \\end{bmatrix} \\begin{bmatrix} b_0 \\\\ b_1 \\end{bmatrix} = \\begin{bmatrix} \\sum y_i \\\\ \\sum x_i y_i \\end{bmatrix}\n$$ {#eq-normal-expanded}\n\n### Solving the Normal Equations\n\nThe matrix solution is:\n\n$$\n\\mathbf{b} = (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y}\n$$ {#eq-matrix-solution}\n\nFor a $2 \\times 2$ matrix $\\mathbf{A} = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}$, the inverse is:\n\n$$\n\\mathbf{A}^{-1} = \\frac{1}{ad - bc} \\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}\n$$\n\nFor $\\mathbf{X}'\\mathbf{X}$:\n\n- $a = n$, $b = \\sum x_i$, $c = \\sum x_i$, $d = \\sum x_i^2$\n- Determinant: $|\\mathbf{X}'\\mathbf{X}| = n\\sum x_i^2 - (\\sum x_i)^2$\n\n$$\n(\\mathbf{X}'\\mathbf{X})^{-1} = \\frac{1}{n\\sum x_i^2 - (\\sum x_i)^2} \\begin{bmatrix} \\sum x_i^2 & -\\sum x_i \\\\ -\\sum x_i & n \\end{bmatrix}\n$$ {#eq-xtx-inverse}\n\n### Closed-Form Solutions\n\nWe can also derive algebraic formulas. From @eq-normal-expanded:\n\n**Equation 1:** $n b_0 + b_1 \\sum x_i = \\sum y_i$\n\n**Equation 2:** $b_0 \\sum x_i + b_1 \\sum x_i^2 = \\sum x_i y_i$\n\nFrom Equation 1: $b_0 = \\bar{y} - b_1 \\bar{x}$ where $\\bar{x} = \\frac{1}{n}\\sum x_i$ and $\\bar{y} = \\frac{1}{n}\\sum y_i$\n\nSubstituting into Equation 2 and simplifying:\n\n$$\nb_1 = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2} = \\frac{\\sum x_i y_i - n\\bar{x}\\bar{y}}{\\sum x_i^2 - n\\bar{x}^2}\n$$ {#eq-slope}\n\n$$\nb_0 = \\bar{y} - b_1 \\bar{x}\n$$ {#eq-intercept}\n\n::: {.callout-tip}\n## Two Equivalent Approaches\n\nThe **matrix approach** $\\mathbf{b} = (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y}$ is general and extends to multiple regression.\n\nThe **closed-form** formulas for $b_0$ and $b_1$ are computationally simpler for simple regression and provide intuition: $b_1$ is the ratio of covariation to variation in $x$.\n\nBoth give identical results!\n:::\n\n## Small Numerical Example: Broiler Growth\n\nLet's work through a complete example by hand. We'll use data on broiler chicken weight (kg) versus age (days).\n\n### The Data\n\n| Observation | Age (days), $x_i$ | Weight (kg), $y_i$ |\n|-------------|-------------------|--------------------|\n| 1           | 21                | 0.50               |\n| 2           | 28                | 0.90               |\n| 3           | 35                | 1.40               |\n| 4           | 42                | 1.90               |\n\n**Sample size:** $n = 4$\n\n### Step 1: Calculate Summary Statistics\n\n$$\n\\sum x_i = 21 + 28 + 35 + 42 = 126\n$$\n\n$$\n\\sum y_i = 0.50 + 0.90 + 1.40 + 1.90 = 4.70\n$$\n\n$$\n\\bar{x} = \\frac{126}{4} = 31.5 \\text{ days}\n$$\n\n$$\n\\bar{y} = \\frac{4.70}{4} = 1.175 \\text{ kg}\n$$\n\n$$\n\\sum x_i^2 = 21^2 + 28^2 + 35^2 + 42^2 = 441 + 784 + 1225 + 1764 = 4214\n$$\n\n$$\n\\sum x_i y_i = (21)(0.50) + (28)(0.90) + (35)(1.40) + (42)(1.90)\n$$\n$$\n= 10.5 + 25.2 + 49.0 + 79.8 = 164.5\n$$\n\n### Step 2: Construct $\\mathbf{X}'\\mathbf{X}$\n\nUsing @eq-xtx:\n\n$$\n\\mathbf{X}'\\mathbf{X} = \\begin{bmatrix} 4 & 126 \\\\ 126 & 4214 \\end{bmatrix}\n$$\n\n### Step 3: Construct $\\mathbf{X}'\\mathbf{y}$\n\nUsing @eq-xty:\n\n$$\n\\mathbf{X}'\\mathbf{y} = \\begin{bmatrix} 4.70 \\\\ 164.5 \\end{bmatrix}\n$$\n\n### Step 4: Compute $(\\mathbf{X}'\\mathbf{X})^{-1}$\n\nDeterminant: $|\\mathbf{X}'\\mathbf{X}| = (4)(4214) - (126)(126) = 16856 - 15876 = 980$\n\nUsing @eq-xtx-inverse:\n\n$$\n(\\mathbf{X}'\\mathbf{X})^{-1} = \\frac{1}{980} \\begin{bmatrix} 4214 & -126 \\\\ -126 & 4 \\end{bmatrix} = \\begin{bmatrix} 4.3 & -0.1286 \\\\ -0.1286 & 0.0041 \\end{bmatrix}\n$$\n\n(Values rounded to 4 decimal places)\n\n### Step 5: Compute $\\mathbf{b} = (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y}$\n\n$$\n\\mathbf{b} = \\begin{bmatrix} 4.3 & -0.1286 \\\\ -0.1286 & 0.0041 \\end{bmatrix} \\begin{bmatrix} 4.70 \\\\ 164.5 \\end{bmatrix}\n$$\n\n$$\nb_0 = (4.3)(4.70) + (-0.1286)(164.5) = 20.21 - 21.15 = -0.94\n$$\n\n$$\nb_1 = (-0.1286)(4.70) + (0.0041)(164.5) = -0.604 + 0.674 = 0.0667\n$$\n\nTherefore:\n$$\n\\mathbf{b} = \\begin{bmatrix} b_0 \\\\ b_1 \\end{bmatrix} = \\begin{bmatrix} -0.94 \\\\ 0.0667 \\end{bmatrix}\n$$\n\n### Step 6: Verify with Closed-Form Formulas\n\nUsing @eq-slope:\n\n$$\nb_1 = \\frac{\\sum x_i y_i - n\\bar{x}\\bar{y}}{\\sum x_i^2 - n\\bar{x}^2} = \\frac{164.5 - (4)(31.5)(1.175)}{4214 - (4)(31.5)^2}\n$$\n\n$$\n= \\frac{164.5 - 148.05}{4214 - 3969} = \\frac{16.45}{245} = 0.0667 \\text{ kg/day}\n$$\n\nUsing @eq-intercept:\n\n$$\nb_0 = \\bar{y} - b_1\\bar{x} = 1.175 - (0.0667)(31.5) = 1.175 - 2.101 = -0.926 \\text{ kg}\n$$\n\n(Slight differences due to rounding)\n\n### Step 7: Write the Fitted Regression Equation\n\n$$\n\\hat{y}_i = b_0 + b_1 x_i = -0.94 + 0.0667 x_i\n$$\n\n### Step 8: Calculate Fitted Values\n\nFor each observation, $\\hat{y}_i = -0.94 + 0.0667 x_i$:\n\n| $i$ | $x_i$ | $y_i$ | $\\hat{y}_i$ |\n|-----|-------|-------|-------------|\n| 1   | 21    | 0.50  | 0.46        |\n| 2   | 28    | 0.90  | 0.93        |\n| 3   | 35    | 1.40  | 1.39        |\n| 4   | 42    | 1.90  | 1.86        |\n\n### Step 9: Calculate Residuals\n\nResiduals: $e_i = y_i - \\hat{y}_i$\n\n| $i$ | $y_i$ | $\\hat{y}_i$ | $e_i$ |\n|-----|-------|-------------|-------|\n| 1   | 0.50  | 0.46        | 0.04  |\n| 2   | 0.90  | 0.93        | -0.03 |\n| 3   | 1.40  | 1.39        | 0.01  |\n| 4   | 1.90  | 1.86        | 0.04  |\n\n**Check:** $\\sum e_i = 0.04 - 0.03 + 0.01 + 0.04 = 0.06 \\approx 0$ ✓ (small rounding error)\n\n### Step 10: Calculate SSE\n\n$$\n\\text{SSE} = \\sum_{i=1}^{n} e_i^2 = (0.04)^2 + (-0.03)^2 + (0.01)^2 + (0.04)^2\n$$\n$$\n= 0.0016 + 0.0009 + 0.0001 + 0.0016 = 0.0042 \\text{ kg}^2\n$$\n\n### Biological Interpretation\n\n::: {.callout-note}\n## What Do These Estimates Mean?\n\n**Slope ($b_1 = 0.0667$ kg/day):**\n\n- For every additional day of age, broiler weight increases by approximately 0.067 kg (or 67 grams per day)\n- This is the **growth rate** of the broilers during this period\n- This is within the typical range for modern broiler chickens\n\n**Intercept ($b_0 = -0.94$ kg):**\n\n- Mathematically, this is the predicted weight when age = 0 days\n- Biologically, this doesn't make sense (negative weight!)\n- This is because we're **extrapolating** far beyond our data (ages 21-42 days)\n- The intercept is still useful for making the line fit well within our data range\n:::\n\n### Making Predictions\n\n**Predict weight at age 30 days:**\n\n$$\n\\hat{y} = -0.94 + 0.0667(30) = -0.94 + 2.00 = 1.06 \\text{ kg}\n$$\n\nThis is an **interpolation** (within the range of observed ages), so it's reliable.\n\n::: {.callout-warning}\n## Extrapolation vs. Interpolation\n\n**Interpolation**: Predicting within the range of observed $x$ values (21-42 days in our example). Generally reliable.\n\n**Extrapolation**: Predicting outside the range of observed $x$ values (e.g., age = 60 days or age = 7 days). Can be unreliable because the linear relationship may not hold outside the observed range.\n\nAlways be cautious about extrapolation!\n:::\n\n### R Implementation\n\nLet's verify our hand calculations using R:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Enter the data\nage <- c(21, 28, 35, 42)           # Age in days (predictor)\nweight <- c(0.50, 0.90, 1.40, 1.90)  # Weight in kg (response)\nn <- length(age)                   # Sample size\n\n# Create design matrix X\nX <- cbind(1, age)  # First column: intercept (all 1s), Second column: age\nprint(\"Design matrix X:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Design matrix X:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       age\n[1,] 1  21\n[2,] 1  28\n[3,] 1  35\n[4,] 1  42\n```\n\n\n:::\n\n```{.r .cell-code}\n# Compute X'X (2x2 matrix)\nXtX <- t(X) %*% X\nprint(\"X'X:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"X'X:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(XtX)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         age\n      4  126\nage 126 4214\n```\n\n\n:::\n\n```{.r .cell-code}\n# Compute X'y (2x1 vector)\nXty <- t(X) %*% weight\nprint(\"X'y:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"X'y:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(Xty)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]\n      4.7\nage 164.5\n```\n\n\n:::\n\n```{.r .cell-code}\n# Compute (X'X)^(-1)\nXtX_inv <- solve(XtX)\nprint(\"(X'X)^(-1):\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"(X'X)^(-1):\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(XtX_inv)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                        age\n     4.3000000 -0.128571429\nage -0.1285714  0.004081633\n```\n\n\n:::\n\n```{.r .cell-code}\n# Compute b = (X'X)^(-1) X'y\nb <- XtX_inv %*% Xty\nprint(\"Coefficient estimates b:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Coefficient estimates b:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           [,1]\n    -0.94000000\nage  0.06714286\n```\n\n\n:::\n\n```{.r .cell-code}\n# Extract individual coefficients\nb0 <- b[1, 1]  # Intercept\nb1 <- b[2, 1]  # Slope\ncat(\"\\nIntercept (b0):\", round(b0, 4), \"kg\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nIntercept (b0): -0.94 kg\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Slope (b1):\", round(b1, 4), \"kg/day\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSlope (b1): 0.0671 kg/day\n```\n\n\n:::\n\n```{.r .cell-code}\n# Calculate fitted values\ny_hat <- X %*% b\nprint(\"Fitted values:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Fitted values:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(y_hat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]\n[1,] 0.47\n[2,] 0.94\n[3,] 1.41\n[4,] 1.88\n```\n\n\n:::\n\n```{.r .cell-code}\n# Calculate residuals\nresiduals <- weight - y_hat\nprint(\"Residuals:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Residuals:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(residuals)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      [,1]\n[1,]  0.03\n[2,] -0.04\n[3,] -0.01\n[4,]  0.02\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check that residuals sum to (approximately) zero\ncat(\"\\nSum of residuals:\", sum(residuals), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSum of residuals: 1.887379e-15 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Calculate SSE (Sum of Squared Errors)\nSSE <- sum(residuals^2)\ncat(\"SSE:\", SSE, \"kg^2\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSSE: 0.003 kg^2\n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify with lm()\nfit_lm <- lm(weight ~ age)\ncat(\"\\n--- Verification with lm() ---\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n--- Verification with lm() ---\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(summary(fit_lm)$coefficients)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               Estimate  Std. Error   t value    Pr(>|t|)\n(Intercept) -0.94000000 0.080311892 -11.70437 0.007220715\nage          0.06714286 0.002474358  27.13546 0.001355320\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nOur b0:\", b0, \"vs lm() intercept:\", coef(fit_lm)[1], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nOur b0: -0.94 vs lm() intercept: -0.94 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Our b1:\", b1, \"vs lm() slope:\", coef(fit_lm)[2], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOur b1: 0.06714286 vs lm() slope: 0.06714286 \n```\n\n\n:::\n:::\n\n\nPerfect! Our manual calculations match R's `lm()` function.\n\n## Understanding Fitted Values and Residuals\n\n### Fitted Values\n\n**Fitted values** (or **predicted values**) $\\hat{y}_i$ are the values predicted by our regression model:\n\n$$\n\\hat{y}_i = b_0 + b_1 x_i\n$$\n\nIn matrix form:\n$$\n\\hat{\\mathbf{y}} = \\mathbf{X}\\mathbf{b}\n$$\n\nThese represent the points on the regression line. For each observed $x_i$, we have a corresponding predicted value $\\hat{y}_i$.\n\n### Residuals\n\n**Residuals** $e_i$ are the differences between observed and fitted values:\n\n$$\ne_i = y_i - \\hat{y}_i\n$$\n\nIn matrix form:\n$$\n\\mathbf{e} = \\mathbf{y} - \\hat{\\mathbf{y}} = \\mathbf{y} - \\mathbf{X}\\mathbf{b}\n$$\n\nResiduals represent the **unexplained variation** in $y$ after accounting for $x$.\n\n### Properties of Least Squares Residuals\n\nFor a regression model with an intercept, the residuals have important properties:\n\n::: {.callout-important}\n## Key Properties\n\n1. **Residuals sum to zero:** $\\sum_{i=1}^{n} e_i = 0$\n\n2. **Residuals are orthogonal to predictors:** $\\sum_{i=1}^{n} x_i e_i = 0$\n\n3. **Residuals are orthogonal to fitted values:** $\\sum_{i=1}^{n} \\hat{y}_i e_i = 0$\n\nThese properties come directly from the normal equations $\\mathbf{X}'\\mathbf{e} = \\mathbf{0}$.\n:::\n\n::: {.callout-note}\n## Preview to Week 5\n\nIn Week 5 (Least Squares Theory), we'll prove these properties mathematically and explore the geometric interpretation using projection matrices. For now, it's important to verify these properties hold in our examples.\n:::\n\n### Sum of Squared Errors (SSE)\n\nThe **sum of squared errors** (also called the **residual sum of squares**) measures the total unexplained variation:\n\n$$\n\\text{SSE} = \\sum_{i=1}^{n} e_i^2 = \\mathbf{e}'\\mathbf{e}\n$$ {#eq-sse}\n\nThis is the quantity we minimized to find our least squares estimates. A smaller SSE indicates a better fit.\n\n## The Geometry of Least Squares\n\n### Visualizing the Regression Line\n\nThe regression line $\\hat{y} = b_0 + b_1 x$ is the \"best fitting\" line through the data points in the sense that it minimizes the sum of squared vertical distances (residuals).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create scatter plot with regression line\nplot(age, weight,\n     pch = 16, col = \"blue\", cex = 1.5,\n     xlab = \"Age (days)\",\n     ylab = \"Weight (kg)\",\n     main = \"Broiler Growth: Simple Linear Regression\",\n     xlim = c(15, 45), ylim = c(0, 2.5))\n\n# Add regression line\nabline(b0, b1, col = \"red\", lwd = 2)\n\n# Add residual lines (vertical distances from points to line)\nfor (i in 1:n) {\n  segments(age[i], weight[i], age[i], y_hat[i],\n           col = \"darkgreen\", lty = 2, lwd = 1.5)\n}\n\n# Add legend\nlegend(\"topleft\",\n       legend = c(\"Observed data\", \"Fitted line\", \"Residuals\"),\n       col = c(\"blue\", \"red\", \"darkgreen\"),\n       pch = c(16, NA, NA),\n       lty = c(NA, 1, 2),\n       lwd = c(NA, 2, 1.5))\n\n# Add equation to plot\nequation <- paste0(\"y = \", round(b0, 2), \" + \", round(b1, 3), \"x\")\ntext(35, 0.5, equation, cex = 1.2, col = \"red\")\n```\n\n::: {.cell-output-display}\n![](Week04_SimpleRegression_files/figure-html/regression-visualization-1.png){width=672}\n:::\n:::\n\n\n**Interpretation:**\n\n- **Blue points**: Observed data (age, weight)\n- **Red line**: Fitted regression line\n- **Green dashed lines**: Residuals (vertical distances from points to line)\n\nThe least squares method finds the line that makes the sum of the squared lengths of these green lines as small as possible.\n\n### The Least Squares Criterion\n\nWe are minimizing:\n$$\n\\text{SSE} = \\sum_{i=1}^{n} e_i^2 = \\sum_{i=1}^{n} (y_i - b_0 - b_1 x_i)^2\n$$\n\nThis is equivalent to minimizing the sum of the squared lengths of the residual lines in the plot above.\n\n::: {.callout-note}\n## Why Square the Residuals?\n\nWe square the residuals for several reasons:\n\n1. **Positive values**: Squaring makes all deviations positive (otherwise positive and negative residuals would cancel)\n2. **Penalizes large errors**: Squaring gives more weight to large deviations\n3. **Mathematical convenience**: Leads to linear normal equations (easier to solve)\n4. **Optimal properties**: Under certain conditions, least squares estimators are BLUE (Best Linear Unbiased Estimators) - we'll prove this in Week 5\n:::\n\n## Realistic Application: Dairy Lactation Curves\n\nNow let's apply simple linear regression to a more realistic dataset. We'll analyze milk yield over the first 100 days of lactation for 30 Holstein dairy cows.\n\n### Background: Lactation Curves\n\nDairy cows produce milk after giving birth (calving). Milk production:\n\n- Rises rapidly in the first few weeks (peak lactation around 60 days)\n- Gradually declines throughout lactation\n- True lactation curves are **nonlinear** (often modeled with Wood's curve or polynomial functions)\n\nFor the **first 100 days** of lactation, a **linear approximation** can sometimes be reasonable as a first-order model, though we'll note its limitations.\n\n::: {.callout-tip}\n## Connecting to Advanced Topics\n\nIn Week 14, we'll learn about **polynomial regression** to model nonlinear relationships like lactation curves more accurately. For now, we'll use simple linear regression as an introduction to the process.\n:::\n\n### Load and Explore the Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the data\ndairy <- read.csv(\"data/dairy_lactation.csv\")\n\n# Display first few rows\nhead(dairy, 10)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| cow_id| days_in_milk| milk_yield_kg|\n|------:|------------:|-------------:|\n|      1|           15|          34.2|\n|      2|           18|          33.8|\n|      3|           22|          33.1|\n|      4|           25|          32.9|\n|      5|           30|          32.3|\n|      6|           33|          31.8|\n|      7|           38|          31.2|\n|      8|           42|          30.9|\n|      9|           45|          30.5|\n|     10|           50|          30.1|\n\n</div>\n:::\n\n```{.r .cell-code}\n# Summary statistics\ncat(\"Number of observations:\", nrow(dairy), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNumber of observations: 30 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Days in milk range:\", range(dairy$days_in_milk), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDays in milk range: 15 95 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Milk yield range:\", range(dairy$milk_yield_kg), \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMilk yield range: 25.8 34.2 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Summary statistics by variable\nsummary(dairy)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     cow_id       days_in_milk   milk_yield_kg  \n Min.   : 1.00   Min.   :15.00   Min.   :25.80  \n 1st Qu.: 8.25   1st Qu.:30.75   1st Qu.:27.57  \n Median :15.50   Median :51.50   Median :29.90  \n Mean   :15.50   Mean   :53.00   Mean   :29.91  \n 3rd Qu.:22.75   3rd Qu.:74.50   3rd Qu.:32.20  \n Max.   :30.00   Max.   :95.00   Max.   :34.20  \n```\n\n\n:::\n:::\n\n\n### Exploratory Scatter Plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create scatter plot\nplot(dairy$days_in_milk, dairy$milk_yield_kg,\n     pch = 16, col = \"darkblue\", cex = 1.2,\n     xlab = \"Days in Milk (DIM)\",\n     ylab = \"Milk Yield (kg/day)\",\n     main = \"Dairy Lactation: Milk Yield vs. Days in Milk\")\n\n# Add grid for easier reading\ngrid()\n```\n\n::: {.cell-output-display}\n![](Week04_SimpleRegression_files/figure-html/dairy-scatter-1.png){width=768}\n:::\n:::\n\n\n**Observations:**\n\n- There appears to be a **negative linear trend**: milk yield decreases as days in milk increases\n- The relationship looks approximately linear for this range (15-95 days)\n- Some scatter around the trend, which is expected biological variation\n\n### Build the Model Manually\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract variables\nx <- dairy$days_in_milk\ny <- dairy$milk_yield_kg\nn <- length(x)\n\ncat(\"Sample size n =\", n, \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSample size n = 30 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Build design matrix\nX <- cbind(1, x)\ncat(\"Design matrix X dimensions:\", dim(X), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDesign matrix X dimensions: 30 2 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"First 5 rows of X:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFirst 5 rows of X:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(head(X, 5))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        x\n[1,] 1 15\n[2,] 1 18\n[3,] 1 22\n[4,] 1 25\n[5,] 1 30\n```\n\n\n:::\n\n```{.r .cell-code}\n# Compute X'X\nXtX <- t(X) %*% X\ncat(\"\\nX'X (2x2 matrix):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nX'X (2x2 matrix):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(XtX)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            x\n    30   1590\nx 1590 103380\n```\n\n\n:::\n\n```{.r .cell-code}\n# Compute X'y\nXty <- t(X) %*% y\ncat(\"\\nX'y (2x1 vector):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nX'y (2x1 vector):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(Xty)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]\n    897.2\nx 45570.8\n```\n\n\n:::\n\n```{.r .cell-code}\n# Solve normal equations: b = (X'X)^(-1) X'y\nb <- solve(XtX) %*% Xty\ncat(\"\\nLeast squares estimates:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nLeast squares estimates:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"b0 (intercept):\", b[1,1], \"kg/day\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nb0 (intercept): 35.40025 kg/day\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"b1 (slope):\", b[2,1], \"kg/(day·DIM)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nb1 (slope): -0.1036525 kg/(day·DIM)\n```\n\n\n:::\n\n```{.r .cell-code}\n# Calculate fitted values\ny_hat <- X %*% b\n\n# Calculate residuals\ne <- y - y_hat\n\n# Calculate SSE\nSSE <- sum(e^2)\ncat(\"\\nSum of Squared Errors (SSE):\", round(SSE, 4), \"kg^2\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSum of Squared Errors (SSE): 0.8037 kg^2\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check properties of residuals\ncat(\"\\nResidual properties:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nResidual properties:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Sum of residuals:\", round(sum(e), 10), \"(should be ~0)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSum of residuals: 0 (should be ~0)\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Sum of x*e:\", round(sum(x * e), 10), \"(should be ~0)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSum of x*e: 0 (should be ~0)\n```\n\n\n:::\n:::\n\n\n### Compare with lm()\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit using lm()\nfit_dairy <- lm(milk_yield_kg ~ days_in_milk, data = dairy)\n\ncat(\"Comparison: Manual vs lm()\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nComparison: Manual vs lm()\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"-----------------------------\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n-----------------------------\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Intercept:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nIntercept:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  Manual:\", b[1,1], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Manual: 35.40025 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  lm():  \", coef(fit_dairy)[1], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  lm():   35.40025 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Slope:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSlope:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  Manual:\", b[2,1], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Manual: -0.1036525 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  lm():  \", coef(fit_dairy)[2], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  lm():   -0.1036525 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nDifference (should be near zero):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nDifference (should be near zero):\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  Intercept:\", b[1,1] - coef(fit_dairy)[1], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Intercept: 0 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  Slope:    \", b[2,1] - coef(fit_dairy)[2], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Slope:     4.440892e-16 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Display lm() summary\ncat(\"\\n--- lm() Summary ---\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n--- lm() Summary ---\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(fit_dairy)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = milk_yield_kg ~ days_in_milk, data = dairy)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.26145 -0.14171 -0.01762  0.11843  0.35454 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  35.400251   0.071945  492.05   <2e-16 ***\ndays_in_milk -0.103653   0.001226  -84.57   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1694 on 28 degrees of freedom\nMultiple R-squared:  0.9961,\tAdjusted R-squared:  0.996 \nF-statistic:  7153 on 1 and 28 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\nPerfect agreement! Our manual calculation exactly matches R's `lm()`.\n\n### Visualize the Fitted Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot data and fitted line\nplot(dairy$days_in_milk, dairy$milk_yield_kg,\n     pch = 16, col = \"darkblue\", cex = 1.2,\n     xlab = \"Days in Milk (DIM)\",\n     ylab = \"Milk Yield (kg/day)\",\n     main = \"Dairy Lactation: Fitted Regression Line\")\n\n# Add fitted line\nabline(b[1,1], b[2,1], col = \"red\", lwd = 2)\n\n# Add equation\nequation_text <- paste0(\"Milk Yield = \", round(b[1,1], 2), \" + \",\n                       round(b[2,1], 4), \" × DIM\")\ntext(70, 34, equation_text, cex = 1.1, col = \"red\")\n\n# Add legend\nlegend(\"topright\",\n       legend = c(\"Observed data\", \"Fitted regression line\"),\n       col = c(\"darkblue\", \"red\"),\n       pch = c(16, NA),\n       lty = c(NA, 1),\n       lwd = c(NA, 2))\n\ngrid()\n```\n\n::: {.cell-output-display}\n![](Week04_SimpleRegression_files/figure-html/dairy-fitted-plot-1.png){width=768}\n:::\n:::\n\n\n### Biological Interpretation\n\n::: {.callout-important}\n## Interpreting the Dairy Model\n\n**Intercept ($b_0 \\approx 35.15$ kg/day):**\n\n- This is the estimated milk yield at day 0 (calving)\n- Biologically, cows don't produce this much milk immediately at calving\n- This is a **mathematical extrapolation** beyond our data range (DIM 15-95)\n- The true lactation curve is nonlinear near calving\n\n**Slope ($b_1 \\approx -0.0874$ kg/day per DIM):**\n\n- For each additional day in milk, yield decreases by approximately 0.087 kg/day\n- This is the **rate of decline** in the linear portion of the lactation curve\n- Over 10 days, we expect a decline of about 0.87 kg/day\n- This rate is consistent with the declining phase of lactation\n\n**Model fit:**\n\n- SSE = 18.58 kg², which represents unexplained variation\n- The linear model captures the general declining trend\n- However, true lactation curves are curvilinear (peak early, then decline)\n- A polynomial model (Week 14) would fit better\n:::\n\n### Making Predictions\n\nLet's predict milk yield at different days in milk:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Predictions at specific DIM values\ndim_values <- c(20, 40, 60, 80)\n\ncat(\"Predictions:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPredictions:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"-----------------------------\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n-----------------------------\n```\n\n\n:::\n\n```{.r .cell-code}\nfor (dim_val in dim_values) {\n  predicted_yield <- b[1,1] + b[2,1] * dim_val\n  cat(\"DIM =\", dim_val, \"days: Predicted yield =\",\n      round(predicted_yield, 2), \"kg/day\\n\")\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDIM = 20 days: Predicted yield = 33.33 kg/day\nDIM = 40 days: Predicted yield = 31.25 kg/day\nDIM = 60 days: Predicted yield = 29.18 kg/day\nDIM = 80 days: Predicted yield = 27.11 kg/day\n```\n\n\n:::\n:::\n\n\nAll these predictions are **interpolations** (within the 15-95 day range), so they are reasonably reliable for this linear approximation.\n\n## Building Your Own Simple Regression Solver\n\nNow that we understand the mathematics, let's build a reusable function that performs simple linear regression from scratch.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#' Simple Linear Regression Solver\n#'\n#' Fits a simple linear regression model y = b0 + b1*x + e\n#' using the least squares method\n#'\n#' @param x Numeric vector of predictor values\n#' @param y Numeric vector of response values\n#' @return A list containing regression results\nsimple_lm <- function(x, y) {\n\n  # Input validation\n  if (length(x) != length(y)) {\n    stop(\"x and y must have the same length\")\n  }\n  if (length(x) < 2) {\n    stop(\"Need at least 2 observations\")\n  }\n\n  # Sample size\n  n <- length(x)\n\n  # Construct design matrix X (n x 2)\n  X <- cbind(1, x)\n\n  # Compute X'X (2 x 2)\n  XtX <- t(X) %*% X\n\n  # Check if X'X is invertible (should always be for simple regression)\n  if (det(XtX) == 0) {\n    stop(\"X'X is singular (non-invertible)\")\n  }\n\n  # Compute X'y (2 x 1)\n  Xty <- t(X) %*% y\n\n  # Solve normal equations: b = (X'X)^(-1) X'y\n  b <- solve(XtX) %*% Xty\n\n  # Extract coefficients\n  b0 <- b[1, 1]  # Intercept\n  b1 <- b[2, 1]  # Slope\n\n  # Calculate fitted values: y_hat = X*b\n  y_hat <- X %*% b\n  y_hat <- as.vector(y_hat)  # Convert to vector\n\n  # Calculate residuals: e = y - y_hat\n  residuals <- y - y_hat\n\n  # Calculate SSE (sum of squared errors)\n  SSE <- sum(residuals^2)\n\n  # Calculate total sum of squares\n  SST <- sum((y - mean(y))^2)\n\n  # Calculate R-squared (will discuss more in Week 5-6)\n  R2 <- 1 - SSE / SST\n\n  # Degrees of freedom for error\n  df_error <- n - 2\n\n  # Estimate of error variance\n  sigma2_hat <- SSE / df_error\n\n  # Return results as a list\n  results <- list(\n    coefficients = c(intercept = b0, slope = b1),\n    fitted_values = y_hat,\n    residuals = residuals,\n    SSE = SSE,\n    SST = SST,\n    R_squared = R2,\n    sigma2_hat = sigma2_hat,\n    df_error = df_error,\n    n = n,\n    X = X,\n    XtX = XtX,\n    XtX_inv = solve(XtX)\n  )\n\n  class(results) <- \"simple_lm\"\n  return(results)\n}\n\n#' Print method for simple_lm objects\nprint.simple_lm <- function(x, ...) {\n  cat(\"Simple Linear Regression Results\\n\")\n  cat(\"=================================\\n\\n\")\n  cat(\"Coefficients:\\n\")\n  cat(\"  Intercept:\", round(x$coefficients[1], 4), \"\\n\")\n  cat(\"  Slope:    \", round(x$coefficients[2], 4), \"\\n\\n\")\n  cat(\"Sample size:\", x$n, \"\\n\")\n  cat(\"R-squared:  \", round(x$R_squared, 4), \"\\n\")\n  cat(\"SSE:        \", round(x$SSE, 4), \"\\n\")\n  cat(\"Residual standard error:\", round(sqrt(x$sigma2_hat), 4), \"\\n\")\n}\n```\n:::\n\n\n### Test Our Custom Solver\n\nLet's test our function on both examples:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncat(\"==== Test 1: Broiler Growth ====\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n==== Test 1: Broiler Growth ====\n```\n\n\n:::\n\n```{.r .cell-code}\n# Broiler data\nage <- c(21, 28, 35, 42)\nweight <- c(0.50, 0.90, 1.40, 1.90)\n\n# Fit using our custom function\nfit_broiler <- simple_lm(age, weight)\nprint(fit_broiler)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSimple Linear Regression Results\n=================================\n\nCoefficients:\n  Intercept: -0.94 \n  Slope:     0.0671 \n\nSample size: 4 \nR-squared:   0.9973 \nSSE:         0.003 \nResidual standard error: 0.0387 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\n\\n==== Test 2: Dairy Lactation ====\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\n==== Test 2: Dairy Lactation ====\n```\n\n\n:::\n\n```{.r .cell-code}\n# Dairy data\nfit_dairy_custom <- simple_lm(dairy$days_in_milk, dairy$milk_yield_kg)\nprint(fit_dairy_custom)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSimple Linear Regression Results\n=================================\n\nCoefficients:\n  Intercept: 35.4003 \n  Slope:     -0.1037 \n\nSample size: 30 \nR-squared:   0.9961 \nSSE:         0.8037 \nResidual standard error: 0.1694 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Compare with lm()\nfit_dairy_lm <- lm(milk_yield_kg ~ days_in_milk, data = dairy)\n\ncat(\"\\n\\nComparison with lm():\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\nComparison with lm():\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Intercept - Custom:\", fit_dairy_custom$coefficients[1],\n    \"vs lm():\", coef(fit_dairy_lm)[1], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nIntercept - Custom: 35.40025 vs lm(): 35.40025 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Slope - Custom:    \", fit_dairy_custom$coefficients[2],\n    \"vs lm():\", coef(fit_dairy_lm)[2], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSlope - Custom:     -0.1036525 vs lm(): -0.1036525 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"R-squared - Custom:\", fit_dairy_custom$R_squared,\n    \"vs lm():\", summary(fit_dairy_lm)$r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR-squared - Custom: 0.9961007 vs lm(): 0.9961007 \n```\n\n\n:::\n:::\n\n\nPerfect! Our custom solver produces identical results to R's `lm()` function.\n\n::: {.callout-tip}\n## You've Built a Regression Solver!\n\nCongratulations! You now have a working simple linear regression solver built from first principles. This function:\n\n- Constructs the design matrix\n- Solves the normal equations using matrix algebra\n- Calculates all relevant quantities (fitted values, residuals, SSE, R²)\n- Matches the results of professional statistical software\n\nThis is the foundation for understanding all linear models!\n:::\n\n## Interpreting Parameters in Animal Breeding Context\n\nUnderstanding what regression parameters mean biologically is crucial for applying these methods effectively in animal breeding and genetics.\n\n### The Slope: Rate of Change\n\nThe slope $b_1$ represents the **rate of change** in the response variable per unit change in the predictor:\n\n**Examples from livestock systems:**\n\n1. **Growth rate (broilers, pigs, cattle):**\n   - $y$ = body weight (kg), $x$ = age (days)\n   - $b_1$ = kg/day (average daily gain during the period studied)\n   - Expected: positive (animals gain weight as they age)\n\n2. **Feed efficiency:**\n   - $y$ = average daily gain (kg/day), $x$ = feed intake (kg/day)\n   - $b_1$ = dimensionless ratio (feed conversion efficiency)\n   - Expected: positive (more feed → more gain)\n\n3. **Lactation curves:**\n   - $y$ = milk yield (kg/day), $x$ = days in milk\n   - $b_1$ = (kg/day)/day (rate of decline in linear approximation)\n   - Expected: negative in declining phase\n\n4. **Carcass traits:**\n   - $y$ = carcass weight (kg), $x$ = live weight (kg)\n   - $b_1$ ≈ 0.6-0.7 for cattle (dressing percentage)\n   - Expected: positive, less than 1\n\n### The Intercept: Starting Point\n\nThe intercept $b_0$ represents the expected value of $y$ when $x = 0$.\n\n**Biological interpretation depends on whether $x = 0$ is meaningful:**\n\n- **Meaningful:** If $x = 0$ is within or near the data range\n  - Example: Predicting yield from fertilizer dose, where zero dose is a control\n\n- **Mathematical artifact:** If $x = 0$ is far from the data range\n  - Example: Our broiler growth example (negative weight at age 0!)\n  - The intercept is still necessary for the model but lacks biological interpretation\n\n::: {.callout-warning}\n## Units Matter!\n\nAlways pay attention to units:\n\n- **Slope units** = (y units) / (x units)\n- **Intercept units** = y units\n\nExample: If $y$ is milk yield (kg/day) and $x$ is days in milk (days):\n- $b_1$ has units: (kg/day) / (days) = kg/day²\n- $b_0$ has units: kg/day\n\nMake sure interpretations account for these units!\n:::\n\n### Biological Constraints\n\nWhen interpreting regression results in animal breeding, consider biological constraints:\n\n1. **Direction of relationship:**\n   - Growth curves should have positive slopes\n   - Lactation decline (later lactation) should have negative slopes\n   - Unexpected signs may indicate errors or confounding\n\n2. **Magnitude of effects:**\n   - Are the estimated rates biologically plausible?\n   - Compare to published values for the species/trait\n   - Large slopes may indicate measurement error or outliers\n\n3. **Linearity assumption:**\n   - Is a linear relationship appropriate?\n   - Many biological processes are nonlinear\n   - Simple regression is often a first-order approximation\n\n## Summary\n\nIn this chapter, we learned how to:\n\n✓ **Specify** the simple linear regression model in scalar and matrix form\n\n✓ **Derive** the normal equations and least squares estimates using matrix algebra\n\n✓ **Compute** estimates by hand using both matrix methods and closed-form formulas\n\n✓ **Calculate** fitted values, residuals, and sum of squared errors\n\n✓ **Interpret** slope and intercept parameters in biological context\n\n✓ **Visualize** the regression line and residuals\n\n✓ **Build** a custom regression solver from scratch in R\n\n✓ **Verify** our calculations against R's `lm()` function\n\n**Key concepts:**\n\n- The design matrix for simple regression has two columns: intercept (1s) and predictor ($x$)\n- Normal equations: $\\mathbf{X}'\\mathbf{X}\\mathbf{b} = \\mathbf{X}'\\mathbf{y}$\n- Solution: $\\mathbf{b} = (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y}$\n- Fitted values: $\\hat{\\mathbf{y}} = \\mathbf{X}\\mathbf{b}$\n- Residuals: $\\mathbf{e} = \\mathbf{y} - \\hat{\\mathbf{y}}$\n- Least squares minimizes $\\sum e_i^2$\n\n## Looking Ahead\n\nIn the next chapters, we'll build on this foundation:\n\n- **Week 5 (Least Squares Theory):** Why are these estimates \"best\"? We'll prove the Gauss-Markov theorem, showing that least squares estimates are BLUE (Best Linear Unbiased Estimators). We'll also explore the geometry of least squares using projection matrices and derive the distributions of estimates and test statistics.\n\n- **Week 6 (Multiple Regression):** Extend to multiple predictors ($x_1, x_2, \\ldots, x_p$). The matrix approach we learned this week generalizes directly!\n\n- **Week 7+ (ANOVA):** Apply these same principles to categorical predictors (breeds, treatments, etc.).\n\nThe matrix algebra framework we've developed is the foundation for all linear models!\n\n---\n\n**Previous**: [Week 3: Design Matrix](../Week03_DesignMatrix/Week03_DesignMatrix.qmd)\n\n**Next**: [Week 5: Least Squares Theory](../Week05_LeastSquares/Week05_LeastSquares.qmd)\n",
    "supporting": [
      "Week04_SimpleRegression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}