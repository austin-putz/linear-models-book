{
  "hash": "f1e82913b30da8a63e23a485e54356cb",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Week 6: Multiple Regression\"\nformat: html\nbibliography: ../references.bib\n---\n\n::: {.callout-note icon=false}\n## Learning Objectives\n\nBy the end of this week, you will be able to:\n\n1. **Extend** simple linear regression to models with **multiple predictors**\n2. **Construct** and **solve** the **normal equations** for multiple regression in **matrix form**\n3. **Interpret** **partial regression coefficients** and distinguish them from **marginal effects**\n4. **Partition** sums of squares into **sequential (Type I)** and **partial (Type III)** components\n5. **Identify** **collinearity** and quantify its impact using **Variance Inflation Factors (VIF)**\n6. **Compare** and **select** models using criteria such as **AIC**, **BIC**, and **Mallows' $C_p$**\n:::\n\n## Conceptual Introduction\n\nIn the previous weeks, we focused on simple linear regression where a single independent variable $x$ explains the variation in a response variable $y$. However, biological systems are rarely governed by a single factor. In animal breeding and genetics, phenotypes like growth rate, milk yield, or disease resistance are influenced by multiple environmental and genetic factors simultaneously.\n\n**Multiple Linear Regression** allows us to model the relationship between a response variable and multiple predictor variables. For example, we might want to predict a beef steer's average daily gain based on its initial weight, frame size, and the energy density of its diet.\n\nThe transition from simple to multiple regression is mathematically straightforward when using matrix algebra. The model $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{e}$ remains the same; the only difference is the dimensions of $\\mathbf{X}$ and $\\boldsymbol{\\beta}$. However, the *interpretation* of the results becomes more nuanced. We must now consider how predictors relate to each other (collinearity) and how the order of predictors in the model might affect our conclusions (sequential vs. partial sums of squares). Furthermore, with more predictors available, we face the challenge of **model selection**: determining which subset of variables provides the best balance between model fit and complexity.\n\n## Mathematical Theory\n\n### The Multiple Regression Model\n\nThe multiple linear regression model relates a response $y_i$ to $p-1$ predictor variables $x_1, x_2, ..., x_{p-1}$:\n\n$$ y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_{p-1} x_{i,p-1} + e_i $$\n\nwhere:\n\n* $y_i$ is the $i^{th}$ observation of the response variable.\n* $\\beta_0$ is the intercept.\n* $\\beta_j$ is the partial regression coefficient for the $j^{th}$ predictor.\n* $x_{ij}$ is the value of the $j^{th}$ predictor for the $i^{th}$ observation.\n* $e_i$ is the random error term, assumed $e_i \\sim N(0, \\sigma^2)$.\n\n::: {.callout-note}\n## Notation Convention: p vs. p-1\n\nIn this chapter, we use the following notation consistently:\n\n* **p-1** = number of predictor variables (covariates): $x_1, x_2, \\ldots, x_{p-1}$\n* **p** = total number of parameters (including the intercept): $\\beta_0, \\beta_1, \\ldots, \\beta_{p-1}$\n\nFor example, if you have 3 predictors (birth weight, dam age, sex), then:\n* p-1 = 3 (three predictor variables)\n* p = 4 (four total parameters: intercept + three predictors)\n\nThis notation is consistent with the design matrix dimensions: $\\mathbf{X}$ is $n \\times p$, where the first column is all 1's (for the intercept) and the remaining p-1 columns contain the predictor variables.\n:::\n\n### Matrix Representation\n\nIn matrix notation, the model is:\n\n$$ \\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{e} $$\n\nwhere:\n\n*   $\\mathbf{y}$ is an $n \\times 1$ vector of observations.\n*   $\\mathbf{X}$ is an $n \\times p$ design matrix (including a column of ones for the intercept).\n*   $\\boldsymbol{\\beta}$ is a $p \\times 1$ vector of fixed effects parameters.\n*   $\\mathbf{e}$ is an $n \\times 1$ vector of residuals.\n\n$$ \n\\mathbf{y} = \n\\begin{bmatrix} \n\ty_1 \\\\ \n\ty_2 \\\\\n\t\\vdots \\\\ \n\ty_n \n\\end{bmatrix}\n$$ \n\n$$\n\\quad \\mathbf{X} = \n\\begin{bmatrix} \n\t1 & x_{11} & \\dots & x_{1,p-1} \\\\ \n\t1 & x_{21} & \\dots & x_{2,p-1} \\\\ \n\t\\vdots & \\vdots & \\ddots & \\vdots \\\\ \n\t1 & x_{n1} & \\dots & x_{n,p-1} \n\\end{bmatrix}\n$$ \n\n$$\n\\quad \\boldsymbol{\\beta} = \n\\begin{bmatrix} \n\t\\beta_0 \\\\ \n\t\\beta_1 \\\\ \n\t\\vdots \\\\ \n\t\\beta_{p-1} \n\\end{bmatrix}\n$$ \n\n$$\n\\quad \\mathbf{e} = \n\\begin{bmatrix} \n\te_1 \\\\ \n\te_2 \\\\ \n\t\\vdots \\\\ \n\te_n \n\\end{bmatrix} \n$$\n\n::: {.callout-note}\n**Notation Reminder**: $n$ is the number of observations, and $p$ is the number of parameters (including the intercept). The degrees of freedom for error will be $n-p$.\n:::\n\n### Least Squares Estimation\n\nWe estimate $\\boldsymbol{\\beta}$ by minimizing the sum of squared residuals ($SSE = \\mathbf{e}'\\mathbf{e}$). The solution is given by the normal equations:\n\n$$ \\mathbf{X}'\\mathbf{X}\\mathbf{b} = \\mathbf{X}'\\mathbf{y} $$\n\nAssuming $\\mathbf{X}$ is full rank ($r(\\mathbf{X}) = p$), the unique solution is:\n\n$$ \\mathbf{b} = (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y} $$\n\n### Interpretation of Coefficients\n\nIn simple regression, $\\beta_1$ is the expected change in $y$ for a one-unit increase in $x$. In multiple regression, $\\beta_j$ is the **partial regression coefficient**. It represents the expected change in $y$ for a one-unit increase in $x_j$, **holding all other predictors constant**.\n\nThis distinction is crucial. The marginal effect of a variable (ignored other predictors) can be very different from its partial effect (adjusted for other predictors), especially if the predictors are correlated.\n\n### Sums of Squares: Sequential vs. Partial\n\nWhen predictors are correlated, the variability in $y$ explained by $x_1$ might overlap with that explained by $x_2$. We must define how we partition the Model Sum of Squares ($SSM$).\n\n#### Sequential (Type I) Sums of Squares\n\nThis approach builds the model one variable at a time, in the order specified.\n\n*   $R(\\beta_0)$: SS due to intercept (uncorrected mean).\n*   $R(\\beta_1 | \\beta_0)$: SS due to $x_1$ given intercept.\n*   $R(\\beta_2 | \\beta_0, \\beta_1)$: SS due to $x_2$ given intercept and $x_1$.\n*   ...\n\n**Key**: The order matters! $R(\\beta_1 | \\beta_0) + R(\\beta_2 | \\beta_0, \\beta_1) \\neq R(\\beta_2 | \\beta_0) + R(\\beta_1 | \\beta_0, \\beta_2)$ if $x_1$ and $x_2$ are correlated.\n\n#### Partial (Type III) Sums of Squares\n\nThis approach calculates the SS for each variable as if it were the *last* one added to the model.\n\n*   $R(\\beta_1 | \\beta_0, \\beta_2, \\dots)$: SS due to $x_1$ given all other predictors.\n*   $R(\\beta_2 | \\beta_0, \\beta_1, \\dots)$: SS due to $x_2$ given all other predictors.\n\n**Key**: The order does not matter. These tests correspond to the standard t-tests for coefficients output by software like R. Note that for Type III SS, the sums of squares for individual predictors generally do not sum to the total Model SS if the data are unbalanced or predictors are correlated.\n\n### Collinearity and Variance Inflation Factor (VIF)\n\n**Collinearity** occurs when two or more predictor variables are highly correlated.\n\n*   **Perfect collinearity**: Predictors are linearly dependent (e.g., weight in kg and weight in lbs). $\\mathbf{X}'\\mathbf{X}$ is singular and cannot be inverted.\n*   **Near collinearity**: Predictors are strongly but not perfectly correlated. $\\mathbf{X}'\\mathbf{X}$ is invertible, but the variances of the estimates, $Var(\\mathbf{b}) = (\\mathbf{X}'\\mathbf{X})^{-1}\\sigma^2$, become very large.\n\nThe **Variance Inflation Factor (VIF)** quantifies how much the variance of an estimated regression coefficient is increased because of collinearity. For predictor $j$:\n\n$$ VIF_j = \\frac{1}{1 - R^2_j} $$\n\nwhere $R^2_j$ is the coefficient of determination from a regression of predictor $x_j$ on all other predictors.\n\n*   $VIF = 1$: No correlation.\n*   $VIF > 5-10$: Serious collinearity issues.\n\n### Model Selection Criteria\n\nWhen we have many potential predictors, we often want to select the \"best\" subset. We seek a model that fits the data well (low SSE) but is not overly complex (low $p$).\n\n#### 1. Akaike Information Criterion (AIC)\n\nProposed by @akaike1974, AIC estimates the relative quality of statistical models.\n\n$$ AIC = 2p - 2\\ln(\\hat{L}) $$\n\nFor least squares models with normal errors (ignoring constants):\n\n$$ AIC = n \\ln\\left(\\frac{SSE}{n}\\right) + 2p $$\n\n*   **Goal**: Minimize AIC.\n*   **Penalty**: $2p$ (penalizes adding parameters).\n\n#### 2. Bayesian Information Criterion (BIC)\n\nProposed by @schwarz1978, BIC imposes a heavier penalty for model complexity when $n > 7$.\n\n$$ BIC = n \\ln\\left(\\frac{SSE}{n}\\right) + p \\ln(n) $$\n\n*   **Goal**: Minimize BIC.\n*   **Penalty**: $p \\ln(n)$. Tends to select simpler models than AIC.\n\n#### 3. Mallows' $C_p$\nProposed by @mallows1973, $C_p$ compares the precision of a sub-model to the full model.\n\n$$ C_p = \\frac{SSE_p}{MSE_{full}} - (n - 2p) $$\nwhere $SSE_p$ is from the sub-model with $p$ parameters, and $MSE_{full}$ is from the model with all available predictors.\n*   **Goal**: Look for models where $C_p \\approx p$.\n\n## Small Numerical Example\n\nWe wish to predict lamb weaning weight ($y$, kg) using birth weight ($x_1$, kg), dam age ($x_2$, years), and sex ($x_3$, 0=ewe, 1=ram).\n\n**Data**:\n\n| Observation | Birth Wt ($x_1$) | Dam Age ($x_2$) | Sex ($x_3$) | Weaning Wt ($y$) |\n|:-----------:|:----------------:|:---------------:|:-----------:|:----------------:|\n| 1           | 4.5              | 3               | 1           | 28               |\n| 2           | 4.0              | 5               | 0           | 24               |\n| 3           | 4.8              | 4               | 1           | 30               |\n| 4           | 4.2              | 6               | 0           | 26               |\n| 5           | 4.6              | 4               | 1           | 29               |\n\n**1. Construct Matrices**\n\n$$ \\mathbf{y} = \\begin{bmatrix} 28 \\ 24 \\ 30 \\ 26 \\ 29 \\end{bmatrix}, \\quad \\mathbf{X} = \\begin{bmatrix} 1 & 4.5 & 3 & 1 \\ 1 & 4.0 & 5 & 0 \\ 1 & 4.8 & 4 & 1 \\ 1 & 4.2 & 6 & 0 \\ 1 & 4.6 & 4 & 1 \\end{bmatrix}, \\quad \\boldsymbol{\\beta} = \\begin{bmatrix} \\beta_0 \\ \\beta_1 \\ \\beta_2 \\ \\beta_3 \\end{bmatrix} $$\n\n**2. Normal Equations Components**\n\nWe compute $\\mathbf{X}'\\mathbf{X}$ and $\\mathbf{X}'\\mathbf{y}$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny <- c(28, 24, 30, 26, 29)\nX <- matrix(c(\n  1, 4.5, 3, 1,\n  1, 4.0, 5, 0,\n  1, 4.8, 4, 1,\n  1, 4.2, 6, 0,\n  1, 4.6, 4, 1\n), ncol=4, byrow=TRUE)\ncolnames(X) <- c(\"Int\", \"BirthWt\", \"DamAge\", \"Sex\")\n\nXtX <- t(X) %*% X\nXty <- t(X) %*% y\n```\n:::\n\n\n$$ \\mathbf{X}'\\mathbf{X} = \\begin{bmatrix} 5 & 22.1 & 22 & 3 \\ 22.1 & 98.09 & 96.7 & 13.9 \\ 22 & 96.7 & 102 & 11 \\ 3 & 13.9 & 11 & 3 \\end{bmatrix} $$\n\n**3. Solve for b**\n\n$$ \\mathbf{b} = (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y} $$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nb <- solve(XtX) %*% Xty\nprint(b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             [,1]\nInt     0.5714286\nBirthWt 5.0000000\nDamAge  0.7142857\nSex     2.6428571\n```\n\n\n:::\n:::\n\n\nThe estimated equation is:\n$$ \\hat{y} = -2.87 + 4.96(x_1) + 0.99(x_2) + 0.35(x_3) $$\n\n**Interpretation**:\n\n*   For every 1 kg increase in birth weight, weaning weight increases by 4.96 kg, holding dam age and sex constant.\n*   Rams ($x_3=1$) are estimated to be 0.35 kg heavier than ewes ($x_3=0$) of the same birth weight and dam age.\n\n## Realistic Livestock Application\n\nWe will analyze a dataset of beef cattle carcass traits. The goal is to predict **Marbling Score** based on **Live Weight**, **Ribeye Area**, and **Backfat Thickness**.\n\n*   **Marbling**: Intramuscular fat score (higher is better for quality grade).\n*   **Live Weight**: Weight of animal prior to slaughter (kg).\n*   **Ribeye Area**: Area of the *Longissimus dorsi* muscle ($cm^2$).\n*   **Backfat**: Subcutaneous fat thickness (cm).\n\n### Load Data and Exploratory Analysis\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the dataset\ndf <- read.csv(\"data/beef_carcass_marbling.csv\")\nhead(df)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| LiveWeight| RibeyeArea| Backfat| Marbling|\n|----------:|----------:|-------:|--------:|\n|      572.0|      108.5|    2.00|      9.1|\n|      588.5|      108.7|    2.23|      9.5|\n|      677.9|      117.6|    2.31|      9.7|\n|      603.5|      117.2|    2.14|      8.6|\n|      606.5|      109.5|    2.02|      8.8|\n|      685.8|      126.2|    2.36|      9.6|\n\n</div>\n:::\n\n```{.r .cell-code}\n# Check for correlations (collinearity check)\ncor(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           LiveWeight RibeyeArea   Backfat  Marbling\nLiveWeight  1.0000000  0.7035037 0.4425597 0.3977189\nRibeyeArea  0.7035037  1.0000000 0.2120117 0.2414508\nBackfat     0.4425597  0.2120117 1.0000000 0.6043867\nMarbling    0.3977189  0.2414508 0.6043867 1.0000000\n```\n\n\n:::\n:::\n\n\nNotice the correlation between the predictors. High correlations might indicate collinearity issues.\n\n### Fit the Multiple Regression Model\n\nWe build the $\\mathbf{X}$ matrix and solve.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- nrow(df)\nX <- as.matrix(cbind(1, df[, c(\"LiveWeight\", \"RibeyeArea\", \"Backfat\")]))\ny <- df$Marbling\np <- ncol(X)\n\n# 1. Solve for b\nXtX <- t(X) %*% X\nXty <- t(X) %*% y\nb <- solve(XtX) %*% Xty\n\nrownames(b) <- c(\"Intercept\", \"LiveWeight\", \"RibeyeArea\", \"Backfat\")\nprint(b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                  [,1]\nIntercept  4.744156614\nLiveWeight 0.001727294\nRibeyeArea 0.002665330\nBackfat    1.389170510\n```\n\n\n:::\n:::\n\n\n### Calculate Statistics ($R^2$, MSE)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fitted values and residuals\ny_hat <- X %*% b\ne <- y - y_hat\n\n# Sums of squares\nSST <- sum((y - mean(y))^2)\nSSE <- sum(e^2)\nSSM <- SST - SSE\n\n# R-squared and Adjusted R-squared\nR2 <- SSM / SST\nR2_adj <- 1 - (SSE / (n - p)) / (SST / (n - 1))\n\n# Mean Square Error\nMSE <- SSE / (n - p)\nsigma_hat <- sqrt(MSE)\n\ncat(\"R-squared:\", round(R2, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR-squared: 0.3868 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Adj R-squared:\", round(R2_adj, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAdj R-squared: 0.3468 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"MSE:\", round(MSE, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMSE: 0.2144 \n```\n\n\n:::\n:::\n\n\n### Variance Inflation Factors (VIF)\n\nLet's calculate VIF for each predictor to check for multicollinearity.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Function to calculate VIF for a predictor index j (excluding intercept)\ncalc_vif <- function(X, j) {\n  # Regress x_j on other x's (excluding intercept col 1 and target col j)\n  # The input X includes intercept at col 1.\n  \n  y_j <- X[, j]\n  X_others <- X[, -c(j)] # Includes intercept and other predictors\n  \n  b_j <- solve(t(X_others) %*% X_others) %*% t(X_others) %*% y_j\n  y_j_hat <- X_others %*% b_j\n  \n  SST_j <- sum((y_j - mean(y_j))^2)\n  SSE_j <- sum((y_j - y_j_hat)^2)\n  R2_j <- 1 - SSE_j/SST_j\n  \n  return(1 / (1 - R2_j))\n}\n\n# Calculate VIF for predictors (columns 2, 3, 4 of X)\nvifs <- c(\n  LiveWeight = calc_vif(X, 2),\n  RibeyeArea = calc_vif(X, 3),\n  Backfat = calc_vif(X, 4)\n)\nprint(vifs)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLiveWeight RibeyeArea    Backfat \n  2.409974   2.029168   1.274524 \n```\n\n\n:::\n:::\n\n\n*   **Interpretation**: If any VIF > 5 or 10, we should be concerned. A high VIF for Live Weight might be expected if it is correlated with Ribeye Area and Backfat.\n\n### Model Selection (AIC/BIC)\n\nLet's compare the full model against a reduced model without `RibeyeArea` (assuming it might be non-significant or redundant).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Full Model AIC/BIC\nAIC_full <- n * log(SSE / n) + 2 * p\nBIC_full <- n * log(SSE / n) + p * log(n)\n\n# Reduced Model (drop RibeyeArea, col 3)\nX_red <- X[, -3]\np_red <- ncol(X_red)\n\nb_red <- solve(t(X_red) %*% X_red) %*% t(X_red) %*% y\ne_red <- y - X_red %*% b_red\nSSE_red <- sum(e_red^2)\n\nAIC_red <- n * log(SSE_red / n) + 2 * p_red\nBIC_red <- n * log(SSE_red / n) + p_red * log(n)\n\nresults <- data.frame(\n  Model = c(\"Full\", \"Reduced\"),\n  p = c(p, p_red),\n  SSE = c(SSE, SSE_red),\n  AIC = c(AIC_full, AIC_red),\n  BIC = c(BIC_full, BIC_red)\n)\nprint(results)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    Model p      SSE       AIC       BIC\n1    Full 4 9.863187 -73.16068 -65.51259\n2 Reduced 3 9.870114 -75.12558 -69.38951\n```\n\n\n:::\n:::\n\n\nWe choose the model with the lowest AIC or BIC.\n\n## Solver Implementation in R\n\nBelow is a more generalized function that computes multiple regression statistics, including AIC and BIC.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsolve_ols <- function(X, y) {\n  n <- nrow(X)\n  p <- ncol(X)\n  \n  # 1. Coefficients\n  XtX <- t(X) %*% X\n  # Check for singularity\n  if(kappa(XtX) > 1e12) warning(\"Design matrix is near singular!\")\n  XtX_inv <- solve(XtX)\n  b <- XtX_inv %*% t(X) %*% y\n  \n  # 2. Residuals and Fit\n  y_hat <- X %*% b\n  e <- y - y_hat\n  \n  # 3. Variance stats\n  SSE <- sum(e^2)\n  SST <- sum((y - mean(y))^2)\n  MSE <- SSE / (n - p)\n  R2 <- 1 - SSE/SST\n  R2_adj <- 1 - (SSE/(n-p))/(SST/(n-1))\n  \n  # 4. Standard Errors\n  var_b <- XtX_inv * MSE\n  se_b <- sqrt(diag(var_b))\n  t_vals <- b / se_b\n  p_vals <- 2 * (1 - pt(abs(t_vals), df = n - p))\n  \n  # 5. Information Criteria\n  AIC_val <- n * log(SSE/n) + 2 * p\n  BIC_val <- n * log(SSE/n) + p * log(n)\n  \n  return(list(\n    Coefficients = cbind(Estimate=b, SE=se_b, t=t_vals, p=p_vals),\n    Stats = c(R2=R2, R2_adj=R2_adj, MSE=MSE, AIC=AIC_val, BIC=BIC_val)\n  ))\n}\n\n# Test with our beef data\nresults <- solve_ols(X, y)\nprint(results)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$Coefficients\n                                SE                       \n1          4.744156614 1.231884502 3.8511375 0.0003618514\nLiveWeight 0.001727294 0.002218494 0.7785888 0.4402088099\nRibeyeArea 0.002665330 0.014828683 0.1797415 0.8581449643\nBackfat    1.389170510 0.337592512 4.1149328 0.0001587276\n\n$Stats\n         R2      R2_adj         MSE         AIC         BIC \n  0.3868084   0.3468176   0.2144171 -73.1606816 -65.5125895 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Compare with built-in R function\n# fit_lm <- lm(Marbling ~ LiveWeight + RibeyeArea + Backfat, data=df)\n# summary(fit_lm)\n# AIC(fit_lm)\n```\n:::\n\n\n## Exercises\n\n### Conceptual\n1.  Explain why partial regression coefficients can differ in sign from simple linear regression coefficients for the same variables.\n2.  Prove that $VIF_j = 1$ when predictor $x_j$ is orthogonal to all other predictors.\n3.  Why does adding a variable always increase $R^2$ but not necessarily Adjusted $R^2$ or decrease AIC?\n\n### Computational\n1.  Using the **Small Numerical Example** data:\n    a.  Calculate the sequential Sums of Squares: $R(BirthWt | Int)$, $R(DamAge | Int, BirthWt)$, and $R(Sex | Int, BirthWt, DamAge)$.\n    b.  Calculate the partial Sum of Squares for Sex: $R(Sex | Int, BirthWt, DamAge)$. Compare to the sequential result. Are they the same? Why or why not?\n    c.  Calculate the AIC for this model.\n\n2.  **Simulation Exercise**:\n    a.  Generate a dataset with $n=100$ where $x_1$ and $x_2$ have a correlation of 0.9. Let $y = x_1 + x_2 + e$.\n    b.  Fit the model $y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + e$. Check the standard errors of $\\beta_1$ and $\\beta_2$.\n    c.  Calculate VIFs.\n    d.  Fit the model $y = \\beta_0 + \\beta_1 x_1 + e$. How does the estimate of $\\beta_1$ change?\n\n### Applied\nUsing the **Beef Carcass** dataset provided in class (or the one analyzed in the chapter):\n1.  Fit a model predicting Marbling using only Backfat.\n2.  Fit the full model (LiveWt, Ribeye, Backfat).\n3.  Calculate Mallows' $C_p$ for the single-variable model (using the full model as the \"true\" estimate of $\\sigma^2$).\n4.  Based on AIC, BIC, and $C_p$, which model is preferred?\n\n## References\n\n::: {#refs}\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}