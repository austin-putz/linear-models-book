{
  "hash": "d9ec11acf2966b155db273a2b677afc0",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Week 5: Least Squares Theory\"\nformat:\n  html:\n    toc: true\n    toc-depth: 3\n    number-sections: true\n    code-fold: false\n    code-tools: true\n---\n\n::: {.callout-note icon=false}\n## Learning Objectives\n\nAfter completing this week's material, you will be able to:\n\n1. **Understand** the theoretical foundation of the **least squares criterion**\n2. **Derive** the **normal equations** from the principle of minimizing **squared error**\n3. **Prove** the key properties of **least squares estimators**, including **unbiasedness**\n4. **Understand** and **apply** the **Gauss-Markov theorem**, recognizing why LS estimators are **BLUE**\n5. **Decompose** the **total sum of squares** and estimate the **residual variance**\n6. **Utilize** **projection matrices** to understand the geometry of least squares\n:::\n\n---\n\n## Conceptual Introduction\n\nIn Week 4, we learned how to solve for the parameters of a simple linear regression model, $\\boldsymbol{b} = (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\boldsymbol{y}$. We took this solution as a given. But why this specific formula? What makes it the \"best\" way to estimate $\\boldsymbol{\\beta}$?\n\nThis week, we dive into the theoretical heart of linear models: the **principle of least squares**. The core idea is simple and intuitive: we want to find the line (or plane, in multiple regression) that is \"closest\" to all the data points simultaneously. \"Closest\" is defined as minimizing the sum of the squared vertical distances between each observation and the fitted line. This sum is called the **Residual Sum of Squares (SSE)** or **Error Sum of Squares**.\n\nBy minimizing this quantity, we find estimators that are not only logical but also possess powerful statistical properties. The most important of these is summarized by the **Gauss-Markov Theorem**, which states that under certain assumptions, the least squares estimator is the **Best Linear Unbiased Estimator (BLUE)**. This means that among a certain class of estimators, it is the one with the minimum variance.\n\nUnderstanding this theory is crucial. It gives us confidence in our estimates, allows us to derive their variances, and provides the foundation for all the hypothesis testing and diagnostics we will perform in the coming weeks.\n\n---\n\n## Mathematical Theory\n\n### The Least Squares Criterion\n\nThe fundamental goal is to find the vector of estimates, $\\boldsymbol{b}$, that minimizes the sum of squared residuals. The vector of residuals is defined as:\n\n$$\n\\boldsymbol{e} = \\boldsymbol{y} - \\hat{\\boldsymbol{y}} \n$$\n$$\n= \\boldsymbol{y} - \\mathbf{X}\\boldsymbol{b}\n$$\n\nThe sum of squared residuals, which we will denote as a function $S(\\boldsymbol{b})$, is the inner product of the residual vector with itself:\n\n$$S(\\boldsymbol{b}) = \\boldsymbol{e}'\\boldsymbol{e} = (\\boldsymbol{y} - \\mathbf{X}\\boldsymbol{b})'(\\boldsymbol{y} - \\mathbf{X}\\boldsymbol{b})$$\n\n**Visual Illustration of the Least Squares Criterion**\n\nTo understand what we're minimizing, let's visualize the residuals (errors) between observed data points and the fitted regression line. The least squares criterion finds the line that minimizes the sum of the squared vertical distances (residuals).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\n# Simple example data: milk fat vs protein\nprotein <- c(3.0, 3.2, 3.4, 3.6, 3.8)\nfat <- c(3.5, 3.8, 4.0, 4.2, 4.5)\n\n# Fit regression\nfit <- lm(fat ~ protein)\nb0 <- coef(fit)[1]\nb1 <- coef(fit)[2]\nfitted_values <- predict(fit)\nresiduals <- residuals(fit)\n\n# Create data frame for plotting\nplot_data <- data.frame(\n  protein = protein,\n  fat = fat,\n  fitted = fitted_values,\n  residual = residuals\n)\n\n# Create visualization\nggplot(plot_data, aes(x = protein, y = fat)) +\n  # Regression line\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\", linewidth = 1) +\n  # Observed points\n  geom_point(size = 3, color = \"black\") +\n  # Residual lines (vertical distances)\n  geom_segment(aes(x = protein, xend = protein,\n                   y = fat, yend = fitted),\n               color = \"red\", linetype = \"dashed\", linewidth = 0.8) +\n  # Fitted points\n  geom_point(aes(y = fitted), color = \"blue\", size = 2, shape = 1) +\n  # Labels\n  labs(title = \"Least Squares Criterion: Minimizing Residuals\",\n       subtitle = \"Minimize S(b) = Σ(yi - ŷi)²\",\n       x = \"Milk Protein %\",\n       y = \"Milk Fat %\") +\n  # Annotations\n  annotate(\"text\", x = 3.15, y = 3.65,\n           label = \"Residual~(hat(e))\",\n           color = \"red\", size = 3.5, hjust = 0, parse = TRUE) +\n  annotate(\"text\", x = 3.5, y = 3.7,\n           label = \"Fitted line (ŷ)\",\n           color = \"blue\", size = 3.5) +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(size = 11))\n```\n\n::: {.cell-output-display}\n![](Week05_LeastSquares_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nThe red dashed lines represent the residuals $e_i = y_i - \\hat{y}_i$. The least squares method finds the values of $b_0$ and $b_1$ that make the sum of the **squared lengths** of these red lines as small as possible: $SSE = \\sum_{i=1}^{n} e_i^2$.\n\nLet's expand this expression:\n\n$$\nS(\\boldsymbol{b}) = (\\boldsymbol{y}' - (\\mathbf{X}\\boldsymbol{b})')(\\boldsymbol{y} - \\mathbf{X}\\boldsymbol{b})\n$$\n$$\n= (\\boldsymbol{y}' - \\boldsymbol{b}'\\mathbf{X}')(\\boldsymbol{y} - \\mathbf{X}\\boldsymbol{b})\n$$\n$$\n= \\boldsymbol{y}'\\boldsymbol{y} - \\boldsymbol{y}'\\mathbf{X}\\boldsymbol{b} - \\boldsymbol{b}'\\mathbf{X}'\\boldsymbol{y} + \\boldsymbol{b}'\\mathbf{X}'\\mathbf{X}\\boldsymbol{b}\n$$\n\nSince $\\boldsymbol{b}'\\mathbf{X}'\\boldsymbol{y}$ is a scalar, its transpose is equal to itself: $(\\boldsymbol{b}'\\mathbf{X}'\\boldsymbol{y})' = \\boldsymbol{y}'(\\mathbf{X}')'(\\boldsymbol{b}')' = \\boldsymbol{y}'\\mathbf{X}\\boldsymbol{b}$. Therefore, the two middle terms are identical.\n\n$$S(\\boldsymbol{b}) = \\boldsymbol{y}'\\boldsymbol{y} - 2\\boldsymbol{b}'\\mathbf{X}'\\boldsymbol{y} + \\boldsymbol{b}'\\mathbf{X}'\\mathbf{X}\\boldsymbol{b}$$\n\n### Deriving the Normal Equations\n\nTo find the vector $\\boldsymbol{b}$ that minimizes this function, we need to take the derivative of $S(\\boldsymbol{b})$ with respect to $\\boldsymbol{b}$ and set it to zero. Using the rules of matrix calculus:\n\n-   $\\frac{\\partial(\\boldsymbol{a}'\\boldsymbol{x})}{\\partial\\boldsymbol{x}} = \\boldsymbol{a}$\n-   $\\frac{\\partial(\\boldsymbol{x}'\\mathbf{A}\\boldsymbol{x})}{\\partial\\boldsymbol{x}} = 2\\mathbf{A}\\boldsymbol{x}$ (for symmetric $\\mathbf{A}$)\n\nApplying these rules to $S(\\boldsymbol{b})$:\n\n$$ \\frac{\\partial S(\\boldsymbol{b})}{\\partial \\boldsymbol{b}} = -2\\mathbf{X}'\\boldsymbol{y} + 2\\mathbf{X}'\\mathbf{X}\\boldsymbol{b} $$\n\nSetting the derivative to a vector of zeros:\n\n$$ -2\\mathbf{X}'\\boldsymbol{y} + 2\\mathbf{X}'\\mathbf{X}\\boldsymbol{b} = \\mathbf{0} $$\n\n$$ \\implies \\mathbf{X}'\\mathbf{X}\\boldsymbol{b} = \\mathbf{X}'\\boldsymbol{y} $$\n\nThis fundamental result is the **Normal Equations**. Any vector $\\boldsymbol{b}$ that satisfies the normal equations is a least squares estimator of $\\boldsymbol{\\beta}$. If the matrix $\\mathbf{X}'\\mathbf{X}$ is full rank (invertible), there is a unique solution:\n\n::: {.callout-important icon=true}\n## The Least Squares Solution\n\nWhen $\\mathbf{X}'\\mathbf{X}$ is invertible (full rank), the unique least squares estimator is:\n\n$$ \\boldsymbol{b} = (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\boldsymbol{y} $$\n\nThis is the **fundamental formula** for least squares estimation. It provides the parameter estimates that minimize the sum of squared residuals. Everything we do in linear models—hypothesis testing, confidence intervals, predictions—builds from this solution.\n:::\n\n### Properties of Least Squares Estimators\n\n#### 1. Unbiasedness\n\nUnder the Gauss-Markov assumption that $E(\\boldsymbol{e}) = \\mathbf{0}$, the LS estimator $\\boldsymbol{b}$ is an unbiased estimator of $\\boldsymbol{\\beta}$.\n\n**Proof:**\n\nWe start with the solution for $\\boldsymbol{b}$ and substitute $\\boldsymbol{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{e}$.\n\n$$ \n\\begin{aligned} \n\tE(\\boldsymbol{b}) &= E[(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\boldsymbol{y}] \\\\ \n\t&= E[(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{e})] \\\\ \n\t&= E[(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{X}\\boldsymbol{\\beta} + (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\boldsymbol{e}] \\\\\n\t&= E[\\mathbf{I}\\boldsymbol{\\beta} + (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\boldsymbol{e}] \\\\ \n\t&= E[\\boldsymbol{\\beta}] + E[(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\boldsymbol{e}] \n\\end{aligned} \n$$\n\nSince $\\mathbf{X}$ and $\\boldsymbol{\\beta}$ are fixed (not random), $E[\\boldsymbol{\\beta}] = \\boldsymbol{\\beta}$. The expectation of the second term is:\n\n$$ E[(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\boldsymbol{e}] = (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'E[\\boldsymbol{e}] $$\n\nBy assumption, $E[\\boldsymbol{e}] = \\mathbf{0}$. Therefore, the second term is zero.\n\n$$ E(\\boldsymbol{b}) = \\boldsymbol{\\beta} + \\mathbf{0} = \\boldsymbol{\\beta} $$\n\nThus, the least squares estimator is unbiased. On average, it will equal the true parameter value.\n\n#### 2. Variance of `b`\n\nUnder the Gauss-Markov assumptions, $E(\\boldsymbol{e})=\\mathbf{0}$ and $Var(\\boldsymbol{e}) = \\sigma^2\\mathbf{I}$, we can derive the variance-covariance matrix of the estimator $\\boldsymbol{b}$.\n\n**Derivation:**\n\nThe variance of a vector $\\boldsymbol{b}$ is defined as $Var(\\boldsymbol{b}) = E[(\\boldsymbol{b} - E(\\boldsymbol{b}))(\\boldsymbol{b} - E(\\boldsymbol{b}))']$.\nSince $E(\\boldsymbol{b}) = \\boldsymbol{\\beta}$, we have $\\boldsymbol{b} - E(\\boldsymbol{b}) = (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\boldsymbol{e}$.\n\n$$ \n\\begin{aligned} \n\tVar(\\boldsymbol{b}) &= E[(\\boldsymbol{b} - \\boldsymbol{\\beta})(\\boldsymbol{b} - \\boldsymbol{\\beta})'] \\\\ \n\t&= E[ ((\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\boldsymbol{e}) ((\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\boldsymbol{e})' ] \\\\\n\t&= E[ (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\boldsymbol{e} \\boldsymbol{e}'\\mathbf{X}(\\mathbf{X}'\\mathbf{X})^{-1} ] \\\\ \n\t&= (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}' E[\\boldsymbol{e}\\boldsymbol{e}'] \\mathbf{X}(\\mathbf{X}'\\mathbf{X})^{-1} \n\\end{aligned} \n$$\n\nBy definition, $Var(\\boldsymbol{e}) = E[\\boldsymbol{e}\\boldsymbol{e}'] = \\sigma^2\\mathbf{I}$.\n\n$$ \n\\begin{aligned} \n\tVar(\\boldsymbol{b}) &= (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}' (\\sigma^2\\mathbf{I}) \\mathbf{X}(\\mathbf{X}'\\mathbf{X})^{-1} \\\\\n\t&= \\sigma^2 (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{X}(\\mathbf{X}'\\mathbf{X})^{-1} \\\\\n\t&= \\sigma^2 (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{I} \\\\ \n\t&= \\sigma^2 (\\mathbf{X}'\\mathbf{X})^{-1} \n\\end{aligned}\n$$\n\n::: {.callout-note}\n## Notation: What is p?\n\nThroughout this course, **p** denotes the **total number of parameters** in the model, including the intercept. For simple linear regression:\n* $\\boldsymbol{\\beta} = (\\beta_0, \\beta_1)'$ contains 2 parameters, so **p = 2**\n* The design matrix $\\mathbf{X}$ has dimensions $n \\times p$ (n rows, p columns)\n* Degrees of freedom for error: $df = n - p$\n\nIn Week 6 (Multiple Regression), we will sometimes write \"p-1 predictor variables\" to emphasize that one of the p parameters is the intercept, leaving p-1 actual covariates.\n:::\n\n::: {.callout-note}\n#### Variance-Covariance Matrix of $\\boldsymbol{b}$\nThe variance-covariance matrix of the least squares estimator is:\n$$ Var(\\boldsymbol{b}) = \\sigma^2 (\\mathbf{X}'\\mathbf{X})^{-1} $$\nThe diagonal elements of this matrix are the variances of the individual parameter estimates ($Var(b_0), Var(b_1), \\dots$), and the off-diagonal elements are the covariances ($Cov(b_i, b_j)$).\n:::\n\n### The Gauss-Markov Theorem\n\nThis is one of the most important theorems in statistics. It gives us the theoretical justification for using least squares.\n\n::: {.callout-important}\n#### The Gauss-Markov Theorem\nUnder the assumptions of the linear model ($E(\\boldsymbol{e}) = \\mathbf{0}$, $Var(\\boldsymbol{e}) = \\sigma^2\\mathbf{I}$, and $\\mathbf{X}$ is fixed), the least squares estimator $\\boldsymbol{b} = (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\boldsymbol{y}$ is the **Best Linear Unbiased Estimator (BLUE)** of $\\boldsymbol{\\beta}$.\n\n-   **Best**: Minimum variance among all linear unbiased estimators.\n-   **Linear**: It is a linear function of the observations $\\boldsymbol{y}$.\n-   **Unbiased**: $E(\\boldsymbol{b}) = \\boldsymbol{\\beta}$.\n:::\n\nThe proof is more involved, but it essentially shows that any other linear unbiased estimator has a variance that is at least as large as the variance of the LS estimator.\n\n### Projection Matrices and the Geometry of Least Squares\n\nThe geometry of least squares provides a powerful way to visualize what's happening. The key is the **hat matrix**, $\\mathbf{H}$.\n\nThe vector of fitted values $\\hat{\\boldsymbol{y}}$ is calculated as:\n$$ \\hat{\\boldsymbol{y}} = \\mathbf{X}\\boldsymbol{b} = \\mathbf{X}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\boldsymbol{y} $$\n\nLet's define the **Hat Matrix (H)** as:\n$$ \\mathbf{H} = \\mathbf{X}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}' $$\n\nSo, $\\hat{\\boldsymbol{y}} = \\mathbf{H}\\boldsymbol{y}$. The hat matrix \"puts the hat on y\". It projects the observed data vector $\\boldsymbol{y}$ onto the column space of the design matrix $\\mathbf{X}$. The fitted values $\\hat{\\boldsymbol{y}}$ are the shadow that $\\boldsymbol{y}$ casts onto the space defined by the model.\n\n**Properties of H:**\n\n1.  **Symmetric**: $\\mathbf{H}' = \\mathbf{H}$\n2.  **Idempotent**: $\\mathbf{H}\\mathbf{H} = \\mathbf{H}$ (Projecting a projection doesn\\'t change anything).\n\nThe residual vector can also be expressed using a projection matrix:\n\n$$ \n\\begin{aligned} \n\t\\boldsymbol{e} = \\boldsymbol{y} - \\hat{\\boldsymbol{y}} \\\\\n\t&= \\boldsymbol{y} - \\mathbf{H}\\boldsymbol{y} \\\\\n\t&= (\\mathbf{I} - \\mathbf{H})\\boldsymbol{y} \n\\end{aligned} \n$$\n\nThe matrix $(\\mathbf{I} - \\mathbf{H})$ is also symmetric and idempotent. It projects $\\boldsymbol{y}$ onto the space orthogonal to the column space of $\\mathbf{X}$. This is why the residuals are orthogonal to the fitted values: $\\hat{\\boldsymbol{y}}'\\boldsymbol{e} = 0$.\n\n### Sum of Squares Decomposition\n\nThe total variation in the data can be partitioned into variation explained by the model and the unexplained (residual) variation.\n\n-   **Total Sum of Squares (SST)**: Measures the total variation of y around its mean.\n    $$ SST = \\sum(y_i - \\bar{y})^2 = \\boldsymbol{y}'\\boldsymbol{y} - n\\bar{y}^2 $$\n-   **Model Sum of Squares (SSM)**: Measures the variation explained by the regression model.\n    $$ SSM = \\sum(\\hat{y}_i - \\bar{y})^2 = \\boldsymbol{b}'\\mathbf{X}'\\boldsymbol{y} - n\\bar{y}^2 $$\n-   **Error Sum of Squares (SSE)**: Measures the unexplained variation.\n    $$ SSE = \\sum(y_i - \\hat{y}_i)^2 = \\boldsymbol{e}'\\boldsymbol{e} = \\boldsymbol{y}'\\boldsymbol{y} - \\boldsymbol{b}'\\mathbf{X}'\\boldsymbol{y} $$\n\nA fundamental identity is:\n$$ SST = SSM + SSE $$\n\n### Variance Estimation\n\nOur $Var(\\boldsymbol{b})$ formula depends on the unknown population parameter $\\sigma^2$. We must estimate it from the data. The estimator for $\\sigma^2$ is the **Mean Square Error (MSE)**.\n\n$$ \\hat{\\sigma}^2 = MSE = \\frac{SSE}{n-p} $$\n\nWhere:\n-   $n$ = number of observations\n-   $p$ = number of parameters in $\\boldsymbol{\\beta}$ (including the intercept), which is equal to the rank of $\\mathbf{X}$.\n\nThe quantity $(n-p)$ is the degrees of freedom for error. We can show that $E(MSE) = \\sigma^2$, so it is an unbiased estimator.\n\nWith this, we can estimate the variance-covariance matrix of $\\boldsymbol{b}$:\n$$ \\widehat{Var}(\\boldsymbol{b}) = \\hat{\\sigma}^2 (\\mathbf{X}'\\mathbf{X})^{-1} $$\nThe standard error of an individual estimate $b_j$ is the square root of the $j$-th diagonal element of this matrix.\n\n### Distribution Theory (Under Normality Assumption)\n\nIf we add the assumption that the errors are normally distributed, $\\boldsymbol{e} \\sim N(\\mathbf{0}, \\sigma^2\\mathbf{I})$, we can derive the distributions of our estimators, which is essential for hypothesis testing.\n\n-   $\boldsymbol{y} \\sim N(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2\\mathbf{I})$\n-   $\boldsymbol{b} \\sim N(\\boldsymbol{\\beta}, \\sigma^2(\\mathbf{X}'\\mathbf{X})^{-1})$\n-   $\frac{SSE}{\\sigma^2} \\sim \\chi^2_{n-p}$ (Chi-squared distribution with $n-p$ degrees of freedom)\n-   $\boldsymbol{b}$ and $SSE$ are statistically independent.\n\nThese results lead directly to the t-tests for individual coefficients and the F-test for the overall model, which we will use extensively.\n\n---\n\n## Small Numerical Example\n\nLet's analyze the relationship between milk protein percentage (x) and milk fat percentage (y) in a sample of 5 dairy cows.\n\n| Protein % (x) | Fat % (y) |\n|:-------------:|\n| 3.0           | 3.5       |\n| 3.2           | 3.8       |\n| 3.4           | 4.0       |\n| 3.6           | 4.2       |\n| 3.8           | 4.5       |\n\nThe model is $y_i = \\beta_0 + \\beta_1 x_i + e_i$.\n\n**1. Construct Matrices**\n\n$$ \\boldsymbol{y} = \\begin{pmatrix} 3.5 \\\\ 3.8 \\\\ 4.0 \\\\ 4.2 \\\\ 4.5 \\end{pmatrix}, \\quad \\mathbf{X} = \\begin{pmatrix} 1 & 3.0 \\\\ 1 & 3.2 \\\\ 1 & 3.4 \\\\ 1 & 3.6 \\\\ 1 & 3.8 \\end{pmatrix} $$\n\n**2. Calculate $\\mathbf{X}'\\mathbf{X}$ and $\\mathbf{X}'\\boldsymbol{y}$**\n\n$$ \\mathbf{X}'\\mathbf{X} = \\begin{pmatrix} 1 & 1 & 1 & 1 & 1 \\\\ 3.0 & 3.2 & 3.4 & 3.6 & 3.8 \\end{pmatrix} \\begin{pmatrix} 1 & 3.0 \\\\ 1 & 3.2 \\\\ 1 & 3.4 \\\\ 1 & 3.6 \\\\ 1 & 3.8 \\end{pmatrix} = \\begin{pmatrix} 5 & 17.0 \\\\ 17.0 & 58.6 \\end{pmatrix} $$\n\n$$ \\mathbf{X}'\\boldsymbol{y} = \\begin{pmatrix} 1 & 1 & 1 & 1 & 1 \\\\ 3.0 & 3.2 & 3.4 & 3.6 & 3.8 \\end{pmatrix} \\begin{pmatrix} 3.5 \\\\ 3.8 \\\\ 4.0 \\\\ 4.2 \\\\ 4.5 \\end{pmatrix} = \\begin{pmatrix} 20.0 \\\\ 68.34 \\end{pmatrix} $$\n\n**3. Find the Inverse $(\\mathbf{X}'\\mathbf{X})^{-1}$**\n\nThe determinant is $det(\\mathbf{X}'\\mathbf{X}) = (5)(58.6) - (17.0)(17.0) = 293 - 289 = 4$.\nThe inverse is:\n$$ (\\mathbf{X}'\\mathbf{X})^{-1} = \\frac{1}{4} \\begin{pmatrix} 58.6 & -17.0 \\\\ -17.0 & 5 \\end{pmatrix} = \\begin{pmatrix} 14.65 & -4.25 \\\\ -4.25 & 1.25 \\end{pmatrix} $$\n\n**4. Solve for $\\boldsymbol{b}$**\n\n$$ \\boldsymbol{b} = (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\boldsymbol{y} = \\begin{pmatrix} 14.65 & -4.25 \\\\ -4.25 & 1.25 \\end{pmatrix} \\begin{pmatrix} 20.0 \\\\ 68.34 \\end{pmatrix} = \\begin{pmatrix} 293 - 290.445 \\\\ -85 + 85.425 \\end{pmatrix} = \\begin{pmatrix} 2.555 \\\\ 0.425 \\end{pmatrix} $$\n\nSo, $b_0 = 2.555$ and $b_1 = 0.425$. The fitted equation is $\\hat{y} = 2.555 + 0.425x$.\n\n**5. Decompose Sums of Squares**\n\nFirst, find $\\boldsymbol{b}'\\mathbf{X}'\\boldsymbol{y}$:\n$$ \\boldsymbol{b}'\\mathbf{X}'\\boldsymbol{y} = \\begin{pmatrix} 2.555 & 0.425 \\end{pmatrix} \\begin{pmatrix} 20.0 \\\\ 68.34 \\end{pmatrix} = 51.1 + 29.0445 = 80.1445 $$\nAnd $\\boldsymbol{y}'\\boldsymbol{y}$:\n$$ \\boldsymbol{y}'\\boldsymbol{y} = 3.5^2 + 3.8^2 + 4.0^2 + 4.2^2 + 4.5^2 = 12.25 + 14.44 + 16.0 + 17.64 + 20.25 = 80.58 $$\nThe mean of y is $\\bar{y} = 20.0 / 5 = 4.0$.\n\n-   $SSE = \\boldsymbol{y}'\\boldsymbol{y} - \\boldsymbol{b}'\\mathbf{X}'\\boldsymbol{y} = 80.58 - 80.1445 = 0.4355$\n-   $SST = \\boldsymbol{y}'\\boldsymbol{y} - n\\bar{y}^2 = 80.58 - 5(4.0)^2 = 80.58 - 80 = 0.58$\n-   $SSM = SST - SSE = 0.58 - 0.4355 = 0.1445$\n\n**6. Estimate Variance and Construct Confidence Intervals**\n\nHere, $n=5$ and $p=2$.\n$$ \\hat{\\sigma}^2 = MSE = \\frac{SSE}{n-p} = \\frac{0.4355}{5-2} = 0.145167 $$\n\nNow find the estimated variance of $\\boldsymbol{b}$:\n$$ \\widehat{Var}(\\boldsymbol{b}) = \\hat{\\sigma}^2 (\\mathbf{X}'\\mathbf{X})^{-1} = 0.145167 \\begin{pmatrix} 14.65 & -4.25 \\\\ -4.25 & 1.25 \\end{pmatrix} = \\begin{pmatrix} 2.1267 & -0.6169 \\\\ -0.6169 & 0.1815 \\end{pmatrix} $$\nSo, $\\widehat{Var}(b_1) = 0.1815$, and the standard error is $se(b_1) = \\sqrt{0.1815} = 0.426$.\n\nLet's construct a 95% confidence interval for $\\beta_1$. The critical t-value for $df = n-p=3$ is $t_{0.025, 3} = 3.182$.\n$$ CI = b_1 \\pm t_{0.025, 3} \\times se(b_1) = 0.425 \\pm 3.182 \\times 0.426 = 0.425 \\pm 1.355 $$\nCI = (-0.93, 1.78). Since this interval contains 0, we cannot conclude that there is a significant linear relationship between milk protein and fat percentage in this very small sample.\n\n---\n\n## Realistic Livestock Application\n\n**Scenario**: We want to predict the weaning weight of lambs based on their birth weight. We have data from 40 lambs from a research flock.\n\n**Objective**: Fit a simple linear regression, compute the full ANOVA table, and test the significance of the regression.\n\nFirst, let's load and view a sample of the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlamb_data <- read.csv(\"data/lamb_data.csv\")\nhead(lamb_data)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| Birth_Weight_kg| Wean_Weight_kg|\n|---------------:|--------------:|\n|             5.3|           42.0|\n|             4.2|           35.1|\n|             4.7|           40.4|\n|             4.9|           37.7|\n|             4.7|           35.1|\n|             4.4|           38.1|\n\n</div>\n:::\n:::\n\n\nWe fit the model: `Wean_Weight_kg` = $\\beta_0 + \\beta_1$`Birth_Weight_kg` + $e$.\n\nWe will perform the calculations manually using matrix algebra in R, then construct the ANOVA table.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Matrix calculations\ny <- lamb_data$Wean_Weight_kg\nx <- lamb_data$Birth_Weight_kg\nn <- length(y)\nX <- cbind(1, x)\n\n# Solve normal equations\nXtX <- t(X) %*% X\nXty <- t(X) %*% y\nb <- solve(XtX) %*% Xty\n\n# Sum of Squares\nybar <- mean(y)\nb_Xty <- t(b) %*% Xty\nyty <- t(y) %*% y\n\nSST <- yty - n * ybar^2\nSSE <- yty - b_Xty\nSSM <- SST - SSE\n\n# ANOVA Table components\np <- 2 # number of parameters\ndf_M <- p - 1\ndf_E <- n - p\ndf_T <- n - 1\n\nMSM <- SSM / df_M\nMSE <- SSE / df_E\nF_stat <- MSM / MSE\n\n# Print results\ncat(\"Parameter Estimates (b0, b1):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nParameter Estimates (b0, b1):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       [,1]\n  15.476140\nx  4.938749\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\n--- ANOVA Table ---\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n--- ANOVA Table ---\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"% -10s %4s %8s %8s %8s\\n\", \"Source\", \"df\", \"SS\", \"MS\", \"F\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSource       df       SS       MS        F\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"% -10s %4d %8.2f %8.2f %8.2f\\n\", \"Model\", df_M, SSM, MSM, F_stat))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel         1   512.41   512.41    95.25\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"% -10s %4d %8.2f %8.2f\\n\", \"Error\", df_E, SSE, MSE))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nError        38   204.43     5.38\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"% -10s %4d %8.2f\\n\", \"Total\", df_T, SST))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTotal        39   716.84\n```\n\n\n:::\n:::\n\n\n**Interpretation**:\n\nThe F-statistic is very large (215.11). The critical F-value for 1 and 38 degrees of freedom at $\\alpha=0.05$ is approximately 4.10. Since our calculated F-statistic is much larger, we reject the null hypothesis $H_0: \\beta_1 = 0$. There is a highly significant linear relationship between birth weight and weaning weight in these lambs. The model explains a large proportion of the total variation in weaning weight.\n\n---\n\n## Solver Implementation in R\n\nLet's implement the core theoretical concepts from this week in R and verify their properties. We will use the small dairy cow data example.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Data from the small example\ny <- c(3.5, 3.8, 4.0, 4.2, 4.5)\nx <- c(3.0, 3.2, 3.4, 3.6, 3.8)\nn <- length(y)\np <- 2\n\n# Design Matrix\nX <- cbind(1, x)\n\n# 1. Hat Matrix (H) and Projection\nH <- X %*% solve(t(X) %*% X) %*% t(X)\ncat(\"Hat Matrix H:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nHat Matrix H:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(H, 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3] [,4] [,5]\n[1,]  0.6  0.4  0.2  0.0 -0.2\n[2,]  0.4  0.3  0.2  0.1  0.0\n[3,]  0.2  0.2  0.2  0.2  0.2\n[4,]  0.0  0.1  0.2  0.3  0.4\n[5,] -0.2  0.0  0.2  0.4  0.6\n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify properties of H\n# H should be symmetric\nis_symmetric <- all.equal(H, t(H))\ncat(\"\\nIs H symmetric? \", is_symmetric, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nIs H symmetric?  TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\n# H should be idempotent (H*H = H)\nH_squared <- H %*% H\nis_idempotent <- all.equal(H, H_squared)\ncat(\"Is H idempotent? \", is_idempotent, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nIs H idempotent?  TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\n# 2. Get fitted values and residuals using H\ny_hat <- H %*% y\nresiduals <- (diag(n) - H) %*% y\n\ncat(\"\\nFitted values (y_hat):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nFitted values (y_hat):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(y_hat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]\n[1,] 3.52\n[2,] 3.76\n[3,] 4.00\n[4,] 4.24\n[5,] 4.48\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nResiduals (e):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nResiduals (e):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(residuals)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              [,1]\n[1,] -2.000000e-02\n[2,]  4.000000e-02\n[3,]  1.432188e-14\n[4,] -4.000000e-02\n[5,]  2.000000e-02\n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify that residuals are orthogonal to fitted values\northogonality_check <- t(y_hat) %*% residuals\ncat(\"\\nInner product of y_hat and e (should be ~0): \", orthogonality_check, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nInner product of y_hat and e (should be ~0):  4.606454e-13 \n```\n\n\n:::\n\n```{.r .cell-code}\n# 3. Sum of Squares calculation\n# Using matrix formulas\nb <- solve(t(X)%*%X) %*% t(X)%*%y\nSST_mat <- t(y) %*% y - n * mean(y)^2\nSSE_mat <- t(y) %*% y - t(b) %*% t(X) %*% y\nSSM_mat <- t(b) %*% t(X) %*% y - n * mean(y)^2\n\ncat(\"\\n--- Sum of Squares ---\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n--- Sum of Squares ---\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"SST: \", SST_mat, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSST:  0.58 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"SSM: \", SSM_mat, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSSM:  0.576 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"SSE: \", SSE_mat, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSSE:  0.004 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"SSM + SSE: \", SSM_mat + SSE_mat, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSSM + SSE:  0.58 \n```\n\n\n:::\n\n```{.r .cell-code}\n# 4. Variance Estimation\nsigma2_hat <- SSE_mat / (n - p)\ncat(\"\\nEstimated Residual Variance (sigma^2): \", c(sigma2_hat), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nEstimated Residual Variance (sigma^2):  0.001333333 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Variance-Covariance matrix of b\nvar_b <- solve(t(X) %*% X) * c(sigma2_hat)\ncat(\"\\nVariance-Covariance Matrix of b:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVariance-Covariance Matrix of b:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(var_b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                         x\n   0.03880000 -0.011333333\nx -0.01133333  0.003333333\n```\n\n\n:::\n\n```{.r .cell-code}\n# Standard error of b1\nse_b1 <- sqrt(var_b[2, 2])\ncat(\"\\nStandard Error of b1: \", se_b1, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nStandard Error of b1:  0.05773503 \n```\n\n\n:::\n\n```{.r .cell-code}\n# 5. Compare with lm() output\nfit <- lm(y ~ x)\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n         1          2          3          4          5 \n-2.000e-02  4.000e-02 -5.204e-18 -4.000e-02  2.000e-02 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.08000    0.19698  -0.406 0.711874    \nx            1.20000    0.05774  20.785 0.000244 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.03651 on 3 degrees of freedom\nMultiple R-squared:  0.9931,\tAdjusted R-squared:  0.9908 \nF-statistic:   432 on 1 and 3 DF,  p-value: 0.0002436\n```\n\n\n:::\n:::\n\n\nAs you can see, our manually calculated values for the estimates, standard errors, residual variance ($\\hat{\\sigma}^2$, called \"Residual standard error\" squared in the `lm` summary), and sums of squares all match the output from R's built-in `lm()` function. This confirms our understanding of the underlying theory.\n\n**Previous**: [Week 4: Simple Linear Regression](../Week04_SimpleRegression/Week04_SimpleRegression.qmd)\n\n**Next**: [Week 6: Multiple Regression](../Week06_MultipleRegression/Week06_MultipleRegression.qmd)\n",
    "supporting": [
      "Week05_LeastSquares_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}