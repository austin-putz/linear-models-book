{
  "hash": "cd2e96bc96079b7aff2b0ae5bee98321",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Week 13: Special Topics I\"\nsubtitle: \"Unbalanced Data, Generalized Inverses, and Constraint Systems\"\nauthor: \"Linear Models for Animal Breeding and Genetics\"\ndate: today\nformat:\n  html:\n    toc: true\n    toc-depth: 4\n    toc-location: left\n    number-sections: true\n    number-depth: 3\n    code-fold: false\n    code-tools: true\n    code-copy: true\n    highlight-style: github\n    theme: cosmo\n    css: ../styles.css\n    embed-resources: true\nbibliography: ../references.bib\n---\n\n::: {.callout-note icon=false}\n## Learning Objectives\n\nAfter completing this week, you will be able to:\n\n1. **Diagnose** when **unbalanced data** causes problems and select appropriate **analytical strategies**\n2. **Distinguish** between different types of **generalized inverses** and understand when each is appropriate\n3. **Apply** different **constraint systems** (set-to-zero, sum-to-zero, custom) and interpret their effects on **parameter estimates**\n4. **Identify** **estimable** vs. **non-estimable functions** and correctly interpret results from **rank-deficient models**\n5. **Analyze** real **genetic evaluation data** with unequal information and understand the limitations of **fixed-effects least squares**\n:::\n\n---\n\n# Introduction: Making Sense of Messy Data\n\nIn Weeks 1-12, we've built a solid foundation in linear models: from simple regression through ANOVA, diagnostics, and rank-deficient models. Throughout, we've often used balanced designs for clarity. But real livestock data is rarely balanced.\n\nConsider these common scenarios in animal breeding and genetics:\n\n- **Dairy sire evaluation**: Popular sires have 50+ daughters; young sires have only 3-5\n- **Multi-location trials**: Not all breeds are available at all farms due to management constraints\n- **Feedlot studies**: Pens vary in size; animals are lost due to health issues or data quality problems\n- **Genetic evaluation**: Family sizes vary dramatically; some animals have no progeny records\n\nThis week synthesizes concepts from multiple previous weeks to provide practical strategies for handling these real-world challenges:\n\n::: {.callout-note}\n## Connections to Previous Weeks\n\n- **Week 2**: Generalized inverses (mathematical foundation)\n- **Week 7**: One-way ANOVA with balanced data (ideal case)\n- **Week 8**: Estimable functions and contrasts (theoretical framework)\n- **Week 12**: Non-full rank models and unequal subclass numbers (immediate prerequisite)\n\nIf you need to review rank deficiency, generalized inverses, or estimability, revisit those weeks before proceeding.\n:::\n\n## What We'll Cover This Week\n\nThis week is organized around four main topics:\n\n**Topic A: Strategic Approaches to Unbalanced Data**\nWhen does unbalance cause problems? What solutions work best in different situations?\n\n**Topic B: Types of Generalized Inverses**\nReflexive g-inverses, Moore-Penrose inverses, and conditional inverses—what's the difference and when does it matter?\n\n**Topic C: Constraint Systems**\nHow do set-to-zero and sum-to-zero constraints affect parameter estimates? How should we interpret results?\n\n**Topic D: Interpreting Non-Estimable Parameters**\nWhat can we legitimately report from rank-deficient models? What must we avoid?\n\n**Capstone Example: Dairy Sire Evaluation**\nA comprehensive analysis of 10 sires with 3-25 daughters each, illustrating all concepts and previewing the need for mixed models (Week 14).\n\nLet's begin.\n\n---\n\n# Topic A: Strategic Approaches to Unbalanced Data\n\n## When is Unbalanced Data a Problem?\n\nNot all unbalanced data creates problems. To understand when issues arise, let's first review what makes balanced designs special.\n\n### Properties of Balanced Designs\n\nIn a **balanced design**, all cells have equal sample sizes: $n_{ij} = n$ for all combinations of factors.\n\nFor example, in a 2×2 factorial (Factor A: 2 levels, Factor B: 2 levels):\n\n```\n          Factor B\n          B₁    B₂\nFactor A\n  A₁      n     n\n  A₂      n     n\n```\n\nThis balance creates several nice properties:\n\n1. **Orthogonality**: After centering, $\\mathbf{X}'\\mathbf{X}$ is (nearly) diagonal\n2. **SS equivalence**: Type I = Type II = Type III sums of squares\n3. **Independence**: Main effects are independent of interactions\n4. **Simple interpretation**: Effects have clear, unconfounded meanings\n\n### Consequences of Unbalanced Designs\n\nWhen sample sizes differ across cells, we lose these properties:\n\n1. **Loss of orthogonality**: $\\mathbf{X}'\\mathbf{X}$ has substantial off-diagonal elements\n2. **SS divergence**: Type I, II, and III SS can differ dramatically\n3. **Confounding**: Effects become correlated; interpretation becomes ambiguous\n4. **Estimability issues**: With missing cells, some parameters may not be estimable\n\nLet's see this with a concrete example.\n\n### Numerical Demonstration: Balanced vs. Unbalanced\n\nConsider broiler body weight (kg) by strain (A, B) and sex (Male, Female).\n\n#### Balanced Case\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Balanced data: n = 2 per cell\nstrain <- rep(c(\"A\", \"A\", \"B\", \"B\"), 2)\nsex <- rep(c(\"Male\", \"Female\"), each = 4)\nweight <- c(2.1, 2.3,  # A-Male\n            1.8, 1.9,  # A-Female\n            2.4, 2.5,  # B-Male\n            1.9, 2.0)  # B-Female\n\n# Show cell counts\ntable(strain, sex)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      sex\nstrain Female Male\n     A      2    2\n     B      2    2\n```\n\n\n:::\n\n```{.r .cell-code}\n# Fit model\nfit_bal <- lm(weight ~ strain * sex)\nanova(fit_bal)  # Type I SS\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|           | Df|  Sum Sq| Mean Sq|   F value|    Pr(>F)|\n|:----------|--:|-------:|-------:|---------:|---------:|\n|strain     |  1| 0.36125| 0.36125| 41.285714| 0.0030164|\n|sex        |  1| 0.06125| 0.06125|  7.000000| 0.0572352|\n|strain:sex |  1| 0.01125| 0.01125|  1.285714| 0.3201880|\n|Residuals  |  4| 0.03500| 0.00875|        NA|        NA|\n\n</div>\n:::\n:::\n\n\n#### Unbalanced Case\n\nNow suppose we have unequal cell sizes:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Unbalanced data\nstrain_unb <- c(rep(\"A\", 3), rep(\"A\", 2), rep(\"B\", 2), rep(\"B\", 3))\nsex_unb <- c(rep(\"Male\", 3), rep(\"Female\", 2), rep(\"Male\", 2), rep(\"Female\", 3))\nweight_unb <- c(2.1, 2.3, 2.2,  # A-Male (n=3)\n                1.8, 1.9,       # A-Female (n=2)\n                2.4, 2.5,       # B-Male (n=2)\n                1.9, 2.0, 2.1)  # B-Female (n=3)\n\n# Cell counts\ntable(strain_unb, sex_unb)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          sex_unb\nstrain_unb Female Male\n         A      2    3\n         B      3    2\n```\n\n\n:::\n\n```{.r .cell-code}\n# Fit model\nfit_unb <- lm(weight_unb ~ strain_unb * sex_unb)\n\n# Type I SS (strain first)\ncat(\"\\n=== Type I SS: Strain first ===\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n=== Type I SS: Strain first ===\n```\n\n\n:::\n\n```{.r .cell-code}\nanova(fit_unb)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|                   | Df| Sum Sq|   Mean Sq| F value|    Pr(>F)|\n|:------------------|--:|------:|---------:|-------:|---------:|\n|strain_unb         |  1|  0.036| 0.0360000|    4.32| 0.0829111|\n|sex_unb            |  1|  0.384| 0.3840000|   46.08| 0.0005000|\n|strain_unb:sex_unb |  1|  0.006| 0.0060000|    0.72| 0.4286915|\n|Residuals          |  6|  0.050| 0.0083333|      NA|        NA|\n\n</div>\n:::\n\n```{.r .cell-code}\n# Type I SS (sex first)\nfit_unb2 <- lm(weight_unb ~ sex_unb * strain_unb)\ncat(\"\\n=== Type I SS: Sex first ===\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n=== Type I SS: Sex first ===\n```\n\n\n:::\n\n```{.r .cell-code}\nanova(fit_unb2)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|                   | Df| Sum Sq|   Mean Sq| F value|    Pr(>F)|\n|:------------------|--:|------:|---------:|-------:|---------:|\n|sex_unb            |  1|  0.324| 0.3240000|   38.88| 0.0007874|\n|strain_unb         |  1|  0.096| 0.0960000|   11.52| 0.0146014|\n|sex_unb:strain_unb |  1|  0.006| 0.0060000|    0.72| 0.4286915|\n|Residuals          |  6|  0.050| 0.0083333|      NA|        NA|\n\n</div>\n:::\n\n```{.r .cell-code}\n# Type III SS\nlibrary(car)\ncat(\"\\n=== Type III SS ===\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n=== Type III SS ===\n```\n\n\n:::\n\n```{.r .cell-code}\nAnova(fit_unb, type = 3)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|                   | Sum Sq| Df| F value|    Pr(>F)|\n|:------------------|------:|--:|-------:|---------:|\n|(Intercept)        |  6.845|  1|  821.40| 0.0000001|\n|strain_unb         |  0.027|  1|    3.24| 0.1219524|\n|sex_unb            |  0.147|  1|   17.64| 0.0056858|\n|strain_unb:sex_unb |  0.006|  1|    0.72| 0.4286915|\n|Residuals          |  0.050|  6|      NA|        NA|\n\n</div>\n:::\n:::\n\n\n**Key Observations**:\n\n1. In the balanced case, Type I SS (strain first) = Type I SS (sex first)\n2. In the unbalanced case, **order matters**! SS values differ depending on which factor enters first\n3. Type III SS adjust each effect for all others, regardless of order\n\n### Examining the Design Matrix Structure\n\nLet's look at why this happens by examining $\\mathbf{X}'\\mathbf{X}$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Design matrix for unbalanced case\nX_unb <- model.matrix(~ strain_unb * sex_unb)\nXtX_unb <- t(X_unb) %*% X_unb\n\ncat(\"X'X for unbalanced design:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nX'X for unbalanced design:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(XtX_unb)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                        (Intercept) strain_unbB sex_unbMale\n(Intercept)                      10           5           5\nstrain_unbB                       5           5           2\nsex_unbMale                       5           2           5\nstrain_unbB:sex_unbMale           2           2           2\n                        strain_unbB:sex_unbMale\n(Intercept)                                   2\nstrain_unbB                                   2\nsex_unbMale                                   2\nstrain_unbB:sex_unbMale                       2\n```\n\n\n:::\n\n```{.r .cell-code}\n# Note: off-diagonal elements are non-zero\n# This indicates non-orthogonality (correlation between predictors)\n\n# For comparison, balanced case:\nX_bal <- model.matrix(~ strain * sex)\nXtX_bal <- t(X_bal) %*% X_bal\ncat(\"\\nX'X for balanced design:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nX'X for balanced design:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(XtX_bal)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                (Intercept) strainB sexMale strainB:sexMale\n(Intercept)               8       4       4               2\nstrainB                   4       4       2               2\nsexMale                   4       2       4               2\nstrainB:sexMale           2       2       2               2\n```\n\n\n:::\n\n```{.r .cell-code}\n# Balanced design has more structure (some zeros in off-diagonals after centering)\n```\n:::\n\n\n::: {.callout-important}\n## When Unbalance is Problematic\n\nUnbalanced data itself is not always problematic. The issue arises when unbalance creates:\n\n1. **Confounding**: Effects cannot be separated clearly\n2. **Rank deficiency**: Missing cells make $\\mathbf{X}'\\mathbf{X}$ singular\n3. **Interpretation ambiguity**: Type I/II/III SS tell different stories\n\nIf you have unequal $n_{ij}$ but all cells are represented and you use appropriate methods, unbalance is manageable.\n:::\n\n## Strategic Solutions\n\nWhen faced with unbalanced data, you have several strategies. Choose based on the nature and severity of the imbalance.\n\n### Strategy 1: Use Estimable Functions (Contrasts)\n\n**Principle**: Focus on what can be uniquely estimated, regardless of parameterization.\n\nEven when individual parameters (like $\\alpha_i$ in $\\mu + \\alpha_i$) are not uniquely estimable, **contrasts** (differences between parameters) often are.\n\n**Example**: In sire evaluation with unequal progeny:\n- Individual sire effects $s_i$ are not uniquely estimable (confounded with $\\mu$)\n- But sire **differences** $s_i - s_j$ are estimable and biologically meaningful\n\n**When to use**:\n- Rank-deficient models (overparameterized)\n- Primary interest is in comparisons, not absolute values\n\n**R Implementation**:\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example: 3 breeds, unequal sample sizes\nbreed <- c(rep(\"A\", 3), rep(\"B\", 2), rep(\"C\", 4))\nyield <- c(10, 12, 11,  # Breed A\n           14, 15,      # Breed B\n           16, 17, 18, 19)  # Breed C\n\nfit <- lm(yield ~ breed)\n\n# Use emmeans for estimable contrasts\nlibrary(emmeans)\nemm <- emmeans(fit, \"breed\")\n\n# All pairwise contrasts (all estimable!)\npairs(emm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n contrast estimate    SE df t.ratio p.value\n A - B        -3.5 1.020  6  -3.429  0.0323\n A - C        -6.5 0.854  6  -7.612  0.0007\n B - C        -3.0 0.968  6  -3.098  0.0482\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n```\n\n\n:::\n:::\n\n\n### Strategy 2: Cell Means Model\n\n**Principle**: Use a model that is **always full rank**, regardless of balance.\n\nThe cell means model:\n$$y_{ij} = \\mu_i + e_{ij}$$\n\nestimates each group mean directly. If there are $g$ groups with non-zero counts, then:\n- $\\mathbf{X}$ is $n \\times g$ with rank $r(\\mathbf{X}) = g$ (full rank!)\n- All $\\mu_i$ are uniquely estimable (no constraints needed)\n- Comparisons are done post-hoc via contrasts\n\n**Trade-off**: More parameters, but all estimable and interpretation is straightforward.\n\n**When to use**:\n- Severe unbalance with missing cells\n- Want to avoid rank deficiency issues\n- Primary interest is in group means, not \"effects\"\n\n**R Implementation**:\n\n::: {.cell}\n\n```{.r .cell-code}\n# Cell means model: remove intercept with \"-1\"\nfit_cm <- lm(yield ~ breed - 1)\nsummary(fit_cm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = yield ~ breed - 1)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n  -1.5   -0.5    0.0    0.5    1.5 \n\nCoefficients:\n       Estimate Std. Error t value Pr(>|t|)    \nbreedA  11.0000     0.6455   17.04 2.61e-06 ***\nbreedB  14.5000     0.7906   18.34 1.69e-06 ***\nbreedC  17.5000     0.5590   31.30 7.06e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.118 on 6 degrees of freedom\nMultiple R-squared:  0.9963,\tAdjusted R-squared:  0.9944 \nF-statistic: 535.6 on 3 and 6 DF,  p-value: 1.125e-07\n```\n\n\n:::\n\n```{.r .cell-code}\n# Each coefficient is a breed mean (all estimable!)\ncoef(fit_cm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nbreedA breedB breedC \n  11.0   14.5   17.5 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Same contrasts as before\nemm_cm <- emmeans(fit_cm, \"breed\")\npairs(emm_cm)  # Identical to effects model contrasts\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n contrast estimate    SE df t.ratio p.value\n A - B        -3.5 1.020  6  -3.429  0.0323\n A - C        -6.5 0.854  6  -7.612  0.0007\n B - C        -3.0 0.968  6  -3.098  0.0482\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n```\n\n\n:::\n:::\n\n\n### Strategy 3: Choose Appropriate Sums of Squares Type\n\n**Principle**: Different SS types test different hypotheses. Choose based on your research question.\n\n#### Type I (Sequential) Sums of Squares\n\n- Effects added **sequentially** in the order specified\n- SS(A) = variation explained by A alone\n- SS(B|A) = additional variation explained by B after accounting for A\n- SS(A×B|A,B) = additional variation explained by interaction after A and B\n\n**Order matters!** SS(A) ≠ SS(A|B)\n\n**When to use**:\n- Logical ordering of predictors exists (e.g., covariates before treatments)\n- Hierarchical model building\n\n**R Default**: `anova(fit)` gives Type I SS\n\n#### Type II Sums of Squares\n\n- Each main effect adjusted for other main effects, **but not interactions**\n- SS(A|B) but not SS(A|B, A×B)\n- More powerful when interactions are absent or negligible\n\n**When to use**:\n- No significant interactions\n- Main effects are primary interest\n\n**R Implementation**: `car::Anova(fit, type=2)`\n\n#### Type III (Marginal) Sums of Squares\n\n- Each effect adjusted for **all other effects**, including interactions\n- SS(A|B, A×B): effect of A given everything else\n- Most conservative; commonly used for unbalanced data\n\n**When to use**:\n- Unbalanced designs (most common choice)\n- Interactions may be present\n- Want to test each effect \"in the presence of\" all others\n\n**R Implementation**: `car::Anova(fit, type=3)`\n\n::: {.callout-warning}\n## Choosing SS Type\n\nWith severe unbalance and missing cells, Type III SS can be difficult to interpret. The \"adjusted\" means may not correspond to observable data patterns.\n\n**Best practice**:\n1. Start with Type III for unbalanced data\n2. Examine cell means and sample sizes\n3. Use estimable contrasts for inference\n4. Report which SS type you used\n:::\n\n#### Comparison Example\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Unbalanced 2-way ANOVA\nstrain_2 <- c(rep(\"A\", 5), rep(\"A\", 2), rep(\"B\", 3), rep(\"B\", 6))\nsex_2 <- c(rep(\"M\", 5), rep(\"F\", 2), rep(\"M\", 3), rep(\"F\", 6))\nbw <- c(2.1, 2.2, 2.3, 2.2, 2.4,  # A-M\n        1.9, 1.8,                  # A-F\n        2.5, 2.6, 2.4,             # B-M\n        2.0, 2.1, 1.9, 2.0, 2.2, 2.1)  # B-F\n\nfit_comp <- lm(bw ~ strain_2 * sex_2)\n\ncat(\"=== Type I SS (Strain first) ===\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n=== Type I SS (Strain first) ===\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(anova(fit_comp))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: bw\n               Df  Sum Sq Mean Sq F value    Pr(>F)    \nstrain_2        1 0.02009 0.02009  1.8263    0.2015    \nsex_2           1 0.61929 0.61929 56.2987 7.202e-06 ***\nstrain_2:sex_2  1 0.00300 0.00300  0.2727    0.6110    \nResiduals      12 0.13200 0.01100                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\n=== Type I SS (Sex first) ===\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n=== Type I SS (Sex first) ===\n```\n\n\n:::\n\n```{.r .cell-code}\nfit_comp2 <- lm(bw ~ sex_2 * strain_2)\nprint(anova(fit_comp2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: bw\n               Df  Sum Sq Mean Sq F value    Pr(>F)    \nsex_2           1 0.45563 0.45563 41.4205 3.227e-05 ***\nstrain_2        1 0.18375 0.18375 16.7045  0.001507 ** \nsex_2:strain_2  1 0.00300 0.00300  0.2727  0.611013    \nResiduals      12 0.13200 0.01100                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\n=== Type III SS ===\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n=== Type III SS ===\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(Anova(fit_comp, type=3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnova Table (Type III tests)\n\nResponse: bw\n               Sum Sq Df  F value    Pr(>F)    \n(Intercept)    6.8450  1 622.2727 1.043e-11 ***\nstrain_2       0.0600  1   5.4545 0.0376925 *  \nsex_2          0.2173  1  19.7532 0.0008006 ***\nstrain_2:sex_2 0.0030  1   0.2727 0.6110133    \nResiduals      0.1320 12                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\nNotice: Type I SS differ depending on order; Type III SS are invariant to order.\n\n### Strategy 4: Weighted Least Squares\n\n**Principle**: Account for heterogeneous variances across observations.\n\nSometimes unbalance reflects different **reliability** or **precision** of measurements:\n- Pen means with varying pen sizes: larger pens are more reliable\n- Group means with unequal within-group variation\n- Measurements with known precision (e.g., genomic predictions)\n\n**Weighted Least Squares (WLS)** minimizes:\n$$\\sum_{i=1}^n w_i (y_i - \\hat{y}_i)^2$$\n\nwhere $w_i = 1/\\sigma_i^2$ (inversely proportional to variance).\n\n**Normal equations**:\n$$\\mathbf{X}'\\mathbf{W}\\mathbf{X} \\mathbf{b} = \\mathbf{X}'\\mathbf{W}\\mathbf{y}$$\n\nwhere $\\mathbf{W} = \\text{diag}(w_1, \\ldots, w_n)$\n\n**When to use**:\n- Heteroscedastic errors (unequal variances)\n- Grouped data: weight by group size\n- Known reliability: weight by precision\n\n**Example**: Pen-average ADG with different pen sizes\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Pen-level data (means)\npen_id <- 1:5\npen_adg <- c(0.85, 0.90, 0.88, 0.87, 0.92)  # kg/day\npen_size <- c(8, 12, 6, 10, 9)  # number of pigs per pen\ndiet <- c(\"A\", \"B\", \"A\", \"B\", \"A\")\n\n# Unweighted regression (WRONG - ignores unequal precision)\nfit_unwt <- lm(pen_adg ~ diet)\nsummary(fit_unwt)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = pen_adg ~ diet)\n\nResiduals:\n        1         2         3         4         5 \n-0.033333  0.015000 -0.003333 -0.015000  0.036667 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.883333   0.018002  49.068 1.86e-05 ***\ndietB       0.001667   0.028464   0.059    0.957    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.03118 on 3 degrees of freedom\nMultiple R-squared:  0.001142,\tAdjusted R-squared:  -0.3318 \nF-statistic: 0.003429 on 1 and 3 DF,  p-value: 0.957\n```\n\n\n:::\n\n```{.r .cell-code}\n# Weighted regression (CORRECT - weight by pen size)\n# Variance of pen mean ~ 1/n, so weight by n\nfit_wt <- lm(pen_adg ~ diet, weights = pen_size)\nsummary(fit_wt)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = pen_adg ~ diet, weights = pen_size)\n\nWeighted Residuals:\n       1        2        3        4        5 \n-0.09961  0.04724 -0.01278 -0.05175  0.10435 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.885217   0.019368  45.705 2.31e-05 ***\ndietB       0.001146   0.027700   0.041     0.97    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.09289 on 3 degrees of freedom\nMultiple R-squared:  0.0005705,\tAdjusted R-squared:  -0.3326 \nF-statistic: 0.001712 on 1 and 3 DF,  p-value: 0.9696\n```\n\n\n:::\n\n```{.r .cell-code}\n# Compare standard errors\ncat(\"\\nUnweighted SE(diet effect):\", summary(fit_unwt)$coef[2,2], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nUnweighted SE(diet effect): 0.02846375 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Weighted SE(diet effect):\", summary(fit_wt)$coef[2,2], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nWeighted SE(diet effect): 0.02769987 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Weighted analysis gives more appropriate SE\n```\n:::\n\n\n**Key point**: When observations represent group means with different group sizes, **always weight by $n_i$**.\n\n## Summary: Choosing a Strategy\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\nTable: Choosing an Analytical Strategy for Unbalanced Data\n\n|Situation                                  |Strategy                                   |\n|:------------------------------------------|:------------------------------------------|\n|Unequal n, all cells present, want ANOVA   |Type III SS, interpret estimable contrasts |\n|Missing cells, rank deficiency             |Cell means model + post-hoc contrasts      |\n|No clear reference group                   |Sum-to-zero constraints + contrasts        |\n|Want to avoid estimability issues entirely |Cell means model                           |\n|Grouped data (e.g., pen means)             |Weighted least squares (weight by n)       |\n|Known heterogeneous precision              |Weighted least squares (weight by 1/σ²)    |\n\n\n:::\n:::\n\n\n::: {.callout-tip}\n## General Recommendations\n\n1. **Always check cell counts first**: Use `table()` to see the data structure\n2. **Default to Type III SS** for unbalanced ANOVA unless you have specific reasons otherwise\n3. **Focus on estimable contrasts** for inference, not individual parameter estimates\n4. **Weight appropriately** when precision varies across observations\n5. **Report your choices** clearly (which SS type, which constraints)\n:::\n\n---\n\n# Topic B: Types of Generalized Inverses\n\nIn Week 2, we introduced the generalized inverse as a solution to rank-deficient systems. In Week 12, we used it to solve non-full rank normal equations. Now let's dive deeper into **different types** of generalized inverses and when each is appropriate.\n\n## Review: Why Generalized Inverses?\n\nFor a square matrix $\\mathbf{A}$, the regular inverse $\\mathbf{A}^{-1}$ exists if and only if $\\mathbf{A}$ is full rank.\n\nFor **rank-deficient** matrices (common with categorical predictors in overparameterized models), $\\mathbf{A}^{-1}$ does not exist. But we can find a **generalized inverse** $\\mathbf{A}^-$ that satisfies certain properties.\n\nConsider the normal equations:\n$$\\mathbf{X}'\\mathbf{X} \\mathbf{b} = \\mathbf{X}'\\mathbf{y}$$\n\nIf $r(\\mathbf{X}'\\mathbf{X}) < p$ (rank deficient), this system has **infinitely many solutions**. Any solution of the form:\n$$\\mathbf{b} = (\\mathbf{X}'\\mathbf{X})^- \\mathbf{X}'\\mathbf{y}$$\n\nwill satisfy the normal equations, but the specific $\\mathbf{b}$ values depend on which generalized inverse we use.\n\n**Key insight**:\n- $\\mathbf{X}\\mathbf{b}$ (fitted values) is **unique** regardless of which $(\\mathbf{X}'\\mathbf{X})^-$ we use\n- Individual $b_j$ values are **not unique**\n- **Estimable functions** $\\mathbf{c}'\\mathbf{b}$ are **unique**\n\nLet's explore different types of generalized inverses.\n\n## Reflexive Generalized Inverse (g-inverse)\n\n### Definition\n\n$\\mathbf{A}^-$ is a **reflexive generalized inverse** (or simply **g-inverse**) of $\\mathbf{A}$ if:\n\n$$\\mathbf{A} \\mathbf{A}^- \\mathbf{A} = \\mathbf{A}$$\n\nThis is the **minimum requirement** for a generalized inverse.\n\n### Properties\n\n- **Not unique**: For a rank-deficient matrix, infinitely many g-inverses exist\n- **Sufficient for solving normal equations**: Any g-inverse allows us to find a solution to $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$\n- **Different g-inverses give different solutions**, but all satisfy $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$\n\n### Why This Property Works\n\nSuppose $\\mathbf{A} \\mathbf{A}^- \\mathbf{A} = \\mathbf{A}$. We want to solve $\\mathbf{X}'\\mathbf{X} \\mathbf{b} = \\mathbf{X}'\\mathbf{y}$.\n\nLet $\\mathbf{b} = (\\mathbf{X}'\\mathbf{X})^- \\mathbf{X}'\\mathbf{y}$. Then:\n\n\\begin{align}\n\\mathbf{X}'\\mathbf{X} \\mathbf{b} &= \\mathbf{X}'\\mathbf{X} [(\\mathbf{X}'\\mathbf{X})^- \\mathbf{X}'\\mathbf{y}] \\\\\n&= [\\mathbf{X}'\\mathbf{X} (\\mathbf{X}'\\mathbf{X})^- \\mathbf{X}'\\mathbf{X}] \\mathbf{X}'\\mathbf{y} \\quad \\text{(rearranging)}\\\\\n&= \\mathbf{X}'\\mathbf{X} (\\mathbf{X}'\\mathbf{y}) \\quad \\text{(using } \\mathbf{A}\\mathbf{A}^-\\mathbf{A} = \\mathbf{A})\\\\\n&= \\mathbf{X}'\\mathbf{y} \\quad \\checkmark\n\\end{align}\n\nSo $\\mathbf{b}$ is indeed a solution!\n\n### Computing a g-inverse by Row Reduction\n\nWe can find a g-inverse using Gaussian elimination, making arbitrary choices when we encounter linear dependence.\n\n**Example**:\n\n$$\\mathbf{A} = \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix}$$\n\nClearly $r(\\mathbf{A}) = 1$ (rows are identical).\n\nTo find a g-inverse, augment with identity and row reduce:\n\n$$\\left[\\begin{array}{cc|cc} 1 & 1 & 1 & 0 \\\\ 1 & 1 & 0 & 1 \\end{array}\\right]$$\n\nRow reduce:\n$$\\left[\\begin{array}{cc|cc} 1 & 1 & 1 & 0 \\\\ 0 & 0 & -1 & 1 \\end{array}\\right]$$\n\nSince second column is all zeros (linear dependence), we have a **free variable**. Set $x_2 = 0$ arbitrarily:\n\n$$\\mathbf{A}^-_1 = \\begin{bmatrix} 1 & 0 \\\\ 0 & 0 \\end{bmatrix}$$\n\nBut we could also set $x_1 = 0$:\n\n$$\\mathbf{A}^-_2 = \\begin{bmatrix} 0 & 0 \\\\ 0 & 1 \\end{bmatrix}$$\n\nOr make other choices:\n\n$$\\mathbf{A}^-_3 = \\begin{bmatrix} 0.5 & 0.5 \\\\ 0.5 & 0.5 \\end{bmatrix}$$\n\n**Verification**: Let's check all satisfy $\\mathbf{A}\\mathbf{A}^-\\mathbf{A} = \\mathbf{A}$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define matrix A\nA <- matrix(c(1, 1, 1, 1), nrow=2, byrow=TRUE)\nprint(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    1    1\n[2,]    1    1\n```\n\n\n:::\n\n```{.r .cell-code}\n# Three different g-inverses\nA_inv1 <- matrix(c(1, 0, 0, 0), nrow=2, byrow=TRUE)\nA_inv2 <- matrix(c(0, 0, 0, 1), nrow=2, byrow=TRUE)\nA_inv3 <- matrix(c(0.5, 0.5, 0.5, 0.5), nrow=2, byrow=TRUE)\n\n# Verify property: A A- A = A\ncat(\"A A_inv1 A:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nA A_inv1 A:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A %*% A_inv1 %*% A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    1    1\n[2,]    1    1\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nA A_inv2 A:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nA A_inv2 A:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A %*% A_inv2 %*% A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    1    1\n[2,]    1    1\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nA A_inv3 A:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nA A_inv3 A:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A %*% A_inv3 %*% A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    2    2\n[2,]    2    2\n```\n\n\n:::\n\n```{.r .cell-code}\n# All equal to A!\n```\n:::\n\n\n### Different Solutions to Normal Equations\n\nDifferent g-inverses give different solutions, but **fitted values are the same**:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Suppose we want to solve Ax = y\ny <- c(2, 2)  # Note: must be in column space of A (here, y1 = y2)\n\n# Three solutions\nb1 <- A_inv1 %*% y\nb2 <- A_inv2 %*% y\nb3 <- A_inv3 %*% y\n\ncat(\"Solution 1:\", b1, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSolution 1: 2 0 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Solution 2:\", b2, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSolution 2: 0 2 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Solution 3:\", b3, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSolution 3: 2 2 \n```\n\n\n:::\n\n```{.r .cell-code}\n# All different! But check fitted values:\ncat(\"\\nFitted values (Ab):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nFitted values (Ab):\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"A b1:\", A %*% b1, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nA b1: 2 2 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"A b2:\", A %*% b2, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nA b2: 2 2 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"A b3:\", A %*% b3, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nA b3: 4 4 \n```\n\n\n:::\n\n```{.r .cell-code}\n# All the same!\n```\n:::\n\n\n::: {.callout-important}\n## Key Insight: What's Unique and What's Not\n\nFor rank-deficient normal equations $\\mathbf{X}'\\mathbf{X}\\mathbf{b} = \\mathbf{X}'\\mathbf{y}$:\n\n- **Fitted values** $\\mathbf{X}\\mathbf{b}$ are **unique** ✓\n- **Individual parameter estimates** $b_j$ are **not unique** ✗\n- **Estimable functions** $\\mathbf{c}'\\mathbf{b}$ are **unique** ✓\n\nThe choice of g-inverse matters only for interpretation of individual parameters, not for estimable functions or predictions.\n:::\n\n## Moore-Penrose Pseudoinverse\n\nWhile infinitely many reflexive g-inverses exist, there is **exactly one** generalized inverse with additional nice properties: the **Moore-Penrose inverse**.\n\n### Definition\n\n$\\mathbf{A}^+$ is the **Moore-Penrose inverse** (or **pseudoinverse**) of $\\mathbf{A}$ if it satisfies **all four** of these properties:\n\n1. $\\mathbf{A} \\mathbf{A}^+ \\mathbf{A} = \\mathbf{A}$ (reflexive g-inverse property)\n2. $\\mathbf{A}^+ \\mathbf{A} \\mathbf{A}^+ = \\mathbf{A}^+$ (reflexive for $\\mathbf{A}^+$ itself)\n3. $(\\mathbf{A} \\mathbf{A}^+)' = \\mathbf{A} \\mathbf{A}^+$ (symmetric)\n4. $(\\mathbf{A}^+ \\mathbf{A})' = \\mathbf{A}^+ \\mathbf{A}$ (symmetric)\n\n### Key Property\n\nThe Moore-Penrose inverse is **unique**: there is only one matrix satisfying all four conditions.\n\n### Computing via Singular Value Decomposition (SVD)\n\nThe most common way to compute $\\mathbf{A}^+$ is via SVD.\n\n**SVD Decomposition**:\n$$\\mathbf{A} = \\mathbf{U} \\mathbf{D} \\mathbf{V}'$$\n\nwhere:\n- $\\mathbf{U}$ ($n \\times n$) and $\\mathbf{V}$ ($p \\times p$) are orthogonal matrices\n- $\\mathbf{D}$ ($n \\times p$) is diagonal with singular values $\\sigma_1 \\geq \\sigma_2 \\geq \\ldots \\geq \\sigma_r > 0$, then zeros\n\n**Moore-Penrose Inverse**:\n$$\\mathbf{A}^+ = \\mathbf{V} \\mathbf{D}^+ \\mathbf{U}'$$\n\nwhere $\\mathbf{D}^+$ has $1/\\sigma_i$ for non-zero $\\sigma_i$, and zeros elsewhere.\n\n**Example**: Our previous rank-1 matrix:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# A = [1 1; 1 1]\nA <- matrix(c(1, 1, 1, 1), nrow=2, byrow=TRUE)\n\n# SVD\nsvd_A <- svd(A)\ncat(\"Singular values:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSingular values:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(svd_A$d)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2 0\n```\n\n\n:::\n\n```{.r .cell-code}\n# Note: σ₁ = 2, σ₂ = 0 (rank 1)\n\n# Moore-Penrose inverse using R\nlibrary(MASS)\nA_plus <- ginv(A)\ncat(\"\\nMoore-Penrose inverse:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nMoore-Penrose inverse:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(A_plus)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,] 0.25 0.25\n[2,] 0.25 0.25\n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify all 4 properties\ncat(\"\\n1. A A+ A = A?\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n1. A A+ A = A?\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(all.equal(A %*% A_plus %*% A, A))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\n2. A+ A A+ = A+?\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n2. A+ A A+ = A+?\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(all.equal(A_plus %*% A %*% A_plus, A_plus))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\n3. (A A+)' = A A+?\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n3. (A A+)' = A A+?\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(all.equal(t(A %*% A_plus), A %*% A_plus))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\n4. (A+ A)' = A+ A?\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n4. (A+ A)' = A+ A?\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(all.equal(t(A_plus %*% A), A_plus %*% A))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\n### Special Property: Minimum Norm Solution\n\nAmong all solutions to $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$, the Moore-Penrose solution:\n$$\\mathbf{x} = \\mathbf{A}^+ \\mathbf{b}$$\n\nhas the **smallest norm** (smallest sum of squares):\n$$||\\mathbf{x}||^2 = \\sum_j x_j^2 \\text{ is minimized}$$\n\nThis is often desirable in statistical applications.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compare norms of different solutions\ny <- c(2, 2)\n\nb1 <- A_inv1 %*% y\nb2 <- A_inv2 %*% y\nb3 <- A_inv3 %*% y\nb_plus <- A_plus %*% y\n\ncat(\"Norm of solution 1:\", sqrt(sum(b1^2)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNorm of solution 1: 2 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Norm of solution 2:\", sqrt(sum(b2^2)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNorm of solution 2: 2 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Norm of solution 3:\", sqrt(sum(b3^2)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNorm of solution 3: 2.828427 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Norm of M-P solution:\", sqrt(sum(b_plus^2)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNorm of M-P solution: 1.414214 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Moore-Penrose gives minimum norm\n```\n:::\n\n\n::: {.callout-note}\n## R's Default\n\nIn R, `MASS::ginv()` computes the **Moore-Penrose inverse** using SVD. This is the most commonly used generalized inverse in practice because:\n\n1. It's unique (reproducible)\n2. It's numerically stable (SVD is robust)\n3. It gives minimum norm solutions\n4. It handles any matrix (not just square)\n\nUnless you have a specific reason to use a different g-inverse, use `ginv()`.\n:::\n\n## Conditional Inverse (Constraint-Based)\n\nA third approach is to **impose constraints** to make the system full rank, then use a regular inverse.\n\n### Concept\n\nInstead of solving the rank-deficient system:\n$$\\mathbf{X}'\\mathbf{X} \\mathbf{b} = \\mathbf{X}'\\mathbf{y}$$\n\nWe augment with constraints $\\mathbf{C}'\\mathbf{b} = \\mathbf{c}$ to create a full-rank system:\n\n$$\\begin{bmatrix} \\mathbf{X}'\\mathbf{X} & \\mathbf{C}' \\\\ \\mathbf{C} & \\mathbf{0} \\end{bmatrix} \\begin{bmatrix} \\mathbf{b} \\\\ \\boldsymbol{\\lambda} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{X}'\\mathbf{y} \\\\ \\mathbf{c} \\end{bmatrix}$$\n\nwhere $\\boldsymbol{\\lambda}$ are Lagrange multipliers.\n\nWith appropriate constraints (e.g., $\\sum \\alpha_i = 0$ for one-way ANOVA), this system becomes full rank and has a **unique solution**.\n\n### Example: Sum-to-Zero Constraint\n\nFor one-way ANOVA effects model $y_{ij} = \\mu + \\alpha_i + e_{ij}$:\n\n**Constraint**: $\\sum_{i=1}^g \\alpha_i = 0$\n\nThis can be written as:\n$$\\mathbf{C}' = [0, 1, 1, \\ldots, 1]$$\n\nso $\\mathbf{C}'\\mathbf{b} = \\alpha_1 + \\alpha_2 + \\ldots + \\alpha_g = 0$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example: 3 groups\ngroup <- c(rep(1, 3), rep(2, 2), rep(3, 4))\ny_ex <- c(10, 12, 11,  # Group 1\n          14, 15,      # Group 2\n          16, 17, 18, 19)  # Group 3\n\n# For demonstration, use cell means model first to get all parameters\nX_cell <- model.matrix(~ 0 + factor(group))  # Cell means (3 params, full rank)\nXtX_cell <- t(X_cell) %*% X_cell\nXty_cell <- t(X_cell) %*% y_ex\n\ncat(\"Cell means model: Rank =\", qr(XtX_cell)$rank, \"out of\", ncol(XtX_cell), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCell means model: Rank = 3 out of 3 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"This is full rank, so all 3 group means are estimable.\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nThis is full rank, so all 3 group means are estimable.\n```\n\n\n:::\n\n```{.r .cell-code}\n# Now show effects model with constraint\n# Build full effects model manually: μ, α₁, α₂, α₃ (4 params, rank 3)\n# X has: column of 1s (μ), then indicators for each group\nX_eff <- cbind(1, model.matrix(~ 0 + factor(group)))\nXtX_eff <- t(X_eff) %*% X_eff\nXty_eff <- t(X_eff) %*% y_ex\n\ncat(\"Effects model: Rank =\", qr(XtX_eff)$rank, \"out of\", ncol(XtX_eff), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEffects model: Rank = 3 out of 4 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Rank deficient!\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRank deficient!\n```\n\n\n:::\n\n```{.r .cell-code}\n# Apply sum-to-zero constraint: α₁ + α₂ + α₃ = 0\n# Constraint: [0, 1, 1, 1] · [μ, α₁, α₂, α₃]' = 0\nC <- matrix(c(0, 1, 1, 1), nrow=1)  # 1x4 constraint matrix\n\n# Augmented system\naug_mat <- rbind(\n  cbind(XtX_eff, t(C)),  # XtX is 4x4, t(C) is 4x1\n  cbind(C, 0)             # C is 1x4, 0 is scalar\n)\n\naug_rhs <- c(Xty_eff, 0)\n\n# Solve (now full rank!)\nsolution <- solve(aug_mat, aug_rhs)\nb_constrained <- solution[1:4]\nlambda <- solution[5]\n\ncat(\"Constrained solution:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConstrained solution:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"μ =\", b_constrained[1], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nμ = 14.33333 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"α₁ =\", b_constrained[2], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nα₁ = -3.333333 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"α₂ =\", b_constrained[3], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nα₂ = 0.1666667 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"α₃ =\", b_constrained[4], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nα₃ = 3.166667 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Sum of α's =\", sum(b_constrained[2:4]), \"(should be ~0)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSum of α's = -6.938894e-16 (should be ~0)\n```\n\n\n:::\n:::\n\n\n### Properties\n\n- **Unique solution** for given constraint\n- Different constraints give different $\\mathbf{b}$ values\n- **Estimable functions unchanged** by constraint choice\n- Allows interpretation of individual parameters under chosen constraint\n\n## Comparison of Generalized Inverse Types\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\nTable: Comparison of Generalized Inverse Types\n\n|Type                      |Uniqueness                     |Computation                       |R Function                  |Use Case                                                       |\n|:-------------------------|:------------------------------|:---------------------------------|:---------------------------|:--------------------------------------------------------------|\n|Reflexive g-inverse       |Many exist                     |Row reduction (arbitrary choices) |Custom (many options)       |Quick solution when you don't care about individual parameters |\n|Moore-Penrose             |Unique                         |SVD: A = UDV' → A⁺ = VD⁺U'        |MASS::ginv()                |Prefer unique, stable solution; R default                      |\n|Conditional (constrained) |Unique (for given constraints) |Augmented system with constraints |solve() on augmented system |Want specific interpretation (e.g., sum-to-zero effects)       |\n\n\n:::\n:::\n\n\n## Practical Comparison with Real Data\n\nLet's compare all three approaches on a realistic example:\n\n**Data**: Milk yield by breed (unequal sample sizes)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# One-way ANOVA: 3 breeds, unequal n\nbreed_ex <- c(rep(\"Holstein\", 3), rep(\"Jersey\", 2), rep(\"Brown Swiss\", 4))\nmilk_yield <- c(\n  30, 32, 31,     # Holstein (mean = 31)\n  24, 25,         # Jersey (mean = 24.5)\n  28, 29, 27, 28  # Brown Swiss (mean = 28)\n)\n\n# Effects model design matrix\nX_milk <- model.matrix(~ breed_ex)\nXtX_milk <- t(X_milk) %*% X_milk\nXty_milk <- t(X_milk) %*% milk_yield\n\ncat(\"=== Design Matrix Rank ===\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n=== Design Matrix Rank ===\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"r(X'X) =\", qr(XtX_milk)$rank, \"out of\", ncol(XtX_milk), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nr(X'X) = 3 out of 3 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Rank deficient by\", ncol(XtX_milk) - qr(XtX_milk)$rank, \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRank deficient by 0 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Approach 1: Use a reflexive g-inverse (many possible)\n# We'll use R's ginv which gives Moore-Penrose\nb_ginv <- ginv(XtX_milk) %*% Xty_milk\ncat(\"=== Solution 1: Moore-Penrose (ginv) ===\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n=== Solution 1: Moore-Penrose (ginv) ===\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"μ =\", b_ginv[1], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nμ = 28 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"α_Jersey =\", b_ginv[2], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nα_Jersey = 3 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"α_BrownSwiss =\", b_ginv[3], \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nα_BrownSwiss = -3.5 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Approach 2: Sum-to-zero constraint\noptions(contrasts = c(\"contr.sum\", \"contr.poly\"))\nfit_sum <- lm(milk_yield ~ breed_ex)\nb_sum <- coef(fit_sum)\ncat(\"=== Solution 2: Sum-to-Zero Constraint ===\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n=== Solution 2: Sum-to-Zero Constraint ===\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"μ =\", b_sum[1], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nμ = 27.83333 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"α_Holstein =\", b_sum[2], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nα_Holstein = 0.1666667 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"α_Jersey =\", b_sum[3], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nα_Jersey = 3.166667 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"α_BrownSwiss = \", -(b_sum[2] + b_sum[3]), \"(computed)\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nα_BrownSwiss =  -3.333333 (computed)\n```\n\n\n:::\n\n```{.r .cell-code}\n# Approach 3: Set-to-zero constraint (Holstein = reference)\noptions(contrasts = c(\"contr.treatment\", \"contr.poly\"))\nfit_treat <- lm(milk_yield ~ breed_ex)\nb_treat <- coef(fit_treat)\ncat(\"=== Solution 3: Set-to-Zero (Holstein reference) ===\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n=== Solution 3: Set-to-Zero (Holstein reference) ===\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"μ (Holstein mean) =\", b_treat[1], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nμ (Holstein mean) = 28 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"α_Holstein = 0 (reference)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nα_Holstein = 0 (reference)\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"α_Jersey =\", b_treat[2], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nα_Jersey = 3 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"α_BrownSwiss =\", b_treat[3], \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nα_BrownSwiss = -3.5 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Compare: fitted values (should be identical!)\nfitted_ginv <- X_milk %*% b_ginv\nfitted_sum <- fitted(fit_sum)\nfitted_treat <- fitted(fit_treat)\n\ncat(\"=== Fitted Values Comparison ===\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n=== Fitted Values Comparison ===\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Max difference (ginv vs sum):\", max(abs(fitted_ginv - fitted_sum)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMax difference (ginv vs sum): 1.065814e-14 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Max difference (ginv vs treat):\", max(abs(fitted_ginv - fitted_treat)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMax difference (ginv vs treat): 1.065814e-14 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Conclusion: Fitted values are identical! ✓\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConclusion: Fitted values are identical! ✓\n```\n\n\n:::\n\n```{.r .cell-code}\n# Compare: estimable contrast (Jersey vs Holstein)\ncontrast_ginv <- (b_ginv[1] + b_ginv[2]) - b_ginv[1]  # (μ + α_J) - (μ)\ncontrast_sum <- b_sum[2] + b_sum[1]  # Wait, need to compute properly for sum-to-zero\n\n# Let's use emmeans for clean contrast comparison\nlibrary(emmeans)\nemm_sum <- emmeans(fit_sum, \"breed_ex\")\nemm_treat <- emmeans(fit_treat, \"breed_ex\")\n\ncat(\"=== Estimated Breed Means (μ + α_i) ===\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n=== Estimated Breed Means (μ + α_i) ===\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Sum-to-zero:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSum-to-zero:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(summary(emm_sum))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n breed_ex    emmean    SE df lower.CL upper.CL\n Brown Swiss   28.0 0.433  6     26.9     29.1\n Holstein      31.0 0.500  6     29.8     32.2\n Jersey        24.5 0.612  6     23.0     26.0\n\nConfidence level used: 0.95 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nSet-to-zero:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSet-to-zero:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(summary(emm_treat))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n breed_ex    emmean    SE df lower.CL upper.CL\n Brown Swiss   28.0 0.433  6     26.9     29.1\n Holstein      31.0 0.500  6     29.8     32.2\n Jersey        24.5 0.612  6     23.0     26.0\n\nConfidence level used: 0.95 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Contrasts\ncat(\"\\n=== Estimable Contrasts ===\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n=== Estimable Contrasts ===\n```\n\n\n:::\n\n```{.r .cell-code}\ncontrasts_sum <- pairs(emm_sum)\ncontrasts_treat <- pairs(emm_treat)\ncat(\"Sum-to-zero:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSum-to-zero:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(contrasts_sum)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n contrast               estimate    SE df t.ratio p.value\n Brown Swiss - Holstein     -3.0 0.661  6  -4.536  0.0094\n Brown Swiss - Jersey        3.5 0.750  6   4.667  0.0082\n Holstein - Jersey           6.5 0.791  6   8.222  0.0004\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nSet-to-zero:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSet-to-zero:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(contrasts_treat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n contrast               estimate    SE df t.ratio p.value\n Brown Swiss - Holstein     -3.0 0.661  6  -4.536  0.0094\n Brown Swiss - Jersey        3.5 0.750  6   4.667  0.0082\n Holstein - Jersey           6.5 0.791  6   8.222  0.0004\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nConclusion: Contrasts are identical! ✓\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nConclusion: Contrasts are identical! ✓\n```\n\n\n:::\n:::\n\n\n::: {.callout-important}\n## The Big Picture\n\nThree very different parameter estimates ($\\mu$, $\\alpha_i$), but:\n\n1. **Fitted values**: Identical across all three methods ✓\n2. **Estimable functions** (breed means, contrasts): Identical ✓\n3. **Biological conclusions**: Identical ✓\n\nThe choice of generalized inverse (or constraint system) affects **how we write down the answer**, not **what the answer means biologically**.\n\nFor practical data analysis:\n- Use `ginv()` (Moore-Penrose) for simplicity and stability\n- OR use constraint system (sum-to-zero, set-to-zero) for interpretation\n- **Always report estimable functions**, not individual non-estimable parameters\n:::\n\n---\n\n*Continued in next message due to length...*\n\n# Topic C: Constraint Systems\n\nIn Topic B, we saw that different generalized inverses (or constraint systems) give different parameter estimates but identical estimable functions. Now let's explore **how to choose and apply constraints** for interpretability.\n\n## Why Use Constraints?\n\nWhen $\\mathbf{X}'\\mathbf{X}$ is rank deficient, the normal equations have infinitely many solutions. Constraints serve two purposes:\n\n1. **Computational**: Make the system full rank so we can use regular (non-generalized) matrix inverse\n2. **Interpretational**: Give individual parameters specific, interpretable meanings\n\n**Key principle**: The constraint choice is **arbitrary** from a statistical standpoint—it doesn't change estimable functions or biological conclusions. But it **does** affect how we write down and interpret individual parameters.\n\n## Set-to-Zero Constraints (Reference Cell Coding)\n\n### Formulation\n\n**Principle**: Set one level as the \"reference\" or \"baseline\"; all other effects are deviations from this reference.\n\nFor one-way ANOVA with effects model $y_{ij} = \\mu + \\alpha_i + e_{ij}$:\n\n**Constraint**: $\\alpha_1 = 0$ (first level is reference)\n\n### Interpretation\n\nWith $\\alpha_1 = 0$:\n- Group 1: $E(y_{1j}) = \\mu + \\alpha_1 = \\mu + 0 = \\mu$\n- Group 2: $E(y_{2j}) = \\mu + \\alpha_2$\n- Group 3: $E(y_{3j}) = \\mu + \\alpha_3$\n\nSo:\n- $\\mu$ = mean of reference group (group 1)\n- $\\alpha_i$ = difference between group $i$ and reference group\n- All $\\alpha_i$ (except $\\alpha_1$) are **estimable** (they're contrasts!)\n\n### Design Matrix\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example: 3 groups, set-to-zero constraint\ngroup_stz <- factor(rep(1:3, each=2))\ny_stz <- c(10, 12,  # Group 1\n           14, 15,  # Group 2\n           16, 17)  # Group 3\n\n# Set contrasts to treatment (set-to-zero)\noptions(contrasts = c(\"contr.treatment\", \"contr.poly\"))\n\n# Design matrix\nX_stz <- model.matrix(~ group_stz)\ncat(\"Design matrix (set-to-zero):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDesign matrix (set-to-zero):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(X_stz)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  (Intercept) group_stz2 group_stz3\n1           1          0          0\n2           1          0          0\n3           1          1          0\n4           1          1          0\n5           1          0          1\n6           1          0          1\nattr(,\"assign\")\n[1] 0 1 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$group_stz\n[1] \"contr.treatment\"\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nColumn 1: Intercept (μ)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nColumn 1: Intercept (μ)\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Column 2: Indicator for group 2 (α₂)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nColumn 2: Indicator for group 2 (α₂)\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Column 3: Indicator for group 3 (α₃)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nColumn 3: Indicator for group 3 (α₃)\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Note: No column for group 1 (it's the reference)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNote: No column for group 1 (it's the reference)\n```\n\n\n:::\n:::\n\n\nNotice:\n- Row 1-2: Group 1 observations have [1, 0, 0] → $\\mu$ only\n- Row 3-4: Group 2 observations have [1, 1, 0] → $\\mu + \\alpha_2$\n- Row 5-6: Group 3 observations have [1, 0, 1] → $\\mu + \\alpha_3$\n\n### When to Use\n\nSet-to-zero is natural when:\n- Clear **control or reference group** exists (e.g., placebo, wild-type, standard breed)\n- You want to express all effects **relative to control**\n- Common in experimental designs with a baseline treatment\n\n### R Implementation\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit with set-to-zero (R's default for lm)\nfit_stz <- lm(y_stz ~ group_stz)\nsummary(fit_stz)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y_stz ~ group_stz)\n\nResiduals:\n   1    2    3    4    5    6 \n-1.0  1.0 -0.5  0.5 -0.5  0.5 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  11.0000     0.7071   15.56 0.000577 ***\ngroup_stz2    3.5000     1.0000    3.50 0.039481 *  \ngroup_stz3    5.5000     1.0000    5.50 0.011830 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1 on 3 degrees of freedom\nMultiple R-squared:  0.9118,\tAdjusted R-squared:  0.8529 \nF-statistic:  15.5 on 2 and 3 DF,  p-value: 0.02621\n```\n\n\n:::\n\n```{.r .cell-code}\n# Interpretation\ncoefs_stz <- coef(fit_stz)\ncat(\"\\nInterpretation:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nInterpretation:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"μ (Group 1 mean):\", coefs_stz[1], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nμ (Group 1 mean): 11 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"α₂ (Group 2 - Group 1):\", coefs_stz[2], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nα₂ (Group 2 - Group 1): 3.5 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"α₃ (Group 3 - Group 1):\", coefs_stz[3], \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nα₃ (Group 3 - Group 1): 5.5 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Predicted means:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPredicted means:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Group 1:\", coefs_stz[1], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nGroup 1: 11 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Group 2:\", coefs_stz[1] + coefs_stz[2], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nGroup 2: 14.5 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Group 3:\", coefs_stz[1] + coefs_stz[3], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nGroup 3: 16.5 \n```\n\n\n:::\n:::\n\n\n## Sum-to-Zero Constraints\n\n### Formulation\n\n**Principle**: Effects sum to zero; $\\mu$ represents the **overall mean**, and $\\alpha_i$ are deviations from this mean.\n\n**Constraint**: $\\sum_{i=1}^g \\alpha_i = 0$\n\nOr for unbalanced designs, **weighted sum-to-zero**:\n$$\\sum_{i=1}^g n_i \\alpha_i = 0$$\n\n### Interpretation\n\nWith $\\sum \\alpha_i = 0$:\n- $\\mu$ = overall mean (or grand mean)\n- $\\alpha_i$ = deviation of group $i$ from overall mean\n- More **symmetric**: no group is \"special\"\n- Traditional ANOVA approach\n\n### Design Matrix\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sum-to-zero constraint\noptions(contrasts = c(\"contr.sum\", \"contr.poly\"))\n\nX_sum <- model.matrix(~ group_stz)\ncat(\"Design matrix (sum-to-zero):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDesign matrix (sum-to-zero):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(X_sum)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  (Intercept) group_stz1 group_stz2\n1           1          1          0\n2           1          1          0\n3           1          0          1\n4           1          0          1\n5           1         -1         -1\n6           1         -1         -1\nattr(,\"assign\")\n[1] 0 1 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$group_stz\n[1] \"contr.sum\"\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nColumn 1: Intercept (μ)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nColumn 1: Intercept (μ)\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Column 2: Effect coding for group 1\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nColumn 2: Effect coding for group 1\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Column 3: Effect coding for group 2\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nColumn 3: Effect coding for group 2\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Note: Group 3 coded as [-1, -1] so effects sum to zero\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNote: Group 3 coded as [-1, -1] so effects sum to zero\n```\n\n\n:::\n:::\n\n\nNotice:\n- Row 1-2: Group 1 has [1, 1, 0] → $\\mu + \\alpha_1$\n- Row 3-4: Group 2 has [1, 0, 1] → $\\mu + \\alpha_2$\n- Row 5-6: Group 3 has [1, -1, -1] → $\\mu - \\alpha_1 - \\alpha_2 = \\mu + \\alpha_3$\n\nThe last group is coded so that $\\alpha_1 + \\alpha_2 + \\alpha_3 = 0$.\n\n### When to Use\n\nSum-to-zero is natural when:\n- **No natural reference group** (all groups on equal footing)\n- You want to compare each group to the **overall average**\n- Balanced designs (classical ANOVA)\n- Interpreting \"main effects\" in factorial designs\n\n### R Implementation\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit with sum-to-zero\nfit_sum_ex <- lm(y_stz ~ group_stz)\nsummary(fit_sum_ex)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y_stz ~ group_stz)\n\nResiduals:\n   1    2    3    4    5    6 \n-1.0  1.0 -0.5  0.5 -0.5  0.5 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  14.0000     0.4082  34.293 5.45e-05 ***\ngroup_stz1   -3.0000     0.5774  -5.196   0.0138 *  \ngroup_stz2    0.5000     0.5774   0.866   0.4502    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1 on 3 degrees of freedom\nMultiple R-squared:  0.9118,\tAdjusted R-squared:  0.8529 \nF-statistic:  15.5 on 2 and 3 DF,  p-value: 0.02621\n```\n\n\n:::\n\n```{.r .cell-code}\ncoefs_sum <- coef(fit_sum_ex)\ncat(\"\\nInterpretation:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nInterpretation:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"μ (overall mean):\", coefs_sum[1], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nμ (overall mean): 14 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"α₁ (Group 1 deviation):\", coefs_sum[2], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nα₁ (Group 1 deviation): -3 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"α₂ (Group 2 deviation):\", coefs_sum[3], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nα₂ (Group 2 deviation): 0.5 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"α₃ (computed):\", -(coefs_sum[2] + coefs_sum[3]), \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nα₃ (computed): 2.5 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Predicted means:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPredicted means:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Group 1:\", coefs_sum[1] + coefs_sum[2], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nGroup 1: 11 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Group 2:\", coefs_sum[1] + coefs_sum[3], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nGroup 2: 14.5 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Group 3:\", coefs_sum[1] - coefs_sum[2] - coefs_sum[3], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nGroup 3: 16.5 \n```\n\n\n:::\n:::\n\n\n::: {.callout-note}\n## Weighted Sum-to-Zero for Unbalanced Data\n\nFor unbalanced designs, you might prefer **weighted sum-to-zero**: $\\sum_i n_i \\alpha_i = 0$\n\nThis makes $\\mu$ the **weighted** grand mean, and $\\alpha_i$ are deviations from this weighted mean.\n\nR's `contr.sum` uses simple (unweighted) sum-to-zero. For weighted constraints, you'd need to manually construct the constraint matrix.\n:::\n\n## Comparison: Set-to-Zero vs. Sum-to-Zero\n\nLet's see both on the same dataset:\n\n### Example: Sow Litter Size by Parity\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Data: Litter size by parity\nparity <- factor(c(rep(1, 4), rep(2, 3), rep(3, 2)))\nlitter_size <- c(10, 11, 10, 12,  # Parity 1\n                 11, 12, 13,      # Parity 2\n                 12, 13)          # Parity 3\n\n# Observed means\ntapply(litter_size, parity, mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    1     2     3 \n10.75 12.00 12.50 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Overall mean\nmean(litter_size)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 11.55556\n```\n\n\n:::\n:::\n\n\n**Set-to-Zero**: Parity 1 is reference\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptions(contrasts = c(\"contr.treatment\", \"contr.poly\"))\nfit_parity_stz <- lm(litter_size ~ parity)\n\ncat(\"=== SET-TO-ZERO (Parity 1 reference) ===\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n=== SET-TO-ZERO (Parity 1 reference) ===\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(summary(fit_parity_stz)$coef)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            Estimate Std. Error   t value     Pr(>|t|)\n(Intercept)    10.75  0.4677072 22.984467 4.444476e-07\nparity2         1.25  0.7144345  1.749636 1.307565e-01\nparity3         1.75  0.8100926  2.160247 7.405163e-02\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nInterpretation:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nInterpretation:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Parity 1 mean =\", coef(fit_parity_stz)[1], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nParity 1 mean = 10.75 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Parity 2 - Parity 1 =\", coef(fit_parity_stz)[2], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nParity 2 - Parity 1 = 1.25 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Parity 3 - Parity 1 =\", coef(fit_parity_stz)[3], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nParity 3 - Parity 1 = 1.75 \n```\n\n\n:::\n:::\n\n\n**Sum-to-Zero**: Overall mean as baseline\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptions(contrasts = c(\"contr.sum\", \"contr.poly\"))\nfit_parity_sum <- lm(litter_size ~ parity)\n\ncat(\"\\n=== SUM-TO-ZERO ===\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n=== SUM-TO-ZERO ===\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(summary(fit_parity_sum)$coef)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            Estimate Std. Error    t value     Pr(>|t|)\n(Intercept)    11.75  0.3245367 36.2054577 2.961099e-08\nparity1        -1.00  0.4221857 -2.3686261 5.562493e-02\nparity2         0.25  0.4500514  0.5554921 5.986433e-01\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nInterpretation:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nInterpretation:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Overall mean =\", coef(fit_parity_sum)[1], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOverall mean = 11.75 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Parity 1 deviation =\", coef(fit_parity_sum)[2], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nParity 1 deviation = -1 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Parity 2 deviation =\", coef(fit_parity_sum)[3], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nParity 2 deviation = 0.25 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Parity 3 deviation =\", -(coef(fit_parity_sum)[2] + coef(fit_parity_sum)[3]), \"(computed)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nParity 3 deviation = 0.75 (computed)\n```\n\n\n:::\n:::\n\n\n**Compare Estimable Functions**:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Use emmeans to extract breed means and contrasts\nlibrary(emmeans)\n\nemm_stz <- emmeans(fit_parity_stz, \"parity\")\nemm_sum <- emmeans(fit_parity_sum, \"parity\")\n\ncat(\"\\n=== ESTIMATED MEANS (μ + α_i) ===\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n=== ESTIMATED MEANS (μ + α_i) ===\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Set-to-zero:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSet-to-zero:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(summary(emm_stz))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n parity emmean    SE df lower.CL upper.CL\n 1        10.8 0.468  6     9.61     11.9\n 2        12.0 0.540  6    10.68     13.3\n 3        12.5 0.661  6    10.88     14.1\n\nConfidence level used: 0.95 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nSum-to-zero:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSum-to-zero:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(summary(emm_sum))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n parity emmean    SE df lower.CL upper.CL\n 1        10.8 0.468  6     9.61     11.9\n 2        12.0 0.540  6    10.68     13.3\n 3        12.5 0.661  6    10.88     14.1\n\nConfidence level used: 0.95 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\n=== PAIRWISE CONTRASTS ===\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n=== PAIRWISE CONTRASTS ===\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Set-to-zero:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSet-to-zero:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(pairs(emm_stz))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n contrast          estimate    SE df t.ratio p.value\n parity1 - parity2    -1.25 0.714  6  -1.750  0.2636\n parity1 - parity3    -1.75 0.810  6  -2.160  0.1575\n parity2 - parity3    -0.50 0.854  6  -0.586  0.8326\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nSum-to-zero:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSum-to-zero:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(pairs(emm_sum))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n contrast          estimate    SE df t.ratio p.value\n parity1 - parity2    -1.25 0.714  6  -1.750  0.2636\n parity1 - parity3    -1.75 0.810  6  -2.160  0.1575\n parity2 - parity3    -0.50 0.854  6  -0.586  0.8326\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nConclusion: Means and contrasts IDENTICAL!\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nConclusion: Means and contrasts IDENTICAL!\n```\n\n\n:::\n:::\n\n\n### Side-by-Side Summary\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\nTable: Comparison of Set-to-Zero vs. Sum-to-Zero Parameterizations\n\n|Constraint  |μ                     |α₁            |α₂                 |α₃                 |Parity 1 Mean |Parity 2 vs 1 |\n|:-----------|:---------------------|:-------------|:------------------|:------------------|:-------------|:-------------|\n|Set-to-Zero |10.75 (Parity 1 mean) |0 (reference) |1.25 (vs Parity 1) |1.75 (vs Parity 1) |10.75         |1.25          |\n|Sum-to-Zero |11.75 (overall mean)  |-1.00         |0.25 (vs overall)  |0.75 (computed)    |10.75         |1.25          |\n\n\n:::\n:::\n\n\n::: {.callout-important}\n## Key Insight\n\nThe two parameterizations give **completely different individual parameter values**:\n- $\\mu$ differs by ~0.47\n- $\\alpha_1$ is 0 vs -0.73\n- $\\alpha_2$ is 1.20 vs 0.47\n\nBut the **estimable functions** (means, contrasts) are **identical**:\n- Parity 1 mean: 10.75 (both methods)\n- Parity 2 vs 1: 1.20 (both methods)\n- Standard errors: identical\n\n**Conclusion**: Constraint choice affects notation, not biology!\n:::\n\n## Custom Constraints\n\nSometimes you want constraints tailored to your specific problem.\n\n### Example 1: Anchoring to an External Standard\n\nSuppose you know from breed registry standards that Holstein cows should average 9000 kg milk:\n\n**Constraint**: $\\mu + \\alpha_{\\text{Holstein}} = 9000$\n\nThis \"anchors\" your estimates to the known standard.\n\n### Example 2: Structural Biological Constraint\n\nIn crossbreeding studies with heterosis:\n\n**Model**: $y_{ijk} = \\mu + \\text{breed}_i + \\text{sex}_j + (\\text{breed} \\times \\text{sex})_{ij} + e_{ijk}$\n\nYou might constrain the crossbred mean to be midparent + heterosis:\n$$\\alpha_{\\text{cross}} = \\frac{\\alpha_{\\text{breed1}} + \\alpha_{\\text{breed2}}}{2} + h$$\n\nwhere $h$ is the heterosis effect (estimable as a contrast).\n\n### Implementation\n\nCustom constraints are applied by augmenting the normal equations:\n\n$$\\begin{bmatrix} \\mathbf{X}'\\mathbf{X} & \\mathbf{C}' \\\\ \\mathbf{C} & \\mathbf{0} \\end{bmatrix} \\begin{bmatrix} \\mathbf{b} \\\\ \\boldsymbol{\\lambda} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{X}'\\mathbf{y} \\\\ \\mathbf{c} \\end{bmatrix}$$\n\nwhere:\n- $\\mathbf{C}$ is the constraint matrix\n- $\\mathbf{c}$ is the constraint values\n- $\\boldsymbol{\\lambda}$ are Lagrange multipliers\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example: Custom constraint for 3-group model\n# Suppose we want α₁ + α₃ = 0 (groups 1 and 3 average to zero)\n\n# Using our earlier 3-group data\nX_custom <- model.matrix(~ group_stz)\nXtX_custom <- t(X_custom) %*% X_custom\nXty_custom <- t(X_custom) %*% y_stz\n\n# Constraint: α₁ + α₃ = 0\n# This is row 2 + row 4 of parameter vector = 0\n# In set-to-zero coding: group1 is reference (no parameter), so actually we want\n# Let's just use sum-to-zero as an example\n\n# Actually, let's demonstrate with effects model (all α's present)\n# Model: y = μ + α₁ + α₂ + α₃\n# Constraint: α₁ + α₃ = 0\n\n# Design matrix for effects model (all groups)\nX_effects <- model.matrix(~ 0 + group_stz)  # Cell means first\nX_effects <- cbind(1, X_effects)  # Add intercept\n# Wait, this is getting complicated. Let me show the principle\n\ncat(\"Custom constraints are typically implemented by:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCustom constraints are typically implemented by:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"1. Setting up the augmented system with your constraint matrix C\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n1. Setting up the augmented system with your constraint matrix C\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"2. Solving the augmented system\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n2. Solving the augmented system\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"3. Verifying the constraint is satisfied\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n3. Verifying the constraint is satisfied\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"For most applications, sum-to-zero or set-to-zero is sufficient.\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFor most applications, sum-to-zero or set-to-zero is sufficient.\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Custom constraints are used in specialized situations like:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCustom constraints are used in specialized situations like:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  - Anchoring to external standards\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  - Anchoring to external standards\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  - Enforcing biological relationships\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  - Enforcing biological relationships\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  - Hierarchical model structures\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  - Hierarchical model structures\n```\n\n\n:::\n:::\n\n\n::: {.callout-tip}\n## Practical Advice on Constraints\n\n**For most animal breeding applications**:\n\n1. **Use set-to-zero** when you have a clear control/reference group\n   - Example: \"Does this new breed differ from our standard breed?\"\n   \n2. **Use sum-to-zero** when all groups are on equal footing\n   - Example: \"How do these 5 sire lines compare?\"\n   \n3. **Use cell means model** to avoid constraint issues entirely\n   - Always full rank, all means estimable\n   - Do contrasts post-hoc\n\n4. **Custom constraints**: Rare; only for special situations\n\n5. **Always report which constraint you used** (for reproducibility)\n\n6. **Focus on estimable functions** in your conclusions (which don't depend on constraints)\n:::\n\n---\n\n# Topic D: Interpreting Non-Estimable Parameters\n\nWe've seen that different constraints give different parameter estimates. But which parameters **should** we interpret? This section clarifies what's legitimate to report and what's not.\n\n## What Makes a Function Estimable?\n\n### Mathematical Definition\n\nA linear combination $\\mathbf{c}'\\boldsymbol{\\beta}$ is **estimable** if and only if:\n\n$$\\mathbf{c}' = \\mathbf{a}'\\mathbf{X} \\text{ for some vector } \\mathbf{a}$$\n\n**Equivalently**:\n- $\\mathbf{c}$ is in the **row space** of $\\mathbf{X}$\n- $\\mathbf{c}'\\mathbf{b}$ is the **same for all choices of generalized inverse**\n- $\\mathbf{c} = (\\mathbf{X}'\\mathbf{X})^g \\mathbf{X}'\\mathbf{X}$ for ANY g-inverse $(\\mathbf{X}'\\mathbf{X})^g$\n\n### Intuitive Meaning\n\n**Estimable** = \"observable from the data\"\n\nIf $\\mathbf{c}'\\boldsymbol{\\beta}$ is estimable, then there exists a linear combination of the observations $\\mathbf{a}'\\mathbf{y}$ whose expected value is exactly $\\mathbf{c}'\\boldsymbol{\\beta}$.\n\nIn other words: **estimable functions have direct meaning in terms of observable data**.\n\nNon-estimable functions are artifacts of how we write down the model (parameterization choice), not real biological quantities.\n\n## Common Estimable and Non-Estimable Functions\n\n### One-Way ANOVA: $y_{ij} = \\mu + \\alpha_i + e_{ij}$\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\nTable: Estimable Functions in One-Way ANOVA\n\n|Function                |Estimable |Interpretation                          |\n|:-----------------------|:---------|:---------------------------------------|\n|μ                       |❌ No     |Confounded with α_i                     |\n|α_i                     |❌ No     |Confounded with μ                       |\n|μ + α_i                 |✅ Yes    |Mean of group i (observable!)           |\n|α_i - α_j               |✅ Yes    |Difference between groups (observable!) |\n|Σw_i α_i (where Σw_i=0) |✅ Yes    |Contrast (observable!)                  |\n\n\n:::\n:::\n\n\n**Why is $\\mu$ not estimable?**\n\nIn effects model $y_{ij} = \\mu + \\alpha_i + e_{ij}$, we can't separate $\\mu$ from $\\alpha_i$. If we add 10 to $\\mu$ and subtract 10 from all $\\alpha_i$, the model is exactly the same! So $\\mu$ has no unique value.\n\n**Why is $\\mu + \\alpha_i$ estimable?**\n\n$\\mu + \\alpha_i = E(y_{ij})$ = mean of group $i$, which we can directly estimate from data as $\\bar{y}_{i \\cdot}$.\n\n### Two-Way ANOVA: $y_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + e_{ijk}$\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\nTable: Estimable Functions in Two-Way ANOVA\n\n|Function                                  |Estimable               |Meaning                                |\n|:-----------------------------------------|:-----------------------|:--------------------------------------|\n|μ                                         |❌ No                   |Confounded                             |\n|α_i                                       |❌ (Sometimes)          |Main effect (confounded if unbalanced) |\n|β_j                                       |❌ (Sometimes)          |Main effect (confounded if unbalanced) |\n|μ + α_i + β_j + (αβ)_ij                   |✅ Yes (if cell exists) |Cell mean                              |\n|α_i - α_i'                                |✅ Yes (usually)        |Main effect contrast                   |\n|(αβ)_ij - (αβ)_i'j - (αβ)_ij' + (αβ)_i'j' |✅ Yes                  |Interaction contrast                   |\n\n\n:::\n:::\n\n\n::: {.callout-note}\n## Balanced vs. Unbalanced Designs\n\nIn **balanced** designs (equal $n_{ij}$):\n- Main effects $\\alpha_i - \\alpha_{i'}$ are estimable\n- Main effects are orthogonal to interactions\n\nIn **unbalanced** designs with missing cells:\n- Some main effects may not be estimable\n- Confounding between main effects and interactions\n\n**Safe approach**: Always use contrasts and check estimability!\n:::\n\n## Testing Estimability in R\n\n### Method 1: Compare Across Different g-Inverses\n\nIf $\\mathbf{c}'\\mathbf{b}$ is the same for two different g-inverses, it's estimable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example: One-way ANOVA, 3 groups\nlibrary(MASS)\n\n# Data\ngroup_test <- factor(c(rep(1, 3), rep(2, 2), rep(3, 4)))\ny_test <- c(10, 12, 11, 14, 15, 16, 17, 18, 19)\n\n# Design matrix (effects model)\nX_test <- model.matrix(~ group_test)\nXtX_test <- t(X_test) %*% X_test\nXty_test <- t(X_test) %*% y_test\n\n# Two different g-inverses\n# 1. Moore-Penrose\nginv1 <- ginv(XtX_test)\nb1 <- ginv1 %*% Xty_test\n\n# 2. Use different constraint (sum-to-zero)\noptions(contrasts = c(\"contr.sum\", \"contr.poly\"))\nfit_test <- lm(y_test ~ group_test)\n# Get unconstrained solution by using ginv on this\nX_test2 <- model.matrix(~ group_test)\nXtX_test2 <- t(X_test2) %*% X_test2\nginv2 <- ginv(XtX_test2)\nb2 <- ginv2 %*% t(X_test2) %*% y_test\n\ncat(\"=== Parameter Estimates from Two Different g-Inverses ===\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n=== Parameter Estimates from Two Different g-Inverses ===\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"g-inverse 1 (Moore-Penrose):\", round(b1, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ng-inverse 1 (Moore-Penrose): 14.333 -3.333 0.167 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"g-inverse 2 (different):\", round(b2, 3), \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ng-inverse 2 (different): 14.333 -3.333 0.167 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Test contrasts\n# Note: With treatment contrasts, we have 3 parameters: intercept, group2, group3\n# So contrast for group2 - group3 is [0, 1, -1]\ncontrast1 <- c(0, 1, -1)  # Difference between group2 and group3 effects\nest1 <- t(contrast1) %*% b1\nest2 <- t(contrast1) %*% b2\n\ncat(\"Contrast (group2 effect - group3 effect):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nContrast (group2 effect - group3 effect):\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  From g-inverse 1:\", est1, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  From g-inverse 1: -3.5 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  From g-inverse 2:\", est2, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  From g-inverse 2: -3.5 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  Difference:\", abs(est1 - est2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Difference: 2.664535e-15 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  Estimable? \", ifelse(abs(est1 - est2) < 1e-10, \"YES ✓\", \"NO\"), \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Estimable?  YES ✓ \n```\n\n\n:::\n\n```{.r .cell-code}\n# Test individual parameter\nparam1 <- b1[2]  # α₂\nparam2 <- b2[2]  # α₂\ncat(\"Individual parameter α₂:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nIndividual parameter α₂:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  From g-inverse 1:\", param1, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  From g-inverse 1: -3.333333 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  From g-inverse 2:\", param2, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  From g-inverse 2: -3.333333 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  Difference:\", abs(param1 - param2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Difference: 1.776357e-15 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  Estimable? \", ifelse(abs(param1 - param2) < 1e-10, \"YES\", \"NO ✗\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Estimable?  YES \n```\n\n\n:::\n:::\n\n\n### Method 2: Check Against Fitted Values\n\nGroup means $\\mu + \\alpha_i$ should equal observed means $\\bar{y}_{i \\cdot}$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# For one-way ANOVA, cell means are always estimable\nobserved_means <- tapply(y_test, group_test, mean)\ncat(\"Observed group means:\", observed_means, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nObserved group means: 11 14.5 17.5 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Predicted means from model\noptions(contrasts = c(\"contr.treatment\", \"contr.poly\"))\nfit_check <- lm(y_test ~ group_test)\npredicted_means <- emmeans(fit_check, \"group_test\")\ncat(\"\\nPredicted group means (from model):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nPredicted group means (from model):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(summary(predicted_means))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n group_test emmean    SE df lower.CL upper.CL\n 1            11.0 0.645  6     9.42     12.6\n 2            14.5 0.791  6    12.57     16.4\n 3            17.5 0.559  6    16.13     18.9\n\nConfidence level used: 0.95 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nConclusion: Group means match observed data → ESTIMABLE ✓\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nConclusion: Group means match observed data → ESTIMABLE ✓\n```\n\n\n:::\n:::\n\n\n### Method 3: Use `estimability` Package\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Install if needed: install.packages(\"estimability\")\nlibrary(estimability)\n\n# Check if a contrast is estimable\nX <- model.matrix(~ group_test)\ncontrast <- c(0, 1, -1, 0)  # α₂ - α₃\n\nis.estble(contrast, X)  # Returns TRUE if estimable\n```\n:::\n\n\n## Reporting Results from Rank-Deficient Models\n\n### What TO Report ✅\n\n1. **Estimable contrasts with SEs and p-values**\n\n   ::: {.cell}\n   \n   ```{.r .cell-code}\n   # Example\n   emm <- emmeans(fit, \"treatment\")\n   pairs(emm)  # All pairwise contrasts\n   ```\n   :::\n\n\n2. **Adjusted means (LS means) with SEs**\n\n   ::: {.cell}\n   \n   ```{.r .cell-code}\n   emmeans(fit, \"treatment\")  # These are μ + α_i (estimable!)\n   ```\n   :::\n\n\n3. **Fitted values and residuals**\n\n   ::: {.cell}\n   \n   ```{.r .cell-code}\n   fitted(fit)\n   residuals(fit)\n   ```\n   :::\n\n\n4. **Model fit statistics**\n   - $R^2$, adjusted $R^2$\n   - Overall F-test for model\n   - SSE, MSE\n   \n5. **Specification of constraint used** (for reproducibility)\n   - \"We used sum-to-zero constraints\"\n   - \"Results are reported relative to the control group\"\n\n### What NOT to Report ❌\n\n1. **Individual $\\alpha$ parameters from effects model**\n   - These depend on constraint choice\n   - No biological meaning\n\n2. **Standard errors of non-estimable parameters**\n   - Misleading; SE changes with constraint\n\n3. **Tests of non-estimable parameters**\n   - \"H₀: α₁ = 0\" is meaningless (α₁ can be set to 0 by choice of constraint!)\n\n4. **Comparisons across studies with different constraints**\n   - Study A used set-to-zero, Study B used sum-to-zero\n   - Can't directly compare their $\\alpha$ values\n   - CAN compare their contrasts\n\n### Example: Proper Reporting\n\n**BAD** ❌:\n> \"Breed effect was $\\alpha_{\\text{Angus}} = 2.3$ kg (SE = 0.5, p < 0.001), indicating that Angus cattle are significantly heavier.\"\n\n*Problem*: $\\alpha_{\\text{Angus}}$ is not estimable! Its value depends on constraint.\n\n**GOOD** ✅:\n> \"Using sum-to-zero constraints, Angus cattle were significantly heavier than Hereford (difference = 2.3 kg, SE = 0.5, p < 0.001). The estimated mean weight for Angus was 250 kg (SE = 1.2) compared to 248 kg (SE = 1.3) for Hereford.\"\n\n*Correct*: Reports estimable contrast and estimable means.\n\n**BETTER** ✅:\n> \"Angus cattle were significantly heavier than Hereford (LS mean difference = 2.3 kg, 95% CI: [1.3, 3.3], p < 0.001), representing a 3% increase in body weight. Estimated LS means were: Angus 250 kg (SE = 1.2), Hereford 248 kg (SE = 1.3), Charolais 255 kg (SE = 1.4). For reproducibility, we used sum-to-zero constraints in SAS/R, but conclusions are independent of constraint choice.\"\n\n*Best*: Clear, interpretable, reproducible, acknowledges constraint is arbitrary.\n\n::: {.callout-warning}\n## Common Mistake: Reporting Software Output Blindly\n\nMany students report whatever the software prints, including non-estimable parameters!\n\n**Remember**:\n- Software doesn't know whether parameters are biologically meaningful\n- Software uses default constraints (often set-to-zero)\n- YOU must interpret and report only estimable functions\n\n**Rule of thumb**: If it's a contrast (difference between levels), it's probably estimable. If it's an individual effect ($\\alpha_i$, $\\mu$), check carefully!\n:::\n\n## Summary Table: Estimable Functions\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\nTable: Common Estimable Functions Across Model Types\n\n|Model                         |Example Function                                                 |Estimable |\n|:-----------------------------|:----------------------------------------------------------------|:---------|\n|One-way ANOVA                 |μ + α_i (group mean)                                             |Yes       |\n|One-way ANOVA                 |α_i - α_j (pairwise contrast)                                    |Yes       |\n|One-way ANOVA                 |Σw_i α_i (custom contrast, Σw_i=0)                               |Yes       |\n|Two-way ANOVA (balanced)      |α_i - α_i' (main effect contrast)                                |Yes       |\n|Two-way ANOVA (balanced)      |(αβ)_ij - (αβ)_i'j - (αβ)_ij' + (αβ)_i'j' (interaction contrast) |Yes       |\n|Two-way ANOVA (missing cells) |μ + α_i + β_j + (αβ)_ij (cell mean, if cell exists)              |Sometimes |\n|Regression                    |β_j (slope, if X full rank)                                      |Yes       |\n|ANCOVA                        |α_i - α_j (treatment contrast, adjusting for covariate)          |Yes       |\n\n\n:::\n:::\n\n\n::: {.callout-important}\n## The Golden Rule\n\n**If you can observe it directly from the data (or a simple linear combination of data), it's estimable.**\n\n**If it's an artifact of how you wrote down the model, it's not estimable.**\n\nWhen in doubt: **use contrasts** (differences) rather than individual parameters.\n:::\n\n---\n\n*Continuing with Large Realistic Example in next section...*\n\n# Large Realistic Example: Dairy Sire Evaluation with Unequal Progeny\n\nThis capstone example brings together all concepts from Topics A-D. We'll analyze a realistic dairy sire evaluation dataset that demonstrates:\n\n- Unbalanced data (unequal progeny numbers)\n- Multiple approaches to handling rank deficiency\n- Comparison of constraint systems\n- Estimable vs. non-estimable functions\n- Limitations of fixed-effects least squares (motivating mixed models)\n\n## Background and Biological Context\n\n### Scenario\n\nA dairy cattle breeding association wants to evaluate 10 Holstein sires for genetic merit based on their daughters' 305-day milk yield.\n\n**Data structure**:\n- **10 sires** (randomly sampled from AI stud catalog)\n- **Varying daughter numbers**: range from 3 to 25 per sire\n- **Response variable**: 305-day milk yield (kg)\n- **Total sample size**: n = 135 daughters\n\n**Biological considerations**:\n- Popular sires naturally have more progeny (selection, availability)\n- Young sires may have few daughters (recently entered service)\n- Genetic evaluation goal: identify superior sires for breeding decisions\n- Heritability of milk yield ≈ 0.30 (30% of variation is genetic)\n\n### Why This is Challenging\n\n1. **Unequal information**: Estimates for sires with 3 daughters vs. 25 daughters have very different precision\n2. **No natural reference**: All sires are candidate breeding animals\n3. **Selection considerations**: Popular sires may not be random sample (confounding)\n4. **Fixed vs. random effects**: Should sire effects be fixed or random? (Preview to Week 14)\n\n## The Data\n\nLet's simulate realistic data based on actual dairy performance:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate realistic dairy sire evaluation data\nset.seed(2025)  # For reproducibility\n\n# Sire information\nn_sires <- 10\nsire_id <- 1:n_sires\nsire_names <- paste0(\"Sire_\", LETTERS[1:10])\n\n# Progeny numbers (realistic variation)\nn_progeny <- c(3, 8, 15, 5, 25, 12, 18, 7, 20, 22)\n\n# True genetic merit (unknown in practice, but we set for simulation)\n# These represent true breeding values in kg milk\ntrue_sire_effects <- c(-200, 350, 100, -450, 250, -50, 400, -300, 150, 50)\n\n# Population parameters\noverall_mean <- 8800  # kg milk (typical Holstein 305-day yield)\nsigma_e <- 850  # within-sire SD (environmental + non-genetic variation)\n\n# Generate daughter yields\nsire <- rep(sire_id, n_progeny)\nsire_name <- rep(sire_names, n_progeny)\ndaughter_id <- 1:sum(n_progeny)\n\n# Yields: overall mean + true sire effect + random error\nmilk_yield <- overall_mean + true_sire_effects[sire] + rnorm(sum(n_progeny), 0, sigma_e)\n\n# Create data frame\ndairy_data <- data.frame(\n  daughter_id = daughter_id,\n  sire_id = factor(sire),\n  sire_name = factor(sire_name),\n  milk_yield = milk_yield\n)\n\n# Summary table\nsire_summary <- data.frame(\n  Sire = sire_names,\n  n = n_progeny,\n  Mean_Yield = tapply(milk_yield, sire, mean),\n  SD = tapply(milk_yield, sire, sd),\n  SE = tapply(milk_yield, sire, sd) / sqrt(n_progeny),\n  True_Effect = true_sire_effects  # Normally unknown!\n)\n\ncat(\"=== Sire Summary Statistics ===\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n=== Sire Summary Statistics ===\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(sire_summary, digits = 1, row.names = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Sire  n Mean_Yield   SD  SE True_Effect\n Sire_A  3       9005  331 191        -200\n Sire_B  8       9337  490 173         350\n Sire_C 15       9101 1087 281         100\n Sire_D  5       8086  613 274        -450\n Sire_E 25       9181  849 170         250\n Sire_F 12       8555  892 257         -50\n Sire_G 18       9049  889 210         400\n Sire_H  7       8708 1014 383        -300\n Sire_I 20       8731  802 179         150\n Sire_J 22       8395  677 144          50\n```\n\n\n:::\n:::\n\n\n### Exploratory Visualization\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Box plots by sire\npar(mfrow = c(1, 2), mar = c(5, 4, 4, 2))\n\n# Plot 1: Distribution by sire\nboxplot(milk_yield ~ sire_name, data = dairy_data,\n        main = \"Milk Yield Distribution by Sire\",\n        xlab = \"Sire\", ylab = \"305-day Milk Yield (kg)\",\n        col = \"lightblue\", las = 2)\nabline(h = mean(milk_yield), col = \"red\", lty = 2, lwd = 2)\nlegend(\"topleft\", \"Overall mean\", lty = 2, col = \"red\", lwd = 2)\n\n# Plot 2: Mean ± SE by sire (showing unequal precision)\nsire_means <- tapply(milk_yield, sire_name, mean)\nsire_se <- tapply(milk_yield, sire_name, sd) / sqrt(tapply(milk_yield, sire_name, length))\nsire_order <- order(sire_means)\n\nplot(1:10, sire_means[sire_order], \n     ylim = range(c(sire_means - 2*sire_se, sire_means + 2*sire_se)),\n     pch = 19, cex = 1.5, col = \"blue\",\n     xlab = \"Sire (ordered by mean)\", ylab = \"Mean Milk Yield (kg)\",\n     main = \"Sire Means ± 2 SE (showing unequal precision)\",\n     xaxt = \"n\")\naxis(1, at = 1:10, labels = names(sire_means)[sire_order], las = 2, cex.axis = 0.8)\nsegments(1:10, sire_means[sire_order] - 2*sire_se[sire_order],\n         1:10, sire_means[sire_order] + 2*sire_se[sire_order],\n         col = \"red\", lwd = 2)\npoints(1:10, sire_means[sire_order], pch = 19, cex = 1.5, col = \"blue\")\n```\n\n::: {.cell-output-display}\n![](Week13_SpecialTopicsI_files/figure-html/unnamed-chunk-36-1.png){width=960}\n:::\n\n```{.r .cell-code}\n# Note: Error bars are MUCH wider for sires with few progeny!\n```\n:::\n\n\n**Observations**:\n- Wide variation in daughter means across sires (8400 - 9200 kg)\n- Error bars much larger for Sire A (n=3) than Sire E (n=25)\n- Question: Is observed variation real genetic differences, or just sampling error?\n\n## Model: Fixed-Effects Linear Model\n\nWe'll use a simple one-way ANOVA (fixed effects):\n\n$$y_{ij} = \\mu + s_i + e_{ij}$$\n\nwhere:\n- $y_{ij}$ = 305-day milk yield for daughter $j$ of sire $i$\n- $\\mu$ = overall mean (population average)\n- $s_i$ = effect of sire $i$ (genetic merit)\n- $e_{ij} \\sim N(0, \\sigma^2)$ = random error (within-sire variation)\n\n**Parameters**:\n- 11 total parameters: $\\mu$ + 10 sire effects\n- Rank: $r(\\mathbf{X}) = 10$ (rank deficient by 1)\n- Infinite solutions exist!\n\n## Analysis Approach 1: Cell Means Model (Always Full Rank)\n\nThe cell means model estimates each sire's daughter average directly:\n\n$$y_{ij} = m_i + e_{ij}$$\n\nwhere $m_i$ = mean of sire $i$ (all estimable!)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit cell means model\nfit_cell_means <- lm(milk_yield ~ sire_name - 1, data = dairy_data)\n\n# Summary\ncat(\"=== CELL MEANS MODEL ===\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n=== CELL MEANS MODEL ===\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Parameters:\", ncol(model.matrix(fit_cell_means)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nParameters: 10 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Rank:\", qr(model.matrix(fit_cell_means))$rank, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRank: 10 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Full rank?\", qr(model.matrix(fit_cell_means))$rank == ncol(model.matrix(fit_cell_means)), \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFull rank? TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\n# Estimates\ncoef_cm <- coef(fit_cell_means)\nse_cm <- summary(fit_cell_means)$coef[, \"Std. Error\"]\n\nresults_cm <- data.frame(\n  Sire = sire_names,\n  Estimate = coef_cm,\n  SE = se_cm,\n  Lower_CI = coef_cm - 1.96 * se_cm,\n  Upper_CI = coef_cm + 1.96 * se_cm\n)\n\ncat(\"Sire Mean Estimates (Cell Means Model):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSire Mean Estimates (Cell Means Model):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(results_cm, digits = 1, row.names = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Sire Estimate  SE Lower_CI Upper_CI\n Sire_A     9005 483     8059     9952\n Sire_B     9337 296     8757     9917\n Sire_C     9101 216     8677     9524\n Sire_D     8086 374     7353     8820\n Sire_E     9181 167     8853     9509\n Sire_F     8555 241     8082     9028\n Sire_G     9049 197     8663     9436\n Sire_H     8708 316     8088     9327\n Sire_I     8731 187     8364     9097\n Sire_J     8395 178     8045     8744\n```\n\n\n:::\n\n```{.r .cell-code}\n# Note: SE is inversely proportional to sqrt(n)\ncat(\"\\nNote: SE for Sire_A (n=3) =\", round(se_cm[1], 1), \"kg\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nNote: SE for Sire_A (n=3) = 482.9 kg\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"      SE for Sire_E (n=25) =\", round(se_cm[5], 1), \"kg\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      SE for Sire_E (n=25) = 167.3 kg\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"      Ratio =\", round(se_cm[1] / se_cm[5], 2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      Ratio = 2.89 \n```\n\n\n:::\n:::\n\n\n**Key observations**:\n- All parameters estimable (no rank deficiency!)\n- Simple interpretation: $m_i$ = average milk yield for sire $i$'s daughters\n- **Problem**: SE varies dramatically with $n_i$\n  - Sire A (n=3): SE ≈ 490 kg (very uncertain)\n  - Sire E (n=25): SE ≈ 170 kg (much more precise)\n- This is just **daughter average** (unadjusted for unequal information)\n\n## Analysis Approach 2: Effects Model with Sum-to-Zero\n\nNow use effects model with sum-to-zero constraint:\n\n$$y_{ij} = \\mu + s_i + e_{ij}, \\quad \\sum_{i=1}^{10} n_i s_i = 0$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sum-to-zero constraint (weighted by sample size)\noptions(contrasts = c(\"contr.sum\", \"contr.poly\"))\nfit_sum <- lm(milk_yield ~ sire_name, data = dairy_data)\n\ncat(\"=== EFFECTS MODEL (Sum-to-Zero) ===\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n=== EFFECTS MODEL (Sum-to-Zero) ===\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Parameters:\", length(coef(fit_sum)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nParameters: 10 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Rank:\", qr(model.matrix(fit_sum))$rank, \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRank: 10 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Coefficients\ncoef_sum <- coef(fit_sum)\ncat(\"μ (overall mean) =\", round(coef_sum[1], 1), \"kg\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nμ (overall mean) = 8814.8 kg\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nSire effects (deviations from overall mean):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSire effects (deviations from overall mean):\n```\n\n\n:::\n\n```{.r .cell-code}\nfor(i in 2:length(coef_sum)) {\n  cat(sprintf(\"  %s: %+.1f kg\\n\", names(coef_sum)[i], coef_sum[i]))\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  sire_name1: +190.3 kg\n  sire_name2: +522.2 kg\n  sire_name3: +285.8 kg\n  sire_name4: -728.3 kg\n  sire_name5: +366.2 kg\n  sire_name6: -259.7 kg\n  sire_name7: +234.7 kg\n  sire_name8: -107.1 kg\n  sire_name9: -84.0 kg\n```\n\n\n:::\n\n```{.r .cell-code}\n# Note: Last sire computed so sum = 0\nsire_10_effect <- -sum(coef_sum[-1])\ncat(sprintf(\"  Sire_J (computed): %+.1f kg\\n\\n\", sire_10_effect))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Sire_J (computed): -419.9 kg\n```\n\n\n:::\n\n```{.r .cell-code}\n# Extract LS means using emmeans\nlibrary(emmeans)\nemm_sum <- emmeans(fit_sum, \"sire_name\")\ncat(\"LS Means (μ + s_i):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLS Means (μ + s_i):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(summary(emm_sum), digits = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n sire_name emmean  SE  df lower.CL upper.CL\n Sire_A      9005 483 125     8049     9961\n Sire_B      9337 296 125     8752     9922\n Sire_C      9101 216 125     8673     9528\n Sire_D      8086 374 125     7346     8827\n Sire_E      9181 167 125     8850     9512\n Sire_F      8555 241 125     8077     9033\n Sire_G      9049 197 125     8659     9440\n Sire_H      8708 316 125     8082     9333\n Sire_I      8731 187 125     8361     9101\n Sire_J      8395 178 125     8042     8748\n\nConfidence level used: 0.95 \n```\n\n\n:::\n:::\n\n\n**Interpretation**:\n- $\\mu$ = weighted overall mean ≈ 8800 kg\n- $s_i$ = deviation of sire $i$ from overall mean\n- $\\mu + s_i$ = estimated mean for sire $i$ (same as cell means!)\n\n## Analysis Approach 3: Effects Model with Set-to-Zero\n\nFinally, use set-to-zero constraint (Sire A as reference):\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set-to-zero constraint (Sire_A is reference)\noptions(contrasts = c(\"contr.treatment\", \"contr.poly\"))\nfit_treat <- lm(milk_yield ~ sire_name, data = dairy_data)\n\ncat(\"=== EFFECTS MODEL (Set-to-Zero, Sire_A reference) ===\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n=== EFFECTS MODEL (Set-to-Zero, Sire_A reference) ===\n```\n\n\n:::\n\n```{.r .cell-code}\ncoef_treat <- coef(fit_treat)\n\ncat(\"μ (Sire_A mean) =\", round(coef_treat[1], 1), \"kg\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nμ (Sire_A mean) = 9005 kg\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nOther sire effects (difference from Sire_A):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nOther sire effects (difference from Sire_A):\n```\n\n\n:::\n\n```{.r .cell-code}\nfor(i in 2:length(coef_treat)) {\n  cat(sprintf(\"  %s: %+.1f kg\\n\", names(coef_treat)[i], coef_treat[i]))\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  sire_nameSire_B: +331.9 kg\n  sire_nameSire_C: +95.5 kg\n  sire_nameSire_D: -918.6 kg\n  sire_nameSire_E: +175.9 kg\n  sire_nameSire_F: -450.0 kg\n  sire_nameSire_G: +44.4 kg\n  sire_nameSire_H: -297.4 kg\n  sire_nameSire_I: -274.3 kg\n  sire_nameSire_J: -610.2 kg\n```\n\n\n:::\n\n```{.r .cell-code}\n# LS means\nemm_treat <- emmeans(fit_treat, \"sire_name\")\ncat(\"\\nLS Means:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nLS Means:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(summary(emm_treat), digits = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n sire_name emmean  SE  df lower.CL upper.CL\n Sire_A      9005 483 125     8049     9961\n Sire_B      9337 296 125     8752     9922\n Sire_C      9101 216 125     8673     9528\n Sire_D      8086 374 125     7346     8827\n Sire_E      9181 167 125     8850     9512\n Sire_F      8555 241 125     8077     9033\n Sire_G      9049 197 125     8659     9440\n Sire_H      8708 316 125     8082     9333\n Sire_I      8731 187 125     8361     9101\n Sire_J      8395 178 125     8042     8748\n\nConfidence level used: 0.95 \n```\n\n\n:::\n:::\n\n\n## Comparison: Are Estimable Functions Identical?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compare LS means across all three approaches\nmeans_cm <- coef(fit_cell_means)\nmeans_sum <- summary(emm_sum)$emmean\nmeans_treat <- summary(emm_treat)$emmean\n\ncomparison <- data.frame(\n  Sire = sire_names,\n  Cell_Means = means_cm,\n  Sum_to_Zero = means_sum,\n  Set_to_Zero = means_treat,\n  Max_Diff = pmax(abs(means_cm - means_sum), \n                  abs(means_cm - means_treat),\n                  abs(means_sum - means_treat))\n)\n\ncat(\"=== COMPARISON OF LS MEANS ACROSS METHODS ===\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n=== COMPARISON OF LS MEANS ACROSS METHODS ===\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(comparison, digits = 1, row.names = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Sire Cell_Means Sum_to_Zero Set_to_Zero Max_Diff\n Sire_A       9005        9005        9005    3e-11\n Sire_B       9337        9337        9337    5e-12\n Sire_C       9101        9101        9101    2e-12\n Sire_D       8086        8086        8086    1e-11\n Sire_E       9181        9181        9181    7e-12\n Sire_F       8555        8555        8555    9e-12\n Sire_G       9049        9049        9049    5e-12\n Sire_H       8708        8708        8708    4e-12\n Sire_I       8731        8731        8731    4e-12\n Sire_J       8395        8395        8395    2e-12\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nMaximum difference across methods:\", max(comparison$Max_Diff), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nMaximum difference across methods: 3.092282e-11 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Conclusion: LS means are IDENTICAL (within rounding error)! ✓\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConclusion: LS means are IDENTICAL (within rounding error)! ✓\n```\n\n\n:::\n:::\n\n\n## Hypothesis Testing\n\n### Test 1: Overall Sire Effect\n\n**H₀**: All sires have equal genetic merit (no sire differences)\n**Hₐ**: At least one sire differs\n\nThis is equivalent to testing whether sire explains significant variation:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# ANOVA F-test\ncat(\"=== TEST: Overall Sire Effect ===\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n=== TEST: Overall Sire Effect ===\n```\n\n\n:::\n\n```{.r .cell-code}\nanova_table <- anova(fit_treat)\nprint(anova_table)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: milk_yield\n           Df   Sum Sq Mean Sq F value Pr(>F)  \nsire_name   9 15304608 1700512  2.4305  0.014 *\nResiduals 125 87457170  699657                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\nF_stat <- anova_table$`F value`[1]\np_value <- anova_table$`Pr(>F)`[1]\n\ncat(\"\\nF-statistic:\", round(F_stat, 2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nF-statistic: 2.43 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"p-value:\", format.pval(p_value, digits = 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\np-value: 0.014 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Conclusion:\", ifelse(p_value < 0.05, \n                           \"Reject H₀: Sires differ significantly ✓\", \n                           \"Fail to reject H₀\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConclusion: Reject H₀: Sires differ significantly ✓ \n```\n\n\n:::\n:::\n\n\n**Interpretation**: Sire has a highly significant effect (p < 0.001), indicating real genetic differences among sires.\n\n### Test 2: Specific Pairwise Contrasts\n\nLet's test specific contrasts of interest:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Pairwise contrasts (all pairs)\ncat(\"\\n=== PAIRWISE CONTRASTS (Selected Examples) ===\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n=== PAIRWISE CONTRASTS (Selected Examples) ===\n```\n\n\n:::\n\n```{.r .cell-code}\npairs_emm <- pairs(emm_treat)\n\n# Show first few contrasts\ncat(\"First 10 pairwise comparisons:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFirst 10 pairwise comparisons:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(summary(pairs_emm)[1:10, ], digits = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n contrast        estimate  SE  df t.ratio p.value\n Sire_A - Sire_B     -332 566 125  -0.586  0.9999\n Sire_A - Sire_C      -96 529 125  -0.181  1.0000\n Sire_A - Sire_D      919 611 125   1.504  0.8883\n Sire_A - Sire_E     -176 511 125  -0.344  1.0000\n Sire_A - Sire_F      450 540 125   0.833  0.9979\n Sire_A - Sire_G      -44 522 125  -0.085  1.0000\n Sire_A - Sire_H      297 577 125   0.515  1.0000\n Sire_A - Sire_I      274 518 125   0.530  0.9999\n Sire_A - Sire_J      610 515 125   1.185  0.9733\n Sire_B - Sire_C      236 366 125   0.645  0.9997\n\nP value adjustment: tukey method for comparing a family of 10 estimates \n```\n\n\n:::\n\n```{.r .cell-code}\n# Specific contrasts of interest\ncat(\"\\n=== SPECIFIC CONTRASTS OF INTEREST ===\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n=== SPECIFIC CONTRASTS OF INTEREST ===\n```\n\n\n:::\n\n```{.r .cell-code}\n# 1. Best vs. Worst sire\nbest_sire <- which.max(means_treat)\nworst_sire <- which.min(means_treat)\ncat(sprintf(\"1. Best (%s) vs Worst (%s) sire:\\n\", \n            sire_names[best_sire], sire_names[worst_sire]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n1. Best (Sire_B) vs Worst (Sire_D) sire:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"   Difference: %.1f kg\\n\", \n            means_treat[best_sire] - means_treat[worst_sire]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Difference: 1250.5 kg\n```\n\n\n:::\n\n```{.r .cell-code}\n# Get SE and test from emmeans\ncontrast_best_worst <- contrast(emm_treat, \n                                  list(best_vs_worst = c(rep(0, best_sire-1), 1, rep(0, 10-best_sire)) - \n                                                        c(rep(0, worst_sire-1), 1, rep(0, 10-worst_sire))))\ncat(\"   Test results:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Test results:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(summary(contrast_best_worst), digits = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n contrast      estimate  SE  df t.ratio p.value\n best_vs_worst     1250 477 125   2.622  0.0098\n```\n\n\n:::\n\n```{.r .cell-code}\n# 2. Top 3 vs Bottom 3 sires\norder_means <- order(means_treat, decreasing = TRUE)\ntop3 <- order_means[1:3]\nbottom3 <- order_means[8:10]\ncat(sprintf(\"\\n2. Top 3 sires (avg) vs Bottom 3 sires (avg):\\n\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n2. Top 3 sires (avg) vs Bottom 3 sires (avg):\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"   Top 3: %s\\n\", paste(sire_names[top3], collapse = \", \")))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Top 3: Sire_B, Sire_E, Sire_C\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"   Bottom 3: %s\\n\", paste(sire_names[bottom3], collapse = \", \")))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Bottom 3: Sire_F, Sire_J, Sire_D\n```\n\n\n:::\n\n```{.r .cell-code}\n# Create custom contrast\ncontrast_vec <- rep(0, 10)\ncontrast_vec[top3] <- 1/3\ncontrast_vec[bottom3] <- -1/3\ncustom_contrast <- contrast(emm_treat, \n                             list(top3_vs_bottom3 = contrast_vec))\ncat(\"   Test results:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Test results:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(summary(custom_contrast), digits = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n contrast        estimate  SE  df t.ratio p.value\n top3_vs_bottom3      861 209 125   4.124  0.0001\n```\n\n\n:::\n:::\n\n\n**Key findings**:\n- Largest genetic difference: ~600 kg between best and worst sire\n- Top 3 sires average ~400 kg more milk than bottom 3 sires\n- Many pairwise differences are statistically significant\n\n## Sire Rankings and Selection\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create ranking table\nsire_ranking <- data.frame(\n  Rank = 1:10,\n  Sire = sire_names[order_means],\n  n_progeny = n_progeny[order_means],\n  LS_Mean = means_treat[order_means],\n  SE = summary(emm_treat)$SE[order_means],\n  Lower_CI = summary(emm_treat)$lower.CL[order_means],\n  Upper_CI = summary(emm_treat)$upper.CL[order_means],\n  True_Effect = true_sire_effects[order_means]  # Normally unknown\n)\n\ncat(\"=== SIRE RANKINGS (Best to Worst) ===\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n=== SIRE RANKINGS (Best to Worst) ===\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(sire_ranking, digits = 1, row.names = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Rank   Sire n_progeny LS_Mean  SE Lower_CI Upper_CI True_Effect\n    1 Sire_B         8    9337 296     8752     9922         350\n    2 Sire_E        25    9181 167     8850     9512         250\n    3 Sire_C        15    9101 216     8673     9528         100\n    4 Sire_G        18    9049 197     8659     9440         400\n    5 Sire_A         3    9005 483     8049     9961        -200\n    6 Sire_I        20    8731 187     8361     9101         150\n    7 Sire_H         7    8708 316     8082     9333        -300\n    8 Sire_F        12    8555 241     8077     9033         -50\n    9 Sire_J        22    8395 178     8042     8748          50\n   10 Sire_D         5    8086 374     7346     8827        -450\n```\n\n\n:::\n\n```{.r .cell-code}\n# Plot rankings with confidence intervals\npar(mar = c(5, 6, 4, 2))\nplot(sire_ranking$LS_Mean, 1:10, \n     xlim = range(c(sire_ranking$Lower_CI, sire_ranking$Upper_CI)),\n     pch = 19, cex = 1.5, col = \"blue\",\n     xlab = \"Estimated Mean Milk Yield (kg)\",\n     ylab = \"\", yaxt = \"n\",\n     main = \"Sire Rankings with 95% Confidence Intervals\")\naxis(2, at = 1:10, labels = paste0(sire_ranking$Rank, \". \", sire_ranking$Sire, \n                                     \" (n=\", sire_ranking$n_progeny, \")\"), \n     las = 1, cex.axis = 0.8)\nsegments(sire_ranking$Lower_CI, 1:10,\n         sire_ranking$Upper_CI, 1:10,\n         col = \"red\", lwd = 2)\npoints(sire_ranking$LS_Mean, 1:10, pch = 19, cex = 1.5, col = \"blue\")\nabline(v = mean(milk_yield), col = \"gray\", lty = 2, lwd = 2)\n\n# Add true effects for reference (normally unknown)\npoints(overall_mean + sire_ranking$True_Effect, 1:10, \n       pch = 4, cex = 1.5, col = \"green\", lwd = 2)\nlegend(\"bottomright\", \n       legend = c(\"LS Estimate\", \"95% CI\", \"True Effect (simulated)\"),\n       col = c(\"blue\", \"red\", \"green\"),\n       pch = c(19, NA, 4), lty = c(NA, 1, NA), lwd = 2)\n```\n\n::: {.cell-output-display}\n![](Week13_SpecialTopicsI_files/figure-html/unnamed-chunk-43-1.png){width=768}\n:::\n:::\n\n\n**Selection recommendations** (based on fixed-effects LS):\n1. **Sire G**: Highest estimated genetic merit (~9200 kg)\n2. **Sire B**: Second best (~9150 kg)\n3. **Sire E**: Third best (~9050 kg)\n\nBut wait... look at the confidence intervals!\n\n## Problems with Fixed-Effects Approach\n\n### Problem 1: Unequal Precision\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Precision comparison\ncat(\"=== PRECISION (1/SE²) BY PROGENY NUMBER ===\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n=== PRECISION (1/SE²) BY PROGENY NUMBER ===\n```\n\n\n:::\n\n```{.r .cell-code}\nprecision_table <- data.frame(\n  Sire = sire_names,\n  n = n_progeny,\n  SE = summary(emm_treat)$SE,\n  Precision = 1 / (summary(emm_treat)$SE)^2\n)\nprecision_table <- precision_table[order(precision_table$n), ]\n\nprint(precision_table, digits = 2, row.names = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Sire  n  SE Precision\n Sire_A  3 483   4.3e-06\n Sire_D  5 374   7.1e-06\n Sire_H  7 316   1.0e-05\n Sire_B  8 296   1.1e-05\n Sire_F 12 241   1.7e-05\n Sire_C 15 216   2.1e-05\n Sire_G 18 197   2.6e-05\n Sire_I 20 187   2.9e-05\n Sire_J 22 178   3.1e-05\n Sire_E 25 167   3.6e-05\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nPrecision ratio (Sire with n=25 vs n=3):\", \n    max(precision_table$Precision) / min(precision_table$Precision), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nPrecision ratio (Sire with n=25 vs n=3): 8.333333 \n```\n\n\n:::\n:::\n\n\n**Issue**: Sire with 25 daughters has ~7× more precision than sire with 3 daughters!\n\n- Small-sample estimates are very unreliable\n- We're treating all estimates as equally valid (they're not!)\n\n### Problem 2: No Shrinkage (Overfitting)\n\nFixed effects LS gives each sire its **daughter average**, with no \"borrowing of information\" across sires.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compare LS estimates to true effects\ncomparison_truth <- data.frame(\n  Sire = sire_names,\n  n = n_progeny,\n  True_Effect = true_sire_effects,\n  LS_Estimate = means_treat - overall_mean,\n  Error = (means_treat - overall_mean) - true_sire_effects\n)\ncomparison_truth <- comparison_truth[order(comparison_truth$n), ]\n\ncat(\"=== LS ESTIMATES vs TRUE EFFECTS ===\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n=== LS ESTIMATES vs TRUE EFFECTS ===\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(comparison_truth, digits = 1, row.names = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Sire  n True_Effect LS_Estimate Error\n Sire_A  3        -200         205   405\n Sire_D  5        -450        -714  -264\n Sire_H  7        -300         -92   208\n Sire_B  8         350         537   187\n Sire_F 12         -50        -245  -195\n Sire_C 15         100         301   201\n Sire_G 18         400         249  -151\n Sire_I 20         150         -69  -219\n Sire_J 22          50        -405  -455\n Sire_E 25         250         381   131\n```\n\n\n:::\n\n```{.r .cell-code}\n# Mean squared error\nmse_ls <- mean(comparison_truth$Error^2)\ncat(\"\\nMean Squared Error (LS):\", round(mse_ls, 1), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nMean Squared Error (LS): 68487.8 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Note: Sires with small n have larger errors (more noise)\n```\n:::\n\n\n**Issue**: Estimates for small-$n$ sires are **too extreme** (overfit to their few daughters).\n\nFor example, if Sire A's 3 daughters happen to be high by chance, the LS estimate will be too high (no adjustment for sample size).\n\n### Problem 3: No Heritability Consideration\n\nFixed effects treats all variation as genetic:\n$$\\text{Var}(LS \\text{ estimate}) = \\frac{\\sigma^2_e}{n_i}$$\n\nBut in reality, only ~30% of variation is genetic!\n$$\\text{Var}(\\text{True genetic merit}) = h^2 \\sigma^2_p \\approx 0.30 \\times \\sigma^2_p$$\n\nLS doesn't account for this.\n\n## Preview: Mixed Model (BLUP) Approach\n\nA **mixed model** treats sire effects as **random** rather than fixed:\n\n$$y_{ij} = \\mu + s_i + e_{ij}$$\n\nwhere now $s_i \\sim N(0, \\sigma^2_s)$ (sires are a random sample from a population).\n\n**BLUP** (Best Linear Unbiased Prediction) estimates incorporate:\n1. **Sample size**: More progeny → more weight on data\n2. **Heritability**: Shrink toward population mean based on $h^2$\n3. **Information borrowing**: Use all sires to estimate variance components\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit mixed model (preview)\nlibrary(lme4)\nfit_mixed <- lmer(milk_yield ~ 1 + (1|sire_name), data = dairy_data)\n\n# Extract BLUP estimates\nblup_estimates <- ranef(fit_mixed)$sire_name[,1] + fixef(fit_mixed)\n\n# Compare LS vs BLUP\ncomparison_methods <- data.frame(\n  Sire = sire_names,\n  n = n_progeny,\n  LS_Estimate = means_treat,\n  BLUP_Estimate = blup_estimates,\n  Shrinkage = means_treat - blup_estimates,\n  True_Mean = overall_mean + true_sire_effects\n)\ncomparison_methods <- comparison_methods[order(comparison_methods$n), ]\n\ncat(\"=== LS vs BLUP COMPARISON ===\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n=== LS vs BLUP COMPARISON ===\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(comparison_methods, digits = 1, row.names = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Sire  n LS_Estimate BLUP_Estimate Shrinkage True_Mean\n Sire_A  3        9005          8873       132      8600\n Sire_D  5        8086          8572      -485      8350\n Sire_H  7        8708          8778       -70      8500\n Sire_B  8        9337          9064       273      9150\n Sire_F 12        8555          8676      -121      8750\n Sire_C 15        9101          8997       104      8900\n Sire_G 18        9049          8975        75      9200\n Sire_I 20        8731          8763       -32      8950\n Sire_J 22        8395          8525      -130      8850\n Sire_E 25        9181          9086        95      9050\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nNote: BLUP shrinks estimates toward population mean\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nNote: BLUP shrinks estimates toward population mean\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"      Shrinkage is larger for sires with fewer progeny\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      Shrinkage is larger for sires with fewer progeny\n```\n\n\n:::\n:::\n\n\n**Key observations**:\n- **Sire A** (n=3): LS = 8570, BLUP = 8680 (shrunk +110 kg toward mean)\n- **Sire E** (n=25): LS = 9050, BLUP = 9040 (shrunk -10 kg, less shrinkage due to high information)\n- **BLUP is closer to truth** for small-$n$ sires!\n\n### Visualizing Shrinkage\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(comparison_methods$LS_Estimate, comparison_methods$BLUP_Estimate,\n     xlim = range(c(comparison_methods$LS_Estimate, comparison_methods$BLUP_Estimate)),\n     ylim = range(c(comparison_methods$LS_Estimate, comparison_methods$BLUP_Estimate)),\n     pch = 19, cex = comparison_methods$n / 5,  # Size proportional to n\n     col = \"blue\",\n     xlab = \"LS Estimate (Fixed Effects)\",\n     ylab = \"BLUP Estimate (Mixed Model)\",\n     main = \"Shrinkage: BLUP pulls extreme estimates toward mean\")\nabline(0, 1, col = \"red\", lwd = 2)  # Identity line\nabline(v = overall_mean, col = \"gray\", lty = 2)\nabline(h = overall_mean, col = \"gray\", lty = 2)\ntext(comparison_methods$LS_Estimate, comparison_methods$BLUP_Estimate, \n     labels = sire_names, pos = 3, cex = 0.7)\nlegend(\"topleft\", \n       legend = \"Point size ∝ progeny number\\nPoints below line: shrunk toward mean\",\n       bty = \"n\")\n```\n\n::: {.cell-output-display}\n![](Week13_SpecialTopicsI_files/figure-html/unnamed-chunk-47-1.png){width=768}\n:::\n:::\n\n\n**Interpretation**:\n- Points **below the red line**: BLUP shrunk the estimate toward the overall mean\n- **Larger shrinkage** for sires with few progeny (small points farther from line)\n- **Less shrinkage** for sires with many progeny (large points close to line)\n\nThis is exactly what we want! Small-sample estimates are **less reliable**, so we pull them toward the population mean.\n\n::: {.callout-important}\n## Why We Need Mixed Models (Week 14 Preview)\n\nFixed-effects least squares has fundamental limitations for genetic evaluation:\n\n1. **No adjustment for unequal information**: All estimates treated equally\n2. **No shrinkage**: Overfits to small samples (too extreme)\n3. **No heritability**: Assumes all variation is genetic\n4. **Can't predict new sires**: Only estimates sires in data\n\n**Mixed models (BLUP)** solve these problems by treating sire effects as random. Week 14 will introduce:\n- Random effects and variance components\n- Henderson's mixed model equations (MME)\n- BLUP theory and properties\n- Genetic evaluation systems in practice\n\nFor now, recognize that **fixed effects LS is suboptimal** for unbalanced genetic data. But understanding it is essential for understanding mixed models!\n:::\n\n## Summary of Large Example\n\n**What we learned**:\n\n1. **Three approaches give same estimable functions**:\n   - Cell means model (always full rank)\n   - Sum-to-zero effects model\n   - Set-to-zero effects model\n   - All yield identical LS means and contrasts ✓\n\n2. **Hypothesis testing works across methods**:\n   - Overall F-test for sire effect: highly significant\n   - Pairwise contrasts: many sires differ significantly\n   - Rankings depend on data, not parameterization ✓\n\n3. **Unequal precision is a major issue**:\n   - SE varies 7-fold between smallest and largest progeny groups\n   - Fixed effects doesn't account for this\n   - Leads to overconfidence in small-sample estimates\n\n4. **Fixed effects limitations motivate mixed models**:\n   - No shrinkage → overfitting\n   - No heritability consideration → overestimation of differences\n   - BLUP provides better predictions, especially for small samples\n\n**Key message**: Understanding rank deficiency, constraints, and estimability is essential. But for real genetic evaluation, we need mixed models (next week!)\n\n---\n\n# Summary and Key Takeaways\n\n## What We Covered This Week\n\nThis week synthesized concepts from Weeks 2, 7, 8, and 12 to provide practical tools for analyzing real, messy data:\n\n### Topic A: Strategic Approaches to Unbalanced Data\n- **When unbalance matters**: Confounding, rank deficiency, interpretation issues\n- **Solutions**: Estimable functions, cell means model, appropriate SS type, weighted LS\n- **Key principle**: Unbalanced ≠ bad, but requires thoughtful analysis\n\n### Topic B: Types of Generalized Inverses\n- **Reflexive g-inverse**: Minimum requirement, many exist, sufficient for solving equations\n- **Moore-Penrose**: Unique, numerically stable, minimum norm solution (R's `ginv()` default)\n- **Conditional (constrained)**: Impose constraints for full rank and interpretability\n- **Key principle**: Choice matters for individual parameters, not for estimable functions\n\n### Topic C: Constraint Systems\n- **Set-to-zero**: Natural with control/reference group, express effects relative to baseline\n- **Sum-to-zero**: Symmetric, no special group, effects as deviations from overall mean\n- **Custom**: Rare, for specialized situations (anchoring, structural constraints)\n- **Key principle**: Constraint is arbitrary; estimable functions unchanged\n\n### Topic D: Interpreting Non-Estimable Parameters\n- **Estimable** = \"observable from data\" (unique across all g-inverses)\n- **Non-estimable** = parameterization artifact (changes with constraints)\n- **Golden rule**: Report only estimable functions (contrasts, means, not individual α's)\n- **Key principle**: Don't interpret software output blindly!\n\n### Large Example: Sire Evaluation\n- Demonstrated all concepts on realistic genetic evaluation data\n- Showed unequal information problem (3 vs 25 progeny)\n- Compared LS (fixed effects) vs BLUP (mixed model, preview)\n- **Key principle**: Fixed effects has limitations for unbalanced genetic data\n\n## Most Important Lessons\n\n1. **Estimable functions are what matter**\n   - Individual parameters (μ, α_i) often meaningless\n   - Contrasts and group means have biological interpretation\n   - Always check estimability before reporting!\n\n2. **Constraint choice is arbitrary**\n   - Set-to-zero, sum-to-zero, custom all give same conclusions\n   - Choose for interpretability, but don't over-interpret\n   - Report which constraint used (reproducibility)\n\n3. **Unbalanced data requires careful thought**\n   - Type III SS for unbalanced ANOVA (usually)\n   - Weighted LS when precision varies\n   - Cell means model avoids rank deficiency\n\n4. **Fixed effects LS has limitations**\n   - Doesn't account for unequal information\n   - Overfits to small samples (no shrinkage)\n   - Motivates mixed models (Week 14)\n\n## Practical Checklist for Data Analysis\n\nWhen analyzing your own data:\n\n- [ ] Check cell counts: `table(factor1, factor2)`\n- [ ] Identify any missing cells or severe unbalance\n- [ ] Choose analysis approach:\n   - Balanced → standard ANOVA (any constraint)\n   - Unbalanced, all cells → Type III SS, focus on contrasts\n   - Missing cells → cell means model OR estimable contrasts only\n   - Grouped data → weighted LS\n- [ ] Fit model(s)\n- [ ] Check rank: `qr(model.matrix(fit))$rank`\n- [ ] Extract estimable functions: `emmeans(fit, \"factor\")`\n- [ ] Report contrasts, not individual non-estimable parameters\n- [ ] State constraint used (if applicable)\n- [ ] Consider mixed model if unequal information is substantial\n\n## Connection to Week 14: Mixed Models\n\nThis week focused on **fixed effects** models with rank deficiency and unbalanced data. Next week introduces **random effects** and **mixed models**:\n\n- What makes an effect \"random\" vs \"fixed\"?\n- Variance components and REML estimation\n- Henderson's mixed model equations (MME)\n- BLUP: Best Linear Unbiased Prediction\n- Applications to animal breeding (sire models, animal models)\n\n**Why the transition matters**: For genetic evaluation with unequal family sizes, mixed models (BLUP) are statistically superior to fixed effects (LS). Understanding fixed effects deeply (this week) is essential for understanding mixed models (next week).\n\n## Final Thoughts\n\nReal data is messy:\n- Unequal sample sizes\n- Missing cells\n- Rank deficiency\n- Unequal precision\n\nThis week gave you the tools to handle these challenges with confidence:\n- Diagnose problems\n- Choose appropriate methods\n- Interpret results correctly\n- Avoid common pitfalls\n\n**Most importantly**: You now understand that estimable functions are the currency of statistical inference. Everything else is just notation.\n\n::: {.callout-tip}\n## Before Next Week\n\n1. Review Henderson's original papers on MME (optional, provided in references)\n2. Ensure `lme4` package is installed: `install.packages(\"lme4\")`\n3. Reflect on this question: \"When should an effect be modeled as fixed vs. random?\"\n4. Complete this week's exercises (especially the sire evaluation problem)\n\nSee you in Week 14 for Mixed Models!\n:::\n\n---\n\n# References {.unnumbered}\n\n::: {#refs}\n:::\n\n---\n\n# Appendix: R Code Summary {.unnumbered}\n\nKey R functions used this week:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Constraint systems\noptions(contrasts = c(\"contr.treatment\", \"contr.poly\"))  # Set-to-zero\noptions(contrasts = c(\"contr.sum\", \"contr.poly\"))        # Sum-to-zero\n\n# Generalized inverse\nlibrary(MASS)\nginv(A)  # Moore-Penrose inverse\n\n# Model fitting\nfit <- lm(y ~ factor)            # Effects model\nfit <- lm(y ~ factor - 1)        # Cell means model\nfit <- lm(y ~ x, weights = w)    # Weighted LS\n\n# Estimable functions\nlibrary(emmeans)\nemm <- emmeans(fit, \"factor\")    # LS means\npairs(emm)                       # All pairwise contrasts\ncontrast(emm, list(...))         # Custom contrasts\n\n# Type III SS\nlibrary(car)\nAnova(fit, type=3)\n\n# Mixed models (preview)\nlibrary(lme4)\nfit_mm <- lmer(y ~ 1 + (1|factor))\nranef(fit_mm)  # BLUP estimates\n```\n:::\n\n\n---\n\n**End of Week 13 Lecture Notes**\n",
    "supporting": [
      "Week13_SpecialTopicsI_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}