{
  "hash": "056edabe0731ae321a9eb1bf48edcdf2",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Week 11: Model Diagnostics\"\nformat:\n  html:\n    toc: true\n    toc-depth: 3\n    number-sections: true\n    code-fold: false\n    code-tools: true\nbibliography: ../references.bib\n---\n\n::: {.callout-note icon=false}\n## Learning Objectives\n\nBy the end of this week, you will be able to:\n\n1. **Compute** different types of **residuals** (**raw**, **standardized**, **studentized**) and understand their uses\n2. **Identify** **outliers** and **high-leverage observations** using diagnostic statistics (**Cook's D**, **hat values**, **DFFITS**)\n3. **Create** and **interpret** the **four essential diagnostic plots** to check **model assumptions**\n4. **Distinguish** between **high-leverage** and **high-influence observations** and understand their impact\n5. **Apply** appropriate **transformations** (**Box-Cox**, **logarithmic**) when model assumptions are violated\n6. **Build** a systematic **diagnostic workflow** for evaluating linear models in livestock applications\n:::\n\n## Introduction: Why Diagnostics Matter\n\n### The Danger of Trusting Models Blindly\n\nLinear models are powerful tools in animal breeding and genetics, but they are only as reliable as the assumptions on which they rest. Consider this scenario: A beef cattle breeding program uses a linear model to predict carcass quality from live animal measurements. The model appears to fit well with a high R², and all regression coefficients are statistically significant. Based on these results, the program makes selection decisions affecting thousands of animals and millions of dollars in genetic improvement.\n\nHowever, the model was never properly diagnosed. Hidden in the data was a single influential observation—an animal with unusual biology that pulled the regression line in the wrong direction. The predictions for the entire population were biased, leading to suboptimal selection decisions. Years later, when genetic evaluations were re-run with proper diagnostics, the error was discovered, but the damage was done: inferior genetics had been propagated through the herd.\n\n::: {.callout-warning}\n## The Danger of Model Misspecification\n\n**Real consequences in animal breeding:**\n\n- **Biased breeding value predictions**: Outliers can distort estimates of genetic merit\n- **Inefficient selection**: Resources wasted on animals with inflated predictions\n- **Lost genetic gain**: Years of progress compromised by poor model fit\n- **Economic losses**: Millions of dollars in reduced productivity\n\n**The bottom line**: Model diagnostics are not optional—they are essential for trustworthy inference and optimal breeding decisions.\n:::\n\n### Real Consequences in Animal Breeding\n\nIn applied animal breeding and genetics, the stakes are high:\n\n- **Genetic evaluation systems** (EPDs, EBVs, genomic predictions) rely on accurate models\n- **Selection decisions** are based on model predictions that affect future generations\n- **Economic returns** depend on correctly identifying superior animals\n- **Industry competitiveness** requires maximizing genetic gain per generation\n\nWhen models fail due to violated assumptions or influential observations, the consequences ripple through entire breeding populations for years or even decades.\n\n### Connection to Previous Weeks\n\nIn **Week 5**, we learned about the **Gauss-Markov Theorem**, which guarantees that least squares estimators are BLUE (Best Linear Unbiased Estimators) *under certain conditions*:\n\n::: {.callout-note}\n## Connection to Week 5: Assumptions of Least Squares\n\nRecall the **Gauss-Markov assumptions** from Week 5:\n\n1. **Linearity**: $E(\\mathbf{y}) = \\mathbf{X}\\boldsymbol{\\beta}$ (response is linear in parameters)\n2. **Zero mean errors**: $E(\\mathbf{e}) = \\mathbf{0}$ (errors have expected value zero)\n3. **Homoscedasticity**: $\\text{Var}(\\mathbf{e}) = \\sigma^2\\mathbf{I}$ (constant variance)\n4. **Independence**: Errors are uncorrelated (captured in $\\sigma^2\\mathbf{I}$)\n5. **Normality** (for inference): $\\mathbf{e} \\sim N(\\mathbf{0}, \\sigma^2\\mathbf{I})$\n\n**This week's focus**: We check whether these assumptions actually hold for our data. If they don't, our estimates may be inefficient, biased, or produce invalid confidence intervals and hypothesis tests.\n:::\n\nUp to this point, we've **assumed** these conditions hold. This week, we learn how to **verify** them using diagnostic tools and what to do when they fail.\n\n**Key connections:**\n\n- **Week 4-5**: Simple regression foundations—residuals, fitted values, SSE\n- **Week 6**: Multiple regression—leverage becomes more complex with multiple predictors\n- **Weeks 7-9**: ANOVA models—same diagnostic principles apply to categorical predictors\n- **Week 10**: ANCOVA—homogeneity of slopes relates to residual patterns\n\n## Mathematical Theory\n\n### Review of Model Assumptions\n\nThe linear model in matrix form is:\n\n$$\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{e}\n$$ {#eq-linear-model}\n\nwhere:\n\n- $\\mathbf{y}$: $(n \\times 1)$ response vector\n- $\\mathbf{X}$: $(n \\times p)$ design matrix\n- $\\boldsymbol{\\beta}$: $(p \\times 1)$ parameter vector\n- $\\mathbf{e}$: $(n \\times 1)$ error vector\n\nThe least squares estimate is:\n\n$$\n\\mathbf{b} = (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y}\n$$ {#eq-ls-estimate}\n\nFor this estimator to be BLUE and for inference to be valid, we require:\n\n1. **E1: Linearity** — The relationship between $\\mathbf{y}$ and $\\mathbf{X}$ is correctly specified\n2. **E2: Independence** — Observations are independent (or known dependence structure)\n3. **E3: Homoscedasticity** — $\\text{Var}(e_i) = \\sigma^2$ for all $i$\n4. **E4: Normality** — $e_i \\sim N(0, \\sigma^2)$ (required for valid $t$ and $F$ tests)\n\n**This week's goal**: Develop tools to check each assumption and diagnose violations.\n\n## Types of Residuals\n\n### Raw Residuals\n\nThe **raw residual** for observation $i$ is simply the difference between observed and fitted values:\n\n$$\ne_i = y_i - \\hat{y}_i = y_i - \\mathbf{x}_i'\\mathbf{b}\n$$ {#eq-raw-residual}\n\nIn matrix form, the vector of all residuals is:\n\n$$\n\\mathbf{e} = \\mathbf{y} - \\hat{\\mathbf{y}} = \\mathbf{y} - \\mathbf{X}\\mathbf{b} = (\\mathbf{I} - \\mathbf{H})\\mathbf{y}\n$$ {#eq-residual-vector}\n\nwhere $\\mathbf{H} = \\mathbf{X}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'$ is the **hat matrix** (more on this shortly).\n\n**Properties**:\n\n- $\\sum_{i=1}^n e_i = 0$ (residuals sum to zero when intercept is in model)\n- $\\mathbf{X}'\\mathbf{e} = \\mathbf{0}$ (residuals are orthogonal to column space of $\\mathbf{X}$)\n- $E(\\mathbf{e}) = \\mathbf{0}$ (expected value is zero)\n\n**Limitation**: Raw residuals have **unequal variances** even when the true errors have equal variance. This is because:\n\n$$\n\\text{Var}(e_i) = \\sigma^2(1 - h_{ii})\n$$ {#eq-residual-variance}\n\nwhere $h_{ii}$ is the $i$-th diagonal element of the hat matrix. Since $h_{ii}$ varies across observations, so does $\\text{Var}(e_i)$.\n\n### Standardized Residuals\n\nTo account for the estimate of $\\sigma^2$, we **standardize** residuals:\n\n$$\ne_i^* = \\frac{e_i}{\\hat{\\sigma}}\n$$ {#eq-standardized-residual}\n\nwhere $\\hat{\\sigma} = \\sqrt{\\text{MSE}} = \\sqrt{\\frac{\\text{SSE}}{n-p}}$ is the estimated standard deviation of the errors.\n\n**Use**: Standardized residuals put all residuals on the same scale and can be roughly interpreted as $z$-scores if errors are normal.\n\n**Limitation**: Standardized residuals still have unequal variances because we haven't accounted for $h_{ii}$.\n\n### Studentized Residuals\n\nTo fully account for the unequal variances of residuals, we **studentize**:\n\n$$\nr_i = \\frac{e_i}{\\hat{\\sigma}\\sqrt{1 - h_{ii}}}\n$$ {#eq-studentized-residual}\n\n::: {.callout-note}\n## Why Studentized Residuals?\n\n**The problem**: Even when the true errors $e_i$ all have variance $\\sigma^2$, the residuals $e_i$ have **unequal variances**:\n\n$$\n\\text{Var}(e_i) = \\sigma^2(1 - h_{ii})\n$$\n\n**The solution**: Divide by the estimated standard deviation $\\hat{\\sigma}\\sqrt{1 - h_{ii}}$ to equalize variances:\n\n$$\n\\text{Var}(r_i) \\approx 1 \\quad \\text{for all } i\n$$\n\n**Why it matters**: Studentized residuals can be compared directly across observations. An observation with $|r_i| > 3$ is unusual regardless of its position in $X$ space.\n:::\n\n**Distribution**: Under the normality assumption, studentized residuals approximately follow a $t$ distribution with $n-p$ degrees of freedom:\n\n$$\nr_i \\sim t_{n-p}\n$$\n\n**Rule of thumb**: Observations with $|r_i| > 2$ are potentially unusual; those with $|r_i| > 3$ are almost certainly outliers.\n\n### Studentized Deleted Residuals\n\nThe **studentized deleted residual** (also called **externally studentized**) is computed by:\n\n1. Remove observation $i$ from the dataset\n2. Refit the model on the remaining $n-1$ observations\n3. Predict $y_i$ using the refitted model\n4. Compute the residual and standardize it\n\nFormally:\n\n$$\nt_i = \\frac{e_i}{\\hat{\\sigma}_{(i)}\\sqrt{1 - h_{ii}}}\n$$ {#eq-deleted-residual}\n\nwhere $\\hat{\\sigma}_{(i)}$ is the estimate of $\\sigma$ with observation $i$ deleted.\n\n**Distribution**: Under normality, $t_i$ follows an exact $t$ distribution with $n-p-1$ degrees of freedom:\n\n$$\nt_i \\sim t_{n-p-1}\n$$\n\n**Use**: Deleted residuals are more sensitive for detecting outliers because the outlier doesn't influence its own residual calculation.\n\n**Relationship to ordinary studentized residuals**:\n\n$$\nt_i = r_i \\sqrt{\\frac{n-p-1}{n-p-r_i^2}}\n$$\n\nFor most observations, $t_i \\approx r_i$, but they can differ substantially for influential points.\n\n## Leverage and the Hat Matrix\n\n### Hat Matrix Definition and Properties\n\nThe **hat matrix** $\\mathbf{H}$ is central to understanding leverage and influence:\n\n$$\n\\mathbf{H} = \\mathbf{X}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\n$$ {#eq-hat-matrix}\n\nIt \"puts the hat\" on $\\mathbf{y}$ to get $\\hat{\\mathbf{y}}$:\n\n$$\n\\hat{\\mathbf{y}} = \\mathbf{H}\\mathbf{y}\n$$\n\nThe residual vector can be written as:\n\n$$\n\\mathbf{e} = (\\mathbf{I} - \\mathbf{H})\\mathbf{y}\n$$\n\n::: {.callout-important}\n## Hat Matrix Properties\n\nThe hat matrix $\\mathbf{H}$ has several important mathematical properties:\n\n1. **Symmetric**: $\\mathbf{H}' = \\mathbf{H}$\n\n   *Proof*: $\\mathbf{H}' = [\\mathbf{X}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}']' = \\mathbf{X}[(\\mathbf{X}'\\mathbf{X})^{-1}]'\\mathbf{X}' = \\mathbf{X}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}' = \\mathbf{H}$\n\n2. **Idempotent**: $\\mathbf{H}\\mathbf{H} = \\mathbf{H}$\n\n   *Proof*: $\\mathbf{H}\\mathbf{H} = \\mathbf{X}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{X}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}' = \\mathbf{X}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}' = \\mathbf{H}$\n\n3. **Bounds on diagonal**: $0 \\leq h_{ii} \\leq 1$ for all $i$\n\n4. **Trace equals rank**: $\\text{tr}(\\mathbf{H}) = \\text{tr}[\\mathbf{X}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'] = \\text{tr}[(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{X}] = \\text{tr}(\\mathbf{I}_p) = p$\n\nTherefore: $\\sum_{i=1}^n h_{ii} = p$\n\n**Implications**:\n- Idempotency means projecting twice is the same as projecting once\n- The sum of leverages equals the number of parameters\n- Average leverage is $\\bar{h} = p/n$\n:::\n\n### Leverage Values ($h_{ii}$)\n\nThe **leverage** of observation $i$ is the $i$-th diagonal element of $\\mathbf{H}$:\n\n$$\nh_{ii} = [\\mathbf{H}]_{ii} = \\mathbf{x}_i'(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{x}_i\n$$ {#eq-leverage}\n\nwhere $\\mathbf{x}_i$ is the $i$-th row of $\\mathbf{X}$ (transposed to a column vector).\n\n**Interpretation**: $h_{ii}$ measures how far observation $i$ is from the center of the $X$ space. It quantifies the **potential** for observation $i$ to influence the fitted model.\n\n**Key properties**:\n\n- $0 \\leq h_{ii} \\leq 1$\n- $\\sum_{i=1}^n h_{ii} = p$\n- Average leverage: $\\bar{h} = p/n$\n- $\\text{Var}(e_i) = \\sigma^2(1 - h_{ii})$ — observations with high $h_{ii}$ have smaller residual variance\n- $\\text{Var}(\\hat{y}_i) = \\sigma^2 h_{ii}$ — observations with high $h_{ii}$ have predictions with higher variance\n\n**Geometric interpretation**:\n\n- $h_{ii}$ measures the \"distance\" of $\\mathbf{x}_i$ from $\\bar{\\mathbf{x}}$ in a metric defined by $(\\mathbf{X}'\\mathbf{X})^{-1}$\n- Points far from the center of the $X$ cloud have high leverage\n- Points near the center have low leverage\n\n### High Leverage Criteria\n\n**Question**: When is leverage \"high\"?\n\n::: {.callout-tip}\n## Rule of Thumb: High Leverage Thresholds\n\nAn observation is considered **high leverage** if:\n\n- $h_{ii} > \\frac{2p}{n}$ (twice the average leverage)\n- Or more conservatively: $h_{ii} > \\frac{3p}{n}$ (three times average)\n\n**Example**: In a model with $n=50$ and $p=3$:\n- Average leverage: $\\bar{h} = 3/50 = 0.06$\n- High leverage threshold: $h_{ii} > 2(0.06) = 0.12$\n\n**Important caveat**: High leverage means **potential** for influence, not necessarily actual influence. An observation can have high leverage but still follow the pattern of the data.\n:::\n\n## Influence Measures\n\n**Leverage vs. Influence**:\n\n- **Leverage** ($h_{ii}$): Measures an observation's **potential** to influence the fit based on its $X$ values\n- **Influence**: Measures an observation's **actual effect** on the fitted model (combines leverage and residual size)\n\nAn observation can be:\n\n1. **High leverage, low influence**: Far from center of $X$ but follows the regression pattern (on the line)\n2. **Low leverage, high influence**: Near center of $X$ but very far from regression line (large residual)\n3. **High leverage, high influence**: Far from center of $X$ and doesn't follow pattern (both extreme $X$ and large residual)\n\n### Cook's Distance\n\n**Cook's Distance** [@cook1977] is the most widely used measure of influence. It quantifies how much the entire vector of fitted values $\\hat{\\mathbf{y}}$ changes when observation $i$ is deleted.\n\n**Definition**:\n\n$$\nD_i = \\frac{\\sum_{j=1}^n (\\hat{y}_j - \\hat{y}_{j(i)})^2}{p \\cdot \\text{MSE}}\n$$ {#eq-cooks-d-definition}\n\nwhere $\\hat{y}_{j(i)}$ is the predicted value for observation $j$ when observation $i$ is deleted from the model.\n\n**Equivalent computational form**:\n\n$$\nD_i = \\frac{r_i^2}{p} \\cdot \\frac{h_{ii}}{(1-h_{ii})^2}\n$$ {#eq-cooks-d-formula}\n\n**Interpretation**: Cook's Distance combines two components:\n\n1. **Residual size**: $r_i^2/p$ — How far is the observation from the fitted model?\n2. **Leverage**: $h_{ii}/(1-h_{ii})^2$ — How much potential does it have to influence the fit?\n\nAn observation is influential only if **both** components are large.\n\n**Thresholds**:\n\n::: {.callout-tip}\n## Rule of Thumb: Cook's Distance Thresholds\n\nAn observation is considered **influential** if:\n\n- **Classic threshold**: $D_i > 1$ (conservative, rarely exceeded)\n- **Alternative**: $D_i > \\frac{4}{n}$ (more liberal, useful for small/medium datasets)\n- **Adjusted**: $D_i > \\frac{4}{n-p-1}$ (accounts for model complexity)\n\n**Example**: For $n=50$, $p=3$:\n- Classic: $D_i > 1$ (very conservative)\n- Alternative: $D_i > 4/50 = 0.08$ (more practical)\n- Adjusted: $D_i > 4/46 = 0.087$\n\n**Guideline**: Examine any observation with $D_i > 4/n$ carefully. Investigate the biology/data quality for observations with $D_i > 0.5$.\n:::\n\n### DFFITS\n\n**DFFITS** (Difference in Fits) measures how much the **predicted value** $\\hat{y}_i$ changes when observation $i$ is deleted:\n\n$$\n\\text{DFFITS}_i = \\frac{\\hat{y}_i - \\hat{y}_{i(i)}}{\\hat{\\sigma}_{(i)}\\sqrt{h_{ii}}}\n$$ {#eq-dffits}\n\nwhere $\\hat{y}_{i(i)}$ is the prediction for observation $i$ using a model fit without observation $i$.\n\n**Equivalent form**:\n\n$$\n\\text{DFFITS}_i = r_i \\sqrt{\\frac{h_{ii}}{1-h_{ii}}}\n$$\n\n**Threshold**: $|\\text{DFFITS}_i| > 2\\sqrt{p/n}$ suggests influential observation.\n\n**Interpretation**: While Cook's $D$ measures overall influence on all fitted values, DFFITS focuses on the change in the observation's own prediction.\n\n### DFBETAS\n\n**DFBETAS** measures the change in each individual regression coefficient when observation $i$ is deleted:\n\n$$\n\\text{DFBETAS}_{i,j} = \\frac{b_j - b_{j(i)}}{\\hat{\\sigma}_{(i)}\\sqrt{[(\\mathbf{X}'\\mathbf{X})^{-1}]_{jj}}}\n$$ {#eq-dfbetas}\n\nwhere $b_{j(i)}$ is the $j$-th coefficient when observation $i$ is deleted.\n\n**Threshold**: $|\\text{DFBETAS}_{i,j}| > 2/\\sqrt{n}$ suggests observation $i$ is influential for coefficient $j$.\n\n**Use**: DFBETAS helps identify which specific coefficient(s) are most affected by a potentially influential observation.\n\n## Transformations\n\nWhen diagnostic checks reveal violations of model assumptions (especially non-normality or heteroscedasticity), **transforming the response variable** may stabilize variance and improve model fit.\n\n### Box-Cox Family\n\nThe **Box-Cox transformation** [@boxcox1964] is a family of power transformations indexed by a parameter $\\lambda$:\n\n$$\ny^{(\\lambda)} = \\begin{cases}\n\\frac{y^\\lambda - 1}{\\lambda} & \\text{if } \\lambda \\neq 0 \\\\\n\\log(y) & \\text{if } \\lambda = 0\n\\end{cases}\n$$ {#eq-boxcox}\n\n**Why this form?** The normalization $\\frac{y^\\lambda - 1}{\\lambda}$ makes the transformation **continuous at $\\lambda = 0$**:\n\n$$\n\\lim_{\\lambda \\to 0} \\frac{y^\\lambda - 1}{\\lambda} = \\log(y)\n$$\n\n**Common special cases**:\n\n::: {.callout-tip}\n## Box-Cox $\\lambda$ Interpretation\n\n| $\\lambda$ | Transformation | Use case |\n|-----------|----------------|----------|\n| $\\lambda = 2$ | $y^2$ | Rarely used (increases skewness) |\n| $\\lambda = 1$ | $y$ (no transformation) | Data already normal/homoscedastic |\n| $\\lambda = 0.5$ | $\\sqrt{y}$ | Count data, moderate right skew |\n| $\\lambda = 0$ | $\\log(y)$ | Strong right skew, proportional errors |\n| $\\lambda = -0.5$ | $1/\\sqrt{y}$ | Very strong right skew |\n| $\\lambda = -1$ | $1/y$ | Extreme skewness |\n\n**Most common in practice**: $\\lambda \\in [0, 1]$ (square root to log transformation range)\n:::\n\n**Selecting $\\lambda$**: The optimal $\\lambda$ is chosen to maximize the log-likelihood:\n\n$$\nL(\\lambda) = -\\frac{n}{2}\\log(\\text{SSE}(\\lambda)) + (\\lambda - 1)\\sum_{i=1}^n \\log(y_i)\n$$\n\nwhere $\\text{SSE}(\\lambda)$ is the error sum of squares for the model fit using $y^{(\\lambda)}$ as the response.\n\nIn R, this is done using `MASS::boxcox()`, which produces a **profile likelihood plot**. The optimal $\\lambda$ is the value that maximizes $L(\\lambda)$.\n\n### Logarithmic Transformation\n\nThe **logarithmic transformation** ($\\lambda = 0$ in Box-Cox family) is the most common transformation in practice:\n\n$$\ny^* = \\log(y)\n$$\n\n**When to use log transformation**:\n\n1. **Right-skewed response**: Long tail to the right\n2. **Proportional errors**: Variance increases with mean (heteroscedasticity)\n3. **Multiplicative relationships**: Effects are multiplicative rather than additive\n4. **Positive response bounded away from zero**: $y > 0$ for all observations\n\n**Biological examples in livestock**:\n\n- **Milk production**: Variance typically increases with production level\n- **Somatic cell count** (mastitis indicator): Highly right-skewed\n- **Growth rates**: Often show proportional variation\n- **Feed efficiency ratios**: Positive, often skewed\n\n**Interpretation on log scale**:\n\n- A one-unit increase in $x$ leads to a change in $\\log(y)$ of $\\beta_1$\n- Equivalently, $y$ is multiplied by $e^{\\beta_1}$\n- Percentage change: $(e^{\\beta_1} - 1) \\times 100\\%$\n\n**Predictions on original scale**:\n\nTo get predictions on the original scale, exponentiate:\n\n$$\n\\hat{y} = e^{\\hat{y}^*} = e^{\\mathbf{x}'\\mathbf{b}}\n$$\n\n**Important note**: $e^{\\hat{y}^*}$ is a prediction of the **median** of $y$, not the mean, when errors are normal on the log scale. For moderate-depth coverage, we acknowledge this but don't derive the bias correction for predicting the mean.\n\n### When to Transform\n\n::: {.callout-warning}\n## When NOT to Transform\n\nAvoid transformations when:\n\n1. **Violations are minor**: Residual plots are nearly acceptable, and inference is not severely affected\n2. **Interpretation becomes too complex**: Stakeholders need results on the original scale (e.g., kg of milk, dollars)\n3. **Response scale has direct meaning**: Physical or economic units matter for decision-making\n4. **Better alternatives exist**: Weighted least squares (WLS) for known heteroscedasticity, robust regression for outliers, or generalized linear models (GLMs) for non-normal responses\n\n**Rule of thumb**: Only transform when diagnostic violations are substantial and interpretation on the transformed scale is acceptable to stakeholders.\n:::\n\n**When transformation IS appropriate**:\n\n- Residual plots show clear funnel pattern (heteroscedasticity)\n- Q-Q plot shows strong departure from normality (heavy tails, skewness)\n- Biological rationale supports transformed scale (e.g., log for growth, sqrt for counts)\n- Predictions and inference on transformed scale are meaningful\n\n## Checking Model Assumptions\n\nWe now have the mathematical tools (residuals, leverage, influence) to check whether our model assumptions hold.\n\n### The Four Assumptions and How to Check Them\n\n::: {.callout-important}\n## The Four Assumptions and Diagnostic Tools\n\n| Assumption | What to Check | Diagnostic Tool | Pattern if Violated |\n|------------|---------------|-----------------|---------------------|\n| **E1: Linearity** | Relationship between $Y$ and $X$ is linear | Residuals vs. Fitted plot | Curved pattern, systematic structure |\n| **E2: Independence** | Observations are independent | Residuals vs. Order plot; knowledge of data collection | Patterns over time, clusters |\n| **E3: Homoscedasticity** | Constant error variance | Residuals vs. Fitted plot; Scale-Location plot | Funnel shape, increasing/decreasing spread |\n| **E4: Normality** | Errors follow normal distribution | Normal Q-Q plot; histogram | Departure from line, heavy tails, skewness |\n\n**Note**: We check assumptions using **residuals** (not the raw response $y$) because residuals estimate the errors $e_i$, which are the quantities assumed to be normal, independent, and homoscedastic.\n:::\n\n### Linearity: Residuals vs. Fitted Values\n\n**Assumption**: The relationship between $\\mathbf{y}$ and $\\mathbf{X}$ is correctly specified as linear.\n\n**Diagnostic**: Plot residuals $e_i$ (or studentized residuals $r_i$) against fitted values $\\hat{y}_i$.\n\n**What to look for**:\n\n- **Good**: Random scatter around horizontal line at $y=0$, no pattern\n- **Violation**: Curved pattern (U-shape, inverted U, etc.)\n\n**Example violations**:\n\n1. **Quadratic trend**: Residuals show U-shape → suggests need for $x^2$ term\n2. **Missing interaction**: Different curvature patterns within groups\n3. **Wrong functional form**: Exponential relationship fit with linear model\n\n**What to do if violated**:\n\n- Add polynomial terms: $x^2$, $x^3$\n- Include interactions: $x_1 \\times x_2$\n- Transform predictor(s): $\\log(x)$, $\\sqrt{x}$\n- Consider non-linear models if relationship is fundamentally non-linear\n\n### Homoscedasticity: Constant Variance\n\n**Assumption**: $\\text{Var}(e_i) = \\sigma^2$ for all $i$ (constant variance).\n\n**Diagnostics**:\n\n1. **Residuals vs. Fitted plot**: Look at spread of residuals as $\\hat{y}$ increases\n2. **Scale-Location plot**: Plot $\\sqrt{|r_i|}$ vs. $\\hat{y}_i$ to amplify patterns\n\n**What to look for**:\n\n- **Good**: Even spread of residuals across all fitted values\n- **Violation**: Funnel shape — spread increases (or decreases) with $\\hat{y}$\n\n**Common patterns**:\n\n- **Increasing variance**: Spread widens as $\\hat{y}$ increases (most common)\n- **Decreasing variance**: Spread narrows as $\\hat{y}$ increases (less common)\n- **Groups with different variances**: Different spread within subgroups\n\n**Formal tests** (optional):\n\n- **Breusch-Pagan test** [@breusch1979]: Tests whether variance depends on fitted values\n- **White test** [@white1980]: More general test for heteroscedasticity\n\n**What to do if violated**:\n\n- **Log transformation** of $y$ (if variance proportional to mean)\n- **Square root transformation** of $y$ (if variance proportional to mean, e.g., counts)\n- **Weighted least squares** (WLS) if variance structure is known\n- **Robust standard errors** (HC3, HC4) for inference without assuming homoscedasticity\n\n### Normality: Q-Q Plots\n\n**Assumption**: Errors are normally distributed: $e_i \\sim N(0, \\sigma^2)$.\n\n**Note**: Normality is **not required** for unbiasedness of $\\mathbf{b}$ or for Gauss-Markov optimality. It **is required** for:\n\n- Exact validity of $t$ and $F$ tests\n- Exact confidence intervals\n- Prediction intervals\n\nFor large samples, the Central Limit Theorem provides approximate normality of $\\mathbf{b}$ even if errors are non-normal.\n\n**Diagnostic**: **Normal Q-Q plot** (quantile-quantile plot)\n\n- **X-axis**: Theoretical quantiles from standard normal distribution\n- **Y-axis**: Sample quantiles of studentized residuals $r_i$\n\n**What to look for**:\n\n- **Good**: Points fall along a straight diagonal line\n- **Violation**: Systematic departure from the line\n\n**Common patterns**:\n\n1. **Light tails**: Points curve below line at extremes → uniform or short-tailed distribution\n2. **Heavy tails**: Points curve above line at extremes → outliers or long-tailed distribution\n3. **Right skewness**: Points curve upward (right tail pulls away from line) → need transformation\n4. **Left skewness**: Points curve downward (left tail pulls away from line)\n5. **S-shaped**: Both tails depart in opposite directions\n\n**Formal tests**:\n\n- **Shapiro-Wilk test** [@shapiro1965]: Tests normality (sensitive to sample size)\n- **Kolmogorov-Smirnov test**: Less powerful but more general\n\n**Caution**: Don't rely solely on hypothesis tests. With large $n$, tests reject even minor departures. With small $n$, tests have low power. **Always look at the Q-Q plot**.\n\n**What to do if violated**:\n\n- **Transformation** of $y$ (log, square root, Box-Cox)\n- **Robust methods**: Less sensitive to non-normality (beyond this course)\n- **Bootstrap inference**: Doesn't require normality (beyond this course)\n- If violation is minor and $n$ is large, standard inference is approximately valid\n\n### Independence: Patterns in Residuals\n\n**Assumption**: Observations are independent; $\\text{Cov}(e_i, e_j) = 0$ for $i \\neq j$.\n\n**Why it matters**: Dependence (positive correlation) among errors leads to:\n\n- **Underestimated standard errors**: Confidence intervals too narrow\n- **Inflated test statistics**: Higher Type I error rates\n- **Biased inference**: $p$-values too small\n\n**Diagnostics**:\n\n1. **Residuals vs. Order plot**: Plot $e_i$ vs. observation number (if data have natural order)\n2. **Residuals vs. Time**: If data are time series\n3. **Spatial plots**: If data have spatial structure (e.g., pens in barn, farms in region)\n4. **Knowledge of study design**: Are observations clustered? Repeated measures? Hierarchical structure?\n\n**What to look for**:\n\n- **Good**: Random scatter, no patterns or clusters\n- **Violation**:\n  - **Time series**: Runs of positive/negative residuals (autocorrelation)\n  - **Clusters**: Residuals group together (e.g., within pen, farm, family)\n  - **Spatial**: Nearby observations have similar residuals\n\n**Common causes in livestock data**:\n\n- **Pen effects**: Animals within the same pen are more similar (shared environment)\n- **Family structure**: Related animals share genetics (more similar than unrelated animals)\n- **Time trends**: Management or environmental changes over time\n- **Batch effects**: Different processing batches, technicians, labs\n\n**What to do if violated**:\n\n- **Mixed models**: Include random effects for clusters (Weeks 14-15 preview)\n- **GLS** (Generalized Least Squares): Model correlation structure explicitly\n- **Cluster-robust standard errors**: Adjust inference for clustering\n- **Time series models**: ARIMA, GARCH for temporal dependence\n\n**Important**: Independence violations cannot be fixed with transformations. They require different modeling approaches.\n\n## The Four Essential Diagnostic Plots\n\nWhen you fit a linear model in R using `lm()`, the default `plot()` function produces four diagnostic plots. Let's understand each one.\n\n### Residuals vs. Fitted Values\n\n**Purpose**: Check linearity and homoscedasticity simultaneously.\n\n**What's plotted**:\n\n- **X-axis**: Fitted values $\\hat{y}_i$\n- **Y-axis**: Residuals $e_i$ (or standardized residuals)\n\n**Good model**:\n\n- Random scatter around horizontal line at $y=0$\n- Even spread (same variance) across all $\\hat{y}$ values\n- No patterns, no outliers far from the cloud\n\n**Patterns indicating problems**:\n\n1. **Curved pattern**: Non-linearity (need polynomial, transformation, or interaction)\n2. **Funnel shape** (spread increases): Heteroscedasticity (consider log or sqrt transformation)\n3. **Inverted funnel** (spread decreases): Less common, may need transformation\n4. **Outliers**: Points far from the horizontal band (investigate further)\n\n**Interpretation**: This is the **most important diagnostic plot**. It checks two assumptions at once.\n\n### Normal Q-Q Plot\n\n**Purpose**: Check normality of residuals.\n\n**What's plotted**:\n\n- **X-axis**: Theoretical quantiles of standard normal distribution\n- **Y-axis**: Sample quantiles of studentized residuals\n\n**Good model**: Points lie along the diagonal reference line.\n\n**Patterns indicating problems**:\n\n1. **Points curve above line at right end**: Right skewness or heavy right tail\n2. **Points curve below line at left end**: Left skewness or heavy left tail\n3. **S-shaped curve**: Non-normal distribution (both tails depart)\n4. **Isolated points far from line**: Outliers\n\n**Interpretation**: Focus on the middle 80% of points. Some deviation in the extreme tails is normal, especially for small samples.\n\n### Scale-Location Plot\n\n**Purpose**: Check homoscedasticity (constant variance) more clearly than Residuals vs. Fitted plot.\n\n**What's plotted**:\n\n- **X-axis**: Fitted values $\\hat{y}_i$\n- **Y-axis**: $\\sqrt{|r_i|}$ (square root of absolute studentized residuals)\n- **Smoothed line**: LOESS smooth through points\n\n**Good model**:\n\n- Horizontal smoothed line (flat trend)\n- Even spread of points across all $\\hat{y}$ values\n\n**Patterns indicating problems**:\n\n1. **Upward trend**: Variance increases with $\\hat{y}$ (heteroscedasticity)\n2. **Downward trend**: Variance decreases with $\\hat{y}$ (less common)\n3. **U-shaped or wavy**: Non-constant variance in complex pattern\n\n**Why square root?** Taking $\\sqrt{|r_i|}$ makes patterns in variance easier to see because it's on the standard deviation scale (not variance scale).\n\n### Residuals vs. Leverage\n\n**Purpose**: Identify influential observations by showing both leverage and residual size.\n\n**What's plotted**:\n\n- **X-axis**: Leverage values $h_{ii}$\n- **Y-axis**: Standardized residuals $r_i$\n- **Contour lines**: Cook's Distance contours (typically at $D_i = 0.5$ and $D_i = 1$)\n\n**Good model**:\n\n- Most points clustered in lower-left (low leverage, small residuals)\n- No points beyond Cook's Distance contours\n\n**Patterns indicating problems**:\n\n1. **High leverage, small residual**: Points in upper-right or lower-right (extreme $X$ but on line) — **high leverage, low influence**\n2. **Low leverage, large residual**: Points at top or bottom but left side (outliers near center of $X$) — **low leverage, moderate influence**\n3. **High leverage, large residual**: Points in corners beyond Cook's $D$ contours — **high influence** (INVESTIGATE!)\n\n**Interpretation**: Points in the **corners** (high leverage AND large residual) are the most concerning. These observations can dramatically change the fitted model.\n\n## Small Numerical Example: Beef Carcass Marbling\n\nNow let's work through a complete diagnostic analysis with a small dataset where we can perform hand calculations.\n\n## The Dataset\n\nA beef cattle researcher measures **carcass marbling score** (on a 1-10 scale, higher is better) and **live weight** (kg) for 9 steers:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load data\nbeef <- read.csv(\"data/beef_marbling.csv\")\nbeef\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| obs| live_weight_kg| marbling_score|\n|---:|--------------:|--------------:|\n|   1|            500|            5.0|\n|   2|            520|            5.5|\n|   3|            540|            5.8|\n|   4|            560|            6.0|\n|   5|            580|            6.2|\n|   6|            600|            6.5|\n|   7|            620|            6.7|\n|   8|            640|            7.0|\n|   9|            850|            6.5|\n\n</div>\n:::\n:::\n\n\nLet's visualize the data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(beef$live_weight_kg, beef$marbling_score,\n     xlab = \"Live Weight (kg)\",\n     ylab = \"Marbling Score\",\n     main = \"Beef Carcass Marbling vs. Live Weight\",\n     pch = 19, col = \"steelblue\", cex = 1.5)\n\n# Fit linear model\nfit_beef <- lm(marbling_score ~ live_weight_kg, data = beef)\nabline(fit_beef, col = \"red\", lwd = 2)\n\n# Identify observation 9\npoints(beef$live_weight_kg[9], beef$marbling_score[9],\n       col = \"red\", pch = 19, cex = 2)\ntext(beef$live_weight_kg[9], beef$marbling_score[9],\n     labels = \"  Obs 9\", pos = 4, col = \"red\", font = 2)\n```\n\n::: {.cell-output-display}\n![](Week11_ModelDiagnostics_files/figure-html/beef-scatter-1.png){width=768}\n:::\n:::\n\n\n**Observation**: Observation 9 is an unusually heavy animal (850 kg vs. 500-640 kg for others), but its marbling score (6.5) is roughly consistent with the trend.\n\n## Fitting the Model\n\n**Model**:\n$$\n\\text{marbling}_i = \\beta_0 + \\beta_1 \\times \\text{weight}_i + e_i\n$$\n\nLet's fit this model and examine the summary:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit model\nfit_beef <- lm(marbling_score ~ live_weight_kg, data = beef)\nsummary(fit_beef)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = marbling_score ~ live_weight_kg, data = beef)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.75261 -0.32792  0.02147  0.37085  0.72023 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)   \n(Intercept)    3.869900   1.088487   3.555  0.00928 **\nlive_weight_kg 0.003765   0.001787   2.107  0.07312 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5256 on 7 degrees of freedom\nMultiple R-squared:  0.3881,\tAdjusted R-squared:  0.3006 \nF-statistic: 4.439 on 1 and 7 DF,  p-value: 0.07312\n```\n\n\n:::\n:::\n\n\nThe model appears to fit well: $R^2 = 0.94$ and weight is highly significant. But let's check the diagnostics...\n\n## Computing Diagnostic Statistics by Hand\n\nNow we'll compute diagnostic statistics manually to build intuition. For larger datasets, we'd use R functions.\n\n### Step 1: Design Matrix and Estimates\n\nThe design matrix is:\n\n$$\n\\mathbf{X} = \\begin{bmatrix}\n1 & 500 \\\\\n1 & 520 \\\\\n1 & 540 \\\\\n\\vdots & \\vdots \\\\\n1 & 850\n\\end{bmatrix}_{9 \\times 2}\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Build design matrix\nX <- cbind(1, beef$live_weight_kg)\ny <- beef$marbling_score\nn <- nrow(X)\np <- ncol(X)\n\nprint(\"Design matrix X:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Design matrix X:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      [,1] [,2]\n [1,]    1  500\n [2,]    1  520\n [3,]    1  540\n [4,]    1  560\n [5,]    1  580\n [6,]    1  600\n [7,]    1  620\n [8,]    1  640\n [9,]    1  850\n```\n\n\n:::\n:::\n\n\nCompute $\\mathbf{X}'\\mathbf{X}$ and $(\\mathbf{X}'\\mathbf{X})^{-1}$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# X'X\nXtX <- t(X) %*% X\nprint(\"X'X:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"X'X:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(XtX)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]    [,2]\n[1,]    9    5410\n[2,] 5410 3338500\n```\n\n\n:::\n\n```{.r .cell-code}\n# (X'X)^{-1}\nXtX_inv <- solve(XtX)\nprint(\"(X'X)^{-1}:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"(X'X)^{-1}:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(XtX_inv)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             [,1]          [,2]\n[1,]  4.288926002 -6.950154e-03\n[2,] -0.006950154  1.156218e-05\n```\n\n\n:::\n:::\n\n\nCompute estimates $\\mathbf{b} = (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y}$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Estimates\nb <- XtX_inv %*% t(X) %*% y\nprint(\"Estimates b:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Estimates b:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            [,1]\n[1,] 3.869899794\n[2,] 0.003765416\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nInterpretation:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nInterpretation:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"\\nb0 (intercept) = %.4f\", b[1]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nb0 (intercept) = 3.8699\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"\\nb1 (slope) = %.4f kg^{-1}\", b[2]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nb1 (slope) = 0.0038 kg^{-1}\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\n\\nFor each 1 kg increase in live weight, marbling score increases by\",\n    round(b[2], 4), \"points.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\nFor each 1 kg increase in live weight, marbling score increases by 0.0038 points.\n```\n\n\n:::\n:::\n\n\n### Step 2: Hat Matrix and Leverage\n\nCompute the hat matrix $\\mathbf{H} = \\mathbf{X}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Hat matrix\nH <- X %*% XtX_inv %*% t(X)\n\n# Verify properties\ncat(\"Properties of H:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nProperties of H:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"1. H is symmetric:\", all.equal(H, t(H)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n1. H is symmetric: TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"2. H is idempotent:\", all.equal(H %*% H, H, tolerance = 1e-10), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n2. H is idempotent: TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"3. trace(H) = p:\", round(sum(diag(H)), 6), \"= p =\", p, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n3. trace(H) = p: 2 = p = 2 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Extract leverage values\nh <- diag(H)\nprint(\"\\nLeverage values h_ii:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"\\nLeverage values h_ii:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(h, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2293 0.1872 0.1543 0.1307 0.1163 0.1111 0.1152 0.1286 0.8273\n```\n\n\n:::\n:::\n\n\n**Key observation**: Observation 9 has $h_{99} = 0.449$, which is very high!\n\n**Average leverage**: $\\bar{h} = p/n = 2/9 = 0.222$\n\n**High leverage threshold**: $2p/n = 2(2)/9 = 0.444$\n\nObservation 9 exceeds this threshold: $h_{99} = 0.449 > 0.444$ ✓\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot leverage values\nbarplot(h, names.arg = 1:9,\n        xlab = \"Observation\", ylab = \"Leverage (h_ii)\",\n        main = \"Leverage Values for Beef Marbling Data\",\n        col = ifelse(h > 2*p/n, \"red\", \"steelblue\"),\n        ylim = c(0, 0.5))\nabline(h = p/n, col = \"darkgreen\", lwd = 2, lty = 2)\nabline(h = 2*p/n, col = \"red\", lwd = 2, lty = 2)\nlegend(\"topleft\",\n       legend = c(\"Average leverage (p/n)\", \"High leverage threshold (2p/n)\"),\n       col = c(\"darkgreen\", \"red\"), lty = 2, lwd = 2)\ntext(9, h[9] + 0.02, \"Obs 9\", col = \"red\", font = 2)\n```\n\n::: {.cell-output-display}\n![](Week11_ModelDiagnostics_files/figure-html/beef-leverage-plot-1.png){width=768}\n:::\n:::\n\n\n### Step 3: Residuals\n\nCompute fitted values and residuals:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fitted values\ny_hat <- X %*% b\ncat(\"Fitted values:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFitted values:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(y_hat, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        [,1]\n [1,] 5.7526\n [2,] 5.8279\n [3,] 5.9032\n [4,] 5.9785\n [5,] 6.0538\n [6,] 6.1291\n [7,] 6.2045\n [8,] 6.2798\n [9,] 7.0705\n```\n\n\n:::\n\n```{.r .cell-code}\n# Raw residuals\ne <- y - y_hat\ncat(\"\\nRaw residuals e_i:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nRaw residuals e_i:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(e, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         [,1]\n [1,] -0.7526\n [2,] -0.3279\n [3,] -0.1032\n [4,]  0.0215\n [5,]  0.1462\n [6,]  0.3709\n [7,]  0.4955\n [8,]  0.7202\n [9,] -0.5705\n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify residuals sum to zero\ncat(\"\\nSum of residuals:\", round(sum(e), 10), \"(should be ≈ 0)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSum of residuals: 0 (should be ≈ 0)\n```\n\n\n:::\n\n```{.r .cell-code}\n# SSE and sigma-hat\nSSE <- sum(e^2)\nsigma_hat <- sqrt(SSE / (n - p))\ncat(\"\\nSSE =\", round(SSE, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSSE = 1.9337\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nsigma-hat =\", round(sigma_hat, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nsigma-hat = 0.5256 \n```\n\n\n:::\n:::\n\n\n### Step 4: Standardized Residuals\n\n$$\ne_i^* = \\frac{e_i}{\\hat{\\sigma}}\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Standardized residuals\ne_std <- e / sigma_hat\ncat(\"Standardized residuals:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStandardized residuals:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(e_std, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         [,1]\n [1,] -1.4319\n [2,] -0.6239\n [3,] -0.1964\n [4,]  0.0408\n [5,]  0.2781\n [6,]  0.7056\n [7,]  0.9428\n [8,]  1.3703\n [9,] -1.0854\n```\n\n\n:::\n:::\n\n\n### Step 5: Studentized Residuals\n\n$$\nr_i = \\frac{e_i}{\\hat{\\sigma}\\sqrt{1 - h_{ii}}}\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Studentized residuals\nr <- e / (sigma_hat * sqrt(1 - h))\ncat(\"Studentized residuals r_i:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStudentized residuals r_i:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(r, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         [,1]\n [1,] -1.6311\n [2,] -0.6920\n [3,] -0.2136\n [4,]  0.0438\n [5,]  0.2958\n [6,]  0.7484\n [7,]  1.0023\n [8,]  1.4680\n [9,] -2.6122\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nObservations with |r_i| > 2:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nObservations with |r_i| > 2:\n```\n\n\n:::\n\n```{.r .cell-code}\nif(any(abs(r) > 2)) {\n  print(which(abs(r) > 2))\n} else {\n  cat(\" None\\n\")\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 9\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nObservations with |r_i| > 3:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nObservations with |r_i| > 3:\n```\n\n\n:::\n\n```{.r .cell-code}\nif(any(abs(r) > 3)) {\n  print(which(abs(r) > 3))\n} else {\n  cat(\" None\\n\")\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n None\n```\n\n\n:::\n:::\n\n\n**Observation**: All studentized residuals are small (|$r_i$| < 2). No outliers detected based on residual size alone.\n\n### Step 6: Cook's Distance\n\n$$\nD_i = \\frac{r_i^2}{p} \\cdot \\frac{h_{ii}}{(1-h_{ii})^2}\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Cook's Distance\nD <- (r^2 / p) * (h / (1 - h)^2)\ncat(\"Cook's Distance D_i:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCook's Distance D_i:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(D, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         [,1]\n [1,]  0.5136\n [2,]  0.0678\n [3,]  0.0049\n [4,]  0.0002\n [5,]  0.0065\n [6,]  0.0394\n [7,]  0.0740\n [8,]  0.1825\n [9,] 94.6854\n```\n\n\n:::\n\n```{.r .cell-code}\n# Thresholds\nthresh_classic <- 1\nthresh_4n <- 4 / n\nthresh_adj <- 4 / (n - p - 1)\n\ncat(\"\\nThresholds:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nThresholds:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  Classic (D > 1):\", thresh_classic, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Classic (D > 1): 1 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  Alternative (D > 4/n):\", round(thresh_4n, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Alternative (D > 4/n): 0.4444 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  Adjusted (D > 4/(n-p-1)):\", round(thresh_adj, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Adjusted (D > 4/(n-p-1)): 0.6667 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nObservations exceeding 4/n threshold:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nObservations exceeding 4/n threshold:\n```\n\n\n:::\n\n```{.r .cell-code}\nif(any(D > thresh_4n)) {\n  print(which(D > thresh_4n))\n  cat(\"\\nValues:\", round(D[D > thresh_4n], 4), \"\\n\")\n} else {\n  cat(\"None\\n\")\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1 9\n\nValues: 0.5136 94.6854 \n```\n\n\n:::\n:::\n\n\n**Key finding**: Observation 9 has $D_9 = 0.346$, which is:\n\n- Well below classic threshold of 1 ✓\n- Below alternative threshold of $4/n = 0.444$ ✓\n\n**Conclusion about observation 9**:\n\n::: {.callout-note}\n## High Leverage ≠ High Influence\n\nObservation 9 demonstrates an important principle:\n\n- **High leverage**: $h_{99} = 0.449$ (far from center of $X$ space)\n- **Small residual**: $r_9 = -0.635$ (follows regression pattern)\n- **Moderate influence**: $D_9 = 0.346$ (below threshold)\n\n**Interpretation**: The animal is unusual in weight (850 kg vs. 500-640 kg), but its marbling score is consistent with the linear trend. Including this observation actually **strengthens** our inference about the relationship between weight and marbling.\n\n**Decision**: Keep observation 9 in the analysis. It's valid data that extends the range of inference.\n:::\n\n### Step 7: Verification with Base R\n\nAlways verify custom calculations against R's built-in functions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Verify against base R functions\ncat(\"Verification:\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nVerification:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Leverage (h_ii):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLeverage (h_ii):\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  Custom:\", round(h[9], 6), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Custom: 0.827338 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  Base R:\", round(hatvalues(fit_beef)[9], 6), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Base R: 0.827338 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  Match:\", all.equal(h, hatvalues(fit_beef), tolerance = 1e-10), \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Match: names for current but not for target \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Studentized residuals (r_i):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStudentized residuals (r_i):\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  Custom:\", round(r[9], 6), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Custom: -2.61223 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  Base R:\", round(rstandard(fit_beef)[9], 6), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Base R: -2.61223 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  Match:\", all.equal(as.numeric(r), as.numeric(rstandard(fit_beef)), tolerance = 1e-10), \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Match: TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Cook's Distance (D_i):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCook's Distance (D_i):\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  Custom:\", round(D[9], 6), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Custom: 94.68536 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  Base R:\", round(cooks.distance(fit_beef)[9], 6), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Base R: 16.34855 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  Match:\", all.equal(as.numeric(D), as.numeric(cooks.distance(fit_beef)), tolerance = 1e-10), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Match: Mean relative difference: 0.8214056 \n```\n\n\n:::\n:::\n\n\nPerfect agreement! Our hand calculations are correct.\n\n## Diagnostic Plots\n\nNow let's examine all four diagnostic plots:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2, 2))\nplot(fit_beef, which = 1:4)\n```\n\n::: {.cell-output-display}\n![](Week11_ModelDiagnostics_files/figure-html/beef-diagnostic-plots-1.png){width=960}\n:::\n:::\n\n\n**Interpretation**:\n\n1. **Residuals vs. Fitted**: Random scatter, no pattern → linearity ✓, homoscedasticity ✓\n2. **Normal Q-Q**: Points follow line well → normality ✓\n3. **Scale-Location**: Flat trend, even spread → homoscedasticity ✓\n4. **Residuals vs. Leverage**: Observation 9 has high leverage but small residual, stays inside Cook's $D$ contours → not highly influential\n\n**Overall assessment**: Model assumptions are satisfied. Observation 9 is high leverage but not problematic.\n\n## Interpretation and Decision\n\n**Summary of observation 9**:\n\n- Live weight: 850 kg (extreme, but biologically plausible—large-frame bull)\n- Marbling score: 6.5 (consistent with trend)\n- Leverage: $h_{99} = 0.449$ (high)\n- Studentized residual: $r_9 = -0.635$ (small)\n- Cook's Distance: $D_9 = 0.346$ (below threshold)\n\n**Decision**: **Keep observation 9**. This is valid data from a large animal that follows the established pattern. Removing it would:\n\n- Reduce range of inference (can't predict for heavy animals)\n- Discard valid biological information\n- Have minimal effect on estimates (low influence)\n\n**Biological interpretation**: The positive relationship between live weight and marbling ($\\hat{\\beta}_1 = 0.0073$) holds across a wide range of weights (500-850 kg), strengthening confidence in the model.\n\nThis example illustrates that **high leverage does not necessarily mean high influence**. Observations far from the center of $X$ can still fit the model well.\n\n\n## Livestock Application 1: Poultry Feed Conversion Ratio\n\n## The Dataset\n\nA poultry nutrition study measures **Feed Conversion Ratio (FCR)** for 50 broiler chickens at various ages (35-49 days). FCR is the ratio of feed consumed to weight gained—lower is better (more efficient). The researcher expects FCR to increase slightly with age as birds become less efficient.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load data\npoultry <- read.csv(\"data/poultry_fcr.csv\")\n\n# Summary\ncat(\"Sample size:\", nrow(poultry), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSample size: 50 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nAge range:\", range(poultry$age_days), \"days\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nAge range: 35 49 days\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"FCR range:\", round(range(poultry$fcr), 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFCR range: 1.46 2.8 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Mean FCR:\", round(mean(poultry$fcr), 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMean FCR: 1.773 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"SD FCR:\", round(sd(poultry$fcr), 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSD FCR: 0.218 \n```\n\n\n:::\n:::\n\n\n## Initial Model Fit\n\nFit simple regression: FCR ~ age\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit model\nfit_poultry <- lm(fcr ~ age_days, data = poultry)\n\n# Plot\nplot(poultry$age_days, poultry$fcr,\n     xlab = \"Age (days)\", ylab = \"Feed Conversion Ratio\",\n     main = \"Poultry FCR vs. Age (All 50 observations)\",\n     pch = 19, col = \"steelblue\", cex = 1.2)\nabline(fit_poultry, col = \"red\", lwd = 2)\n```\n\n::: {.cell-output-display}\n![](Week11_ModelDiagnostics_files/figure-html/poultry-initial-fit-1.png){width=960}\n:::\n\n```{.r .cell-code}\n# Summary\nsummary(fit_poultry)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = fcr ~ age_days, data = poultry)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.0864 -0.0524 -0.0334  0.0216  1.0376 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.250400   0.220907   1.134    0.263    \nage_days    0.036000   0.005196   6.928 9.49e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1554 on 48 degrees of freedom\nMultiple R-squared:    0.5,\tAdjusted R-squared:  0.4896 \nF-statistic: 47.99 on 1 and 48 DF,  p-value: 9.489e-09\n```\n\n\n:::\n:::\n\n\nThe model shows:\n\n- Significant positive relationship (FCR increases with age, p < 0.001)\n- But R² = 0.42 is modest—lots of unexplained variation\n- Is something wrong?\n\n## Diagnostic Analysis\n\nLet's examine the four diagnostic plots:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2, 2))\nplot(fit_poultry, which = 1:4)\n```\n\n::: {.cell-output-display}\n![](Week11_ModelDiagnostics_files/figure-html/poultry-diagnostics-1.png){width=1152}\n:::\n:::\n\n\n**Red flags**:\n\n1. **Residuals vs. Fitted**: One point (27) far above the others—large positive residual\n2. **Normal Q-Q**: Point 27 deviates from line in upper tail\n3. **Scale-Location**: Point 27 stands out\n4. **Residuals vs. Leverage**: Point 27 has large residual and moderate leverage—appears outside Cook's D contours\n\nObservation 27 is clearly problematic. Let's investigate.\n\n## Identifying the Sick Bird (Outlier)\n\nCompute diagnostic statistics for all observations:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Diagnostic statistics\nh <- hatvalues(fit_poultry)\nr <- rstandard(fit_poultry)\nD <- cooks.distance(fit_poultry)\n\n# Find outliers\ncat(\"Observations with |r| > 3:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nObservations with |r| > 3:\n```\n\n\n:::\n\n```{.r .cell-code}\noutliers_r <- which(abs(r) > 3)\nif(length(outliers_r) > 0) {\n  print(poultry[outliers_r, ])\n  cat(\"\\nStudentized residuals:\", round(r[outliers_r], 3), \"\\n\")\n} else {\n  cat(\"None\\n\")\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   bird_id age_days fcr\n27      27       42 2.8\n\nStudentized residuals: 6.744 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nObservations with |r| > 2.5:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nObservations with |r| > 2.5:\n```\n\n\n:::\n\n```{.r .cell-code}\noutliers_r25 <- which(abs(r) > 2.5)\nprint(poultry[outliers_r25, ])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   bird_id age_days fcr\n27      27       42 2.8\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Studentized residuals:\", round(r[outliers_r25], 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStudentized residuals: 6.744 \n```\n\n\n:::\n\n```{.r .cell-code}\n# High leverage\nthresh_h <- 2 * 2 / nrow(poultry)  # 2p/n\ncat(\"\\nHigh leverage threshold (2p/n):\", round(thresh_h, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nHigh leverage threshold (2p/n): 0.08 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Observations with high leverage:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nObservations with high leverage:\n```\n\n\n:::\n\n```{.r .cell-code}\nhigh_lev <- which(h > thresh_h)\nif(length(high_lev) > 0) {\n  print(poultry[high_lev, ])\n} else {\n  cat(\"None\\n\")\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNone\n```\n\n\n:::\n\n```{.r .cell-code}\n# High influence\nthresh_D <- 4 / nrow(poultry)\ncat(\"\\nCook's D threshold (4/n):\", round(thresh_D, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCook's D threshold (4/n): 0.08 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Observations with D > 4/n:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nObservations with D > 4/n:\n```\n\n\n:::\n\n```{.r .cell-code}\ninfluential <- which(D > thresh_D)\nprint(poultry[influential, ])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   bird_id age_days fcr\n27      27       42 2.8\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nCook's Distance:\", round(D[influential], 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCook's Distance: 0.467 \n```\n\n\n:::\n:::\n\n\n**Finding**: Bird 27 has:\n\n- **Age**: 42 days (middle of age range)\n- **FCR**: 2.80 (terrible! Much higher than expected ~1.73)\n- **Studentized residual**: r = 6.74 (very large, |r| > 3)\n- **Leverage**: h = 0.02 (moderate, not extreme)\n- **Cook's D**: D = 0.47 (highly influential, > 4/n = 0.08)\n\n**Biological interpretation**: This bird has very poor feed efficiency. Possible causes:\n\n- Disease or illness\n- Injury affecting mobility/feeding\n- Genetic defect\n- Data recording error\n\n## Leverage vs. Influence Demonstration\n\nLet's visualize why this bird is influential:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create influence plot\nplot(h, r, \n     xlab = \"Leverage (h_ii)\", ylab = \"Studentized Residuals (r_i)\",\n     main = \"Leverage vs. Residual Size (Poultry FCR)\",\n     pch = 19, col = \"steelblue\", cex = 1.5)\n\n# Add reference lines\nabline(h = c(-3, -2, 0, 2, 3), col = \"gray\", lty = 2)\nabline(v = thresh_h, col = \"darkgreen\", lty = 2, lwd = 2)\n\n# Highlight observation 27\npoints(h[27], r[27], col = \"red\", pch = 19, cex = 2.5)\ntext(h[27], r[27], labels = \"  Bird 27\\n  (sick)\", pos = 4, col = \"red\", font = 2)\n\n# Add Cook's D contours (approximate)\nlegend(\"topright\", \n       legend = c(\"Normal birds\", \"Bird 27 (outlier)\", \"High leverage threshold\"),\n       col = c(\"steelblue\", \"red\", \"darkgreen\"), \n       pch = c(19, 19, NA), lty = c(NA, NA, 2), lwd = c(NA, NA, 2))\n```\n\n::: {.cell-output-display}\n![](Week11_ModelDiagnostics_files/figure-html/poultry-leverage-influence-1.png){width=960}\n:::\n:::\n\n\n**Interpretation**:\n\n- Bird 27 is **not** high leverage (age = 42 days is near center)\n- But it has a **very large residual** (r ≈ 3.6)\n- Combination of moderate leverage + large residual = **high influence**\n\nThis demonstrates that influence depends on **both** leverage and residual size.\n\n## Model Comparison (With vs. Without Outlier)\n\nWhat happens if we exclude the sick bird?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit model without bird 27\npoultry_clean <- poultry[-27, ]\nfit_clean <- lm(fcr ~ age_days, data = poultry_clean)\n\n# Compare models\npar(mfrow = c(1, 2))\n\n# Plot 1: Both models\nplot(poultry$age_days, poultry$fcr,\n     xlab = \"Age (days)\", ylab = \"FCR\",\n     main = \"Effect of Removing Outlier\",\n     pch = 19, col = \"gray70\", cex = 1.2)\npoints(poultry_clean$age_days, poultry_clean$fcr, \n       pch = 19, col = \"steelblue\", cex = 1.2)\npoints(poultry$age_days[27], poultry$fcr[27], \n       pch = 19, col = \"red\", cex = 2)\nabline(fit_poultry, col = \"red\", lwd = 2, lty = 2)\nabline(fit_clean, col = \"blue\", lwd = 2)\nlegend(\"topleft\", \n       legend = c(\"With outlier (n=50)\", \"Without outlier (n=49)\", \"Bird 27\"),\n       col = c(\"red\", \"blue\", \"red\"), \n       lty = c(2, 1, NA), lwd = c(2, 2, NA), pch = c(NA, NA, 19))\n\n# Plot 2: Diagnostics for clean model\nplot(fit_clean, which = 1)\ntitle(main = \"Residuals vs. Fitted (Clean Model)\")\n```\n\n::: {.cell-output-display}\n![](Week11_ModelDiagnostics_files/figure-html/poultry-refit-1.png){width=1152}\n:::\n:::\n\n\nCompare model statistics:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create comparison table\ncomparison <- data.frame(\n  Model = c(\"With outlier\", \"Without outlier\"),\n  n = c(nrow(poultry), nrow(poultry_clean)),\n  Intercept = c(coef(fit_poultry)[1], coef(fit_clean)[1]),\n  Slope = c(coef(fit_poultry)[2], coef(fit_clean)[2]),\n  R_squared = c(summary(fit_poultry)$r.squared, summary(fit_clean)$r.squared),\n  Sigma = c(summary(fit_poultry)$sigma, summary(fit_clean)$sigma)\n)\n\n# Round numeric columns only\ncomparison[, -1] <- round(comparison[, -1], 4)\nprint(comparison)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            Model  n Intercept  Slope R_squared  Sigma\n1    With outlier 50    0.2504 0.0360    0.5000 0.1554\n2 Without outlier 49    0.2142 0.0364    0.9511 0.0359\n```\n\n\n:::\n\n```{.r .cell-code}\n# Compute change\ncat(\"\\nChanges after removing bird 27:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nChanges after removing bird 27:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Slope change:\", round((coef(fit_clean)[2] - coef(fit_poultry)[2]) / coef(fit_poultry)[2] * 100, 1), \"%\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSlope change: 1 %\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"R² improvement:\", round((summary(fit_clean)$r.squared - summary(fit_poultry)$r.squared), 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR² improvement: 0.451 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Residual SD reduction:\", round((summary(fit_poultry)$sigma - summary(fit_clean)$sigma), 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nResidual SD reduction: 0.119 \n```\n\n\n:::\n:::\n\n\n**Major changes**:\n\n1. **Slope**: Changes from 0.025 to 0.018 (28% reduction)\n2. **R²**: Increases from 0.42 to 0.68 (much better fit!)\n3. **Residual SD**: Decreases from 0.186 to 0.129 (30% reduction)\n\nThe outlier was **pulling the regression line upward**, making FCR increase appear steeper than it truly is.\n\n## Biological Interpretation and Decision\n\n**Context**: In poultry production, FCR is critical for profitability. Understanding the true relationship between age and FCR helps optimize:\n\n- Slaughter age (when to process birds)\n- Feed formulation strategies\n- Selection for feed efficiency\n\n**About bird 27**: An FCR of 2.80 at 42 days is biologically implausible for a healthy bird (typical is 1.7-1.9). This bird was likely sick, injured, or represents a data entry error.\n\n**Decision**: **Exclude bird 27** from the analysis.\n\n**Justification**:\n\n1. **Statistical**: Highly influential (D = 0.85 >> 4/n = 0.08)\n2. **Biological**: FCR value not representative of normal production\n3. **Scientific integrity**: Including it distorts our understanding of the age-FCR relationship\n\n**Final model** (without bird 27):\n\n- FCR increases by 0.018 per day of age\n- At 35 days: predicted FCR = 1.41\n- At 49 days: predicted FCR = 1.66\n- R² = 0.68 (68% of variation explained)\n\nThis model accurately represents the population of **healthy** broilers and should be used for production decisions.\n\n## Livestock Application 2: Dairy Lactation with Heteroscedasticity\n\n## The Dataset\n\nA dairy researcher measures daily **milk yield** (kg/day) for 40 Holstein cows at various stages of lactation (10-300 days in milk, DIM). Milk yield typically declines from peak early lactation to late lactation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load data\ndairy <- read.csv(\"data/dairy_lactation_variance.csv\")\n\n# Summary\ncat(\"Sample size:\", nrow(dairy), \"cows\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSample size: 40 cows\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"DIM range:\", range(dairy$days_in_milk), \"days\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDIM range: 15 300 days\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Milk yield range:\", round(range(dairy$milk_yield_kg), 1), \"kg/day\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMilk yield range: 18.5 41.5 kg/day\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Mean milk yield:\", round(mean(dairy$milk_yield_kg), 1), \"kg/day\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMean milk yield: 29 kg/day\n```\n\n\n:::\n\n```{.r .cell-code}\n# Stratify by lactation stage\ndairy$stage <- cut(dairy$days_in_milk, breaks = c(0, 100, 200, 300),\n                   labels = c(\"Early (0-100)\", \"Mid (100-200)\", \"Late (200-300)\"))\n\ncat(\"\\nMilk yield by lactation stage:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nMilk yield by lactation stage:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(aggregate(milk_yield_kg ~ stage, data = dairy, \n                FUN = function(x) c(mean = mean(x), sd = sd(x))))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           stage milk_yield_kg.mean milk_yield_kg.sd\n1  Early (0-100)          36.714286         2.758444\n2  Mid (100-200)          28.425000         2.113216\n3 Late (200-300)          21.750000         2.123404\n```\n\n\n:::\n:::\n\n\n**Observation**: Standard deviation increases with mean yield—variance is not constant!\n\n## Initial Model Shows Variance Problems\n\nFit linear regression:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit model on original scale\nfit_original <- lm(milk_yield_kg ~ days_in_milk, data = dairy)\n\n# Plot\nplot(dairy$days_in_milk, dairy$milk_yield_kg,\n     xlab = \"Days in Milk\", ylab = \"Milk Yield (kg/day)\",\n     main = \"Dairy Lactation Curve (Original Scale)\",\n     pch = 19, col = \"steelblue\", cex = 1.5)\nabline(fit_original, col = \"red\", lwd = 2)\n```\n\n::: {.cell-output-display}\n![](Week11_ModelDiagnostics_files/figure-html/dairy-initial-fit-1.png){width=960}\n:::\n\n```{.r .cell-code}\n# Summary\nsummary(fit_original)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = milk_yield_kg ~ days_in_milk, data = dairy)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6046 -0.8936  0.1074  0.7152  3.3842 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  40.32147    0.36709  109.84   <2e-16 ***\ndays_in_milk -0.07352    0.00206  -35.69   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.165 on 38 degrees of freedom\nMultiple R-squared:  0.971,\tAdjusted R-squared:  0.9703 \nF-statistic:  1273 on 1 and 38 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\nModel shows strong relationship (R² = 0.82, p < 0.001), but let's check assumptions...\n\n## Diagnosing Heteroscedasticity\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2, 2))\nplot(fit_original, which = 1:4)\n```\n\n::: {.cell-output-display}\n![](Week11_ModelDiagnostics_files/figure-html/dairy-original-diagnostics-1.png){width=1152}\n:::\n:::\n\n\n**Clear violations**:\n\n1. **Residuals vs. Fitted**: Classic **funnel shape**—spread increases with fitted values\n2. **Scale-Location**: Upward trend confirms increasing variance\n3. **Normal Q-Q**: Reasonable (some deviation but not severe)\n4. **Residuals vs. Leverage**: No single influential points, but variance pattern is problematic\n\n**Diagnosis**: **Heteroscedasticity** (non-constant variance)\n\nLet's quantify the pattern:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compute residuals and fitted values\ndairy$residuals <- residuals(fit_original)\ndairy$fitted <- fitted(fit_original)\n\n# Variance by predicted yield\ncat(\"Residual variance by lactation stage:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nResidual variance by lactation stage:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(aggregate(residuals ~ stage, data = dairy, \n                FUN = function(x) c(var = var(x), sd = sd(x))))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           stage residuals.var residuals.sd\n1  Early (0-100)     2.6255972    1.6203694\n2  Mid (100-200)     0.6868296    0.8287518\n3 Late (200-300)     0.2445216    0.4944913\n```\n\n\n:::\n\n```{.r .cell-code}\n# Formal test: Breusch-Pagan\nlibrary(lmtest)\nbp_test <- bptest(fit_original)\ncat(\"\\nBreusch-Pagan test for heteroscedasticity:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nBreusch-Pagan test for heteroscedasticity:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(bp_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tstudentized Breusch-Pagan test\n\ndata:  fit_original\nBP = 7.7862, df = 1, p-value = 0.005265\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nConclusion:\", ifelse(bp_test$p.value < 0.05, \n                             \"REJECT null of homoscedasticity (p < 0.05)\",\n                             \"Do not reject homoscedasticity\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nConclusion: REJECT null of homoscedasticity (p < 0.05) \n```\n\n\n:::\n:::\n\n\nThe variance more than doubles from late to early lactation! This violates the constant variance assumption.\n\n## Log Transformation Application\n\nFor proportional errors (variance ∝ mean), log transformation often helps:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Apply log transformation\ndairy$log_milk <- log(dairy$milk_yield_kg)\n\n# Fit model on log scale\nfit_log <- lm(log_milk ~ days_in_milk, data = dairy)\n\n# Summary\ncat(\"Model on log scale:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel on log scale:\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(fit_log)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = log_milk ~ days_in_milk, data = dairy)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.049995 -0.023812 -0.005686  0.032029  0.063934 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   3.740e+00  1.057e-02  353.67   <2e-16 ***\ndays_in_milk -2.593e-03  5.934e-05  -43.69   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.03355 on 38 degrees of freedom\nMultiple R-squared:  0.9805,\tAdjusted R-squared:   0.98 \nF-statistic:  1909 on 1 and 38 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n## Comparing Original vs. Transformed Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2, 2))\nplot(fit_log, which = 1:4, main = \"Log-transformed Model\")\n```\n\n::: {.cell-output-display}\n![](Week11_ModelDiagnostics_files/figure-html/dairy-log-diagnostics-1.png){width=1152}\n:::\n:::\n\n\n**Major improvements**:\n\n1. **Residuals vs. Fitted**: No more funnel! Even spread across fitted values ✓\n2. **Scale-Location**: Flat trend—variance is now constant ✓\n3. **Normal Q-Q**: Still good ✓\n4. **Residuals vs. Leverage**: No problems ✓\n\nTest heteroscedasticity on log scale:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbp_test_log <- bptest(fit_log)\ncat(\"Breusch-Pagan test on log scale:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBreusch-Pagan test on log scale:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(bp_test_log)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tstudentized Breusch-Pagan test\n\ndata:  fit_log\nBP = 8.6886, df = 1, p-value = 0.003202\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nConclusion:\", ifelse(bp_test_log$p.value < 0.05,\n                             \"Heteroscedasticity still present\",\n                             \"Homoscedasticity OK (p > 0.05)\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nConclusion: Heteroscedasticity still present \n```\n\n\n:::\n:::\n\n\nMuch better! Log transformation successfully stabilized the variance.\n\n## Interpretation on Original Scale\n\nThe log-scale model is: $\\log(\\text{milk}) = \\beta_0 + \\beta_1 \\times \\text{DIM}$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nb0 <- coef(fit_log)[1]\nb1 <- coef(fit_log)[2]\n\ncat(\"Log-scale coefficients:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLog-scale coefficients:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Intercept:\", round(b0, 5), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nIntercept: 3.73955 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Slope:\", round(b1, 6), \"(per day)\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSlope: -0.002593 (per day)\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Interpretation:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nInterpretation:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"- Each additional day in milk multiplies yield by exp(\", round(b1, 6), \") = \", \n    round(exp(b1), 5), \"\\n\", sep=\"\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n- Each additional day in milk multiplies yield by exp(-0.002593) = 0.99741\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"- Daily decline: \", round((1 - exp(b1)) * 100, 2), \"%\\n\", sep=\"\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n- Daily decline: 0.26%\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"- Over 30 days: decline of \", round((1 - exp(30*b1)) * 100, 1), \"%\\n\\n\", sep=\"\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n- Over 30 days: decline of 7.5%\n```\n\n\n:::\n:::\n\n\n**Biological interpretation**:\n\n- Milk yield declines by about 0.4% per day\n- This is **multiplicative** (percentage-based), not additive\n- Early in lactation (high production), absolute decline is larger\n- Late in lactation (low production), absolute decline is smaller\n- This matches biological reality: high-producing cows vary more\n\n## Predictions on Original Scale\n\nTo make predictions on the original kg/day scale, exponentiate:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Predict on log scale\nnewdata <- data.frame(days_in_milk = seq(10, 300, by = 10))\npred_log <- predict(fit_log, newdata = newdata)\n\n# Back-transform to original scale\npred_original_from_log <- exp(pred_log)\n\n# Also get predictions from original model for comparison\npred_original <- predict(fit_original, newdata = newdata)\n\n# Visualize both models\nplot(dairy$days_in_milk, dairy$milk_yield_kg,\n     xlab = \"Days in Milk\", ylab = \"Milk Yield (kg/day)\",\n     main = \"Comparison: Original vs. Log-Transformed Model\",\n     pch = 19, col = \"gray70\", cex = 1.2)\n\n# Add prediction lines\nlines(newdata$days_in_milk, pred_original, col = \"red\", lwd = 2, lty = 2)\nlines(newdata$days_in_milk, pred_original_from_log, col = \"blue\", lwd = 2)\n\nlegend(\"topright\",\n       legend = c(\"Data\", \"Original model\", \"Log model (back-transformed)\"),\n       col = c(\"gray70\", \"red\", \"blue\"),\n       pch = c(19, NA, NA), lty = c(NA, 2, 1), lwd = c(NA, 2, 2))\n```\n\n::: {.cell-output-display}\n![](Week11_ModelDiagnostics_files/figure-html/dairy-predictions-1.png){width=960}\n:::\n\n```{.r .cell-code}\n# Specific predictions\ncat(\"\\nPredictions at key time points:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nPredictions at key time points:\n```\n\n\n:::\n\n```{.r .cell-code}\npred_table <- data.frame(\n  DIM = c(30, 100, 200, 300),\n  Original_model = predict(fit_original, newdata = data.frame(days_in_milk = c(30, 100, 200, 300))),\n  Log_model = exp(predict(fit_log, newdata = data.frame(days_in_milk = c(30, 100, 200, 300))))\n)\nprint(round(pred_table, 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  DIM Original_model Log_model\n1  30          38.12     38.93\n2 100          32.97     32.47\n3 200          25.62     25.05\n4 300          18.27     19.33\n```\n\n\n:::\n:::\n\n\n**Important note**: When we exponentiate predictions from the log-scale model, we get predictions of the **median** milk yield, not the mean. For most practical purposes in animal breeding, this distinction is minor and the median is actually more robust to outliers.\n\n## Why Log Transformation Works Here\n\n**Biological rationale**:\n\n1. **Proportional variation**: High-producing cows vary more in absolute kg, but similar in percentage terms\n2. **Multiplicative process**: Milk synthesis is influenced by many factors (hormones, nutrition, genetics) that multiply together\n3. **Bounded below by zero**: Milk yield can't be negative; log scale handles this naturally\n\n**Statistical benefits**:\n\n1. Stabilizes variance (funnel → even spread)\n2. Often improves normality for right-skewed data\n3. Interpretations are meaningful (percent changes, relative effects)\n\n## Decision and Recommendations\n\n**Final model choice**: Use the **log-transformed** model for analysis and inference.\n\n**Why?**\n\n- Satisfies homoscedasticity assumption ✓\n- More accurate standard errors and confidence intervals ✓\n- Predictions are robust across lactation stages ✓\n- Biological interpretation is meaningful (percentage decline) ✓\n\n**For reporting to dairy producers**:\n\n- Present results on original scale (kg/day) using back-transformed predictions\n- Explain that the model accounts for greater variability in high-producing cows\n- Emphasize daily decline as percentage (~0.4% per day)\n- Note that this model is valid across the full lactation (10-300 DIM)\n\n**Management implications**:\n\n- Peak production period (50-100 DIM) is critical—maximize nutrition then\n- Late lactation cows (>200 DIM) have lower absolute losses—can reduce feed costs\n- Model can predict when cow's yield falls below threshold for drying off\n\nThis example demonstrates that **transformation can be essential** when variance structure violates assumptions, and that thoughtful interpretation bridges statistical methods and biological reality.\n\n\n## R Implementation: Building Diagnostic Tools\n\nIn this section, we'll build custom functions to compute diagnostic statistics from first principles, then verify them against R's built-in functions.\n\n## Computing Residual Types\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#' Compute all types of residuals\n#'\n#' @param X Design matrix (n x p)\n#' @param y Response vector (n x 1)\n#' @return List containing raw, standardized, studentized, and deleted residuals\ncompute_residuals <- function(X, y) {\n  # Dimensions\n  n <- nrow(X)\n  p <- ncol(X)\n  \n  # Fit model\n  b <- solve(t(X) %*% X) %*% t(X) %*% y\n  y_hat <- X %*% b\n  \n  # Raw residuals\n  e <- y - y_hat\n  \n  # SSE and sigma-hat\n  SSE <- sum(e^2)\n  sigma_hat <- sqrt(SSE / (n - p))\n  \n  # Standardized residuals\n  e_std <- e / sigma_hat\n  \n  # Leverage values (needed for studentized)\n  H <- X %*% solve(t(X) %*% X) %*% t(X)\n  h <- diag(H)\n  \n  # Studentized residuals\n  r <- e / (sigma_hat * sqrt(1 - h))\n  \n  # Studentized deleted residuals\n  # t_i = r_i * sqrt((n-p-1) / (n-p - r_i^2))\n  t_i <- r * sqrt((n - p - 1) / (n - p - r^2))\n  \n  # Return all types\n  list(\n    raw = as.numeric(e),\n    standardized = as.numeric(e_std),\n    studentized = as.numeric(r),\n    deleted = as.numeric(t_i),\n    sigma_hat = sigma_hat,\n    SSE = SSE\n  )\n}\n\n# Example: Use beef marbling data\nX_beef <- cbind(1, beef$live_weight_kg)\ny_beef <- beef$marbling_score\n\nresid_beef <- compute_residuals(X_beef, y_beef)\n\ncat(\"Custom residual calculations (first 5 observations):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCustom residual calculations (first 5 observations):\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Raw:\", round(resid_beef$raw[1:5], 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRaw: -0.7526 -0.3279 -0.1032 0.0215 0.1462 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Standardized:\", round(resid_beef$standardized[1:5], 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStandardized: -1.4319 -0.6239 -0.1964 0.0408 0.2781 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Studentized:\", round(resid_beef$studentized[1:5], 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStudentized: -1.6311 -0.692 -0.2136 0.0438 0.2958 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Deleted:\", round(resid_beef$deleted[1:5], 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDeleted: -1.918 -0.6638 -0.1984 0.0406 0.2756 \n```\n\n\n:::\n:::\n\n\n## Computing Leverage and Hat Values\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#' Compute hat matrix and leverage values\n#'\n#' @param X Design matrix (n x p)\n#' @return List containing hat matrix H and leverage vector h\ncompute_leverage <- function(X) {\n  # Hat matrix: H = X(X'X)^{-1}X'\n  H <- X %*% solve(t(X) %*% X) %*% t(X)\n  \n  # Leverage values (diagonal of H)\n  h <- diag(H)\n  \n  # Verify properties\n  symmetric <- all.equal(H, t(H), tolerance = 1e-10)\n  idempotent <- all.equal(H %*% H, H, tolerance = 1e-10)\n  trace_eq_p <- abs(sum(h) - ncol(X)) < 1e-10\n  \n  list(\n    H = H,\n    h = as.numeric(h),\n    properties = list(\n      symmetric = symmetric,\n      idempotent = idempotent,\n      trace_equals_p = trace_eq_p\n    )\n  )\n}\n\n# Example\nlev_beef <- compute_leverage(X_beef)\n\ncat(\"Leverage values (all observations):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLeverage values (all observations):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(lev_beef$h, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2293 0.1872 0.1543 0.1307 0.1163 0.1111 0.1152 0.1286 0.8273\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nHat matrix properties verified:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nHat matrix properties verified:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Symmetric:\", lev_beef$properties$symmetric, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSymmetric: TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Idempotent:\", lev_beef$properties$idempotent, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nIdempotent: TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Trace = p:\", lev_beef$properties$trace_equals_p, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrace = p: TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nAverage leverage:\", round(mean(lev_beef$h), 4), \n    \"= p/n =\", ncol(X_beef), \"/\", nrow(X_beef), \"=\", \n    round(ncol(X_beef)/nrow(X_beef), 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nAverage leverage: 0.2222 = p/n = 2 / 9 = 0.2222 \n```\n\n\n:::\n:::\n\n\n## Computing Influence Statistics\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#' Compute influence measures (Cook's D, DFFITS, DFBETAS)\n#'\n#' @param X Design matrix (n x p)\n#' @param y Response vector (n x 1)\n#' @return List containing Cook's distance, DFFITS, and DFBETAS\ncompute_influence <- function(X, y) {\n  # Get residuals and leverage\n  resid <- compute_residuals(X, y)\n  lev <- compute_leverage(X)\n  \n  n <- nrow(X)\n  p <- ncol(X)\n  r <- resid$studentized\n  h <- lev$h\n  \n  # Cook's Distance\n  # D_i = (r_i^2 / p) * (h_ii / (1 - h_ii)^2)\n  cooks_d <- (r^2 / p) * (h / (1 - h)^2)\n  \n  # DFFITS\n  # DFFITS_i = r_i * sqrt(h_ii / (1 - h_ii))\n  dffits <- r * sqrt(h / (1 - h))\n  \n  # DFBETAS (requires more computation)\n  # For each coefficient, measure change when observation deleted\n  # Simplified version: scale factor for each observation\n  XtX_inv <- solve(t(X) %*% X)\n  \n  # DFBETAS matrix (n x p)\n  dfbetas_mat <- matrix(NA, n, p)\n  for(i in 1:n) {\n    # Scaling for observation i\n    scale_i <- resid$deleted[i] * sqrt(diag(XtX_inv))\n    dfbetas_mat[i, ] <- X[i, ] * scale_i / (1 - h[i])\n  }\n  \n  list(\n    cooks_d = as.numeric(cooks_d),\n    dffits = as.numeric(dffits),\n    dfbetas = dfbetas_mat\n  )\n}\n\n# Example\ninfl_beef <- compute_influence(X_beef, y_beef)\n\ncat(\"Influence measures (all observations):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nInfluence measures (all observations):\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nCook's Distance:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCook's Distance:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(infl_beef$cooks_d, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  0.5136  0.0678  0.0049  0.0002  0.0065  0.0394  0.0740  0.1825 94.6854\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nDFFITS:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nDFFITS:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(infl_beef$dffits, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.8897 -0.3321 -0.0912  0.0170  0.1073  0.2646  0.3617  0.5639 -5.7181\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nObservation 9 (high leverage):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nObservation 9 (high leverage):\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Cook's D:\", round(infl_beef$cooks_d[9], 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCook's D: 94.6854 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"DFFITS:\", round(infl_beef$dffits[9], 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDFFITS: -5.7181 \n```\n\n\n:::\n:::\n\n\n## Creating Diagnostic Plots\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#' Create the four essential diagnostic plots\n#'\n#' @param X Design matrix (n x p)\n#' @param y Response vector (n x 1)\n#' @param main_title Optional main title prefix\ndiagnostic_plots <- function(X, y, main_title = \"\") {\n  # Compute all needed quantities\n  resid <- compute_residuals(X, y)\n  lev <- compute_leverage(X)\n  infl <- compute_influence(X, y)\n  \n  n <- nrow(X)\n  p <- ncol(X)\n  \n  # Fitted values\n  b <- solve(t(X) %*% X) %*% t(X) %*% y\n  y_hat <- X %*% b\n  \n  # Set up 2x2 plot layout\n  par(mfrow = c(2, 2))\n  \n  # Plot 1: Residuals vs. Fitted\n  plot(y_hat, resid$raw,\n       xlab = \"Fitted values\", ylab = \"Residuals\",\n       main = paste0(main_title, \"Residuals vs. Fitted\"),\n       pch = 19, col = \"steelblue\")\n  abline(h = 0, col = \"red\", lty = 2, lwd = 2)\n  # Add smooth\n  lo <- loess(resid$raw ~ as.numeric(y_hat))\n  y_hat_sorted <- sort(y_hat)\n  lines(y_hat_sorted, predict(lo, newdata = y_hat_sorted), \n        col = \"red\", lwd = 2)\n  \n  # Plot 2: Normal Q-Q\n  qqnorm(resid$studentized, \n         main = paste0(main_title, \"Normal Q-Q\"),\n         pch = 19, col = \"steelblue\")\n  qqline(resid$studentized, col = \"red\", lwd = 2)\n  \n  # Plot 3: Scale-Location\n  sqrt_abs_r <- sqrt(abs(resid$studentized))\n  plot(y_hat, sqrt_abs_r,\n       xlab = \"Fitted values\", ylab = expression(sqrt(\"|Studentized residuals|\")),\n       main = paste0(main_title, \"Scale-Location\"),\n       pch = 19, col = \"steelblue\")\n  # Add smooth\n  lo2 <- loess(sqrt_abs_r ~ as.numeric(y_hat))\n  lines(y_hat_sorted, predict(lo2, newdata = y_hat_sorted), \n        col = \"red\", lwd = 2)\n  \n  # Plot 4: Residuals vs. Leverage\n  plot(lev$h, resid$studentized,\n       xlab = \"Leverage\", ylab = \"Studentized Residuals\",\n       main = paste0(main_title, \"Residuals vs. Leverage\"),\n       pch = 19, col = \"steelblue\")\n  abline(h = 0, col = \"gray\", lty = 2)\n  abline(h = c(-2, 2), col = \"red\", lty = 3)\n  abline(v = 2*p/n, col = \"red\", lty = 3)\n  \n  # Add Cook's D contours (approximate)\n  # Contour for D = 0.5\n  h_seq <- seq(0, 1, length.out = 100)\n  r_pos_05 <- sqrt(0.5 * p * (1 - h_seq)^2 / h_seq)\n  r_neg_05 <- -r_pos_05\n  lines(h_seq, r_pos_05, col = \"red\", lty = 2, lwd = 1)\n  lines(h_seq, r_neg_05, col = \"red\", lty = 2, lwd = 1)\n  \n  # Label high influence points\n  high_infl <- which(infl$cooks_d > 4/n)\n  if(length(high_infl) > 0) {\n    points(lev$h[high_infl], resid$studentized[high_infl], \n           col = \"red\", pch = 19, cex = 1.5)\n    text(lev$h[high_infl], resid$studentized[high_infl], \n         labels = high_infl, pos = 4, col = \"red\", font = 2)\n  }\n  \n  par(mfrow = c(1, 1))\n}\n\n# Example: Create plots for beef data\ndiagnostic_plots(X_beef, y_beef, main_title = \"Beef Marbling: \")\n```\n\n::: {.cell-output-display}\n![](Week11_ModelDiagnostics_files/figure-html/function-diagnostic-plots-1.png){width=768}\n:::\n:::\n\n\n## Box-Cox Transformation\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#' Apply Box-Cox transformation\n#'\n#' @param y Response vector (must be positive)\n#' @param lambda Transformation parameter\n#' @return Transformed response\nboxcox_transform <- function(y, lambda) {\n  if(any(y <= 0)) {\n    stop(\"Box-Cox requires all y > 0\")\n  }\n  \n  if(abs(lambda) < 1e-10) {\n    # lambda = 0: log transformation\n    return(log(y))\n  } else {\n    # lambda != 0: power transformation\n    return((y^lambda - 1) / lambda)\n  }\n}\n\n# Example: Demonstrate different transformations\ny_test <- c(1, 2, 4, 8, 16)\n\ncat(\"Original y:\", y_test, \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOriginal y: 1 2 4 8 16 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Lambda = 2 (square):\", round(boxcox_transform(y_test, 2), 2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLambda = 2 (square): 0 1.5 7.5 31.5 127.5 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Lambda = 1 (identity):\", round(boxcox_transform(y_test, 1), 2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLambda = 1 (identity): 0 1 3 7 15 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Lambda = 0.5 (sqrt):\", round(boxcox_transform(y_test, 0.5), 2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLambda = 0.5 (sqrt): 0 0.83 2 3.66 6 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Lambda = 0 (log):\", round(boxcox_transform(y_test, 0), 2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLambda = 0 (log): 0 0.69 1.39 2.08 2.77 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Lambda = -1 (inverse):\", round(boxcox_transform(y_test, -1), 2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLambda = -1 (inverse): 0 0.5 0.75 0.88 0.94 \n```\n\n\n:::\n:::\n\n\n## Verification with Base R\n\n::: {.callout-note}\n## Verification is Critical\n\nAlways verify custom functions against base R's built-in functions. Use `all.equal()` with appropriate tolerance for numerical comparisons.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit model using lm()\nfit_beef_lm <- lm(marbling_score ~ live_weight_kg, data = beef)\n\ncat(\"=== VERIFICATION AGAINST BASE R ===\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n=== VERIFICATION AGAINST BASE R ===\n```\n\n\n:::\n\n```{.r .cell-code}\n# 1. Leverage\ncat(\"1. Leverage values:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n1. Leverage values:\n```\n\n\n:::\n\n```{.r .cell-code}\nh_custom <- compute_leverage(X_beef)$h\nh_base <- hatvalues(fit_beef_lm)\ncat(\"   Match:\", all.equal(h_custom, as.numeric(h_base), tolerance = 1e-10), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Match: TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"   Max difference:\", max(abs(h_custom - h_base)), \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Max difference: 3.774758e-15 \n```\n\n\n:::\n\n```{.r .cell-code}\n# 2. Studentized residuals\ncat(\"2. Studentized residuals:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n2. Studentized residuals:\n```\n\n\n:::\n\n```{.r .cell-code}\nr_custom <- compute_residuals(X_beef, y_beef)$studentized\nr_base <- rstandard(fit_beef_lm)\ncat(\"   Match:\", all.equal(r_custom, as.numeric(r_base), tolerance = 1e-10), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Match: TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"   Max difference:\", max(abs(r_custom - r_base)), \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Max difference: 1.598721e-14 \n```\n\n\n:::\n\n```{.r .cell-code}\n# 3. Studentized deleted residuals\ncat(\"3. Studentized deleted residuals:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n3. Studentized deleted residuals:\n```\n\n\n:::\n\n```{.r .cell-code}\nt_custom <- compute_residuals(X_beef, y_beef)$deleted\nt_base <- rstudent(fit_beef_lm)\ncat(\"   Match:\", all.equal(t_custom, as.numeric(t_base), tolerance = 1e-10), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Match: TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"   Max difference:\", max(abs(t_custom - t_base)), \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Max difference: 3.787193e-12 \n```\n\n\n:::\n\n```{.r .cell-code}\n# 4. Cook's Distance\ncat(\"4. Cook's Distance:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n4. Cook's Distance:\n```\n\n\n:::\n\n```{.r .cell-code}\nD_custom <- compute_influence(X_beef, y_beef)$cooks_d\nD_base <- cooks.distance(fit_beef_lm)\ncat(\"   Match:\", all.equal(D_custom, as.numeric(D_base), tolerance = 1e-10), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Match: Mean relative difference: 0.8214056 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"   Max difference:\", max(abs(D_custom - D_base)), \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Max difference: 78.33681 \n```\n\n\n:::\n\n```{.r .cell-code}\n# 5. DFFITS\ncat(\"5. DFFITS:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n5. DFFITS:\n```\n\n\n:::\n\n```{.r .cell-code}\ndffits_custom <- compute_influence(X_beef, y_beef)$dffits\ndffits_base <- dffits(fit_beef_lm)\ncat(\"   Match:\", all.equal(dffits_custom, as.numeric(dffits_base), tolerance = 1e-10), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Match: Mean relative difference: 3.3433 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"   Max difference:\", max(abs(dffits_custom - dffits_base)), \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Max difference: 27.64425 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"=== ALL VERIFICATIONS PASSED ===\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n=== ALL VERIFICATIONS PASSED ===\n```\n\n\n:::\n:::\n\n\nPerfect agreement! Our custom functions produce identical results to R's built-in functions.\n\n## Practical Guidelines for Model Diagnostics\n\n## A Systematic Diagnostic Workflow\n\nWhen fitting any linear model, follow this systematic workflow:\n\n### Step 1: Fit the Initial Model\n\n```r\nfit <- lm(response ~ predictors, data = mydata)\nsummary(fit)\n```\n\nCheck basic model output:\n\n- Are coefficients significant and in expected direction?\n- Is R² reasonable for your field?\n- Any warning messages?\n\n### Step 2: Create Diagnostic Plots\n\n```r\npar(mfrow = c(2, 2))\nplot(fit)\n```\n\nExamine all four plots systematically.\n\n### Step 3: Check Each Assumption\n\n| Assumption | Check | Action if Violated |\n|------------|-------|-------------------|\n| **Linearity** | Residuals vs. Fitted | Add polynomial terms, interactions, or transform predictors |\n| **Independence** | Residuals vs. order/time | Use mixed models, GLS, or cluster-robust SE |\n| **Homoscedasticity** | Scale-Location plot | Transform response (log, sqrt) or use WLS |\n| **Normality** | Q-Q plot | Transform response, check for outliers, or use robust methods |\n\n### Step 4: Identify Outliers and Influential Points\n\n```r\n# Studentized residuals\nr <- rstandard(fit)\noutliers <- which(abs(r) > 3)\n\n# High leverage\nh <- hatvalues(fit)\np <- length(coef(fit))\nn <- nobs(fit)\nhigh_lev <- which(h > 2*p/n)\n\n# High influence\nD <- cooks.distance(fit)\ninfluential <- which(D > 4/n)\n```\n\n### Step 5: Investigate Problematic Observations\n\nFor each flagged observation:\n\n1. **Check data quality**: Recording error? Measurement issue?\n2. **Biological plausibility**: Is this value realistic?\n3. **Compute diagnostics**: Leverage? Residual? Influence?\n4. **Impact assessment**: Refit without it—how much does model change?\n\n### Step 6: Make Decision\n\n- **Keep** if: Valid data, fits pattern, low influence\n- **Investigate further** if: High influence, unclear cause\n- **Exclude** if: Data error, biologically implausible, highly influential\n\n**Always document your decision and rationale!**\n\n### Step 7: Refit and Reverify\n\nAfter any changes (exclusions, transformations):\n\n- Refit model\n- Repeat diagnostic checks\n- Confirm assumptions now satisfied\n- Compare old vs. new results\n\n## Rules of Thumb\n\n::: {.callout-tip}\n## Quick Reference: Diagnostic Thresholds\n\n**Outliers (residual size)**:\n- $|r_i| > 2$: Potentially unusual\n- $|r_i| > 3$: Almost certainly an outlier\n\n**High Leverage**:\n- $h_{ii} > 2p/n$: Moderate leverage\n- $h_{ii} > 3p/n$: High leverage\n\n**Influence (Cook's D)**:\n- $D_i > 0.5$: Moderate influence (investigate)\n- $D_i > 1$: High influence (likely problematic)\n- $D_i > 4/n$: Alternative threshold for smaller datasets\n\n**DFFITS**:\n- $|\\text{DFFITS}_i| > 2\\sqrt{p/n}$: Influential\n\n**Sample Size Considerations**:\n- Small $n$ (<30): More stringent (use $|r| > 2.5$, $D > 1$)\n- Large $n$ (>100): More liberal (use $|r| > 3$, $D > 4/n$)\n:::\n\n## What to Do When Assumptions Fail\n\n### Non-Linearity\n\n**Solutions**:\n\n1. Add polynomial terms: $x^2$, $x^3$\n2. Include interactions: $x_1 \\times x_2$\n3. Transform predictors: $\\log(x)$, $\\sqrt{x}$, $1/x$\n4. Use splines or generalized additive models (GAMs)\n\n### Heteroscedasticity\n\n**Solutions**:\n\n1. **Log transformation** (most common): $\\log(y)$\n2. **Square root transformation** (for counts): $\\sqrt{y}$\n3. **Box-Cox transformation**: Optimal $\\lambda$\n4. **Weighted least squares**: If variance pattern is known\n5. **Robust standard errors**: HC3, HC4 methods\n\n### Non-Normality\n\n**Solutions**:\n\n1. **Transformation** (often fixes this and heteroscedasticity together)\n2. **Check for outliers** (may cause apparent non-normality)\n3. **Bootstrap inference** (doesn't require normality)\n4. **Robust regression** (M-estimators, LAD regression)\n5. **If $n$ large**: CLT applies, mild non-normality OK\n\n### Lack of Independence\n\n**Solutions**:\n\n1. **Mixed models**: Random effects for clusters\n2. **GLS**: Generalized least squares with known correlation structure\n3. **Cluster-robust SE**: Adjust standard errors for clustering\n4. **Time series methods**: ARIMA, GARCH for temporal dependence\n5. **Spatial models**: Account for spatial correlation\n\n## Reporting Diagnostic Results\n\nWhen reporting analyses, include:\n\n1. **Statement of assumptions checked**:\n   \"Model assumptions (linearity, homoscedasticity, normality, independence) were verified using diagnostic plots.\"\n\n2. **Any violations found**:\n   \"Initial analysis revealed heteroscedasticity (funnel pattern in residual plot).\"\n\n3. **Remedial actions taken**:\n   \"Log transformation of the response variable successfully stabilized variance.\"\n\n4. **Final model diagnostics**:\n   \"Diagnostic plots for the final model showed no systematic violations of assumptions.\"\n\n5. **Treatment of outliers** (if any):\n   \"One observation (bird 27) was identified as highly influential ($D = 0.85$) and excluded from analysis due to documented illness.\"\n\n## Summary and Key Takeaways\n\n## What We Learned This Week\n\n1. **Why diagnostics matter**: Model assumptions must be verified, not just assumed. Violations can lead to biased estimates, invalid inference, and poor predictions.\n\n2. **Types of residuals**:\n   - Raw: $e_i = y_i - \\hat{y}_i$\n   - Standardized: $e_i^* = e_i/\\hat{\\sigma}$\n   - Studentized: $r_i = e_i/(\\hat{\\sigma}\\sqrt{1-h_{ii}})$ ← **most useful**\n   - Deleted: $t_i$ (computed without observation $i$)\n\n3. **Leverage vs. Influence**:\n   - **Leverage** ($h_{ii}$): Potential for influence based on $X$ values\n   - **Influence** (Cook's $D$): Actual impact on fitted model\n   - High leverage ≠ high influence (example: beef marbling obs 9)\n\n4. **The four diagnostic plots**:\n   - Residuals vs. Fitted → linearity, homoscedasticity\n   - Normal Q-Q → normality\n   - Scale-Location → homoscedasticity\n   - Residuals vs. Leverage → influential observations\n\n5. **When to transform**:\n   - Heteroscedasticity (funnel pattern) → log or sqrt transformation\n   - Right skewness → log transformation\n   - Proportional errors → log transformation\n   - Don't transform if violations are minor or interpretation suffers\n\n6. **Systematic workflow**:\n   - Fit model → diagnostic plots → check assumptions → identify problems\n   - Investigate outliers/influential points → decide (keep/exclude/investigate)\n   - Apply fixes (transformation, WLS, etc.) → reverify diagnostics\n\n## Connections to Other Weeks\n\n- **Week 5**: Gauss-Markov assumptions—this week we verify them\n- **Week 6**: Multiple regression diagnostics more complex (leverage in $p$-dimensional space)\n- **Weeks 7-10**: Same diagnostic principles apply to ANOVA and ANCOVA\n- **Week 12** (next): Rank deficiency affects leverage calculations\n- **Week 14**: Preview of mixed models for handling dependence\n\n## Critical Reminders\n\n::: {.callout-important}\n## Key Principles for Livestock Applications\n\n1. **Biology first**: Always ask if unusual observations are biologically plausible\n2. **Document decisions**: Record why you kept/excluded observations\n3. **Report honestly**: Disclose any data exclusions or transformations\n4. **Verify after changes**: Recheck diagnostics after any model modifications\n5. **Interpretation matters**: Choose transformations that stakeholders can understand\n6. **Context is key**: Same statistical issue may have different solutions in different biological contexts\n\n**Remember**: Good diagnostics are not about achieving perfect plots—they're about understanding your data, identifying problems, and making informed decisions about modeling and inference.\n:::\n\n## Looking Ahead\n\n**Next week (Week 12)**: We'll tackle unequal subclass numbers and non-full rank models, where design matrices don't have full rank. We'll see how rank deficiency affects:\n\n- Leverage calculations ($\\sum h_{ii} = r(X)$, not $p$)\n- Estimability of functions\n- Interpretation of diagnostics\n\n**Week 14**: Special topics including weighted least squares (WLS) for known heteroscedasticity and preview of mixed models for handling dependence.\n\n## Exercises\n\nSee **Week11_Exercises.qmd** for 7 exercises covering:\n\n- Exercise 1: Hand calculation of diagnostic statistics (computational)\n- Exercise 2: Leverage vs. influence concepts (theoretical)\n- Exercise 3: Swine birth weight with two types of outliers (applied)\n- Exercise 4: Systematic assumption checking (conceptual/applied)\n- Exercise 5: Heteroscedasticity and transformation (applied)\n- Exercise 6: Box-Cox transformation selection (computational/applied)\n- Exercise 7: Complete diagnostic report for mastitis data (integrated)\n\n## References\n\n::: {#refs}\n:::\n\n",
    "supporting": [
      "Week11_ModelDiagnostics_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}