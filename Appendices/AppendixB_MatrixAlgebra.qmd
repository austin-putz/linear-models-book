---
title: "Appendix B: Matrix Algebra Review"
number-sections: true
---

# Appendix B: Matrix Algebra Review {#sec-appendix-b .unnumbered}

## Introduction {#sec-matrix-intro}

### Purpose of This Appendix

This appendix serves as a comprehensive reference for matrix algebra concepts used throughout the 15-week linear models course. It is designed to complement **Week 2: Linear Algebra Essentials** by providing:

- **Deeper coverage** of fundamental concepts
- **Complete property listings** with proofs and examples
- **Comprehensive R implementations** for every operation
- **Quick reference tables** for identities and formulas
- **Cross-references** to specific course weeks where concepts are applied

:::{.callout-note}
## Relationship to Week 2

Week 2 introduces essential matrix algebra concepts needed to get started with linear models. This appendix provides the detailed reference material you'll need as you progress through the course and encounter more advanced applications.
:::

### How to Use This Appendix

- **As a quick reference**: Look up specific properties, formulas, or R functions
- **For deeper understanding**: Read detailed explanations and work through examples
- **To verify your work**: Use the provided R code templates to check your calculations
- **To connect concepts**: Follow cross-references to see how matrix operations appear in different contexts

### Organization

The appendix is organized into 13 main sections:

1. **Introduction** (this section)
2. **Basic Matrix Operations** - Addition, multiplication, transpose
3. **Special Types of Matrices** - Identity, diagonal, symmetric, orthogonal, idempotent
4. **Matrix Properties** - Rank, trace, determinant
5. **Matrix Inverses** - Regular, generalized, identities
6. **Projection Matrices and Quadratic Forms** - Critical for least squares theory
7. **Eigenvalues and Eigenvectors** - Spectral decomposition, SVD
8. **Matrix Calculus** - Derivatives, deriving normal equations
9. **Kronecker Products** - For Animal models and multi-trait analysis
10. **Matrix Identities** - Quick reference tables
11. **Computational Considerations** - Stability, efficiency, verification
12. **Cross-Reference Table** - Concept to week mapping
13. **R Code Templates** - Ready-to-use functions

:::{.callout-tip}
## Navigation Tip

Use the table of contents sidebar to quickly jump to any section. Within sections, look for **Cross-references** boxes that link to relevant course weeks.
:::

### R Package Setup

Throughout this appendix, we'll use the following R packages:

```{r}
#| label: setup
#| message: false
#| warning: false

# Base R (always available)
# - Matrix operations: %*%, t(), solve()
# - Basic functions: matrix(), diag(), det()

# MASS package for generalized inverse
library(MASS)  # ginv()

# Matrix package for advanced operations
library(Matrix)  # rankMatrix(), sparse matrices, cond()

# Set display options
options(digits = 4)  # Display 4 decimal places
```

:::{.callout-important}
## Required Packages

Make sure to install these packages if you haven't already:

```r
install.packages("MASS")
install.packages("Matrix")
```
:::

---

## Basic Matrix Operations {#sec-basic-operations}

### Matrix Addition and Scalar Multiplication {#sec-addition}

**Definition:**

For matrices **A** and **B** of the same dimensions (m × n), matrix addition is element-wise:

$$
(\mathbf{A} + \mathbf{B})_{ij} = a_{ij} + b_{ij}
$$ {#eq-matrix-addition}

Scalar multiplication multiplies every element by a constant c:

$$
(c\mathbf{A})_{ij} = c \cdot a_{ij}
$$ {#eq-scalar-mult}

**Properties:**

- **Commutative**: $\mathbf{A} + \mathbf{B} = \mathbf{B} + \mathbf{A}$
- **Associative**: $(\mathbf{A} + \mathbf{B}) + \mathbf{C} = \mathbf{A} + (\mathbf{B} + \mathbf{C})$
- **Distributive**: $c(\mathbf{A} + \mathbf{B}) = c\mathbf{A} + c\mathbf{B}$
- **Identity element**: $\mathbf{A} + \mathbf{0} = \mathbf{A}$ where **0** is the zero matrix

**R Implementation:**

```{r}
#| label: addition-example

# Define two 2x3 matrices
A <- matrix(c(1, 2, 3, 4, 5, 6), nrow = 2, ncol = 3, byrow = TRUE)
B <- matrix(c(2, 1, 0, 1, 2, 1), nrow = 2, ncol = 3, byrow = TRUE)

cat("Matrix A:\n")
print(A)

cat("\nMatrix B:\n")
print(B)

# Addition
cat("\nA + B:\n")
print(A + B)

# Scalar multiplication
c <- 3
cat("\n3A:\n")
print(c * A)

# Verify commutative property
cat("\nVerify A + B = B + A:\n")
all.equal(A + B, B + A)
```

**Livestock Example:**

Consider average daily gain (kg/day) for two groups of beef steers on different rations:

```{r}
#| label: addition-livestock

# Pen 1 ADG (3 steers)
pen1 <- matrix(c(1.2, 1.3, 1.1), nrow = 1)

# Pen 2 ADG (3 steers)
pen2 <- matrix(c(1.0, 1.1, 0.9), nrow = 1)

cat("Pen 1 ADG:\n")
print(pen1)

cat("\nPen 2 ADG:\n")
print(pen2)

# Combined average
cat("\nAverage of both pens:\n")
print((pen1 + pen2) / 2)
```

:::{.callout-note}
## Cross-Reference

Matrix addition is used throughout the course, starting in **Week 1** for basic computations and appearing in every subsequent week.
:::

---

### Matrix Multiplication {#sec-multiplication}

**Definition:**

For matrix **A** of dimension (m × n) and matrix **B** of dimension (n × p), the product **AB** is an (m × p) matrix where:

$$
(\mathbf{AB})_{ij} = \sum_{k=1}^{n} a_{ik}b_{kj}
$$ {#eq-matrix-mult}

The element in row i, column j of **AB** is the dot product of row i of **A** with column j of **B**.

**Critical Requirement:** The number of columns in **A** must equal the number of rows in **B**.

**Dimension Rule:**
$$
\underbrace{\mathbf{A}}_{m \times n} \times \underbrace{\mathbf{B}}_{n \times p} = \underbrace{\mathbf{AB}}_{m \times p}
$$ {#eq-dimension-rule}

**Properties:**

1. **Non-commutative**: In general, $\mathbf{AB} \neq \mathbf{BA}$ (even when both are defined)
2. **Associative**: $(\mathbf{AB})\mathbf{C} = \mathbf{A}(\mathbf{BC})$
3. **Distributive**: $\mathbf{A}(\mathbf{B} + \mathbf{C}) = \mathbf{AB} + \mathbf{AC}$
4. **Scalar multiplication**: $(c\mathbf{A})\mathbf{B} = c(\mathbf{AB}) = \mathbf{A}(c\mathbf{B})$
5. **Identity**: $\mathbf{AI} = \mathbf{IA} = \mathbf{A}$ where **I** is identity matrix of appropriate dimension

**R Implementation:**

```{r}
#| label: multiplication-basic

# Example 1: Basic multiplication
A <- matrix(c(1, 2, 3, 4, 5, 6), nrow = 2, ncol = 3, byrow = TRUE)
B <- matrix(c(1, 0, 2, 1, 0, 1), nrow = 3, ncol = 2, byrow = TRUE)

cat("Matrix A (2×3):\n")
print(A)

cat("\nMatrix B (3×2):\n")
print(B)

cat("\nA %*% B (2×2):\n")
AB <- A %*% B  # %*% is matrix multiplication in R
print(AB)

# Manual calculation of element (1,1)
cat("\nManual calculation of (AB)[1,1]:\n")
cat("= A[1,1]*B[1,1] + A[1,2]*B[2,1] + A[1,3]*B[3,1]\n")
cat(sprintf("= %d*%d + %d*%d + %d*%d = %d\n",
            A[1,1], B[1,1], A[1,2], B[2,1], A[1,3], B[3,1],
            A[1,1]*B[1,1] + A[1,2]*B[2,1] + A[1,3]*B[3,1]))
```

```{r}
#| label: multiplication-noncommutative

# Verify non-commutativity
BA <- B %*% A  # Different result!

cat("B %*% A (3×3):\n")
print(BA)

cat("\nNote: A %*% B is 2×2, but B %*% A is 3×3")
cat("\nMatrix multiplication is NOT commutative!\n")
```

**Livestock Example - Normal Equations:**

The most important matrix multiplication in linear models is forming the normal equations **X'Xb = X'y**.

```{r}
#| label: multiplication-normal-eqs

# Simple regression: milk yield (y) vs days in milk (x)
# n = 4 dairy cows
days <- c(30, 60, 90, 120)
yield <- c(35, 32, 28, 25)  # kg/day

# Design matrix (n×2): intercept and days
X <- cbind(1, days)
y <- matrix(yield, ncol = 1)

cat("Design matrix X (4×2):\n")
print(X)

cat("\nResponse vector y (4×1):\n")
print(y)

# Form X'X (the "information matrix")
XtX <- t(X) %*% X
cat("\nX'X (2×2):\n")
print(XtX)

# Form X'y (the "right-hand side")
Xty <- t(X) %*% y
cat("\nX'y (2×1):\n")
print(Xty)

# Solve for regression coefficients
b <- solve(XtX) %*% Xty
cat("\nRegression coefficients b = (X'X)^(-1)X'y:\n")
print(b)
cat(sprintf("\nInterpretation: Intercept = %.2f kg/day, Slope = %.4f kg/day per day\n",
            b[1], b[2]))
```

**Important Special Cases:**

1. **Quadratic forms**: $\mathbf{x}'\mathbf{Ax}$ (scalar result)
   - Example: Sum of squares = $\mathbf{e}'\mathbf{e}$ where **e** is residual vector

2. **Outer product**: For vectors **u** (n×1) and **v** (m×1), $\mathbf{uv}'$ is n×m matrix

3. **Inner product**: For vectors **u** and **v** (both n×1), $\mathbf{u}'\mathbf{v}$ is scalar

```{r}
#| label: multiplication-special-cases

# Quadratic form: sum of squares
e <- c(-1.2, 0.5, 0.8, -0.1)  # Residuals
SSE <- t(e) %*% e  # Same as sum(e^2)
cat("Sum of squared errors (e'e):\n")
print(SSE)
cat(sprintf("Verification: sum(e^2) = %.4f\n", sum(e^2)))

# Outer product
u <- matrix(c(1, 2, 3), ncol = 1)
v <- matrix(c(4, 5), ncol = 1)
outer_prod <- u %*% t(v)
cat("\nOuter product u v' (3×2):\n")
print(outer_prod)

# Inner product (dot product)
w <- c(1, 2, 3)
z <- c(4, 5, 6)
inner_prod <- t(w) %*% z  # Or: sum(w * z)
cat("\nInner product w'z (scalar):\n")
print(inner_prod)
```

**Livestock Example - Multi-Trait Analysis:**

```{r}
#| label: multiplication-multitrait

# Swine: 3 pigs, 2 traits (backfat mm, loin depth mm)
# Raw data matrix
Y <- matrix(c(12, 15, 10,   # Backfat for pigs 1, 2, 3
              65, 70, 68),  # Loin depth for pigs 1, 2, 3
            nrow = 3, ncol = 2)
colnames(Y) <- c("Backfat", "LoinDepth")

cat("Multi-trait data Y (3 pigs × 2 traits):\n")
print(Y)

# Compute trait covariance matrix: (Y'Y) / (n-1)
# This involves matrix multiplication
YtY <- t(Y) %*% Y
cat("\nY'Y (sums of squares and cross-products):\n")
print(YtY)

cov_matrix <- YtY / (nrow(Y) - 1)
cat("\nCovariance matrix (2×2):\n")
print(cov_matrix)
cat(sprintf("\nInterpretation:\n"))
cat(sprintf("  Var(Backfat) = %.2f\n", cov_matrix[1,1]))
cat(sprintf("  Var(LoinDepth) = %.2f\n", cov_matrix[2,2]))
cat(sprintf("  Cov(Backfat, LoinDepth) = %.2f\n", cov_matrix[1,2]))
```

:::{.callout-warning}
## Common Mistakes with Matrix Multiplication

1. **Wrong operator**: Use `%*%` for matrix multiplication, not `*` (which is element-wise)
2. **Dimension mismatch**: Check that ncol(A) == nrow(B) before computing A %*% B
3. **Order matters**: AB ≠ BA in general
4. **Vectors**: R treats vectors flexibly; be explicit with `matrix()` or transpose `t()` to avoid confusion
:::

:::{.callout-note}
## Cross-References

Matrix multiplication appears constantly in linear models:

- **Week 3**: Building design matrix **X** and forming **X'X**
- **Week 4-6**: Normal equations **X'Xb = X'y**
- **Week 5**: Hat matrix **H = X(X'X)^(-1)X'** involves three matrix multiplications
- **Week 11**: Computing fitted values **ŷ = Xb** and residuals **e = y - Xb**
:::

---

### Matrix Transpose {#sec-transpose}

**Definition:**

The **transpose** of a matrix **A** (denoted **A'** or **A**^T^) is obtained by interchanging rows and columns:

$$
(\mathbf{A}')_{ij} = (\mathbf{A})_{ji}
$$ {#eq-transpose-def}

If **A** is m × n, then **A'** is n × m.

**Properties:**

1. **Double transpose**: $(\mathbf{A}')' = \mathbf{A}$
2. **Sum transpose**: $(\mathbf{A} + \mathbf{B})' = \mathbf{A}' + \mathbf{B}'$
3. **Product transpose**: $(\mathbf{AB})' = \mathbf{B}'\mathbf{A}'$ (reverse order!)
4. **Scalar multiply**: $(c\mathbf{A})' = c\mathbf{A}'$
5. **Inverse transpose**: $(\mathbf{A}^{-1})' = (\mathbf{A}')^{-1}$ (for invertible matrices)

**Critical Property for Linear Models:**

The transpose of a product **reverses the order**: $(\mathbf{ABC})' = \mathbf{C}'\mathbf{B}'\mathbf{A}'$

This is essential when manipulating expressions like $(\mathbf{X}'\mathbf{X})^{-1}$ or $(\mathbf{y} - \mathbf{Xb})'(\mathbf{y} - \mathbf{Xb})$.

**R Implementation:**

```{r}
#| label: transpose-basic

# Create a 3x2 matrix
A <- matrix(c(1, 2, 3, 4, 5, 6), nrow = 3, ncol = 2, byrow = TRUE)

cat("Matrix A (3×2):\n")
print(A)

cat("\nTranspose A' (2×3):\n")
At <- t(A)
print(At)

# Verify double transpose
cat("\nVerify (A')' = A:\n")
all.equal(t(At), A)

# Product transpose rule
B <- matrix(c(1, 0, 2, 1), nrow = 2, ncol = 2, byrow = TRUE)
cat("\nMatrix B (2×2):\n")
print(B)

AB <- A %*% B
cat("\nA %*% B (3×2):\n")
print(AB)

cat("\n(AB)':\n")
print(t(AB))

cat("\nB' %*% A' (should equal (AB)'):\n")
print(t(B) %*% t(A))

cat("\nVerify (AB)' = B'A':\n")
all.equal(t(AB), t(B) %*% t(A))
```

**Livestock Example - Normal Equations:**

The normal equations **X'Xb = X'y** rely fundamentally on the transpose operation.

```{r}
#| label: transpose-normal-equations

# Beef cattle: ADG (kg/day) for 5 steers
adg <- c(1.2, 1.4, 1.1, 1.3, 1.5)
y <- matrix(adg, ncol = 1)  # Column vector (5×1)

cat("Response vector y (5×1):\n")
print(y)

cat("\nTranspose y' (1×5):\n")
print(t(y))

# Simple model: y = μ + e (estimate the mean)
X <- matrix(1, nrow = 5, ncol = 1)  # Column of ones

cat("\nDesign matrix X (5×1):\n")
print(X)

# Form X'X
XtX <- t(X) %*% X
cat("\nX'X (1×1 scalar):\n")
print(XtX)
cat(sprintf("Interpretation: X'X = %d (the sample size)\n", XtX[1,1]))

# Form X'y
Xty <- t(X) %*% y
cat("\nX'y (1×1 scalar):\n")
print(Xty)
cat(sprintf("Interpretation: X'y = %.1f (the sum of observations)\n", Xty[1,1]))

# Solve: b = (X'X)^(-1) X'y = sum(y) / n
b <- solve(XtX) %*% Xty
cat(sprintf("\nEstimated mean: %.3f kg/day\n", b[1,1]))
cat(sprintf("Verification: mean(y) = %.3f\n", mean(adg)))
```

**Symmetric Matrices:**

A matrix is **symmetric** if **A = A'**. This property is crucial in linear models.

```{r}
#| label: transpose-symmetric

# X'X is always symmetric
X <- matrix(c(1, 1, 1, 1,
              2, 3, 1, 4), nrow = 4, ncol = 2)

cat("Design matrix X (4×2):\n")
print(X)

XtX <- t(X) %*% X
cat("\nX'X (2×2):\n")
print(XtX)

cat("\n(X'X)' (transpose of X'X):\n")
print(t(XtX))

cat("\nVerify X'X is symmetric:\n")
all.equal(XtX, t(XtX))

# Why is X'X always symmetric? Proof:
# (X'X)' = X'(X')' = X'X ✓
```

**Livestock Example - Sum of Squares:**

The sum of squared errors **SSE = e'e** uses transpose to convert a column vector to a scalar.

```{r}
#| label: transpose-sse

# Residuals from a regression
e <- matrix(c(-0.5, 0.8, -0.3, 0.2, -0.2), ncol = 1)

cat("Residual vector e (5×1):\n")
print(e)

cat("\nTranspose e' (1×5):\n")
print(t(e))

# Sum of squared errors
SSE <- t(e) %*% e  # (1×5) %*% (5×1) = (1×1) scalar
cat("\nSSE = e'e:\n")
print(SSE)

cat(sprintf("\nInterpretation: SSE = %.4f\n", SSE[1,1]))
cat(sprintf("Verification: sum(e^2) = %.4f\n", sum(e^2)))

# Equivalent computations
cat("\nThree ways to compute SSE:\n")
cat(sprintf("1. t(e) %%*%% e = %.4f\n", (t(e) %*% e)[1,1]))
cat(sprintf("2. sum(e^2) = %.4f\n", sum(e^2)))
cat(sprintf("3. crossprod(e) = %.4f\n", crossprod(e)[1,1]))  # More efficient
```

**Useful R Functions:**

- `t(A)`: Transpose of matrix A
- `crossprod(X, y)`: Computes **X'y** efficiently (more efficient than `t(X) %*% y`)
- `crossprod(X)`: Computes **X'X** efficiently (more efficient than `t(X) %*% X`)
- `tcrossprod(X, Y)`: Computes **XY'** efficiently

```{r}
#| label: transpose-efficient

# Efficiency comparison for large matrices
X <- matrix(rnorm(1000), nrow = 100, ncol = 10)

# Standard way
system.time({
  XtX_standard <- t(X) %*% X
})

# Efficient way
system.time({
  XtX_efficient <- crossprod(X)
})

cat("Results are identical:\n")
all.equal(XtX_standard, XtX_efficient)

cat("\nFor normal equations, always use:\n")
cat("  XtX <- crossprod(X)     # Instead of t(X) %*% X\n")
cat("  Xty <- crossprod(X, y)  # Instead of t(X) %*% y\n")
```

:::{.callout-important}
## Critical for Linear Models

The transpose appears in virtually every linear model calculation:

1. **Normal equations**: **X'Xb = X'y**
2. **Sum of squares**: **SSE = (y - Xb)'(y - Xb) = e'e**
3. **Variance of estimates**: **Var(b) = (X'X)^(-1)σ²**
4. **Hat matrix**: **H = X(X'X)^(-1)X'**

Always remember: $(\mathbf{AB})' = \mathbf{B}'\mathbf{A}'$ (reverse the order!)
:::

:::{.callout-note}
## Cross-References

The transpose operation is used in:

- **Week 1-2**: Basic matrix operations
- **Week 3**: Forming **X'X** from design matrix
- **Week 4-6**: All regression calculations
- **Week 5**: Deriving least squares via $\partial(\mathbf{e}'\mathbf{e})/\partial\mathbf{β}$
- **Week 8**: Testing contrasts $\mathbf{c}'\mathbf{b}$
- **Week 11**: Computing residuals and diagnostics
:::

---

## Special Types of Matrices {#sec-special-matrices}

### Identity Matrix {#sec-identity}

**Definition:**

The **identity matrix** **I**~n~ is an n × n square matrix with 1's on the main diagonal and 0's elsewhere:

$$
\mathbf{I}_n = \begin{bmatrix}
1 & 0 & 0 & \cdots & 0 \\
0 & 1 & 0 & \cdots & 0 \\
0 & 0 & 1 & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & 1
\end{bmatrix}
$$ {#eq-identity-matrix}

Equivalently: $(\mathbf{I})_{ij} = \begin{cases} 1 & \text{if } i = j \\ 0 & \text{if } i \neq j \end{cases}$

**Key Properties:**

1. **Multiplicative identity**: $\mathbf{AI} = \mathbf{IA} = \mathbf{A}$ for any matrix **A** of compatible dimension
2. **Inverse of itself**: $\mathbf{I}^{-1} = \mathbf{I}$
3. **Idempotent**: $\mathbf{I}^2 = \mathbf{I}$
4. **Symmetric**: $\mathbf{I}' = \mathbf{I}$
5. **Trace**: $\text{tr}(\mathbf{I}_n) = n$
6. **Determinant**: $\det(\mathbf{I}_n) = 1$
7. **Rank**: $r(\mathbf{I}_n) = n$ (full rank)

**R Implementation:**

```{r}
#| label: identity-basic

# Create identity matrices of various sizes
I2 <- diag(2)
I3 <- diag(3)
I5 <- diag(5)

cat("I_2 (2×2 identity):\n")
print(I2)

cat("\nI_3 (3×3 identity):\n")
print(I3)

# Verify multiplicative identity property
A <- matrix(c(1, 2, 3, 4, 5, 6), nrow = 2, ncol = 3)
cat("\nMatrix A (2×3):\n")
print(A)

cat("\nI_2 %*% A (should equal A):\n")
print(I2 %*% A)

cat("\nA %*% I_3 (should equal A):\n")
print(A %*% I3)

cat("\nVerify IA = A:\n")
all.equal(I2 %*% A, A)

cat("\nVerify AI = A:\n")
all.equal(A %*% I3, A)
```

**Role in Linear Models:**

The identity matrix appears frequently in variance-covariance structures and residual matrices.

**1. Homoscedastic, Independent Errors:**

The assumption $\text{Var}(\mathbf{e}) = \sigma^2\mathbf{I}_n$ means:
- All errors have the same variance $\sigma^2$ (homoscedasticity)
- Errors are uncorrelated (independence)

```{r}
#| label: identity-variance

# Example: n = 4 observations
n <- 4
sigma_sq <- 2.5

# Variance-covariance matrix of errors
Var_e <- sigma_sq * diag(n)

cat("Var(e) = σ²I where σ² = 2.5:\n")
print(Var_e)

cat("\nInterpretation:\n")
cat("  - Diagonal elements = 2.5 (common variance)\n")
cat("  - Off-diagonal elements = 0 (no correlation)\n")
```

**2. Residual Projection:**

The matrix $\mathbf{I} - \mathbf{H}$ projects observations onto the residual space (see @sec-projection-matrices).

```{r}
#| label: identity-projection

# Simple example with n=3
X <- cbind(1, c(1, 2, 3))  # Intercept and slope
n <- nrow(X)

# Hat matrix
H <- X %*% solve(t(X) %*% X) %*% t(X)

cat("Hat matrix H (3×3):\n")
print(H)

# Residual projection matrix
I_minus_H <- diag(n) - H

cat("\nI - H (3×3):\n")
print(I_minus_H)

# Verify idempotence
cat("\nVerify (I-H)² = I-H:\n")
all.equal((I_minus_H) %*% (I_minus_H), I_minus_H)
```

**Livestock Example - Weighted Least Squares:**

When observations have different precisions, we use $\mathbf{W} = \text{diag}(w_1, ..., w_n)$ instead of $\mathbf{I}$.

```{r}
#| label: identity-weighted

# Pen-average ADG with different pen sizes
pen_avg <- c(1.2, 1.3, 1.1, 1.4)  # kg/day
pen_size <- c(10, 15, 8, 12)      # Number of pigs per pen

# Weights proportional to pen size (more pigs = more precise)
weights <- pen_size / mean(pen_size)  # Standardized weights

cat("Pen averages and sizes:\n")
print(data.frame(ADG = pen_avg, PenSize = pen_size, Weight = weights))

# Weight matrix (diagonal)
W <- diag(weights)
cat("\nWeight matrix W (4×4):\n")
print(W)

# Weighted mean: (1'W1)^(-1) 1'Wy
ones <- matrix(1, nrow = 4, ncol = 1)
y <- matrix(pen_avg, ncol = 1)

weighted_mean <- solve(t(ones) %*% W %*% ones) %*% t(ones) %*% W %*% y
cat(sprintf("\nWeighted mean ADG: %.3f kg/day\n", weighted_mean[1,1]))

# Compare with unweighted mean
cat(sprintf("Unweighted mean ADG: %.3f kg/day\n", mean(pen_avg)))
```

:::{.callout-note}
## Creating Identity Matrices in R

Three equivalent ways:
```r
I <- diag(n)           # Most common
I <- diag(1, n)        # Explicit
I <- diag(rep(1, n))   # Using rep()
```
:::

:::{.callout-note}
## Cross-References

Identity matrix appears in:

- **Week 5**: Variance assumption $\text{Var}(\mathbf{e}) = \sigma^2\mathbf{I}$
- **Week 6**: Residual projection matrix $\mathbf{I} - \mathbf{H}$
- **Week 10**: Weighted least squares with $\mathbf{W}$ replacing $\mathbf{I}$
- **Week 14**: Generalizations to non-identity covariance structures
:::

---

### Diagonal Matrices {#sec-diagonal}

**Definition:**

A **diagonal matrix** **D** is a square matrix with non-zero elements only on the main diagonal:

$$
\mathbf{D} = \begin{bmatrix}
d_1 & 0 & 0 & \cdots & 0 \\
0 & d_2 & 0 & \cdots & 0 \\
0 & 0 & d_3 & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & d_n
\end{bmatrix}
$$ {#eq-diagonal-matrix}

Notation: $\mathbf{D} = \text{diag}(d_1, d_2, ..., d_n)$

Equivalently: $(\mathbf{D})_{ij} = \begin{cases} d_i & \text{if } i = j \\ 0 & \text{if } i \neq j \end{cases}$

**Key Properties:**

1. **Multiplication is commutative**: $\mathbf{D}_1\mathbf{D}_2 = \mathbf{D}_2\mathbf{D}_1$ (rare for matrices!)
2. **Simple inverse**: $\mathbf{D}^{-1} = \text{diag}(1/d_1, 1/d_2, ..., 1/d_n)$ if all $d_i \neq 0$
3. **Determinant**: $\det(\mathbf{D}) = \prod_{i=1}^{n} d_i$ (product of diagonal elements)
4. **Trace**: $\text{tr}(\mathbf{D}) = \sum_{i=1}^{n} d_i$ (sum of diagonal elements)
5. **Rank**: $r(\mathbf{D})$ = number of non-zero $d_i$
6. **Powers**: $\mathbf{D}^k = \text{diag}(d_1^k, d_2^k, ..., d_n^k)$
7. **Symmetric**: $\mathbf{D}' = \mathbf{D}$

**R Implementation:**

```{r}
#| label: diagonal-basic

# Create diagonal matrices
D1 <- diag(c(2, 3, 5))
D2 <- diag(c(1, 4, 2))

cat("D1:\n")
print(D1)

cat("\nD2:\n")
print(D2)

# Multiplication is commutative
cat("\nD1 %*% D2:\n")
print(D1 %*% D2)

cat("\nD2 %*% D1:\n")
print(D2 %*% D1)

cat("\nVerify D1*D2 = D2*D1:\n")
all.equal(D1 %*% D2, D2 %*% D1)

# Inverse
D1_inv <- solve(D1)
cat("\nD1^(-1):\n")
print(D1_inv)

cat("\nManual inverse: diag(1/2, 1/3, 1/5):\n")
print(diag(1/c(2, 3, 5)))

# Verify inverse
cat("\nVerify D1 * D1^(-1) = I:\n")
print(D1 %*% D1_inv)
```

**Scaling Rows and Columns:**

Diagonal matrices are useful for scaling. Pre-multiplying scales rows; post-multiplying scales columns.

```{r}
#| label: diagonal-scaling

# Example matrix
A <- matrix(c(1, 2, 3, 4, 5, 6, 7, 8, 9), nrow = 3, ncol = 3)
cat("Matrix A (3×3):\n")
print(A)

# Scale rows by diag(2, 3, 4)
D_row <- diag(c(2, 3, 4))
DA <- D_row %*% A

cat("\nD %*% A (scales rows):\n")
print(DA)
cat("Row 1 scaled by 2, Row 2 by 3, Row 3 by 4\n")

# Scale columns by diag(10, 20, 30)
D_col <- diag(c(10, 20, 30))
AD <- A %*% D_col

cat("\nA %*% D (scales columns):\n")
print(AD)
cat("Column 1 scaled by 10, Column 2 by 20, Column 3 by 30\n")
```

**Livestock Example - Weighted Least Squares:**

In weighted regression, observations with different variances get different weights.

```{r}
#| label: diagonal-wls

# Swine ADG data with heterogeneous variances
# 4 treatment groups with different precisions
adg <- c(0.85, 0.90, 0.88, 0.92)
n_pigs <- c(20, 30, 15, 25)  # Group sizes

# Variance inversely proportional to group size
# Var(mean) = σ²/n, so weight = n/σ²
weights <- n_pigs / mean(n_pigs)  # Standardized

cat("Treatment data:\n")
print(data.frame(Treatment = 1:4, ADG = adg, n = n_pigs, Weight = round(weights, 2)))

# Weight matrix
W <- diag(weights)
cat("\nWeight matrix W (4×4):\n")
print(round(W, 2))

# Weighted least squares: (X'WX)^(-1) X'Wy
X <- matrix(1, nrow = 4, ncol = 1)  # Just intercept (estimate overall mean)
y <- matrix(adg, ncol = 1)

# Normal equations with weights
XtWX <- t(X) %*% W %*% X
XtWy <- t(X) %*% W %*% y

b_weighted <- solve(XtWX) %*% XtWy
b_unweighted <- mean(adg)

cat(sprintf("\nWeighted mean: %.4f kg/day\n", b_weighted[1,1]))
cat(sprintf("Unweighted mean: %.4f kg/day\n", b_unweighted))
cat("\nWeighted mean gives more influence to groups with larger n\n")
```

**Livestock Example - Variance Components:**

Diagonal matrices represent variances when effects are independent.

```{r}
#| label: diagonal-variance

# Genetic evaluation: 3 sires, assume independent
# Sire breeding values have different reliabilities
reliability <- c(0.90, 0.75, 0.85)
genetic_variance <- 10  # Additive genetic variance

# Variance of estimated breeding value = (1-r²) * σ²_a
var_ebv <- (1 - reliability^2) * genetic_variance

cat("Sire reliabilities and EBV variances:\n")
print(data.frame(Sire = 1:3, Reliability = reliability,
                 Var_EBV = round(var_ebv, 2)))

# Variance-covariance matrix (diagonal because sires are independent)
V <- diag(var_ebv)
cat("\nVariance-covariance matrix V (3×3):\n")
print(round(V, 2))

cat("\nInterpretation:\n")
cat("  - Diagonal: variances of each sire's EBV\n")
cat("  - Off-diagonal zeros: sires are unrelated (independent)\n")
```

**Extracting and Creating Diagonals:**

```{r}
#| label: diagonal-extract

# Extract diagonal from matrix
A <- matrix(c(1, 2, 3, 4, 5, 6, 7, 8, 9), nrow = 3)
cat("Matrix A:\n")
print(A)

diag_A <- diag(A)
cat("\nDiagonal of A:\n")
print(diag_A)

# Create diagonal matrix from vector
v <- c(10, 20, 30)
D <- diag(v)
cat("\nDiagonal matrix from vector:\n")
print(D)

# Replace diagonal
A_new <- A
diag(A_new) <- c(99, 99, 99)
cat("\nA with diagonal replaced:\n")
print(A_new)
```

**Special Case - Singular Diagonal Matrix:**

If any $d_i = 0$, the matrix is singular (not invertible).

```{r}
#| label: diagonal-singular

# Diagonal matrix with a zero
D_singular <- diag(c(2, 0, 5))
cat("Singular diagonal matrix:\n")
print(D_singular)

cat(sprintf("\nDeterminant: %.1f\n", det(D_singular)))
cat(sprintf("Rank: %d (less than 3)\n", qr(D_singular)$rank))

cat("\nThis matrix is not invertible because det = 0\n")

# But we can compute generalized inverse
library(MASS)
D_ginv <- ginv(D_singular)
cat("\nGeneralized inverse:\n")
print(D_ginv)
```

:::{.callout-tip}
## Computational Advantages

Diagonal matrices are computationally efficient:

1. **Storage**: Store only n values instead of n²
2. **Multiplication**: O(n) instead of O(n³)
3. **Inversion**: O(n) instead of O(n³)
4. **Determinant**: Simple product instead of complex calculation

For large-scale problems, use sparse matrix representations.
:::

:::{.callout-note}
## Cross-References

Diagonal matrices appear in:

- **Week 5**: Variance matrix $\mathbf{Var}(\mathbf{e}) = \sigma^2\mathbf{I}$ (special case)
- **Week 10**: Weighted least squares with weight matrix $\mathbf{W}$
- **Week 11**: Leverage values (diagonal of hat matrix)
- **Week 14**: Variance components in genetic evaluation
:::

---

### Symmetric Matrices {#sec-symmetric}

**Definition:**

A matrix **A** is **symmetric** if $\mathbf{A} = \mathbf{A}'$ (equals its own transpose).

This requires:
1. **A** must be square (n × n)
2. $a_{ij} = a_{ji}$ for all i, j

$$
\mathbf{A} = \begin{bmatrix}
a_{11} & a_{12} & a_{13} \\
a_{12} & a_{22} & a_{23} \\
a_{13} & a_{23} & a_{33}
\end{bmatrix}
$$ {#eq-symmetric-matrix}

**Key Properties:**

1. **Transpose**: $\mathbf{A}' = \mathbf{A}$ (by definition)
2. **Sum**: If **A** and **B** are symmetric, then $\mathbf{A} + \mathbf{B}$ is symmetric
3. **Scalar multiple**: If **A** is symmetric, then $c\mathbf{A}$ is symmetric
4. **Product with transpose**: $\mathbf{X}'\mathbf{X}$ and $\mathbf{XX}'$ are always symmetric for any **X**
5. **Inverse**: If **A** is symmetric and invertible, then $\mathbf{A}^{-1}$ is symmetric
6. **Eigenvalues**: All eigenvalues are real
7. **Eigenvectors**: Eigenvectors corresponding to distinct eigenvalues are orthogonal

**Why Symmetric Matrices Are Crucial for Linear Models:**

The matrix $\mathbf{X}'\mathbf{X}$ is **always** symmetric, and this matrix appears in every linear model calculation.

**Proof that X'X is symmetric:**
$$
(\mathbf{X}'\mathbf{X})' = \mathbf{X}'(\mathbf{X}')' = \mathbf{X}'\mathbf{X} \quad \checkmark
$$

**R Implementation:**

```{r}
#| label: symmetric-basic

# Create a symmetric matrix
A <- matrix(c(4, 2, 1,
              2, 5, 3,
              1, 3, 6), nrow = 3, ncol = 3, byrow = TRUE)

cat("Symmetric matrix A:\n")
print(A)

cat("\nTranspose A':\n")
print(t(A))

cat("\nVerify A = A':\n")
all.equal(A, t(A))

# Check if a matrix is symmetric
is_symmetric <- function(M) {
  isTRUE(all.equal(M, t(M)))
}

cat("\nIs A symmetric?", is_symmetric(A), "\n")

# Non-symmetric matrix
B <- matrix(c(1, 2, 3, 4, 5, 6, 7, 8, 9), nrow = 3, ncol = 3, byrow = TRUE)
cat("\nNon-symmetric matrix B:\n")
print(B)
cat("Is B symmetric?", is_symmetric(B), "\n")
```

**Creating Symmetric Matrices - X'X:**

```{r}
#| label: symmetric-XtX

# Design matrix for simple regression
# n = 5 observations
X <- cbind(1, c(10, 20, 30, 40, 50))

cat("Design matrix X (5×2):\n")
print(X)

# Form X'X
XtX <- t(X) %*% X

cat("\nX'X (2×2):\n")
print(XtX)

cat("\nVerify X'X is symmetric:\n")
all.equal(XtX, t(XtX))

cat("\nStructure of X'X:\n")
cat("  [1,1] = sum of 1² = n = 5\n")
cat("  [1,2] = [2,1] = sum of x = 150\n")
cat("  [2,2] = sum of x² = 5500\n")
```

**Livestock Example - Variance-Covariance Matrix:**

Sample covariance matrices are always symmetric.

```{r}
#| label: symmetric-covariance

# Broiler data: 6 birds, 3 traits (body weight, breast yield, leg yield)
traits <- matrix(c(
  2.5, 0.85, 0.45,  # Bird 1
  2.8, 0.90, 0.48,  # Bird 2
  2.3, 0.80, 0.42,  # Bird 3
  2.7, 0.88, 0.47,  # Bird 4
  2.6, 0.87, 0.46,  # Bird 5
  2.9, 0.92, 0.50   # Bird 6
), nrow = 6, ncol = 3, byrow = TRUE)
colnames(traits) <- c("BodyWt_kg", "BreastYield", "LegYield")

cat("Broiler trait data (6 birds × 3 traits):\n")
print(traits)

# Compute covariance matrix
cov_matrix <- cov(traits)

cat("\nCovariance matrix (3×3):\n")
print(round(cov_matrix, 4))

cat("\nVerify symmetry:\n")
all.equal(cov_matrix, t(cov_matrix))

cat("\nInterpretation:\n")
cat(sprintf("  Var(BodyWt) = %.4f\n", cov_matrix[1,1]))
cat(sprintf("  Cov(BodyWt, BreastYield) = %.4f\n", cov_matrix[1,2]))
cat(sprintf("  Note: cov[1,2] = cov[2,1] = %.4f (symmetric!)\n", cov_matrix[1,2]))
```

**Symmetric Matrices in ANOVA:**

For a one-way ANOVA with balanced data, $\mathbf{X}'\mathbf{X}$ has special structure.

```{r}
#| label: symmetric-anova

# One-way ANOVA: 3 breeds, 2 observations per breed
breed <- factor(rep(1:3, each = 2))
X <- model.matrix(~ breed - 1)  # Cell means model

cat("Design matrix X (6×3):\n")
print(X)

XtX <- t(X) %*% X

cat("\nX'X (3×3):\n")
print(XtX)

cat("\nVerify X'X is symmetric:\n")
all.equal(XtX, t(XtX))

cat("\nFor balanced design, X'X is diagonal:\n")
cat("Each breed has n=2 observations, so diagonal = 2\n")
```

**Inverse of Symmetric Matrix:**

If **A** is symmetric, then $\mathbf{A}^{-1}$ is also symmetric.

```{r}
#| label: symmetric-inverse

# Symmetric matrix
A <- matrix(c(4, 1, 1, 3), nrow = 2, ncol = 2, byrow = TRUE)

cat("Symmetric matrix A:\n")
print(A)

# Inverse
A_inv <- solve(A)

cat("\nInverse A^(-1):\n")
print(A_inv)

cat("\nVerify A^(-1) is symmetric:\n")
all.equal(A_inv, t(A_inv))

cat("\nVerify A * A^(-1) = I:\n")
print(round(A %*% A_inv, 10))
```

**Livestock Example - Normal Equations:**

The entire normal equation system is symmetric.

```{r}
#| label: symmetric-normal-equations

# Dairy cow milk yield (kg/day) vs days in milk
days <- c(30, 60, 90, 120, 150)
yield <- c(35, 32, 28, 25, 22)

# Design matrix (n×2): intercept and days
X <- cbind(1, days)
y <- matrix(yield, ncol = 1)

cat("Design matrix X (5×2):\n")
print(X)

# Normal equations: X'Xb = X'y
XtX <- t(X) %*% X
Xty <- t(X) %*% y

cat("\nX'X (symmetric coefficient matrix):\n")
print(XtX)

cat("\nX'y (right-hand side):\n")
print(Xty)

# Solve
b <- solve(XtX) %*% Xty

cat("\nSolution b:\n")
print(b)

cat(sprintf("\nRegression equation: y = %.2f + %.4f * days\n", b[1], b[2]))

# Variance of estimates: (X'X)^(-1) σ²
XtX_inv <- solve(XtX)

cat("\n(X'X)^(-1) (also symmetric):\n")
print(round(XtX_inv, 6))

cat("\nVerify (X'X)^(-1) is symmetric:\n")
all.equal(XtX_inv, t(XtX_inv))
```

**Checking Symmetry Numerically:**

Due to floating-point arithmetic, sometimes need to check "near symmetry."

```{r}
#| label: symmetric-numerical

# Create matrix with small numerical errors
A_perfect <- matrix(c(1, 0.5, 0.5, 2), nrow = 2, ncol = 2)
A_noisy <- A_perfect
A_noisy[1,2] <- A_noisy[1,2] + 1e-15  # Tiny numerical error

cat("Matrix with numerical noise:\n")
print(A_noisy, digits = 20)

cat("\nExact equality fails:\n")
print(identical(A_noisy, t(A_noisy)))

cat("\nBut all.equal() handles tolerance:\n")
print(all.equal(A_noisy, t(A_noisy)))

# Force exact symmetry
make_symmetric <- function(M) {
  (M + t(M)) / 2
}

A_fixed <- make_symmetric(A_noisy)
cat("\nForced symmetric matrix:\n")
print(A_fixed, digits = 20)

cat("\nNow exactly symmetric:\n")
print(identical(A_fixed, t(A_fixed)))
```

:::{.callout-important}
## X'X is Always Symmetric

In every linear model calculation, the matrix $\mathbf{X}'\mathbf{X}$ is symmetric. This property:

1. Reduces computational cost (only need to compute upper or lower triangle)
2. Guarantees real eigenvalues
3. Enables specialized algorithms (Cholesky decomposition)
4. Ensures variance-covariance matrix $\text{Var}(\mathbf{b}) = (\mathbf{X}'\mathbf{X})^{-1}\sigma^2$ is symmetric

Always verify symmetry when implementing linear model solvers!
:::

:::{.callout-note}
## Cross-References

Symmetric matrices are central to:

- **Week 2**: Properties of $\mathbf{X}'\mathbf{X}$
- **Week 5**: Variance-covariance matrices
- **Week 6**: Correlation matrices
- **Week 8**: Testing contrasts with $\mathbf{c}'(\mathbf{X}'\mathbf{X})^{-1}\mathbf{c}$
- **Week 14**: Genetic covariance matrices **G** and **A**
:::

---

### Orthogonal Matrices {#sec-orthogonal}

**Definition:**

A square matrix **Q** is **orthogonal** if its columns are orthonormal (orthogonal unit vectors).

Equivalently: $\mathbf{Q}'\mathbf{Q} = \mathbf{QQ}' = \mathbf{I}$

This means: $\mathbf{Q}' = \mathbf{Q}^{-1}$ (transpose equals inverse!)

**Orthonormal Columns:**

The columns $\mathbf{q}_1, \mathbf{q}_2, ..., \mathbf{q}_n$ satisfy:
$$
\mathbf{q}_i'\mathbf{q}_j = \begin{cases}
1 & \text{if } i = j \text{ (unit length)} \\
0 & \text{if } i \neq j \text{ (orthogonal)}
\end{cases}
$$ {#eq-orthonormal}

**Key Properties:**

1. **Inverse is transpose**: $\mathbf{Q}^{-1} = \mathbf{Q}'$ (very fast to compute!)
2. **Preserves lengths**: $||\mathbf{Qx}|| = ||\mathbf{x}||$ for any vector **x**
3. **Preserves angles**: Inner products preserved
4. **Determinant**: $\det(\mathbf{Q}) = \pm 1$
5. **Product**: If **Q**₁ and **Q**₂ are orthogonal, so is **Q**₁**Q**₂
6. **Eigenvalues**: All eigenvalues have magnitude 1

**R Implementation:**

```{r}
#| label: orthogonal-basic

# Simple 2×2 rotation matrix (rotation by 45°)
theta <- pi/4  # 45 degrees
Q <- matrix(c(cos(theta), sin(theta),
              -sin(theta), cos(theta)), nrow = 2, ncol = 2, byrow = TRUE)

cat("Orthogonal matrix Q (2×2 rotation):\n")
print(round(Q, 4))

# Verify Q'Q = I
QtQ <- t(Q) %*% Q
cat("\nQ'Q (should be I):\n")
print(round(QtQ, 10))

# Verify QQ' = I
QQt <- Q %*% t(Q)
cat("\nQQ' (should be I):\n")
print(round(QQt, 10))

# Verify Q' = Q^(-1)
Q_inv <- solve(Q)
cat("\nQ^(-1):\n")
print(round(Q_inv, 4))

cat("\nQ' (should equal Q^(-1)):\n")
print(round(t(Q), 4))

cat("\nVerify Q' = Q^(-1):\n")
all.equal(t(Q), Q_inv)
```

**Creating Orthogonal Matrices - Gram-Schmidt:**

Convert a set of vectors into orthonormal vectors.

```{r}
#| label: orthogonal-gram-schmidt

# Start with two non-orthogonal vectors
v1 <- c(1, 1, 0)
v2 <- c(1, 0, 1)

cat("Original vectors:\n")
cat("v1 =", v1, "\n")
cat("v2 =", v2, "\n")

# Gram-Schmidt orthogonalization
# Step 1: Normalize v1
q1 <- v1 / sqrt(sum(v1^2))
cat("\nq1 (normalized v1):\n")
print(q1)

# Step 2: Remove component of v2 in direction of q1
v2_perp <- v2 - sum(v2 * q1) * q1
cat("\nv2_perp (v2 minus projection onto q1):\n")
print(v2_perp)

# Step 3: Normalize
q2 <- v2_perp / sqrt(sum(v2_perp^2))
cat("\nq2 (normalized v2_perp):\n")
print(q2)

# Verify orthonormality
cat("\nq1'q1 (should be 1):", sum(q1 * q1), "\n")
cat("q2'q2 (should be 1):", sum(q2 * q2), "\n")
cat("q1'q2 (should be 0):", sum(q1 * q2), "\n")

# Built-in QR decomposition does this automatically
A <- cbind(v1, v2)
QR <- qr(A)
Q_auto <- qr.Q(QR)
cat("\nQ from qr() decomposition:\n")
print(Q_auto)
```

**Length Preservation:**

Orthogonal transformations preserve vector lengths.

```{r}
#| label: orthogonal-length

# Vector
x <- c(3, 4)
cat("Original vector x:", x, "\n")
cat("Length ||x||:", sqrt(sum(x^2)), "\n")

# Rotate by 45°
Q <- matrix(c(cos(pi/4), sin(pi/4),
              -sin(pi/4), cos(pi/4)), nrow = 2, ncol = 2, byrow = TRUE)

# Transform
Qx <- Q %*% x
cat("\nTransformed vector Qx:", Qx, "\n")
cat("Length ||Qx||:", sqrt(sum(Qx^2)), "\n")

cat("\nLengths are preserved!\n")
```

**Orthogonal Matrices in Linear Models - QR Decomposition:**

Any matrix **X** can be factored as **X = QR** where **Q** is orthogonal and **R** is upper triangular.

```{r}
#| label: orthogonal-qr

# Design matrix for simple regression
X <- cbind(1, c(1, 2, 3, 4, 5))

cat("Design matrix X (5×2):\n")
print(X)

# QR decomposition
qr_obj <- qr(X)
Q <- qr.Q(qr_obj)
R <- qr.R(qr_obj)

cat("\nQ (orthogonal, 5×2):\n")
print(round(Q, 4))

cat("\nR (upper triangular, 2×2):\n")
print(round(R, 4))

# Verify X = QR
cat("\nVerify X = QR:\n")
QR_product <- Q %*% R
print(round(QR_product, 4))
all.equal(X, QR_product, check.attributes = FALSE)

# Verify Q'Q = I
cat("\nVerify Q'Q = I:\n")
QtQ <- t(Q) %*% Q
print(round(QtQ, 10))
```

**Solving Normal Equations with QR:**

If **X = QR**, then **X'X = R'Q'QR = R'R** (since Q'Q = I).

The normal equations simplify: **R'Rb = R'Q'y**

Or: **Rb = Q'y** (much more stable numerically!)

```{r}
#| label: orthogonal-solve

# Example data
y <- c(2, 4, 6, 8, 10)

# Traditional method: (X'X)^(-1)X'y
XtX <- t(X) %*% X
Xty <- t(X) %*% y
b_traditional <- solve(XtX) %*% Xty

cat("Traditional solution b:\n")
print(b_traditional)

# QR method: R^(-1)Q'y
b_qr <- solve(R) %*% t(Q) %*% y

cat("\nQR solution b:\n")
print(b_qr)

cat("\nVerify both methods give same answer:\n")
all.equal(b_traditional, b_qr, check.attributes = FALSE)

cat("\nQR method is numerically more stable for ill-conditioned X'X\n")
```

**Livestock Example - Orthogonal Contrasts:**

In balanced ANOVA, orthogonal contrasts correspond to orthogonal directions.

```{r}
#| label: orthogonal-contrasts

# Three feed types, n=3 per feed
# Treatment means (estimated)
mu <- c(25, 28, 22)  # kg, body weight gain

cat("Treatment means:", mu, "\n")

# Two orthogonal contrasts
# C1: Treatment 1 vs Treatment 2
c1 <- c(1, -1, 0)

# C2: Average of (1,2) vs Treatment 3
c2 <- c(1, 1, -2)

cat("\nContrast 1:", c1, "\n")
cat("Contrast 2:", c2, "\n")

# Check orthogonality: c1'c2 = 0
cat("\nc1'c2 =", sum(c1 * c2), "(orthogonal!)\n")

# Estimates
psi1 <- sum(c1 * mu)
psi2 <- sum(c2 * mu)

cat(sprintf("\nContrast 1 estimate: %.1f kg (T1 - T2)\n", psi1))
cat(sprintf("Contrast 2 estimate: %.1f kg (Avg(T1,T2) - T3)\n", psi2))

cat("\nOrthogonal contrasts partition the treatment sum of squares\n")
```

**Orthogonal Polynomials:**

```{r}
#| label: orthogonal-polynomials

# Five time points
time <- 1:5

# Orthogonal polynomials (up to degree 4)
poly_matrix <- poly(time, degree = 4)

cat("Orthogonal polynomial matrix (5×4):\n")
print(round(poly_matrix, 4))

# Verify orthonormality
cat("\nX'X (should be identity):\n")
XtX <- t(poly_matrix) %*% poly_matrix
print(round(XtX, 10))

cat("\nThese orthogonal polynomials reduce collinearity in polynomial regression\n")
```

:::{.callout-tip}
## Why Orthogonal Matrices Matter

1. **Numerical stability**: $\mathbf{Q}^{-1} = \mathbf{Q}'$ is trivial to compute
2. **QR decomposition**: More stable than solving $(X'X)^{-1}$ directly
3. **Geometry**: Represents rotations and reflections
4. **Condition number**: $\kappa(\mathbf{Q}) = 1$ (perfectly conditioned)
5. **Orthogonal contrasts**: Partition sums of squares cleanly
:::

:::{.callout-note}
## Cross-References

Orthogonal matrices appear in:

- **Week 2**: QR decomposition
- **Week 5**: Stable solution of normal equations
- **Week 8**: Orthogonal contrasts in balanced ANOVA
- **Week 14**: Polynomial regression with orthogonal polynomials
- **Appendix**: Eigenvalue decomposition **A = QΛQ'** for symmetric **A**
:::

---

### Idempotent Matrices {#sec-idempotent}

**Definition:**

A matrix **P** is **idempotent** if $\mathbf{P}^2 = \mathbf{P}$ (applying it twice gives same result as applying once).

Equivalently: $\mathbf{PP} = \mathbf{P}$

**Geometric Interpretation:**

Idempotent matrices represent **projections**. Projecting onto a subspace twice is the same as projecting once.

**Key Properties:**

1. **Repeated application**: $\mathbf{P}^k = \mathbf{P}$ for all $k \geq 1$
2. **Eigenvalues**: Only 0 or 1 (no other values possible!)
3. **Rank equals trace**: $r(\mathbf{P}) = \text{tr}(\mathbf{P})$ (very useful!)
4. **Complement is idempotent**: If **P** is idempotent, so is $\mathbf{I} - \mathbf{P}$
5. **Orthogonal projections**: If **P** is also symmetric, it's an orthogonal projection

**Proof that I - P is idempotent:**
$$
(\mathbf{I} - \mathbf{P})^2 = \mathbf{I}^2 - 2\mathbf{P} + \mathbf{P}^2 = \mathbf{I} - 2\mathbf{P} + \mathbf{P} = \mathbf{I} - \mathbf{P} \quad \checkmark
$$

**Most Important Examples in Linear Models:**

1. **Hat matrix**: $\mathbf{H} = \mathbf{X}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'$ (projects onto column space of **X**)
2. **Residual matrix**: $\mathbf{I} - \mathbf{H}$ (projects onto orthogonal complement)
3. **Centering matrix**: $\mathbf{C} = \mathbf{I} - n^{-1}\mathbf{J}_n$ where $\mathbf{J}_n = \mathbf{11}'$

**R Implementation:**

```{r}
#| label: idempotent-basic

# Simple example: projection onto first coordinate
P <- matrix(c(1, 0, 0, 0, 0, 0, 0, 0, 0), nrow = 3, ncol = 3, byrow = TRUE)

cat("Projection matrix P:\n")
print(P)

# Test idempotence
P2 <- P %*% P

cat("\nP² (should equal P):\n")
print(P2)

cat("\nVerify P² = P:\n")
all.equal(P2, P)

# Apply to vector
x <- c(3, 4, 5)
Px <- P %*% x

cat("\nOriginal vector x:", x, "\n")
cat("Projected vector Px:", Px, "\n")
cat("(Projects onto first coordinate, zeros out others)\n")

# Apply twice
PPx <- P %*% Px
cat("P(Px) (should equal Px):", PPx, "\n")
```

**Hat Matrix - THE Central Example:**

```{r}
#| label: idempotent-hat-matrix

# Simple regression: n=4 observations
X <- cbind(1, c(1, 2, 3, 4))

cat("Design matrix X (4×2):\n")
print(X)

# Construct hat matrix
H <- X %*% solve(t(X) %*% X) %*% t(X)

cat("\nHat matrix H (4×4):\n")
print(round(H, 4))

# Verify idempotence: H² = H
H2 <- H %*% H

cat("\nH² (should equal H):\n")
print(round(H2, 4))

cat("\nVerify H² = H:\n")
all.equal(H2, H)

# Verify rank = trace
rank_H <- qr(H)$rank
trace_H <- sum(diag(H))

cat(sprintf("\nrank(H) = %d\n", rank_H))
cat(sprintf("trace(H) = %.4f\n", trace_H))
cat("rank(H) = trace(H) ✓\n")

cat("\nInterpretation: trace(H) = p = 2 (number of parameters)\n")
```

**Residual Matrix - Orthogonal Complement:**

```{r}
#| label: idempotent-residual

# Residual matrix
I <- diag(4)
M <- I - H

cat("Residual matrix I - H (4×4):\n")
print(round(M, 4))

# Verify idempotence: (I-H)² = I-H
M2 <- M %*% M

cat("\n(I-H)² (should equal I-H):\n")
print(round(M2, 4))

cat("\nVerify (I-H)² = I-H:\n")
all.equal(M2, M)

# Verify rank = trace
rank_M <- qr(M)$rank
trace_M <- sum(diag(M))

cat(sprintf("\nrank(I-H) = %d\n", rank_M))
cat(sprintf("trace(I-H) = %.4f\n", trace_M))
cat("rank(I-H) = trace(I-H) ✓\n")

cat(sprintf("\nInterpretation: trace(I-H) = n-p = %d - %d = %d (degrees of freedom for error)\n",
            nrow(X), ncol(X), nrow(X) - ncol(X)))
```

**Orthogonal Decomposition:**

The hat matrix **H** and residual matrix **I-H** provide an orthogonal decomposition of any vector.

```{r}
#| label: idempotent-decomposition

# Any observation vector y
y <- c(2, 4, 5, 7)

cat("Observation vector y:", y, "\n")

# Fitted values (projection onto column space of X)
y_hat <- H %*% y
cat("\nFitted values ŷ = Hy:", as.vector(y_hat), "\n")

# Residuals (projection onto orthogonal complement)
e <- (I - H) %*% y
cat("Residuals e = (I-H)y:", as.vector(e), "\n")

# Verify y = ŷ + e
cat("\nVerify y = ŷ + e:\n")
all.equal(y, as.vector(y_hat + e))

# Verify orthogonality: ŷ'e = 0
cat(sprintf("\nŷ'e = %.10f (should be 0)\n", sum(y_hat * e)))

cat("\nThis is the fundamental decomposition: y = Hy + (I-H)y\n")
cat("Fitted values + Residuals = Observations\n")
```

**Centering Matrix:**

The centering matrix removes the mean from data.

```{r}
#| label: idempotent-centering

# Data vector
x <- c(10, 20, 30, 40, 50)
n <- length(x)

cat("Data x:", x, "\n")
cat("Mean:", mean(x), "\n")

# Centering matrix: C = I - (1/n)J where J = 11'
J <- matrix(1, nrow = n, ncol = n)  # All ones
C <- diag(n) - (1/n) * J

cat("\nCentering matrix C (5×5):\n")
print(round(C, 2))

# Apply centering matrix
x_centered <- C %*% x

cat("\nCentered data Cx:\n")
print(x_centered)

cat("\nVerify mean of centered data is 0:\n")
cat(sprintf("Mean = %.10f\n", mean(x_centered)))

# Verify idempotence
C2 <- C %*% C
cat("\nVerify C² = C:\n")
all.equal(C2, C)

cat("\nCentering is a projection (idempotent operation)\n")
```

**Livestock Example - Sum of Squares Decomposition:**

Idempotent matrices decompose total sum of squares into components.

```{r}
#| label: idempotent-ss-decomposition

# Broiler weights (kg)
weight <- c(2.1, 2.3, 2.5, 2.7, 2.9)
y <- matrix(weight, ncol = 1)
n <- length(weight)

cat("Broiler weights (kg):", weight, "\n")

# Design matrix (just intercept - estimate mean)
X <- matrix(1, nrow = n, ncol = 1)

# Hat matrix
H <- X %*% solve(t(X) %*% X) %*% t(X)
# For intercept-only: H = (1/n)J (all elements = 1/n)

# Centering matrix
J_n <- matrix(1/n, nrow = n, ncol = n)
C <- diag(n) - J_n

cat("\nHat matrix H (all elements 1/5 = 0.2):\n")
print(round(H, 2))

# Total sum of squares: y'Cy
SST <- t(y) %*% C %*% y
cat(sprintf("\nSST = y'(I - n⁻¹J)y = %.4f\n", SST[1,1]))

# Verify with formula
SST_formula <- sum((weight - mean(weight))^2)
cat(sprintf("Verification: Σ(y - ȳ)² = %.4f\n", SST_formula))

cat("\nThe centering matrix C = I - n⁻¹J is idempotent and computes SST\n")
```

**Properties of Symmetric Idempotent Matrices:**

If **P** is symmetric and idempotent, it's an **orthogonal projection**.

```{r}
#| label: idempotent-symmetric

# Hat matrix is both symmetric and idempotent
X <- cbind(1, c(1, 2, 3))
H <- X %*% solve(t(X) %*% X) %*% t(X)

cat("Hat matrix H:\n")
print(round(H, 4))

cat("\nCheck symmetry:\n")
all.equal(H, t(H))

cat("\nCheck idempotence:\n")
all.equal(H %*% H, H)

cat("\nH is symmetric AND idempotent → orthogonal projection\n")

# Eigenvalues are only 0 or 1
eigenvalues <- eigen(H)$values
cat("\nEigenvalues of H:\n")
print(round(eigenvalues, 10))
cat("Only 0's and 1's (characteristic of idempotent matrices)\n")
cat(sprintf("Number of 1's = %d = rank(H) = trace(H)\n", sum(eigenvalues > 0.5)))
```

**Why Idempotent Matrices Matter:**

```{r}
#| label: idempotent-summary

# Example: repeated projection doesn't change result
X <- cbind(1, 1:5)
y <- c(2, 4, 6, 8, 10)
H <- X %*% solve(t(X) %*% X) %*% t(X)

# Fitted values
y_hat1 <- H %*% y
y_hat2 <- H %*% y_hat1  # Project fitted values again
y_hat3 <- H %*% y_hat2  # And again

cat("Original y:", y, "\n")
cat("Hy (1st projection):", round(as.vector(y_hat1), 4), "\n")
cat("H(Hy) (2nd projection):", round(as.vector(y_hat2), 4), "\n")
cat("H(H(Hy)) (3rd projection):", round(as.vector(y_hat3), 4), "\n")

cat("\nAll identical! Projection is idempotent.\n")
cat("Once you're in the subspace, projecting again does nothing.\n")
```

:::{.callout-important}
## Hat Matrix is Idempotent

The hat matrix $\mathbf{H} = \mathbf{X}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'$ is:

1. **Symmetric**: $\mathbf{H}' = \mathbf{H}$
2. **Idempotent**: $\mathbf{H}^2 = \mathbf{H}$
3. **Rank = Trace**: $r(\mathbf{H}) = \text{tr}(\mathbf{H}) = p$ (number of parameters)

The residual matrix $\mathbf{I} - \mathbf{H}$ has the same properties with:
- $r(\mathbf{I} - \mathbf{H}) = \text{tr}(\mathbf{I} - \mathbf{H}) = n - p$ (error degrees of freedom)

This is why trace gives degrees of freedom!
:::

:::{.callout-note}
## Cross-References

Idempotent matrices are fundamental to:

- **Week 5**: Hat matrix **H** and fitted values $\hat{\mathbf{y}} = \mathbf{Hy}$
- **Week 5**: Residuals $\mathbf{e} = (\mathbf{I} - \mathbf{H})\mathbf{y}$
- **Week 5**: Degrees of freedom = $\text{tr}(\mathbf{I} - \mathbf{H}) = n - p$
- **Week 6**: Sum of squares decomposition using centering matrix
- **Week 11**: Leverage values (diagonal of **H**)
- **Section @sec-projection-matrices**: Complete coverage of projection matrices
:::

---

## Matrix Properties {#sec-properties}

Understanding the fundamental properties of matrices - rank, trace, and determinant - is essential for working with linear models. These properties determine invertibility, degrees of freedom, and the structure of solutions.

### Rank {#sec-rank}

The **rank** of a matrix is one of its most important properties, especially in linear models where it determines whether we can uniquely solve for parameters.

#### Definition

The **rank** of an m × n matrix **A**, denoted r(**A**), is:

- The **maximum number of linearly independent rows**, OR
- The **maximum number of linearly independent columns**, OR
- The dimension of the **column space** (or row space) of **A**

These three definitions are equivalent.

**Key fact**: For any matrix **A**, $r(\mathbf{A}^{\text{rows}}) = r(\mathbf{A}^{\text{columns}})$

#### Properties and Theorems

**1. Basic Properties**

- $0 \leq r(\mathbf{A}) \leq \min(m, n)$ for m × n matrix
- $r(\mathbf{A}) = r(\mathbf{A}')$ (rank of transpose equals rank)
- $r(c\mathbf{A}) = r(\mathbf{A})$ for any scalar $c \neq 0$

**2. Full Rank**

An m × n matrix **A** is **full rank** if:
- $r(\mathbf{A}) = \min(m, n)$
- If m < n: full row rank (r = m)
- If m > n: full column rank (r = n)
- If m = n: **A** is square and invertible

**3. Rank Deficient**

A matrix is **rank deficient** if $r(\mathbf{A}) < \min(m, n)$

For square n × n matrices:
- Full rank: r(**A**) = n (invertible, det(**A**) ≠ 0)
- Rank deficient: r(**A**) < n (singular, det(**A**) = 0)

**4. Product Rule**

$$
r(\mathbf{AB}) \leq \min(r(\mathbf{A}), r(\mathbf{B}))
$$ {#eq-rank-product}

Proof: The columns of **AB** are linear combinations of columns of **A**, so column space of **AB** is a subspace of column space of **A**.

**5. Sum Rule**

$$
r(\mathbf{A} + \mathbf{B}) \leq r(\mathbf{A}) + r(\mathbf{B})
$$ {#eq-rank-sum}

**6. Critical Result for Linear Models**

$$
r(\mathbf{X}'\mathbf{X}) = r(\mathbf{X}\mathbf{X}') = r(\mathbf{X})
$$ {#eq-rank-xtx}

This is **fundamental** for understanding when normal equations have unique solutions.

**Proof**: We need to show $r(\mathbf{X}'\mathbf{X}) = r(\mathbf{X})$.

First, $r(\mathbf{X}'\mathbf{X}) \leq r(\mathbf{X})$ by the product rule.

For the reverse inequality, suppose $\mathbf{X}\mathbf{c} = \mathbf{0}$ for some vector **c**. Then:
$$
\mathbf{c}'\mathbf{X}'\mathbf{X}\mathbf{c} = (\mathbf{X}\mathbf{c})'(\mathbf{X}\mathbf{c}) = \mathbf{0}'\mathbf{0} = 0
$$

This implies $\mathbf{X}'\mathbf{X}\mathbf{c} = \mathbf{0}$ (since the quadratic form equals zero).

Therefore, null space of **X** equals null space of **X'X**, which means they have the same rank.

:::{.callout-important}
## Why This Matters for Linear Models

The result $r(\mathbf{X}'\mathbf{X}) = r(\mathbf{X})$ tells us:

- If **X** is full column rank (r = p), then **X'X** is invertible
- If **X** is rank deficient, **X'X** is also rank deficient (singular)
- This determines whether normal equations have a unique solution

This is central to understanding **Week 12: Non-Full Rank Models**!
:::

#### Computing Rank in R

```{r}
#| label: rank-computation

# Method 1: QR decomposition (most reliable)
A <- matrix(c(1, 2, 3,
              2, 4, 6,
              1, 1, 1), nrow = 3, byrow = TRUE)

cat("Matrix A:\n")
print(A)

rank_qr <- qr(A)$rank
cat("\nRank using QR:", rank_qr, "\n")

# Method 2: Using Matrix package
library(Matrix)
rank_matrix <- rankMatrix(A)
cat("Rank using rankMatrix:", rank_matrix[1], "\n")

# Check: Second row is 2 times first row, so rank < 3
cat("\nRow 2 = 2 * Row 1?", all.equal(A[2,], 2 * A[1,]), "\n")
cat("Therefore rank should be 2 (not 3)\n")
```

#### Livestock Example: Design Matrix Rank

```{r}
#| label: rank-design-matrix

# Example: Sheep fleece weight by breed (unbalanced data)
breed <- c(rep("Merino", 3), rep("Suffolk", 2), rep("Dorset", 1))
weight <- c(5.2, 5.4, 5.3, 4.8, 5.0, 5.6)

# Cell means model (always full rank)
X_cell <- model.matrix(~ breed - 1)
cat("Cell Means Model Design Matrix:\n")
print(X_cell)
cat("Rank:", qr(X_cell)$rank, "\n")
cat("Dimensions:", nrow(X_cell), "×", ncol(X_cell), "\n")
cat("Full column rank?", qr(X_cell)$rank == ncol(X_cell), "\n\n")

# Effects model (overparameterized, not full rank)
X_effects <- model.matrix(~ breed)
cat("Effects Model Design Matrix:\n")
print(X_effects)
cat("Rank:", qr(X_effects)$rank, "\n")
cat("Dimensions:", nrow(X_effects), "×", ncol(X_effects), "\n")
cat("Full column rank?", qr(X_effects)$rank == ncol(X_effects), "\n")
cat("Rank deficient by:", ncol(X_effects) - qr(X_effects)$rank, "\n\n")

# Verify X'X has same rank as X
XtX_cell <- t(X_cell) %*% X_cell
XtX_effects <- t(X_effects) %*% X_effects

cat("Rank verification:\n")
cat("X (cell means): rank =", qr(X_cell)$rank, "\n")
cat("X'X (cell means): rank =", qr(XtX_cell)$rank, "\n")
cat("Agreement:", qr(X_cell)$rank == qr(XtX_cell)$rank, "\n\n")

cat("X (effects): rank =", qr(X_effects)$rank, "\n")
cat("X'X (effects): rank =", qr(XtX_effects)$rank, "\n")
cat("Agreement:", qr(X_effects)$rank == qr(XtX_effects)$rank, "\n")
```

#### Implications for Estimability

When **X** is rank deficient:

- **X'Xb = X'y** has **infinitely many solutions**
- Individual parameters $\beta_j$ are **not uniquely estimable**
- But **linear combinations** $\mathbf{c}'\boldsymbol{\beta}$ may still be estimable if **c** is in the row space of **X**

Example: In effects model μ + α₁ + α₂ + α₃ with $\sum \alpha_i = 0$ constraint:
- μ is not uniquely estimable (confounded with αᵢ)
- Individual αᵢ are not uniquely estimable
- **Contrasts** α₁ - α₂ **are uniquely estimable**

:::{.callout-note}
## Cross-References

- **Week 2: Linear Algebra Essentials** - Introduction to rank
- **Week 7: One-Way ANOVA** - Full rank cell means vs. rank deficient effects model
- **Week 8: Contrasts and Estimable Functions** - What's estimable when rank deficient
- **Week 12: Non-Full Rank Models** - Complete treatment of rank deficiency
:::

---

### Trace {#sec-trace}

The **trace** of a square matrix is the sum of its diagonal elements. It has elegant properties that make it useful in linear models.

#### Definition

For an n × n matrix **A**:

$$
\text{tr}(\mathbf{A}) = \sum_{i=1}^n a_{ii}
$$ {#eq-trace-def}

The trace is a **scalar** (single number).

#### Properties

**1. Linearity**

$$
\text{tr}(\mathbf{A} + \mathbf{B}) = \text{tr}(\mathbf{A}) + \text{tr}(\mathbf{B})
$$

$$
\text{tr}(c\mathbf{A}) = c \cdot \text{tr}(\mathbf{A})
$$

**2. Cyclic Property** (Most Important!)

$$
\text{tr}(\mathbf{ABC}) = \text{tr}(\mathbf{BCA}) = \text{tr}(\mathbf{CAB})
$$ {#eq-trace-cyclic}

Special cases:
- $\text{tr}(\mathbf{AB}) = \text{tr}(\mathbf{BA})$ (even if **AB** ≠ **BA**)
- $\text{tr}(\mathbf{A}'\mathbf{A}) = \text{tr}(\mathbf{A}\mathbf{A}') = \sum_{i,j} a_{ij}^2$ (sum of all squared elements)

**3. Trace of Transpose**

$$
\text{tr}(\mathbf{A}') = \text{tr}(\mathbf{A})
$$

**4. Trace and Eigenvalues**

$$
\text{tr}(\mathbf{A}) = \sum_{i=1}^n \lambda_i
$$ {#eq-trace-eigenvalues}

where λᵢ are the eigenvalues of **A** (including multiplicities).

**5. Trace of Idempotent Matrix**

For an idempotent matrix **P** (where **P²** = **P**):

$$
\text{tr}(\mathbf{P}) = \text{rank}(\mathbf{P})
$$ {#eq-trace-rank-idempotent}

This is **crucial** for projection matrices!

**Proof**: Since **P** is idempotent, its eigenvalues are 0 or 1. If **P** has rank r, then r eigenvalues equal 1 and n - r equal 0. Therefore:
$$
\text{tr}(\mathbf{P}) = \sum \lambda_i = r \cdot 1 + (n-r) \cdot 0 = r = \text{rank}(\mathbf{P})
$$

#### Applications in Linear Models

**1. Degrees of Freedom**

For hat matrix $\mathbf{H} = \mathbf{X}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'$:

$$
\text{tr}(\mathbf{H}) = p
$$

where p = rank(**X**) = number of parameters.

This is the **model degrees of freedom**.

For residual projection $\mathbf{I} - \mathbf{H}$:

$$
\text{tr}(\mathbf{I} - \mathbf{H}) = n - p
$$

This is the **error degrees of freedom**.

**2. Sum of Squared Elements**

$$
\text{tr}(\mathbf{A}'\mathbf{A}) = \sum_{i=1}^m \sum_{j=1}^n a_{ij}^2 = ||\mathbf{A}||_F^2
$$

This is the squared **Frobenius norm** of **A**.

**3. Quadratic Forms**

For any matrices **A** (n × n) and **B** (n × n):

$$
\text{tr}(\mathbf{AB}) = \text{tr}(\mathbf{BA})
$$

Special case: If **y** is n × 1 and **A** is n × n:

$$
\mathbf{y}'\mathbf{A}\mathbf{y} = \text{tr}(\mathbf{A}\mathbf{y}\mathbf{y}')
$$ {#eq-trace-quadratic}

This connects quadratic forms to traces.

#### R Examples

```{r}
#| label: trace-examples

# Define trace function (R doesn't have built-in)
tr <- function(A) sum(diag(A))

# Example 1: Basic trace
A <- matrix(c(1, 2, 3, 4), 2, 2)
cat("Matrix A:\n")
print(A)
cat("Trace:", tr(A), "= 1 + 4 =", 1 + 4, "\n\n")

# Example 2: Cyclic property tr(AB) = tr(BA)
B <- matrix(c(5, 6, 7, 8), 2, 2)
AB <- A %*% B
BA <- B %*% A

cat("Cyclic property:\n")
cat("tr(AB) =", tr(AB), "\n")
cat("tr(BA) =", tr(BA), "\n")
cat("Equal?", all.equal(tr(AB), tr(BA)), "\n")
cat("But AB ≠ BA:\n")
cat("AB =\n")
print(AB)
cat("BA =\n")
print(BA)
cat("\n")

# Example 3: Trace of A'A = sum of squared elements
cat("tr(A'A) = sum of all squared elements:\n")
cat("tr(A'A) =", tr(t(A) %*% A), "\n")
cat("Sum of squared elements =", sum(A^2), "\n")
cat("Agreement:", all.equal(tr(t(A) %*% A), sum(A^2)), "\n\n")

# Example 4: Trace of idempotent matrix = rank
# Use projection matrix from earlier
X <- cbind(1, c(1, 2, 3, 4, 5))  # 5x2 design matrix
H <- X %*% solve(t(X) %*% X) %*% t(X)

cat("Projection matrix H:\n")
cat("Trace(H) =", tr(H), "\n")
cat("Rank(H) =", qr(H)$rank, "\n")
cat("Number of parameters p =", ncol(X), "\n")
cat("All equal?", all.equal(tr(H), qr(H)$rank), "\n")
```

#### Livestock Example: Degrees of Freedom

```{r}
#| label: trace-df-example

# Beef cattle: ADG for 10 steers, 2 breeds (5 per breed)
adg <- c(1.2, 1.3, 1.1, 1.4, 1.2,  # Angus
         1.0, 1.1, 0.9, 1.2, 1.0)  # Hereford
breed <- factor(rep(c("Angus", "Hereford"), each = 5))

# Design matrix (cell means model)
X <- model.matrix(~ breed - 1)
n <- nrow(X)
p <- ncol(X)

# Compute projection matrices
H <- X %*% solve(t(X) %*% X) %*% t(X)
I_minus_H <- diag(n) - H

cat("Sample size n =", n, "\n")
cat("Number of parameters p =", p, "\n\n")

cat("Model degrees of freedom:\n")
cat("  tr(H) =", tr(H), "\n")
cat("  rank(H) =", qr(H)$rank, "\n")
cat("  p =", p, "\n\n")

cat("Error degrees of freedom:\n")
cat("  tr(I - H) =", tr(I_minus_H), "\n")
cat("  rank(I - H) =", qr(I_minus_H)$rank, "\n")
cat("  n - p =", n - p, "\n\n")

cat("Verification: tr(H) + tr(I-H) = n\n")
cat("  ", tr(H), "+", tr(I_minus_H), "=", tr(H) + tr(I_minus_H), "\n")
cat("  n =", n, "\n")
```

:::{.callout-note}
## Cross-References

- **Week 2: Linear Algebra Essentials** - Introduction to trace
- **Week 5: Least Squares Theory** - Uses tr(H) = p for degrees of freedom
- **Week 6: Multiple Regression** - Degrees of freedom in ANOVA tables
- **Week 11: Model Diagnostics** - Leverage as diagonal elements of H
:::

---

### Determinant {#sec-determinant}

The **determinant** is a scalar value computed from a square matrix that provides information about invertibility, volume scaling, and system solvability.

#### Definition

For a square n × n matrix **A**, the determinant det(**A**) or |**A**| is defined recursively:

**1 × 1 matrix**: det([a]) = a

**2 × 2 matrix**:
$$
\det\begin{pmatrix} a & b \\ c & d \end{pmatrix} = ad - bc
$$ {#eq-det-2x2}

**n × n matrix**: Using cofactor expansion along row i:
$$
\det(\mathbf{A}) = \sum_{j=1}^n (-1)^{i+j} a_{ij} M_{ij}
$$

where $M_{ij}$ is the minor (determinant of (n-1) × (n-1) submatrix obtained by deleting row i and column j).

#### Properties

**1. Invertibility**

$$
\det(\mathbf{A}) \neq 0 \iff \mathbf{A} \text{ is invertible}
$$ {#eq-det-invertible}

$$
\det(\mathbf{A}) = 0 \iff \mathbf{A} \text{ is singular}
$$

**2. Product Rule**

$$
\det(\mathbf{AB}) = \det(\mathbf{A}) \cdot \det(\mathbf{B})
$$ {#eq-det-product}

**3. Inverse**

$$
\det(\mathbf{A}^{-1}) = \frac{1}{\det(\mathbf{A})}
$$ {#eq-det-inverse}

**4. Transpose**

$$
\det(\mathbf{A}') = \det(\mathbf{A})
$$

**5. Scalar Multiplication**

For n × n matrix:
$$
\det(c\mathbf{A}) = c^n \det(\mathbf{A})
$$ {#eq-det-scalar}

**6. Determinant and Eigenvalues**

$$
\det(\mathbf{A}) = \prod_{i=1}^n \lambda_i
$$ {#eq-det-eigenvalues}

where λᵢ are the eigenvalues.

**7. Geometric Interpretation**

|det(**A**)| is the volume (or area in 2D) of the parallelepiped formed by the column vectors of **A**.

If det(**A**) < 0, the transformation reverses orientation.

#### Computing Determinants in R

```{r}
#| label: determinant-computation

# Example 1: 2x2 matrix
A_2x2 <- matrix(c(3, 1, 2, 4), 2, 2)
cat("2×2 Matrix A:\n")
print(A_2x2)
cat("det(A) =", det(A_2x2), "\n")
cat("Manual: (3)(4) - (1)(2) =", 3*4 - 1*2, "\n\n")

# Example 2: Singular matrix (det = 0)
A_sing <- matrix(c(1, 2, 2, 4), 2, 2)  # Row 2 = 2 * Row 1
cat("Singular Matrix (row 2 = 2*row 1):\n")
print(A_sing)
cat("det(A) =", det(A_sing), "\n")
cat("Is singular?", abs(det(A_sing)) < 1e-10, "\n\n")

# Example 3: Product rule
B_2x2 <- matrix(c(1, 3, 2, 1), 2, 2)
cat("Product rule: det(AB) = det(A) * det(B)\n")
cat("det(A) =", det(A_2x2), "\n")
cat("det(B) =", det(B_2x2), "\n")
cat("det(A) * det(B) =", det(A_2x2) * det(B_2x2), "\n")
cat("det(AB) =", det(A_2x2 %*% B_2x2), "\n")
cat("Agreement:", all.equal(det(A_2x2) * det(B_2x2),
                            det(A_2x2 %*% B_2x2)), "\n\n")

# Example 4: Inverse rule
cat("Inverse rule: det(A^{-1}) = 1/det(A)\n")
A_inv <- solve(A_2x2)
cat("det(A) =", det(A_2x2), "\n")
cat("1/det(A) =", 1/det(A_2x2), "\n")
cat("det(A^{-1}) =", det(A_inv), "\n")
cat("Agreement:", all.equal(1/det(A_2x2), det(A_inv)), "\n")
```

#### Application: Testing Rank Deficiency

```{r}
#| label: det-rank-deficiency

# Create rank-deficient design matrix (overparameterized ANOVA)
treatment <- factor(rep(1:3, each = 3))
X_full <- model.matrix(~ treatment)  # Intercept + 2 treatment effects

cat("Full effects model design matrix:\n")
print(X_full)
cat("Rank:", qr(X_full)$rank, "\n")
cat("Columns:", ncol(X_full), "\n")
cat("Rank deficient?", qr(X_full)$rank < ncol(X_full), "\n\n")

# Compute X'X
XtX <- t(X_full) %*% X_full
cat("X'X:\n")
print(XtX)
cat("det(X'X) =", det(XtX), "\n")
cat("Is X'X singular?", abs(det(XtX)) < 1e-10, "\n\n")

# Compare with full rank (cell means model)
X_cell <- model.matrix(~ treatment - 1)
XtX_cell <- t(X_cell) %*% X_cell
cat("Cell means model:\n")
cat("X'X:\n")
print(XtX_cell)
cat("det(X'X) =", det(XtX_cell), "\n")
cat("Is X'X invertible?", abs(det(XtX_cell)) > 1e-10, "\n")
```

#### Livestock Example: Checking Invertibility

```{r}
#| label: det-livestock-example

# Swine growth: 6 pigs, measure weight at 3 time points
# Question: Can we fit polynomial growth curve?

pig_id <- rep(1:6, each = 3)
age_days <- rep(c(30, 60, 90), 6)
weight <- c(15, 35, 60,   # Pig 1
            14, 33, 58,   # Pig 2
            16, 36, 62,   # Pig 3
            15, 34, 59,   # Pig 4
            14, 35, 61,   # Pig 5
            16, 37, 63)   # Pig 6

# Try to fit quadratic model: weight ~ age + age²
# Design matrix for one pig's data
age_one_pig <- c(30, 60, 90)
X_quad <- cbind(1, age_one_pig, age_one_pig^2)

cat("Quadratic model design matrix (one pig):\n")
print(X_quad)
cat("Rank:", qr(X_quad)$rank, "\n")
cat("Dimensions:", nrow(X_quad), "×", ncol(X_quad), "\n\n")

cat("Can we fit quadratic (3 parameters) with 3 observations?\n")
XtX_quad <- t(X_quad) %*% X_quad
cat("X'X:\n")
print(XtX_quad)
cat("det(X'X) =", det(XtX_quad), "\n")
cat("Is X'X invertible?", abs(det(XtX_quad)) > 1e-10, "\n")
cat("\nYes! Exactly identified (3 parameters, 3 observations)\n")

# What about cubic?
X_cubic <- cbind(X_quad, age_one_pig^3)
cat("\nCubic model (4 parameters) with 3 observations:\n")
cat("Rank:", qr(X_cubic)$rank, "\n")
cat("Dimensions:", nrow(X_cubic), "×", ncol(X_cubic), "\n")
cat("Can estimate 4 parameters with 3 observations? No - rank deficient\n")
```

:::{.callout-warning}
## Numerical Issues

For large matrices or nearly singular matrices, computed determinants can be unreliable due to:

1. **Overflow/underflow**: det(**A**) can be extremely large or small
2. **Rounding errors**: Near-zero determinants may not be exactly zero

**Better approaches**:
- Use rank to check singularity: `qr(A)$rank < ncol(A)`
- Use condition number to check "near singularity": `kappa(A)`
- Don't use det() for large matrices (computational cost is O(n³))
:::

:::{.callout-note}
## Cross-References

- **Week 2: Linear Algebra Essentials** - Introduction to determinants
- **Week 12: Non-Full Rank Models** - Determinant zero for rank-deficient X'X
- **Week 14: Computational Considerations** - Numerical stability issues
:::

:::{.callout-important}
## Summary: Matrix Properties

**Rank**: Determines uniqueness of solutions
- r(**X'X**) = r(**X**) - fundamental result
- Full rank → unique least squares solution
- Rank deficient → need constraints or estimable functions

**Trace**: Sum of diagonal elements
- tr(**H**) = p (model degrees of freedom)
- tr(**I - H**) = n - p (error degrees of freedom)
- tr(**P**) = rank(**P**) for idempotent **P**

**Determinant**: Tests invertibility
- det(**X'X**) ≠ 0 → **X'X** invertible
- det(**X'X**) = 0 → rank deficient
- Use rank for numerical stability
:::

---

## Matrix Inverses {#sec-inverses}

Matrix inverses are fundamental for solving linear systems, including the normal equations in linear models. When **X** is rank deficient, we need generalized inverses.

### Regular Inverse {#sec-regular-inverse}

The **regular inverse** (or simply "inverse") of a matrix exists only for square, full-rank matrices.

#### Definition

For a square n × n matrix **A**, the inverse **A**⁻¹ (if it exists) satisfies:

$$
\mathbf{A}\mathbf{A}^{-1} = \mathbf{A}^{-1}\mathbf{A} = \mathbf{I}
$$ {#eq-inverse-def}

**Existence conditions:**
- **A** must be **square** (n × n)
- **A** must be **full rank** (rank n)
- Equivalently: det(**A**) ≠ 0

#### Properties

**1. Uniqueness**: If **A**⁻¹ exists, it is unique

**2. Reverse order**:
$$
(\mathbf{AB})^{-1} = \mathbf{B}^{-1}\mathbf{A}^{-1}
$$ {#eq-inverse-product}

**3. Transpose**:
$$
(\mathbf{A}')^{-1} = (\mathbf{A}^{-1})'
$$ {#eq-inverse-transpose}

**4. Inverse of inverse**:
$$
(\mathbf{A}^{-1})^{-1} = \mathbf{A}
$$

**5. Scalar multiplication**:
$$
(c\mathbf{A})^{-1} = \frac{1}{c}\mathbf{A}^{-1}, \quad c \neq 0
$$

#### Formula for 2 × 2 Matrix

For quick reference:
$$
\begin{pmatrix} a & b \\ c & d \end{pmatrix}^{-1} = \frac{1}{ad-bc} \begin{pmatrix} d & -b \\ -c & a \end{pmatrix}
$$ {#eq-inverse-2x2}

provided $ad - bc \neq 0$ (determinant is non-zero).

#### Computing Inverses in R

```{r}
#| label: regular-inverse

# Example 1: 2x2 matrix
A <- matrix(c(3, 1, 2, 4), 2, 2)
cat("Matrix A:\n")
print(A)

# Compute inverse
A_inv <- solve(A)
cat("\nA inverse:\n")
print(A_inv)

# Verify AA^{-1} = I
I_check <- A %*% A_inv
cat("\nA * A^{-1} =\n")
print(round(I_check, 10))
cat("\nIs identity?", all.equal(I_check, diag(2)), "\n\n")

# Example 2: Verify manual 2x2 formula
det_A <- det(A)
A_inv_manual <- (1/det_A) * matrix(c(4, -1, -2, 3), 2, 2)
cat("Manual 2x2 inverse formula:\n")
print(A_inv_manual)
cat("Agreement with solve():", all.equal(A_inv, A_inv_manual), "\n\n")

# Example 3: Transpose property (A')^{-1} = (A^{-1})'
At_inv <- solve(t(A))
Ainv_t <- t(A_inv)
cat("Transpose property:\n")
cat("(A')^{-1} = (A^{-1})'?\n")
cat(all.equal(At_inv, Ainv_t), "\n")
```

#### Application: Solving Normal Equations

When **X** is full rank, the normal equations $\mathbf{X}'\mathbf{X}\mathbf{b} = \mathbf{X}'\mathbf{y}$ have unique solution:

$$
\mathbf{b} = (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{y}
$$ {#eq-ls-solution-inverse}

```{r}
#| label: inverse-normal-equations

# Simple regression example
x <- c(1, 2, 3, 4, 5)
y <- c(2.1, 3.9, 6.2, 7.8, 10.1)

# Design matrix
X <- cbind(1, x)
cat("Design matrix X:\n")
print(X)
cat("Rank:", qr(X)$rank, "Columns:", ncol(X), "\n")
cat("Full rank?", qr(X)$rank == ncol(X), "\n\n")

# Normal equations: X'Xb = X'y
XtX <- t(X) %*% X
Xty <- t(X) %*% y

cat("X'X:\n")
print(XtX)
cat("det(X'X) =", det(XtX), "\n")
cat("Is X'X invertible?", abs(det(XtX)) > 1e-10, "\n\n")

# Solve using inverse
XtX_inv <- solve(XtX)
b <- XtX_inv %*% Xty

cat("Solution b = (X'X)^{-1}X'y:\n")
print(b)

# Verify with lm()
fit <- lm(y ~ x)
cat("\nComparison with lm():\n")
cat("Our b:", c(b), "\n")
cat("lm() coef:", coef(fit), "\n")
```

:::{.callout-note}
## Cross-References

- **Week 2: Linear Algebra Essentials** - Introduction to matrix inverses
- **Week 4-6: Regression** - Uses (**X'X**)⁻¹ throughout
- **Week 5: Least Squares** - Derives b = (**X'X**)⁻¹**X'y**
:::

---

### Generalized Inverse {#sec-generalized-inverse}

When a matrix is not square or not full rank, the regular inverse doesn't exist. **Generalized inverses** extend the concept of inversion to such matrices.

#### Definition: Reflexive Generalized Inverse

A matrix **A**⁻ is a **generalized inverse** (or **g-inverse**) of **A** if:

$$
\mathbf{A}\mathbf{A}^{-}\mathbf{A} = \mathbf{A}
$$ {#eq-ginv-def}

**Key properties:**
- **A**⁻ is **not unique** (many g-inverses exist)
- If **A** is square and invertible, then **A**⁻ = **A**⁻¹ (unique)
- Works for any m × n matrix **A** (doesn't need to be square)

#### Moore-Penrose Inverse

The **Moore-Penrose inverse** **A**⁺ is a **unique** generalized inverse satisfying four conditions:

1. $\mathbf{A}\mathbf{A}^{+}\mathbf{A} = \mathbf{A}$ (reflexive)
2. $\mathbf{A}^{+}\mathbf{A}\mathbf{A}^{+} = \mathbf{A}^{+}$ (reflexive for **A**⁺)
3. $(\mathbf{A}\mathbf{A}^{+})' = \mathbf{A}\mathbf{A}^{+}$ (symmetric)
4. $(\mathbf{A}^{+}\mathbf{A})' = \mathbf{A}^{+}\mathbf{A}$ (symmetric)

**A**⁺ is the **unique** matrix satisfying all four conditions.

#### Computing Generalized Inverses

**Method 1: Using SVD (Most Stable)**

If $\mathbf{A} = \mathbf{U}\mathbf{D}\mathbf{V}'$ is the singular value decomposition, then:

$$
\mathbf{A}^{+} = \mathbf{V}\mathbf{D}^{+}\mathbf{U}'
$$ {#eq-ginv-svd}

where **D**⁺ is formed by taking reciprocals of non-zero diagonal elements of **D** and transposing.

**Method 2: Using `ginv()` from MASS package**

```r
library(MASS)
A_ginv <- ginv(A)
```

#### Properties of Generalized Inverses

**1. Not unique** (except Moore-Penrose):

Different g-inverses give different solutions to **Ax** = **b**, BUT:

**2. Estimable functions are unique**:

If $\mathbf{c}'\boldsymbol{\beta}$ is estimable (i.e., **c**' is in row space of **X**), then $\mathbf{c}'\mathbf{b}$ is the **same** for all g-inverses used to compute **b** = (**X'X**)⁻**X'y**.

This is **crucial** for rank-deficient models!

**3. For symmetric **A**:**
- (**A**⁻)' is also a g-inverse of **A**
- **AA**⁻ and **A**⁻**A** are symmetric and idempotent (projection matrices)

#### R Examples

```{r}
#| label: ginv-examples

library(MASS)

# Example 1: Rank-deficient matrix
A_rd <- matrix(c(1, 2, 3,
                 2, 4, 6,
                 1, 1, 1), 3, 3, byrow = TRUE)

cat("Rank-deficient matrix A:\n")
print(A_rd)
cat("Rank:", qr(A_rd)$rank, "\n")
cat("Dimensions: 3 x 3\n")
cat("det(A) =", det(A_rd), "\n\n")

# Try regular inverse (will fail)
cat("Attempting regular inverse...\n")
tryCatch({
  A_rd_inv <- solve(A_rd)
  cat("Success!\n")
}, error = function(e) {
  cat("Error:", conditionMessage(e), "\n")
})

# Compute generalized inverse
A_rd_ginv <- ginv(A_rd)
cat("\nGeneralized inverse A^-:\n")
print(round(A_rd_ginv, 4))

# Verify AA^-A = A
AAminusA <- A_rd %*% A_rd_ginv %*% A_rd
cat("\nVerify AA^-A = A:\n")
cat("Max difference:", max(abs(AAminusA - A_rd)), "\n")
cat("Agreement:", all.equal(AAminusA, A_rd), "\n")
```

#### Application: Non-Full Rank Design Matrix

```{r}
#| label: ginv-design-matrix

# Three-breed ANOVA with cell means model
breed <- factor(rep(c("A", "B", "C"), each = 3))
y <- c(10, 11, 12,  # Breed A
       15, 16, 14,  # Breed B
       18, 19, 20)  # Breed C

# Cell means model: y = μ_breed + e (one mean per breed)
X <- model.matrix(~ breed - 1)  # -1 removes intercept
cat("Design matrix (cell means model):\n")
print(X)
cat("Rank:", qr(X)$rank, "\n")
cat("Columns:", ncol(X), "\n")
cat("Full rank?", qr(X)$rank == ncol(X), "\n\n")

# Compute X'X (will be full rank for cell means model)
XtX <- t(X) %*% X
cat("X'X:\n")
print(XtX)
cat("det(X'X) =", det(XtX), "\n")
cat("Is invertible?", abs(det(XtX)) > 1e-10, "\n\n")

# Solve for breed means
XtX_inv <- solve(XtX)
Xty <- t(X) %*% y
b <- XtX_inv %*% Xty

cat("Breed means (μ_A, μ_B, μ_C):\n")
print(b)

# Contrasts are directly estimable
# Contrast: μ_A - μ_B (breed A vs breed B)
contrast <- c(1, -1, 0)  # μ_A - μ_B
contrast_estimate <- t(contrast) %*% b
cat("\nContrast μ_A - μ_B =", c(contrast_estimate), "\n")

# Verify this equals difference in breed means
mean_A <- mean(y[breed == "A"])
mean_B <- mean(y[breed == "B"])
cat("Direct calculation:", mean_A - mean_B, "\n")
cat("Agreement:", all.equal(c(contrast_estimate), mean_A - mean_B), "\n")
```

#### Computing Moore-Penrose via SVD

```{r}
#| label: moore-penrose-svd

# Demonstrate computing Moore-Penrose inverse manually
A <- matrix(c(1, 2, 2, 4), 2, 2)  # Rank 1 matrix

cat("Matrix A (rank deficient):\n")
print(A)
cat("Rank:", qr(A)$rank, "\n\n")

# Compute SVD
svd_A <- svd(A)
U <- svd_A$u
D_diag <- svd_A$d
V <- svd_A$v

cat("Singular values:", D_diag, "\n")
cat("Number of non-zero singular values:", sum(D_diag > 1e-10), "\n\n")

# Create D^+ (reciprocals of non-zero singular values)
D_plus_diag <- ifelse(D_diag > 1e-10, 1/D_diag, 0)
D_plus <- diag(D_plus_diag)

# A^+ = V D^+ U'
A_plus_manual <- V %*% D_plus %*% t(U)

cat("Moore-Penrose inverse (manual via SVD):\n")
print(round(A_plus_manual, 6))

# Compare with ginv()
A_plus_ginv <- ginv(A)
cat("\nMoore-Penrose inverse (ginv):\n")
print(round(A_plus_ginv, 6))

cat("\nAgreement:", all.equal(A_plus_manual, A_plus_ginv, tolerance = 1e-6), "\n")

# Verify four Moore-Penrose conditions
cat("\n Verifying four Moore-Penrose conditions:\n")
cat("1. AA^+A = A?", all.equal(A %*% A_plus_ginv %*% A, A), "\n")
cat("2. A^+AA^+ = A^+?", all.equal(A_plus_ginv %*% A %*% A_plus_ginv, A_plus_ginv), "\n")
cat("3. (AA^+)' = AA^+?", all.equal(t(A %*% A_plus_ginv), A %*% A_plus_ginv), "\n")
cat("4. (A^+A)' = A^+A?", all.equal(t(A_plus_ginv %*% A), A_plus_ginv %*% A), "\n")
```

:::{.callout-important}
## Key Insight: Estimability

When using generalized inverses for rank-deficient **X**:

- **Non-estimable functions**: $\mathbf{c}'\mathbf{b}$ depends on which g-inverse is used
- **Estimable functions**: $\mathbf{c}'\mathbf{b}$ is **unique** regardless of g-inverse

**Example**: In effects model μ + αᵢ:
- μ alone: NOT estimable (changes with g-inverse)
- αᵢ alone: NOT estimable
- αᵢ - αⱼ: **ESTIMABLE** (unique across all g-inverses)

This is why we focus on **contrasts** in Week 8!
:::

:::{.callout-note}
## Cross-References

- **Week 2: Linear Algebra Essentials** - Introduction to generalized inverses
- **Week 8: Contrasts and Estimable Functions** - Which functions are estimable
- **Week 12: Non-Full Rank Models** - Extensive use of g-inverses
- **Week 13: Special Topics I** - Different g-inverse choices
:::

---

### Useful Inverse Identities {#sec-inverse-identities}

Several matrix identities make working with inverses more efficient, especially for structured matrices.

#### Woodbury Matrix Identity

Also called **matrix inversion lemma**:

$$
(\mathbf{A} + \mathbf{UCV})^{-1} = \mathbf{A}^{-1} - \mathbf{A}^{-1}\mathbf{U}(\mathbf{C}^{-1} + \mathbf{V}\mathbf{A}^{-1}\mathbf{U})^{-1}\mathbf{V}\mathbf{A}^{-1}
$$ {#eq-woodbury}

**Special cases:**

**Sherman-Morrison Formula** (rank-1 update):

If **A** is n × n invertible and **u**, **v** are n × 1 vectors:

$$
(\mathbf{A} + \mathbf{uv}')^{-1} = \mathbf{A}^{-1} - \frac{\mathbf{A}^{-1}\mathbf{uv}'\mathbf{A}^{-1}}{1 + \mathbf{v}'\mathbf{A}^{-1}\mathbf{u}}
$$ {#eq-sherman-morrison}

This is useful when updating an inverse after a rank-1 change.

#### Block Matrix Inversion

For a block matrix:
$$
\mathbf{M} = \begin{pmatrix} \mathbf{A} & \mathbf{B} \\ \mathbf{C} & \mathbf{D} \end{pmatrix}
$$

If **A** and $\mathbf{D} - \mathbf{CA}^{-1}\mathbf{B}$ (the Schur complement) are invertible:

$$
\mathbf{M}^{-1} = \begin{pmatrix}
\mathbf{A}^{-1} + \mathbf{A}^{-1}\mathbf{B}\mathbf{S}^{-1}\mathbf{CA}^{-1} & -\mathbf{A}^{-1}\mathbf{B}\mathbf{S}^{-1} \\
-\mathbf{S}^{-1}\mathbf{CA}^{-1} & \mathbf{S}^{-1}
\end{pmatrix}
$$ {#eq-block-inverse}

where $\mathbf{S} = \mathbf{D} - \mathbf{CA}^{-1}\mathbf{B}$ is the Schur complement.

#### Inverse of Sum

In general, $(\mathbf{A} + \mathbf{B})^{-1} \neq \mathbf{A}^{-1} + \mathbf{B}^{-1}$

But if **A** and **B** commute (**AB** = **BA**):
$$
(\mathbf{A} + \mathbf{B})^{-1}\mathbf{A} = \mathbf{A}(\mathbf{A} + \mathbf{B})^{-1}
$$

#### R Examples

```{r}
#| label: inverse-identities

# Example 1: Sherman-Morrison formula
A <- matrix(c(4, 1, 1, 3), 2, 2)
u <- c(1, 2)
v <- c(3, 1)

cat("Sherman-Morrison Formula:\n")
cat("A =\n")
print(A)
cat("u =", u, "\n")
cat("v =", v, "\n\n")

# Direct computation
A_plus_uv <- A + u %*% t(v)
A_plus_uv_inv_direct <- solve(A_plus_uv)

cat("Direct: (A + uv')^{-1} =\n")
print(round(A_plus_uv_inv_direct, 4))

# Sherman-Morrison formula
A_inv <- solve(A)
numerator <- A_inv %*% u %*% t(v) %*% A_inv
denominator <- 1 + c(t(v) %*% A_inv %*% u)
A_plus_uv_inv_SM <- A_inv - numerator / denominator

cat("\nSherman-Morrison: (A + uv')^{-1} =\n")
print(round(A_plus_uv_inv_SM, 4))

cat("\nAgreement:", all.equal(A_plus_uv_inv_direct, A_plus_uv_inv_SM), "\n")

cat("\nComputational advantage: Don't need to invert (A + uv'),\n")
cat("just use existing A^{-1} with vector operations\n")
```

```{r}
#| label: block-inverse

# Example 2: Block matrix inversion
# Useful for partitioned models
A_block <- matrix(c(2, 0, 0, 3), 2, 2)
B_block <- matrix(c(1, 0, 0, 1), 2, 2)
C_block <- matrix(c(0, 1, 1, 0), 2, 2)
D_block <- matrix(c(4, 1, 1, 5), 2, 2)

M <- rbind(cbind(A_block, B_block),
           cbind(C_block, D_block))

cat("Block matrix M:\n")
print(M)

# Direct inversion
M_inv_direct <- solve(M)
cat("\nDirect M^{-1}:\n")
print(round(M_inv_direct, 4))

# Block inversion formula
A_inv <- solve(A_block)
S <- D_block - C_block %*% A_inv %*% B_block  # Schur complement
S_inv <- solve(S)

M11 <- A_inv + A_inv %*% B_block %*% S_inv %*% C_block %*% A_inv
M12 <- -A_inv %*% B_block %*% S_inv
M21 <- -S_inv %*% C_block %*% A_inv
M22 <- S_inv

M_inv_block <- rbind(cbind(M11, M12),
                     cbind(M21, M22))

cat("\nBlock formula M^{-1}:\n")
print(round(M_inv_block, 4))

cat("\nAgreement:", all.equal(M_inv_direct, M_inv_block), "\n")
```

:::{.callout-tip}
## When to Use These Identities

**Sherman-Morrison**:
- Updating regression after adding one observation
- Sequential estimation
- Rank-1 modifications to covariance matrices

**Woodbury**:
- Weighted least squares
- Ridge regression
- Kalman filtering

**Block inversion**:
- Partitioned regression models
- Testing subsets of parameters
- Mixed models (Week 14 preview)

These identities can dramatically reduce computation when you have structure to exploit!
:::

:::{.callout-note}
## Cross-References

- **Week 14: Special Topics II** - Weighted least squares uses these identities
- Advanced mixed models courses - Block inversion for MME
:::

:::{.callout-important}
## Summary: Matrix Inverses

**Regular Inverse**:
- Only for square, full-rank matrices
- Unique: **AA**⁻¹ = **A**⁻¹**A** = **I**
- R: `solve(A)`

**Generalized Inverse**:
- For any matrix (rank deficient OK)
- Not unique: **AA**⁻**A** = **A**
- Moore-Penrose **A**⁺ is unique
- R: `ginv(A)` from MASS
- **Key**: Estimable functions are unique across all g-inverses

**Identities**:
- Sherman-Morrison: Rank-1 updates
- Woodbury: Low-rank updates
- Block inversion: Partitioned matrices
:::

---

## Projection Matrices and Quadratic Forms {#sec-projections}

:::{.callout-important}
## Critical Section

This section is foundational for understanding least squares theory in Week 5. It receives extra emphasis with additional examples and detailed coverage. The concepts here are central to all of linear models.
:::

### Projection Matrices {#sec-projection-matrices}

Projection matrices are fundamental to least squares theory. They provide the geometric interpretation of fitting linear models and decomposing variation into model and error components.

#### Definition

A **projection matrix** **P** is a square matrix satisfying:

$$
\mathbf{P}^2 = \mathbf{P}
$$ {#eq-idempotent}

This property is called **idempotence**. If additionally **P** is symmetric ($\mathbf{P}' = \mathbf{P}$), then **P** is an **orthogonal projection matrix**.

:::{.callout-note}
## Geometric Interpretation

An orthogonal projection matrix projects vectors onto a subspace and is "idempotent" because projecting twice is the same as projecting once - you're already in the subspace after the first projection.
:::

#### The Hat Matrix **H**

The most important projection matrix in linear models is the **hat matrix**:

$$
\mathbf{H} = \mathbf{X}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'
$$ {#eq-hat-matrix}

where **X** is the n × p design matrix with full column rank (rank p).

**Why "hat matrix"?** Because it puts the "hat" on **y**:

$$
\hat{\mathbf{y}} = \mathbf{H}\mathbf{y}
$$ {#eq-y-hat}

where $\hat{\mathbf{y}}$ is the vector of fitted values.

#### Properties of the Hat Matrix

The hat matrix **H** has several critical properties:

1. **Symmetric**: $\mathbf{H}' = \mathbf{H}$

   Proof: $\mathbf{H}' = [\mathbf{X}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}']' = \mathbf{X}[(\mathbf{X}'\mathbf{X})^{-1}]'\mathbf{X}' = \mathbf{X}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}' = \mathbf{H}$

2. **Idempotent**: $\mathbf{H}^2 = \mathbf{H}$

   Proof:
   $$\begin{align}
   \mathbf{H}^2 &= \mathbf{X}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}' \cdot \mathbf{X}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}' \\
   &= \mathbf{X}(\mathbf{X}'\mathbf{X})^{-1}[\mathbf{X}'\mathbf{X}](\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}' \\
   &= \mathbf{X}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}' = \mathbf{H}
   \end{align}$$

3. **Rank and Trace**: $\text{rank}(\mathbf{H}) = \text{tr}(\mathbf{H}) = p$

   For idempotent matrices, rank equals trace. Since **H** projects onto the p-dimensional column space of **X**, its rank is p.

4. **Eigenvalues**: All eigenvalues of **H** are either 0 or 1

   - p eigenvalues equal 1 (corresponding to column space of **X**)
   - n - p eigenvalues equal 0 (corresponding to orthogonal complement)

5. **Projects onto column space of X**: $\mathbf{H}\mathbf{X} = \mathbf{X}$

   Proof: $\mathbf{H}\mathbf{X} = \mathbf{X}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{X} = \mathbf{X}$

#### The Residual Projection Matrix **I - H**

The complement of the hat matrix is equally important:

$$
\mathbf{I} - \mathbf{H}
$$ {#eq-residual-matrix}

This matrix projects onto the space orthogonal to the column space of **X**.

**Properties of I - H:**

1. **Symmetric**: $(\mathbf{I} - \mathbf{H})' = \mathbf{I} - \mathbf{H}$

2. **Idempotent**: $(\mathbf{I} - \mathbf{H})^2 = \mathbf{I} - \mathbf{H}$

   Proof:
   $$\begin{align}
   (\mathbf{I} - \mathbf{H})^2 &= \mathbf{I} - 2\mathbf{H} + \mathbf{H}^2 \\
   &= \mathbf{I} - 2\mathbf{H} + \mathbf{H} \\
   &= \mathbf{I} - \mathbf{H}
   \end{align}$$

3. **Rank and Trace**: $\text{rank}(\mathbf{I} - \mathbf{H}) = \text{tr}(\mathbf{I} - \mathbf{H}) = n - p$

4. **Produces residuals**: $\mathbf{e} = (\mathbf{I} - \mathbf{H})\mathbf{y}$

5. **Orthogonal to X**: $(\mathbf{I} - \mathbf{H})\mathbf{X} = \mathbf{0}$

   Proof: $(\mathbf{I} - \mathbf{H})\mathbf{X} = \mathbf{X} - \mathbf{H}\mathbf{X} = \mathbf{X} - \mathbf{X} = \mathbf{0}$

#### Orthogonality of **H** and **I - H**

A key relationship:

$$
\mathbf{H}(\mathbf{I} - \mathbf{H}) = \mathbf{0}
$$ {#eq-orthogonal}

Proof:
$$\begin{align}
\mathbf{H}(\mathbf{I} - \mathbf{H}) &= \mathbf{H} - \mathbf{H}^2 \\
&= \mathbf{H} - \mathbf{H} = \mathbf{0}
\end{align}$$

This shows that fitted values $\hat{\mathbf{y}} = \mathbf{H}\mathbf{y}$ and residuals $\mathbf{e} = (\mathbf{I} - \mathbf{H})\mathbf{y}$ are orthogonal:

$$
\hat{\mathbf{y}}'\mathbf{e} = (\mathbf{H}\mathbf{y})'[(\mathbf{I} - \mathbf{H})\mathbf{y}] = \mathbf{y}'\mathbf{H}'(\mathbf{I} - \mathbf{H})\mathbf{y} = 0
$$ {#eq-fitted-resid-orthogonal}

#### Hat Values (Leverage)

The **diagonal elements** of **H** are called **hat values** or **leverage values**:

$$
h_{ii} = [\mathbf{H}]_{ii}
$$ {#eq-leverage}

Properties:
- $0 \leq h_{ii} \leq 1$ for all i
- $\sum_{i=1}^n h_{ii} = \text{tr}(\mathbf{H}) = p$
- Average hat value: $\bar{h} = p/n$

**Interpretation**: $h_{ii}$ measures the leverage or influence that observation i has on its own fitted value. High leverage points have the potential to strongly influence the regression.

**Rule of thumb**: Observations with $h_{ii} > 2p/n$ or $h_{ii} > 3p/n$ are considered high leverage.

:::{.callout-note}
## Cross-Reference

Hat values are used extensively in **Week 11: Model Diagnostics** to identify influential observations.
:::

#### R Implementation

```{r}
#| label: projection-example

# Example: Simple regression with 5 observations
# Dairy cow milk yield (kg/day) vs days in milk

days <- c(30, 60, 90, 120, 150)
yield <- c(28, 32, 30, 27, 24)

# Build design matrix
X <- cbind(1, days)  # Intercept and slope
n <- nrow(X)
p <- ncol(X)

cat("Design matrix X:\n")
print(X)

# Compute hat matrix H = X(X'X)^{-1}X'
XtX <- t(X) %*% X
XtX_inv <- solve(XtX)
H <- X %*% XtX_inv %*% t(X)

cat("\nHat matrix H:\n")
print(round(H, 4))

# Verify H is symmetric
cat("\nIs H symmetric?\n")
cat(all.equal(H, t(H)), "\n")

# Verify H is idempotent (H^2 = H)
cat("\nIs H idempotent (H^2 = H)?\n")
H2 <- H %*% H
cat(all.equal(H, H2), "\n")

# Check rank and trace
cat("\nRank of H:", qr(H)$rank, "\n")
cat("Trace of H:", sum(diag(H)), "\n")
cat("Expected (p):", p, "\n")

# Compute I - H
I_minus_H <- diag(n) - H

cat("\nResidual projection matrix (I - H):\n")
print(round(I_minus_H, 4))

# Verify (I-H) is idempotent
I_minus_H2 <- I_minus_H %*% I_minus_H
cat("\nIs (I-H) idempotent?\n")
cat(all.equal(I_minus_H, I_minus_H2), "\n")

# Check trace of I-H
cat("\nTrace of (I-H):", sum(diag(I_minus_H)), "\n")
cat("Expected (n-p):", n - p, "\n")

# Verify orthogonality: H(I-H) = 0
H_times_IminusH <- H %*% I_minus_H
cat("\nIs H(I-H) = 0?\n")
cat(all(abs(H_times_IminusH) < 1e-10), "\n")

# Compute fitted values and residuals
y_hat <- H %*% yield
e <- (I_minus_H) %*% yield

cat("\nFitted values:\n")
print(round(y_hat, 2))

cat("\nResiduals:\n")
print(round(e, 2))

# Verify fitted and residuals are orthogonal
cat("\nAre fitted and residuals orthogonal (sum to 0)?\n")
cat(round(sum(y_hat * e), 10), "\n")

# Hat values (leverage)
h_ii <- diag(H)
cat("\nHat values (leverage):\n")
print(round(h_ii, 4))

cat("\nAverage leverage:", p/n, "\n")
cat("High leverage threshold (2p/n):", 2*p/n, "\n")
cat("Observations with high leverage:", which(h_ii > 2*p/n), "\n")
```

#### Extended Example: Multiple Regression

```{r}
#| label: projection-multiple

# Beef cattle: ADG (kg/day) vs initial weight (kg) and days on feed
# n = 8 steers

initial_wt <- c(250, 260, 245, 270, 255, 265, 240, 275)
days_feed <- c(120, 120, 120, 120, 150, 150, 150, 150)
adg <- c(1.2, 1.3, 1.1, 1.4, 1.0, 1.1, 0.9, 1.2)

# Design matrix: intercept, initial weight, days on feed
X <- cbind(1, initial_wt, days_feed)
n <- nrow(X)
p <- ncol(X)

cat("Design matrix X (first 4 rows):\n")
print(head(X, 4))

# Compute H
H <- X %*% solve(t(X) %*% X) %*% t(X)

# Verify all properties
cat("\nVerification of H properties:\n")
cat("1. Symmetric:", all.equal(H, t(H)), "\n")
cat("2. Idempotent:", all.equal(H, H %*% H), "\n")
cat("3. Rank:", qr(H)$rank, "= p =", p, "\n")
cat("4. Trace:", round(sum(diag(H)), 6), "= p =", p, "\n")

# Hat values
h_ii <- diag(H)
cat("\nHat values:\n")
print(round(h_ii, 4))

# Identify high leverage points
threshold <- 2 * p / n
cat("\nLeverage threshold (2p/n):", round(threshold, 4), "\n")
high_leverage <- which(h_ii > threshold)
cat("High leverage observations:", high_leverage, "\n")

# Compute fitted and residual
y_hat <- H %*% adg
e <- adg - y_hat

cat("\nModel fit:\n")
cat("R² =", 1 - sum(e^2) / sum((adg - mean(adg))^2), "\n")
cat("Residual sum of squares =", sum(e^2), "\n")
```

:::{.callout-tip}
## Computational Note

In practice, never compute **H** explicitly for large datasets. Instead:

1. Use `lm()` and `hatvalues()` to get leverage values
2. Compute projections via QR decomposition (more stable)
3. For fitted values: $\hat{\mathbf{y}} = \mathbf{X}\mathbf{b}$ (no need for **H**)
4. For residuals: $\mathbf{e} = \mathbf{y} - \mathbf{X}\mathbf{b}$

```{r}
#| label: practical-computation

# Practical approach
fit <- lm(adg ~ initial_wt + days_feed)

# Get leverage directly
h_practical <- hatvalues(fit)
cat("Leverage from lm():\n")
print(round(h_practical, 4))

# Compare with our manual calculation
cat("\nAgreement with manual H:\n")
cat(all.equal(diag(H), h_practical, check.attributes = FALSE), "\n")
```
:::

---

### Quadratic Forms {#sec-quadratic-forms}

A **quadratic form** is a scalar-valued function of a vector obtained by "sandwiching" a matrix between the vector and its transpose:

$$
q = \mathbf{y}'\mathbf{A}\mathbf{y}
$$ {#eq-quadratic-form}

where **y** is n × 1 and **A** is n × n symmetric.

#### Properties of Quadratic Forms

1. **Result is a scalar**: $\mathbf{y}'\mathbf{A}\mathbf{y}$ is a 1 × 1 matrix (scalar)

2. **Symmetry matters**: For $\mathbf{y}'\mathbf{A}\mathbf{y}$ to be well-defined, **A** should be symmetric
   - If **A** is not symmetric, only $(\mathbf{A} + \mathbf{A}')/2$ contributes to the quadratic form

3. **Trace relationship**:
   $$
   \mathbf{y}'\mathbf{A}\mathbf{y} = \text{tr}(\mathbf{A}\mathbf{y}\mathbf{y}')
   $$ {#eq-qf-trace}

4. **Expected value**: If $\mathbf{y} \sim N(\boldsymbol{\mu}, \boldsymbol{\Sigma})$, then:
   $$
   E(\mathbf{y}'\mathbf{A}\mathbf{y}) = \text{tr}(\mathbf{A}\boldsymbol{\Sigma}) + \boldsymbol{\mu}'\mathbf{A}\boldsymbol{\mu}
   $$ {#eq-qf-expectation}

5. **Variance**: If $\mathbf{y} \sim N(\boldsymbol{\mu}, \sigma^2\mathbf{I})$ and **A** is symmetric:
   $$
   \text{Var}(\mathbf{y}'\mathbf{A}\mathbf{y}) = 2\sigma^4 \text{tr}(\mathbf{A}^2) + 4\sigma^2\boldsymbol{\mu}'\mathbf{A}^2\boldsymbol{\mu}
   $$ {#eq-qf-variance}

#### Distribution of Quadratic Forms

**Theorem**: If $\mathbf{y} \sim N(\mathbf{0}, \sigma^2\mathbf{I})$ and **A** is symmetric and idempotent with rank r, then:

$$
\frac{\mathbf{y}'\mathbf{A}\mathbf{y}}{\sigma^2} \sim \chi^2_r
$$ {#eq-qf-chi-squared}

This is **Cochran's Theorem** and is fundamental for hypothesis testing in linear models.

#### Quadratic Forms in Linear Models

Quadratic forms appear everywhere in linear models analysis. The key decomposition is:

$$
\mathbf{y}'\mathbf{y} = \mathbf{y}'\mathbf{H}\mathbf{y} + \mathbf{y}'(\mathbf{I} - \mathbf{H})\mathbf{y}
$$ {#eq-qf-decomposition}

This leads to the sum of squares decomposition:

**Total Sum of Squares (SST)**:
$$
\text{SST} = \mathbf{y}'(\mathbf{I} - n^{-1}\mathbf{1}\mathbf{1}')\mathbf{y} = \sum_{i=1}^n (y_i - \bar{y})^2
$$ {#eq-sst}

where $\mathbf{1}$ is an n × 1 vector of ones and $\bar{y} = n^{-1}\mathbf{1}'\mathbf{y}$.

**Model Sum of Squares (SSM or SSR)**:
$$
\text{SSM} = \mathbf{y}'[\mathbf{H} - n^{-1}\mathbf{1}\mathbf{1}']\mathbf{y} = \sum_{i=1}^n (\hat{y}_i - \bar{y})^2
$$ {#eq-ssm}

Alternatively:
$$
\text{SSM} = \mathbf{b}'\mathbf{X}'\mathbf{y} - n\bar{y}^2
$$ {#eq-ssm-alt}

**Error Sum of Squares (SSE or RSS)**:
$$
\text{SSE} = \mathbf{y}'(\mathbf{I} - \mathbf{H})\mathbf{y} = \mathbf{e}'\mathbf{e} = \sum_{i=1}^n e_i^2
$$ {#eq-sse}

Alternatively:
$$
\text{SSE} = \mathbf{y}'\mathbf{y} - \mathbf{b}'\mathbf{X}'\mathbf{y}
$$ {#eq-sse-alt}

**Decomposition**:
$$
\text{SST} = \text{SSM} + \text{SSE}
$$ {#eq-ss-decomposition}

with degrees of freedom:
- SST: n - 1
- SSM: p - 1 (or p if no intercept)
- SSE: n - p

#### Variance Estimation via Quadratic Forms

The variance estimate is:

$$
\hat{\sigma}^2 = \frac{\text{SSE}}{n - p} = \frac{\mathbf{y}'(\mathbf{I} - \mathbf{H})\mathbf{y}}{n - p}
$$ {#eq-sigma-hat}

Under normality, $\text{SSE}/\sigma^2 \sim \chi^2_{n-p}$, so:

$$
E(\hat{\sigma}^2) = \sigma^2
$$ {#eq-sigma-hat-unbiased}

#### Variance of Estimates

The variance of the least squares estimates **b** is also a quadratic form:

$$
\text{Var}(\mathbf{b}) = (\mathbf{X}'\mathbf{X})^{-1}\sigma^2
$$ {#eq-var-b}

For a linear combination $\mathbf{c}'\mathbf{b}$:

$$
\text{Var}(\mathbf{c}'\mathbf{b}) = \mathbf{c}'(\mathbf{X}'\mathbf{X})^{-1}\mathbf{c} \cdot \sigma^2
$$ {#eq-var-contrast}

This is a quadratic form in **c**.

#### R Implementation

```{r}
#| label: quadratic-forms-example

# Continue with dairy cow example from earlier
days <- c(30, 60, 90, 120, 150)
yield <- c(28, 32, 30, 27, 24)
n <- length(yield)

# Design matrix
X <- cbind(1, days)
p <- ncol(X)

# Compute projection matrices
H <- X %*% solve(t(X) %*% X) %*% t(X)
I_minus_H <- diag(n) - H
J_n <- matrix(1/n, n, n)  # n^{-1}11'

# Demonstrate y'y = y'Hy + y'(I-H)y
yty <- t(yield) %*% yield
ytHy <- t(yield) %*% H %*% yield
yt_IminusH_y <- t(yield) %*% I_minus_H %*% yield

cat("Decomposition of y'y:\n")
cat("y'y =", c(yty), "\n")
cat("y'Hy =", c(ytHy), "\n")
cat("y'(I-H)y =", c(yt_IminusH_y), "\n")
cat("y'Hy + y'(I-H)y =", c(ytHy + yt_IminusH_y), "\n")
cat("Check:", all.equal(c(yty), c(ytHy + yt_IminusH_y)), "\n\n")

# Sum of squares decomposition
y_bar <- mean(yield)

# SST = y'(I - n^{-1}11')y
SST <- t(yield) %*% (diag(n) - J_n) %*% yield
SST_alt <- sum((yield - y_bar)^2)

cat("Total Sum of Squares (SST):\n")
cat("Quadratic form:", c(SST), "\n")
cat("Direct calculation:", SST_alt, "\n")
cat("Check:", all.equal(c(SST), SST_alt), "\n\n")

# SSM = y'(H - n^{-1}11')y
SSM <- t(yield) %*% (H - J_n) %*% yield
y_hat <- H %*% yield
SSM_alt <- sum((y_hat - y_bar)^2)

cat("Model Sum of Squares (SSM):\n")
cat("Quadratic form:", c(SSM), "\n")
cat("Direct calculation:", SSM_alt, "\n")
cat("Check:", all.equal(c(SSM), SSM_alt, tolerance = 1e-10), "\n\n")

# SSE = y'(I-H)y
SSE <- t(yield) %*% I_minus_H %*% yield
e <- (I_minus_H) %*% yield
SSE_alt <- sum(e^2)

cat("Error Sum of Squares (SSE):\n")
cat("Quadratic form:", c(SSE), "\n")
cat("Direct calculation:", SSE_alt, "\n")
cat("Check:", all.equal(c(SSE), SSE_alt), "\n\n")

# Verify SST = SSM + SSE
cat("Sum of Squares Decomposition:\n")
cat("SST =", c(SST), "\n")
cat("SSM + SSE =", c(SSM + SSE), "\n")
cat("Check:", all.equal(c(SST), c(SSM + SSE)), "\n\n")

# Degrees of freedom
df_total <- n - 1
df_model <- p - 1
df_error <- n - p

cat("Degrees of Freedom:\n")
cat("Total:", df_total, "\n")
cat("Model:", df_model, "\n")
cat("Error:", df_error, "\n")
cat("Check:", df_model + df_error, "=", df_total, "\n\n")

# Variance estimate
sigma2_hat <- c(SSE) / df_error
cat("Variance estimate: sigma^2 =", sigma2_hat, "\n\n")

# R-squared
R2 <- c(SSM) / c(SST)
R2_alt <- 1 - c(SSE) / c(SST)
cat("R-squared:\n")
cat("SSM/SST =", R2, "\n")
cat("1 - SSE/SST =", R2_alt, "\n")
```

#### Complete Example: Beef Cattle ADG

```{r}
#| label: qf-complete-example

# Beef cattle: ADG (kg/day) for 8 steers
# Model: ADG ~ breed (Angus vs Hereford, 4 steers each)

adg <- c(1.2, 1.3, 1.1, 1.4,  # Angus
         1.0, 1.1, 0.9, 1.2)  # Hereford
breed <- factor(rep(c("Angus", "Hereford"), each = 4))
n <- length(adg)

# Cell means model: X has 2 columns (one per breed)
X <- model.matrix(~ breed - 1)
p <- ncol(X)

cat("Design matrix X:\n")
print(X)

# Compute all needed matrices
H <- X %*% solve(t(X) %*% X) %*% t(X)
I_minus_H <- diag(n) - H

# Compute sum of squares
y_bar <- mean(adg)
SST <- sum((adg - y_bar)^2)

y_hat <- H %*% adg
SSM <- sum((y_hat - y_bar)^2)

e <- adg - y_hat
SSE <- sum(e^2)

cat("\nANOVA Table:\n")
cat("Source       SS      df    MS       F\n")
cat("-------------------------------------------\n")
cat(sprintf("Breed     %6.4f   %2d   %6.4f   %6.2f\n",
            SSM, p-1, SSM/(p-1), (SSM/(p-1))/(SSE/(n-p))))
cat(sprintf("Error     %6.4f   %2d   %6.4f\n",
            SSE, n-p, SSE/(n-p)))
cat(sprintf("Total     %6.4f   %2d\n", SST, n-1))
cat("-------------------------------------------\n")

# Verify with lm()
fit <- lm(adg ~ breed - 1)
cat("\nComparison with lm():\n")
print(anova(lm(adg ~ breed)))

# Demonstrate quadratic form properties
cat("\n\nQuadratic Form Verification:\n")
cat("1. SSE = y'(I-H)y = e'e:\n")
SSE_qf <- c(t(adg) %*% I_minus_H %*% adg)
SSE_ee <- c(t(e) %*% e)
cat("   Quadratic form:", SSE_qf, "\n")
cat("   e'e:", SSE_ee, "\n")

cat("\n2. SSM = y'(H - n^{-1}11')y:\n")
J_n <- matrix(1/n, n, n)
SSM_qf <- c(t(adg) %*% (H - J_n) %*% adg)
cat("   Quadratic form:", SSM_qf, "\n")
cat("   Sum of squared deviations:", SSM, "\n")

cat("\n3. SST = SSM + SSE:\n")
cat("   SST:", SST, "\n")
cat("   SSM + SSE:", SSM + SSE, "\n")
cat("   Agreement:", all.equal(SST, SSM + SSE), "\n")
```

:::{.callout-important}
## Key Takeaways

1. **Hat matrix H** projects **y** onto the column space of **X** to give fitted values
2. **I - H** projects onto the orthogonal complement to give residuals
3. **Both are idempotent and symmetric** - essential for distribution theory
4. **Quadratic forms** express sums of squares in matrix notation
5. **Sum of squares decomposition** SST = SSM + SSE follows from orthogonality of projections
6. **Cochran's Theorem** provides the distributional basis for F-tests and t-tests

These concepts are the mathematical foundation of **everything** in linear models!
:::

:::{.callout-note}
## Cross-References

- **Week 5: Least Squares Theory** - Full derivation of these results
- **Week 6: Multiple Regression** - Application to multiple predictors
- **Week 7-10: ANOVA models** - Using quadratic forms for hypothesis tests
- **Week 11: Model Diagnostics** - Leverage values and residual analysis
:::

---

## Eigenvalues and Eigenvectors {#sec-eigenvalues}

### Definitions {#sec-eigen-definitions}

**Definition:**

Let **A** be an n × n square matrix. A scalar λ (lambda) is an **eigenvalue** of **A** if there exists a non-zero vector **v** such that:

$$
\mathbf{Av} = \lambda\mathbf{v}
$$ {#eq-eigenvalue-def}

The vector **v** is called an **eigenvector** corresponding to eigenvalue λ.

**Geometric Interpretation:**

When **A** acts on eigenvector **v**, it simply scales **v** by factor λ (no rotation, just scaling).

**Characteristic Equation:**

Rearranging: $\mathbf{Av} = \lambda\mathbf{v} \Rightarrow (\mathbf{A} - \lambda\mathbf{I})\mathbf{v} = \mathbf{0}$

For non-trivial solution (**v** ≠ 0), the matrix $(\mathbf{A} - \lambda\mathbf{I})$ must be singular:

$$
\det(\mathbf{A} - \lambda\mathbf{I}) = 0
$$ {#eq-characteristic}

This **characteristic equation** is a polynomial of degree n in λ, yielding n eigenvalues (counting multiplicity).

**Key Properties:**

1. **Trace**: $\sum_{i=1}^{n} \lambda_i = \text{tr}(\mathbf{A})$ (sum of eigenvalues equals trace)
2. **Determinant**: $\prod_{i=1}^{n} \lambda_i = \det(\mathbf{A})$ (product of eigenvalues equals determinant)
3. **Eigenvectors for distinct eigenvalues**: Linearly independent
4. **Matrix power**: If $\mathbf{Av} = \lambda\mathbf{v}$, then $\mathbf{A}^k\mathbf{v} = \lambda^k\mathbf{v}$

**R Implementation:**

```{r}
#| label: eigen-basic

# Simple 2x2 example
A <- matrix(c(4, 1, 2, 3), nrow = 2, ncol = 2, byrow = TRUE)

cat("Matrix A:\n")
print(A)

# Compute eigenvalues and eigenvectors
eigen_A <- eigen(A)

eigenvalues <- eigen_A$values
eigenvectors <- eigen_A$vectors

cat("\nEigenvalues:\n")
print(eigenvalues)

cat("\nEigenvectors (as columns):\n")
print(eigenvectors)

# Verify: Av = λv for first eigenvalue
lambda1 <- eigenvalues[1]
v1 <- eigenvectors[, 1]

Av1 <- A %*% v1
lambda_v1 <- lambda1 * v1

cat(sprintf("\nVerify Av = λv for λ₁ = %.4f:\n", lambda1))
cat("Av₁:\n")
print(Av1)
cat("λ₁v₁:\n")
print(lambda_v1)

cat("\nAre they equal?\n")
all.equal(Av1, lambda_v1)
```

**Characteristic Equation Example:**

```{r}
#| label: eigen-characteristic

# 2×2 matrix
A <- matrix(c(5, 2, 2, 5), nrow = 2, ncol = 2, byrow = TRUE)

cat("Matrix A:\n")
print(A)

# Characteristic equation: det(A - λI) = 0
# For 2×2: det([5-λ, 2; 2, 5-λ]) = (5-λ)² - 4 = 0
# λ² - 10λ + 21 = 0
# (λ-7)(λ-3) = 0
# λ = 7 or λ = 3

eigen_A <- eigen(A)
cat("\nEigenvalues (computed):\n")
print(eigen_A$values)

cat("\nManual calculation:\n")
cat("det(A - λI) = (5-λ)² - 4 = λ² - 10λ + 21 = 0\n")
cat("Solutions: λ = 7 and λ = 3\n")

# Verify trace and determinant properties
cat(sprintf("\nSum of eigenvalues = %.1f\n", sum(eigen_A$values)))
cat(sprintf("Trace of A = %.1f\n", sum(diag(A))))

cat(sprintf("\nProduct of eigenvalues = %.1f\n", prod(eigen_A$values)))
cat(sprintf("Determinant of A = %.1f\n", det(A)))
```

**Finding Eigenvectors:**

```{r}
#| label: eigen-vectors

# Using eigenvalue λ = 7, find eigenvector
A <- matrix(c(5, 2, 2, 5), nrow = 2, ncol = 2, byrow = TRUE)
lambda <- 7

# Solve (A - λI)v = 0
A_minus_lambdaI <- A - lambda * diag(2)

cat("A - 7I:\n")
print(A_minus_lambdaI)

# Null space of this matrix gives eigenvector
# Row 1: -2v₁ + 2v₂ = 0 → v₁ = v₂
# So v = [1, 1]' (or any multiple)

v <- c(1, 1)
cat("\nEigenvector v = [1, 1]':\n")
print(v)

# Verify Av = 7v
Av <- A %*% v
cat("\nAv:\n")
print(Av)

cat("\n7v:\n")
print(7 * v)

cat("\nAv = 7v ✓\n")
```

**Livestock Example - Growth Model:**

Eigenvalues describe long-term behavior of dynamic systems.

```{r}
#| label: eigen-population

# Simplified population model: 2 age classes (juveniles, adults)
# Transition matrix: rows = next state, columns = current state
A <- matrix(c(
  0.2, 2.0,   # Juveniles: 20% survive, 2 offspring per adult
  0.5, 0.8    # Adults: 50% juveniles mature, 80% adults survive
), nrow = 2, ncol = 2, byrow = TRUE)

cat("Population transition matrix A:\n")
print(A)

# Eigenvalues
eigen_A <- eigen(A)
lambda_max <- max(eigen_A$values)

cat(sprintf("\nDominant eigenvalue: %.4f\n", lambda_max))

if (lambda_max > 1) {
  cat("λ > 1: Population grows\n")
} else if (lambda_max < 1) {
  cat("λ < 1: Population declines\n")
} else {
  cat("λ = 1: Population stable\n")
}

cat(sprintf("\nGrowth rate: %.2f%% per time period\n", (lambda_max - 1) * 100))

# Stable age distribution (eigenvector for dominant eigenvalue)
v_stable <- eigen_A$vectors[, 1]
v_stable <- v_stable / sum(v_stable)  # Normalize to sum to 1

cat("\nStable age distribution:\n")
cat(sprintf("  Juveniles: %.2f%%\n", v_stable[1] * 100))
cat(sprintf("  Adults: %.2f%%\n", v_stable[2] * 100))
```

**Diagonalization:**

If **A** has n linearly independent eigenvectors, we can write:

$$
\mathbf{A} = \mathbf{V\Lambda V}^{-1}
$$ {#eq-diagonalization}

where:
- **V** = matrix with eigenvectors as columns
- **Λ** = diagonal matrix with eigenvalues on diagonal

```{r}
#| label: eigen-diagonalization

A <- matrix(c(4, 1, 2, 3), nrow = 2, ncol = 2, byrow = TRUE)

eigen_A <- eigen(A)
V <- eigen_A$vectors
Lambda <- diag(eigen_A$values)

cat("Matrix A:\n")
print(A)

cat("\nEigenvector matrix V:\n")
print(V)

cat("\nEigenvalue matrix Λ:\n")
print(Lambda)

# Reconstruct A
A_reconstructed <- V %*% Lambda %*% solve(V)

cat("\nReconstructed A = VΛV⁻¹:\n")
print(A_reconstructed)

cat("\nVerify reconstruction:\n")
all.equal(A, A_reconstructed)
```

**Matrix Powers via Eigenvalues:**

Computing **A**^k^ is easy with diagonalization: $\mathbf{A}^k = \mathbf{V\Lambda}^k\mathbf{V}^{-1}$

```{r}
#| label: eigen-powers

A <- matrix(c(0.8, 0.2, 0.3, 0.7), nrow = 2, ncol = 2, byrow = TRUE)

cat("Transition matrix A:\n")
print(A)

# Compute A^10 directly (slow for large k)
A10_direct <- A
for (i in 1:9) A10_direct <- A10_direct %*% A

cat("\nA^10 (direct calculation):\n")
print(round(A10_direct, 4))

# Compute A^10 via eigendecomposition (fast!)
eigen_A <- eigen(A)
V <- eigen_A$vectors
Lambda <- diag(eigen_A$values)

Lambda10 <- diag(eigen_A$values^10)  # Just raise eigenvalues to power 10
A10_eigen <- V %*% Lambda10 %*% solve(V)

cat("\nA^10 (via eigenvalues):\n")
print(round(A10_eigen, 4))

cat("\nVerify both methods agree:\n")
all.equal(A10_direct, A10_eigen)

cat("\nNote: As k → ∞, matrix stabilizes (dominant eigenvalue < 1)\n")
```

:::{.callout-note}
## Cross-References

Eigenvalues and eigenvectors appear in:

- **Week 2**: Conceptual introduction
- **Week 14**: Variance components in genetic evaluation
- **Section @sec-symmetric-eigen**: Special properties for symmetric matrices
- **Section @sec-positive-definite**: Positive definite matrices (all eigenvalues > 0)
- **Section @sec-svd**: Singular value decomposition
:::

---

### Properties for Symmetric Matrices {#sec-symmetric-eigen}

Symmetric matrices have **very special** eigenvalue properties that make them particularly important in linear models.

**Spectral Theorem for Symmetric Matrices:**

If **A** is a real symmetric matrix (n × n), then:

1. **All eigenvalues are real** (no complex numbers!)
2. **Eigenvectors corresponding to distinct eigenvalues are orthogonal**
3. **A** can be diagonalized by an orthogonal matrix: $\mathbf{A} = \mathbf{Q\Lambda Q}'$

where:
- **Q** is orthogonal ($\mathbf{Q}'\mathbf{Q} = \mathbf{I}$)
- **Λ** is diagonal with eigenvalues

This is called the **spectral decomposition** or **eigenvalue decomposition**.

$$
\mathbf{A} = \mathbf{Q\Lambda Q}' = \sum_{i=1}^{n} \lambda_i \mathbf{q}_i\mathbf{q}_i'
$$ {#eq-spectral-decomposition}

**Why This Matters for Linear Models:**

Since $\mathbf{X}'\mathbf{X}$ is symmetric, it has:
- Real eigenvalues only
- Orthogonal eigenvectors
- Clean spectral decomposition

**R Implementation:**

```{r}
#| label: symmetric-eigen-basic

# Symmetric matrix
A <- matrix(c(4, 2, 1,
              2, 5, 3,
              1, 3, 6), nrow = 3, ncol = 3, byrow = TRUE)

cat("Symmetric matrix A:\n")
print(A)

# Verify symmetry
cat("\nIs A symmetric?", isTRUE(all.equal(A, t(A))), "\n")

# Eigenvalues and eigenvectors
eigen_A <- eigen(A)

cat("\nEigenvalues (all real!):\n")
print(eigen_A$values)

cat("\nEigenvectors Q:\n")
Q <- eigen_A$vectors
print(round(Q, 4))

# Verify Q is orthogonal
cat("\nVerify Q'Q = I:\n")
QtQ <- t(Q) %*% Q
print(round(QtQ, 10))

# Verify Q orthogonal
cat("\nIs Q orthogonal?", isTRUE(all.equal(QtQ, diag(3))), "\n")
```

**Orthogonality of Eigenvectors:**

For symmetric matrices, eigenvectors for different eigenvalues are orthogonal.

```{r}
#| label: symmetric-eigen-orthogonal

# Symmetric matrix
A <- matrix(c(3, 1, 1, 3), nrow = 2, ncol = 2, byrow = TRUE)

cat("Symmetric matrix A:\n")
print(A)

eigen_A <- eigen(A)

cat("\nEigenvalues:\n")
print(eigen_A$values)

v1 <- eigen_A$vectors[, 1]
v2 <- eigen_A$vectors[, 2]

cat("\nEigenvector 1:\n")
print(v1)

cat("\nEigenvector 2:\n")
print(v2)

# Check orthogonality
cat(sprintf("\nv1'v2 = %.10f (should be 0)\n", sum(v1 * v2)))

# Check unit length
cat(sprintf("||v1|| = %.4f\n", sqrt(sum(v1^2))))
cat(sprintf("||v2|| = %.4f\n", sqrt(sum(v2^2))))

cat("\nEigenvectors are orthonormal (orthogonal + unit length)\n")
```

**Spectral Decomposition:**

$\mathbf{A} = \mathbf{Q\Lambda Q}'$ where **Q'** = **Q**^(-1)^

```{r}
#| label: symmetric-eigen-spectral

A <- matrix(c(4, 2, 2, 3), nrow = 2, ncol = 2, byrow = TRUE)

cat("Symmetric matrix A:\n")
print(A)

eigen_A <- eigen(A)
Q <- eigen_A$vectors
Lambda <- diag(eigen_A$values)

cat("\nOrthogonal matrix Q:\n")
print(round(Q, 4))

cat("\nEigenvalue matrix Λ:\n")
print(round(Lambda, 4))

# Reconstruct A using A = QΛQ'
A_reconstructed <- Q %*% Lambda %*% t(Q)

cat("\nReconstructed A = QΛQ':\n")
print(round(A_reconstructed, 4))

cat("\nVerify reconstruction:\n")
all.equal(A, A_reconstructed)

cat("\nNote: For symmetric matrices, Q' = Q⁻¹ (transpose = inverse!)\n")
```

**Livestock Example - Covariance Matrix:**

Covariance matrices are symmetric, so they have real eigenvalues and orthogonal eigenvectors.

```{r}
#| label: symmetric-eigen-covariance

# Broiler data: body weight (kg), breast yield (proportion), leg yield
# Covariance matrix (realistic values)
Sigma <- matrix(c(
  0.16,  0.04,  0.03,  # Var(BW) = 0.16, Cov(BW, Breast) = 0.04, Cov(BW, Leg) = 0.03
  0.04,  0.02,  0.01,  # Cov(Breast, BW) = 0.04, Var(Breast) = 0.02, Cov(Breast, Leg) = 0.01
  0.03,  0.01,  0.02   # Cov(Leg, BW) = 0.03, Cov(Leg, Breast) = 0.01, Var(Leg) = 0.02
), nrow = 3, ncol = 3, byrow = TRUE)

colnames(Sigma) <- rownames(Sigma) <- c("BodyWt", "Breast", "Leg")

cat("Covariance matrix Σ:\n")
print(round(Sigma, 4))

# Eigendecomposition
eigen_Sigma <- eigen(Sigma)

cat("\nEigenvalues (variances of principal components):\n")
print(round(eigen_Sigma$values, 4))

cat("\nEigenvectors (loadings of principal components):\n")
PC_loadings <- eigen_Sigma$vectors
colnames(PC_loadings) <- c("PC1", "PC2", "PC3")
rownames(PC_loadings) <- c("BodyWt", "Breast", "Leg")
print(round(PC_loadings, 4))

# Proportion of variance explained
prop_var <- eigen_Sigma$values / sum(eigen_Sigma$values)

cat("\nProportion of variance explained:\n")
cat(sprintf("  PC1: %.2f%%\n", prop_var[1] * 100))
cat(sprintf("  PC2: %.2f%%\n", prop_var[2] * 100))
cat(sprintf("  PC3: %.2f%%\n", prop_var[3] * 100))

cat("\nThis is the basis of Principal Component Analysis (PCA)\n")
```

**Application to X'X Matrix:**

```{r}
#| label: symmetric-eigen-xtx

# Design matrix for simple regression
X <- cbind(1, c(1, 2, 3, 4, 5))

cat("Design matrix X (5×2):\n")
print(X)

# Form X'X
XtX <- t(X) %*% X

cat("\nX'X (symmetric):\n")
print(XtX)

# Eigendecomposition
eigen_XtX <- eigen(XtX)

cat("\nEigenvalues of X'X:\n")
print(round(eigen_XtX$values, 4))

cat("\nBoth eigenvalues > 0 (X'X is positive definite)\n")

# Condition number (ratio of largest to smallest eigenvalue)
kappa <- max(eigen_XtX$values) / min(eigen_XtX$values)
cat(sprintf("\nCondition number κ(X'X) = %.2f\n", kappa))

if (kappa < 30) {
  cat("Well-conditioned matrix (good numerical stability)\n")
} else if (kappa < 1000) {
  cat("Moderately conditioned (acceptable)\n")
} else {
  cat("Ill-conditioned (numerical problems likely)\n")
}
```

**Sum of Outer Products Form:**

The spectral decomposition can be written as a sum of rank-1 matrices:

$$
\mathbf{A} = \sum_{i=1}^{n} \lambda_i \mathbf{q}_i\mathbf{q}_i'
$$ {#eq-outer-product-sum}

```{r}
#| label: symmetric-eigen-outer

A <- matrix(c(5, 2, 2, 2), nrow = 2, ncol = 2, byrow = TRUE)

cat("Symmetric matrix A:\n")
print(A)

eigen_A <- eigen(A)

# Reconstruct using sum of outer products
lambda1 <- eigen_A$values[1]
lambda2 <- eigen_A$values[2]
q1 <- eigen_A$vectors[, 1]
q2 <- eigen_A$vectors[, 2]

term1 <- lambda1 * (q1 %*% t(q1))
term2 <- lambda2 * (q2 %*% t(q2))

cat("\nλ₁q₁q₁' (first term):\n")
print(round(term1, 4))

cat("\nλ₂q₂q₂' (second term):\n")
print(round(term2, 4))

A_reconstructed <- term1 + term2

cat("\nA = λ₁q₁q₁' + λ₂q₂q₂':\n")
print(round(A_reconstructed, 4))

cat("\nVerify reconstruction:\n")
all.equal(A, A_reconstructed)
```

**Numerical Verification:**

```{r}
#| label: symmetric-eigen-verify

# Random symmetric matrix
set.seed(123)
B <- matrix(rnorm(9), nrow = 3)
A <- (B + t(B)) / 2  # Force symmetry

cat("Random symmetric matrix A:\n")
print(round(A, 4))

eigen_A <- eigen(A)

# Check all eigenvalues are real (imaginary part = 0)
cat("\nImaginary parts of eigenvalues (should all be 0):\n")
print(Im(eigen_A$values))

cat("\nAll eigenvalues are real ✓\n")

# Check eigenvectors are orthogonal
Q <- eigen_A$vectors
QtQ <- t(Q) %*% Q

cat("\nQ'Q (should be identity):\n")
print(round(QtQ, 10))

cat("\nEigenvectors are orthogonal ✓\n")

# Verify spectral decomposition
Lambda <- diag(eigen_A$values)
A_check <- Q %*% Lambda %*% t(Q)

cat("\nVerify A = QΛQ':\n")
all.equal(A, A_check)
```

:::{.callout-important}
## Critical for X'X in Linear Models

Since $\mathbf{X}'\mathbf{X}$ is symmetric:

1. **All eigenvalues are real** (no complex numbers to worry about)
2. **Eigenvectors are orthogonal** (clean geometry)
3. **Spectral decomposition**: $\mathbf{X}'\mathbf{X} = \mathbf{Q\Lambda Q}'$
4. **Condition number**: $\kappa = \lambda_{\max}/\lambda_{\min}$ measures numerical stability
5. **Rank = number of non-zero eigenvalues**

If any eigenvalue is zero, $\mathbf{X}'\mathbf{X}$ is singular (rank deficient).
:::

:::{.callout-note}
## Cross-References

Properties of symmetric matrices are central to:

- **Week 2**: Understanding structure of $\mathbf{X}'\mathbf{X}$
- **Week 5**: Variance-covariance matrix $\text{Var}(\mathbf{b}) = (\mathbf{X}'\mathbf{X})^{-1}\sigma^2$
- **Week 11**: Condition number and numerical stability
- **Section @sec-positive-definite**: When all eigenvalues > 0
- **Section @sec-computation**: Numerical considerations for eigenvalue computation
:::

---

### Positive Definite Matrices {#sec-positive-definite}

**Definition:**

A symmetric matrix **A** (n × n) is **positive definite** if:

$$
\mathbf{x}'\mathbf{Ax} > 0 \quad \text{for all non-zero vectors } \mathbf{x}
$$ {#eq-positive-definite}

The quadratic form $\mathbf{x}'\mathbf{Ax}$ is always strictly positive (except when **x** = 0).

**Related Definitions:**

- **Positive semi-definite**: $\mathbf{x}'\mathbf{Ax} \geq 0$ for all **x** (allows zero)
- **Negative definite**: $\mathbf{x}'\mathbf{Ax} < 0$ for all non-zero **x**
- **Indefinite**: $\mathbf{x}'\mathbf{Ax}$ can be positive or negative depending on **x**

**Equivalent Characterizations:**

For a symmetric matrix **A**, the following are **equivalent**:

1. **A** is positive definite
2. All eigenvalues of **A** are positive: $\lambda_i > 0$ for all i
3. All leading principal minors are positive (Sylvester's criterion)
4. There exists a non-singular matrix **B** such that $\mathbf{A} = \mathbf{B}'\mathbf{B}$
5. **A** is invertible (det(**A**) > 0)

**Why This Matters for Linear Models:**

For full-rank design matrix **X**, the matrix $\mathbf{X}'\mathbf{X}$ is **positive definite**:
- All eigenvalues > 0
- Invertible (unique solution exists!)
- Numerically stable

**R Implementation:**

```{r}
#| label: positive-definite-basic

# Positive definite matrix
A <- matrix(c(4, 1, 1, 3), nrow = 2, ncol = 2, byrow = TRUE)

cat("Matrix A:\n")
print(A)

# Check eigenvalues
eigen_A <- eigen(A)
cat("\nEigenvalues:\n")
print(eigen_A$values)

all_positive <- all(eigen_A$values > 0)
cat(sprintf("\nAll eigenvalues > 0? %s\n", all_positive))

if (all_positive) {
  cat("A is positive definite!\n")
}

# Test quadratic form for several vectors
test_vectors <- list(
  c(1, 0),
  c(0, 1),
  c(1, 1),
  c(1, -1),
  c(2, 3)
)

cat("\nQuadratic forms x'Ax:\n")
for (i in seq_along(test_vectors)) {
  x <- test_vectors[[i]]
  qf <- t(x) %*% A %*% x
  cat(sprintf("x = [%d, %d]: x'Ax = %.4f\n", x[1], x[2], qf[1,1]))
}

cat("\nAll values > 0 (positive definite) ✓\n")
```

**Cholesky Decomposition:**

Every positive definite matrix **A** can be uniquely factored as:

$$
\mathbf{A} = \mathbf{L}\mathbf{L}'
$$ {#eq-cholesky}

where **L** is lower triangular with positive diagonal elements.

This is the **Cholesky decomposition** - extremely useful for solving linear systems efficiently.

```{r}
#| label: positive-definite-cholesky

A <- matrix(c(4, 2, 2, 5), nrow = 2, ncol = 2, byrow = TRUE)

cat("Positive definite matrix A:\n")
print(A)

# Cholesky decomposition
L <- chol(A)  # Note: R's chol() returns UPPER triangular
# So we need to transpose to get lower triangular

L_lower <- t(L)  # Lower triangular

cat("\nLower triangular matrix L:\n")
print(round(L_lower, 4))

# Verify A = LL'
A_reconstructed <- L_lower %*% t(L_lower)

cat("\nReconstructed A = LL':\n")
print(round(A_reconstructed, 4))

cat("\nVerify reconstruction:\n")
all.equal(A, A_reconstructed)

cat("\nCholesky is fast and numerically stable for positive definite matrices\n")
```

**Testing Positive Definiteness:**

```{r}
#| label: positive-definite-test

# Function to test positive definiteness
is_positive_definite <- function(A, tol = 1e-8) {
  # Check symmetry
  if (!isTRUE(all.equal(A, t(A)))) {
    cat("Not symmetric!\n")
    return(FALSE)
  }

  # Check eigenvalues
  eigenvalues <- eigen(A, symmetric = TRUE, only.values = TRUE)$values

  if (all(eigenvalues > tol)) {
    return(TRUE)
  } else {
    return(FALSE)
  }
}

# Test various matrices
A1 <- matrix(c(2, 1, 1, 2), nrow = 2, ncol = 2, byrow = TRUE)
A2 <- matrix(c(2, 3, 3, 2), nrow = 2, ncol = 2, byrow = TRUE)  # Not PD
A3 <- matrix(c(1, 0, 0, 1), nrow = 2, ncol = 2, byrow = TRUE)  # Identity (PD)
A4 <- matrix(c(1, 1, 1, 1), nrow = 2, ncol = 2, byrow = TRUE)  # Singular (not PD)

cat("Matrix A1:\n")
print(A1)
cat("Eigenvalues:", eigen(A1)$values, "\n")
cat("Positive definite?", is_positive_definite(A1), "\n\n")

cat("Matrix A2:\n")
print(A2)
cat("Eigenvalues:", round(eigen(A2)$values, 4), "\n")
cat("Positive definite?", is_positive_definite(A2), "\n\n")

cat("Matrix A3 (Identity):\n")
print(A3)
cat("Eigenvalues:", eigen(A3)$values, "\n")
cat("Positive definite?", is_positive_definite(A3), "\n\n")

cat("Matrix A4 (Singular):\n")
print(A4)
cat("Eigenvalues:", eigen(A4)$values, "\n")
cat("Positive definite?", is_positive_definite(A4), "\n")
```

**Positive Semi-Definite:**

$\mathbf{x}'\mathbf{Ax} \geq 0$ (allows zero). Eigenvalues $\geq$ 0 (some may be zero).

```{r}
#| label: positive-semidefinite

# Positive semi-definite (but not positive definite)
A <- matrix(c(1, 1, 1, 1), nrow = 2, ncol = 2, byrow = TRUE)

cat("Matrix A:\n")
print(A)

eigen_A <- eigen(A)
cat("\nEigenvalues:\n")
print(eigen_A$values)

cat("\nOne eigenvalue = 0 (positive semi-definite but not positive definite)\n")
cat("This matrix is singular (not invertible)\n")

# Rank
cat(sprintf("Rank: %d (less than 2)\n", qr(A)$rank))
```

**Livestock Example - X'X is Positive Definite:**

```{r}
#| label: positive-definite-xtx

# Full-rank design matrix
X <- cbind(1, c(1, 2, 3, 4, 5), c(2, 3, 1, 5, 4))

cat("Design matrix X (5×3):\n")
print(X)

cat(sprintf("\nRank of X: %d (full rank!)\n", qr(X)$rank))

# Form X'X
XtX <- t(X) %*% X

cat("\nX'X (3×3):\n")
print(XtX)

# Check positive definiteness
eigen_XtX <- eigen(XtX)
cat("\nEigenvalues of X'X:\n")
print(round(eigen_XtX$values, 4))

cat("\nAll eigenvalues > 0 ✓\n")
cat("X'X is positive definite (because X has full column rank)\n")

# This guarantees unique solution to normal equations
cat("\nConsequence: Normal equations X'Xb = X'y have unique solution\n")
```

**Condition Number:**

For positive definite matrices, the **condition number** measures numerical stability:

$$
\kappa(\mathbf{A}) = \frac{\lambda_{\max}}{\lambda_{\min}}
$$ {#eq-condition-number}

- $\kappa = 1$: Perfectly conditioned (e.g., identity matrix)
- $\kappa < 30$: Well-conditioned
- $\kappa > 1000$: Ill-conditioned (numerical problems)

```{r}
#| label: positive-definite-condition

# Well-conditioned matrix
A1 <- diag(c(10, 9, 8))

eigen_A1 <- eigen(A1)
kappa1 <- max(eigen_A1$values) / min(eigen_A1$values)

cat("Well-conditioned matrix A1:\n")
print(A1)
cat(sprintf("\nCondition number: %.2f (well-conditioned)\n", kappa1))

# Ill-conditioned matrix
A2 <- matrix(c(1, 0.999, 0.999, 1), nrow = 2, ncol = 2, byrow = TRUE)

eigen_A2 <- eigen(A2)
kappa2 <- max(eigen_A2$values) / min(eigen_A2$values)

cat("\n\nIll-conditioned matrix A2:\n")
print(A2)
cat(sprintf("\nCondition number: %.2f (ill-conditioned!)\n", kappa2))

cat("\nHigh condition number means:\n")
cat("  - Small changes in data cause large changes in solution\n")
cat("  - Numerical errors magnified\n")
cat("  - Avoid if possible (center predictors, remove collinearity)\n")
```

**Livestock Example - Multicollinearity:**

Highly correlated predictors lead to near-singular $\mathbf{X}'\mathbf{X}$ (large condition number).

```{r}
#| label: positive-definite-collinearity

# Swine data: body weight and two highly correlated measures
# x1 = backfat (mm), x2 = backfat measured at different location (highly correlated!)
x1 <- c(10, 12, 11, 13, 14)
x2 <- x1 + rnorm(5, 0, 0.1)  # Nearly identical to x1

X <- cbind(1, x1, x2)

cat("Design matrix X with multicollinearity:\n")
print(round(X, 2))

cat("\nCorrelation between x1 and x2:\n")
cat(sprintf("r = %.4f (very high!)\n", cor(x1, x2)))

XtX <- t(X) %*% X

cat("\nX'X:\n")
print(round(XtX, 2))

# Check condition number
eigen_XtX <- eigen(XtX)
kappa <- max(eigen_XtX$values) / min(eigen_XtX$values)

cat("\nEigenvalues:\n")
print(round(eigen_XtX$values, 4))

cat(sprintf("\nCondition number: %.2f\n", kappa))

if (kappa > 30) {
  cat("WARNING: Ill-conditioned matrix due to multicollinearity!\n")
  cat("Solution:\n")
  cat("  - Remove one of the highly correlated predictors\n")
  cat("  - Use ridge regression\n")
  cat("  - Use principal components\n")
}
```

**Using Cholesky for Solving Linear Systems:**

For positive definite **A**, solve $\mathbf{Ax = b}$ using Cholesky:

1. Factor: $\mathbf{A = LL}'$
2. Solve $\mathbf{Ly = b}$ (forward substitution)
3. Solve $\mathbf{L'x = y}$ (backward substitution)

This is **faster** and **more stable** than direct inversion.

```{r}
#| label: positive-definite-solve

# Positive definite system
A <- matrix(c(4, 2, 2, 5), nrow = 2, ncol = 2, byrow = TRUE)
b <- c(6, 8)

cat("System Ax = b:\n")
cat("A:\n")
print(A)
cat("b:", b, "\n")

# Method 1: Direct inverse (slower, less stable)
x1 <- solve(A) %*% b
cat("\nSolution via A^(-1)b:\n")
print(x1)

# Method 2: Cholesky (faster, more stable)
L <- t(chol(A))  # Lower triangular

# Forward substitution: Ly = b
y <- forwardsolve(L, b)

# Backward substitution: L'x = y
x2 <- backsolve(t(L), y)

cat("\nSolution via Cholesky:\n")
print(x2)

cat("\nBoth methods give same result:\n")
all.equal(x1, x2, check.attributes = FALSE)

cat("\nBut Cholesky is faster for large systems!\n")
```

**Livestock Example - Variance-Covariance Matrix:**

Variance-covariance matrices are always positive semi-definite (positive definite if full rank).

```{r}
#| label: positive-definite-variance

# Multi-trait data: 10 broilers, 3 traits
set.seed(456)
n <- 10
traits <- matrix(c(
  rnorm(n, 2.5, 0.3),   # Body weight (kg)
  rnorm(n, 0.85, 0.05),  # Breast yield (proportion)
  rnorm(n, 0.45, 0.03)   # Leg yield (proportion)
), nrow = n, ncol = 3)

colnames(traits) <- c("BodyWt", "Breast", "Leg")

cat("First 5 observations:\n")
print(round(head(traits, 5), 3))

# Covariance matrix
Sigma <- cov(traits)

cat("\nCovariance matrix Σ:\n")
print(round(Sigma, 4))

# Check positive definiteness
eigen_Sigma <- eigen(Sigma)

cat("\nEigenvalues:\n")
print(round(eigen_Sigma$values, 6))

cat("\nAll eigenvalues > 0 ✓\n")
cat("Σ is positive definite (sample covariance with n > p)\n")

# Cholesky decomposition
L <- t(chol(Sigma))

cat("\nCholesky factor L:\n")
print(round(L, 4))

cat("\nThis allows us to simulate correlated data: X = LZ where Z ~ N(0, I)\n")
```

:::{.callout-important}
## X'X and Positive Definiteness

For linear models:

1. **Full rank X** → $\mathbf{X}'\mathbf{X}$ is **positive definite**
   - All eigenvalues > 0
   - Invertible (unique solution exists)
   - Use Cholesky for fast, stable solution

2. **Rank deficient X** → $\mathbf{X}'\mathbf{X}$ is **positive semi-definite**
   - Some eigenvalues = 0
   - Not invertible (need generalized inverse)
   - Infinite solutions (use constraints)

**Rule**: Check eigenvalues of $\mathbf{X}'\mathbf{X}$ to detect rank deficiency!
:::

:::{.callout-note}
## Cross-References

Positive definite matrices appear in:

- **Week 2**: Understanding when $\mathbf{X}'\mathbf{X}$ is invertible
- **Week 5**: Variance-covariance matrix $\text{Var}(\mathbf{b})$ is positive definite
- **Week 11**: Condition number for numerical stability
- **Week 12**: Rank deficiency and positive semi-definite matrices
- **Section @sec-computation**: Cholesky decomposition for solving systems
:::

---

### Singular Value Decomposition (SVD) {#sec-svd}

**Definition:**

Every m × n matrix **A** (rectangular or square) can be decomposed as:

$$
\mathbf{A} = \mathbf{U\Sigma V}'
$$ {#eq-svd}

where:
- **U** is m × m orthogonal matrix (left singular vectors)
- **Σ** is m × n diagonal matrix with singular values σ₁ ≥ σ₂ ≥ ... ≥ σᵣ ≥ 0
- **V** is n × n orthogonal matrix (right singular vectors)

**Key Properties:**

1. **Always exists** (even for non-square, rank-deficient matrices!)
2. **Singular values**: σᵢ² are eigenvalues of $\mathbf{A}'\mathbf{A}$ (or $\mathbf{AA}'$)
3. **Rank**: r(**A**) = number of non-zero singular values
4. **Condition number**: κ(**A**) = σ₁/σᵣ (ratio of largest to smallest non-zero singular value)
5. **Moore-Penrose inverse**: $\mathbf{A}^+ = \mathbf{V\Sigma}^+\mathbf{U}'$ where Σ⁺ has 1/σᵢ for non-zero σᵢ

**Why SVD Matters for Linear Models:**

- Works for **any** matrix (square, rectangular, full rank, rank deficient)
- Provides **most stable** way to compute generalized inverses
- Reveals **numerical rank** of design matrix **X**
- Used in **principal component regression**
- Foundation for many modern algorithms

**R Implementation:**

```{r}
#| label: svd-basic

# Example matrix
A <- matrix(c(4, 2, 2, 3), nrow = 2, ncol = 2, byrow = TRUE)

cat("Matrix A (2×2):\n")
print(A)

# Singular value decomposition
svd_A <- svd(A)

U <- svd_A$u
Sigma_values <- svd_A$d
V <- svd_A$v

cat("\nLeft singular vectors U (2×2):\n")
print(round(U, 4))

cat("\nSingular values:\n")
print(round(Sigma_values, 4))

cat("\nRight singular vectors V (2×2):\n")
print(round(V, 4))

# Reconstruct A
Sigma <- diag(Sigma_values)
A_reconstructed <- U %*% Sigma %*% t(V)

cat("\nReconstructed A = UΣV':\n")
print(round(A_reconstructed, 4))

cat("\nVerify reconstruction:\n")
all.equal(A, A_reconstructed)
```

**SVD for Rectangular Matrices:**

SVD works perfectly for non-square matrices (unlike eigenvalue decomposition).

```{r}
#| label: svd-rectangular

# Design matrix (5×3)
X <- matrix(c(
  1, 1, 2,
  1, 2, 3,
  1, 3, 1,
  1, 4, 5,
  1, 5, 4
), nrow = 5, ncol = 3, byrow = TRUE)

cat("Design matrix X (5×3):\n")
print(X)

# SVD
svd_X <- svd(X)

cat("\nSingular values:\n")
print(round(svd_X$d, 4))

cat(sprintf("\nRank: %d (number of non-zero singular values)\n", sum(svd_X$d > 1e-10)))

# U is 5×5, V is 3×3
cat(sprintf("\nDimensions: U is %d×%d, V is %d×%d\n",
            nrow(svd_X$u), ncol(svd_X$u),
            nrow(svd_X$v), ncol(svd_X$v)))
```

**Relationship to Eigenvalues:**

For symmetric matrix **A**, SVD and eigenvalue decomposition are closely related.

```{r}
#| label: svd-eigen-relationship

A <- matrix(c(4, 2, 2, 3), nrow = 2, ncol = 2, byrow = TRUE)

# SVD
svd_A <- svd(A)
cat("Singular values from SVD:\n")
print(round(svd_A$d, 4))

# Eigenvalues
eigen_A <- eigen(A)
cat("\nEigenvalues from eigen():\n")
print(round(eigen_A$values, 4))

cat("\nFor symmetric matrices, singular values ≈ |eigenvalues|\n")
```

**Computing Moore-Penrose Inverse via SVD:**

$\mathbf{A}^+ = \mathbf{V\Sigma}^+\mathbf{U}'$ where $(\Sigma^+)_{ii} = 1/\sigma_i$ if $\sigma_i > 0$, else 0.

```{r}
#| label: svd-pseudoinverse

# Rank-deficient matrix
A <- matrix(c(1, 2, 2, 4), nrow = 2, ncol = 2, byrow = TRUE)

cat("Singular matrix A:\n")
print(A)
cat(sprintf("Determinant: %.1f (singular!)\n", det(A)))

# SVD
svd_A <- svd(A)

cat("\nSingular values:\n")
print(svd_A$d)

# Moore-Penrose inverse via SVD
tol <- 1e-10
Sigma_plus <- ifelse(svd_A$d > tol, 1/svd_A$d, 0)

A_plus <- svd_A$v %*% diag(Sigma_plus) %*% t(svd_A$u)

cat("\nMoore-Penrose inverse A⁺:\n")
print(round(A_plus, 4))

# Verify AA⁺A = A
cat("\nVerify AA⁺A = A:\n")
AAminusA <- A %*% A_plus %*% A
print(round(AAminusA, 10))

cat("\nCompare with ginv():\n")
library(MASS)
A_ginv <- ginv(A)
print(round(A_ginv, 4))

cat("\nBoth give Moore-Penrose inverse ✓\n")
```

**Condition Number via SVD:**

The condition number measures how close a matrix is to being singular.

```{r}
#| label: svd-condition

# Well-conditioned matrix
A1 <- diag(c(10, 9, 8))
svd1 <- svd(A1)
kappa1 <- max(svd1$d) / min(svd1$d)

cat("Well-conditioned matrix A1:\n")
print(A1)
cat(sprintf("Condition number: %.2f\n", kappa1))

# Ill-conditioned matrix
A2 <- matrix(c(1, 1, 1.001, 1.001), nrow = 2, ncol = 2, byrow = TRUE)
svd2 <- svd(A2)
kappa2 <- max(svd2$d) / min(svd2$d)

cat("\n\nIll-conditioned matrix A2:\n")
print(A2)
cat("Singular values:\n")
print(round(svd2$d, 6))
cat(sprintf("Condition number: %.2f (very high!)\n", kappa2))

cat("\nSmall singular value indicates near-singularity\n")
```

**Livestock Example - Detecting Rank Deficiency:**

```{r}
#| label: svd-rank-deficiency

# ANOVA with missing cells: 3 breeds × 2 sexes, but some combinations missing
# breed 1, sex 1: observations 1, 2
# breed 1, sex 2: observation 3
# breed 2, sex 1: observations 4, 5
# breed 2, sex 2: missing!
# breed 3, sex 1: observation 6
# breed 3, sex 2: observations 7, 8

# Cell means model (will be rank deficient due to missing cell)
X <- matrix(0, nrow = 8, ncol = 6)
# Columns: B1S1, B1S2, B2S1, B2S2, B3S1, B3S2

X[1:2, 1] <- 1   # breed 1, sex 1
X[3, 2] <- 1     # breed 1, sex 2
X[4:5, 3] <- 1   # breed 2, sex 1
# Column 4 (B2S2) has no observations!
X[6, 5] <- 1     # breed 3, sex 1
X[7:8, 6] <- 1   # breed 3, sex 2

cat("Design matrix X (8×6) with missing cell:\n")
print(X)

# SVD to detect rank
svd_X <- svd(X)

cat("\nSingular values:\n")
print(round(svd_X$d, 6))

# Determine numerical rank
tol <- 1e-10
rank_X <- sum(svd_X$d > tol)

cat(sprintf("\nNumerical rank: %d (out of 6 parameters)\n", rank_X))
cat("Matrix is rank deficient due to missing cell!\n")

cat("\nConsequence: Must use generalized inverse or constraints\n")
```

**Truncated SVD (Low-Rank Approximation):**

Keep only largest k singular values to approximate **A**.

$$
\mathbf{A}_k = \sum_{i=1}^{k} \sigma_i \mathbf{u}_i\mathbf{v}_i'
$$ {#eq-truncated-svd}

```{r}
#| label: svd-truncated

# Create noisy data matrix
set.seed(789)
true_signal <- matrix(c(1, 2, 3, 2, 4, 6), nrow = 3, ncol = 2)
noise <- matrix(rnorm(6, 0, 0.1), nrow = 3, ncol = 2)
A <- true_signal + noise

cat("Noisy matrix A (3×2):\n")
print(round(A, 2))

# Full SVD
svd_A <- svd(A)

cat("\nSingular values:\n")
print(round(svd_A$d, 4))

# Keep only largest singular value (rank-1 approximation)
A1 <- svd_A$d[1] * svd_A$u[, 1] %*% t(svd_A$v[, 1])

cat("\nRank-1 approximation (truncated SVD):\n")
print(round(A1, 2))

# Error
error_norm <- norm(A - A1, "F")
cat(sprintf("\nFrobenius norm of error: %.4f\n", error_norm))

cat("\nTruncated SVD removes noise, keeps signal\n")
```

**Livestock Example - Principal Component Regression:**

SVD provides principal components for regression.

```{r}
#| label: svd-pcr

# Swine data: highly correlated predictors
# y = average daily gain, X = backfat, loin depth, weight (all correlated)
set.seed(234)
n <- 20

# Create correlated predictors
backfat <- rnorm(n, 15, 2)
loin <- 70 - 2*backfat + rnorm(n, 0, 1)  # Negatively correlated
weight <- 100 + 3*backfat + 0.5*loin + rnorm(n, 0, 2)

# Response
adg <- 0.8 + 0.01*backfat - 0.005*loin + 0.002*weight + rnorm(n, 0, 0.05)

# Design matrix (centered)
X <- scale(cbind(backfat, loin, weight), center = TRUE, scale = FALSE)

cat("Correlation matrix of predictors:\n")
print(round(cor(X), 3))

cat("\nHigh correlations → multicollinearity!\n")

# SVD of X
svd_X <- svd(X)

cat("\nSingular values:\n")
print(round(svd_X$d, 4))

# Condition number
kappa <- max(svd_X$d) / min(svd_X$d)
cat(sprintf("\nCondition number: %.2f (ill-conditioned)\n", kappa))

# Principal components
PC <- X %*% svd_X$v
colnames(PC) <- c("PC1", "PC2", "PC3")

cat("\nPrincipal components (first 3 observations):\n")
print(round(head(PC, 3), 3))

cat("\nPCs are uncorrelated (orthogonal):\n")
print(round(cor(PC), 6))

cat("\nCan regress on PCs instead of original X to avoid multicollinearity\n")
```

**Comparing Decompositions:**

| Method | Matrix Type | Output | Use Case |
|--------|------------|--------|----------|
| **Eigenvalue** | Square only | $\mathbf{A} = \mathbf{Q\Lambda Q}'$ | Symmetric matrices, $\mathbf{X}'\mathbf{X}$ |
| **SVD** | Any (m×n) | $\mathbf{A} = \mathbf{U\Sigma V}'$ | Rank deficiency, generalized inverse |
| **Cholesky** | Pos. definite | $\mathbf{A} = \mathbf{LL}'$ | Fast solving, simulation |
| **QR** | Any (m×n) | $\mathbf{A} = \mathbf{QR}$ | Stable LS solution |

```{r}
#| label: svd-comparison

A <- matrix(c(4, 2, 2, 5), nrow = 2, ncol = 2, byrow = TRUE)

cat("Symmetric positive definite matrix A:\n")
print(A)

# Eigenvalue decomposition
eigen_A <- eigen(A)
cat("\nEigenvalues:", round(eigen_A$values, 4), "\n")

# SVD
svd_A <- svd(A)
cat("Singular values:", round(svd_A$d, 4), "\n")

cat("\nFor symmetric matrices: singular values ≈ |eigenvalues|\n")

# Cholesky
L <- t(chol(A))
cat("\nCholesky factor L:\n")
print(round(L, 4))

cat("\nAll decompositions reveal different aspects of same matrix\n")
```

:::{.callout-important}
## SVD is THE Most General Decomposition

SVD works for **any** matrix:
- Square or rectangular
- Full rank or rank deficient
- Symmetric or non-symmetric

**Applications in Linear Models:**

1. **Detect rank deficiency**: Count non-zero singular values
2. **Compute generalized inverse**: $\mathbf{A}^+ = \mathbf{V\Sigma}^+\mathbf{U}'$
3. **Assess numerical stability**: Condition number = σ₁/σᵣ
4. **Principal component regression**: Use **V** to transform predictors
5. **Data compression**: Truncated SVD for dimensionality reduction

**Computational Note:** SVD is more expensive than eigenvalue decomposition but more robust.
:::

:::{.callout-note}
## Cross-References

SVD appears in:

- **Week 2**: Generalized inverse computation
- **Week 11**: Detecting numerical problems via condition number
- **Week 12**: Rank deficiency and generalized inverses
- **Week 14**: Principal component analysis and regression
- **Section @sec-computation**: Numerical considerations
:::

---

## Matrix Calculus for Linear Models {#sec-matrix-calculus}

:::{.callout-important}
## Critical Section

This section provides the mathematical foundation for deriving normal equations. It receives extra emphasis with detailed step-by-step derivations. Understanding these derivatives is essential for understanding where least squares estimates come from.
:::

Matrix calculus extends ordinary calculus to functions involving vectors and matrices. In linear models, we use matrix derivatives primarily to minimize the sum of squared errors and derive the normal equations.

### Vector and Matrix Derivatives {#sec-derivatives}

#### Notation and Conventions

When taking derivatives with respect to vectors, we must be careful about **layout conventions**. There are two main conventions:

1. **Numerator layout** (preferred in this course): Result has same orientation as numerator
2. **Denominator layout**: Result has same orientation as denominator

We use **numerator layout** throughout this course.

#### Scalar Function of a Vector

Let $f: \mathbb{R}^n \to \mathbb{R}$ be a scalar-valued function of a vector $\mathbf{x} = [x_1, x_2, \ldots, x_n]'$.

The **gradient** is:

$$
\frac{\partial f}{\partial \mathbf{x}} = \begin{bmatrix}
\frac{\partial f}{\partial x_1} \\
\frac{\partial f}{\partial x_2} \\
\vdots \\
\frac{\partial f}{\partial x_n}
\end{bmatrix}
$$ {#eq-gradient}

This is an n × 1 **column vector** (numerator layout).

**Example**: If $f(\mathbf{x}) = x_1^2 + 2x_2$, then:

$$
\frac{\partial f}{\partial \mathbf{x}} = \begin{bmatrix} 2x_1 \\ 2 \end{bmatrix}
$$

#### Dimensions Matter

Always check dimensions:
- If $\mathbf{x}$ is n × 1 and $f$ is scalar, then $\frac{\partial f}{\partial \mathbf{x}}$ is n × 1
- If $\mathbf{y}$ is m × 1 and $\mathbf{x}$ is n × 1, then $\frac{\partial \mathbf{y}}{\partial \mathbf{x}}$ is m × n (Jacobian)

#### Second Derivatives (Hessian Matrix)

The **Hessian** is the matrix of second partial derivatives:

$$
\frac{\partial^2 f}{\partial \mathbf{x} \partial \mathbf{x}'} = \mathbf{H} = \begin{bmatrix}
\frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \cdots \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \cdots \\
\vdots & \vdots & \ddots
\end{bmatrix}
$$ {#eq-hessian}

This is an n × n matrix. If $f$ has continuous second derivatives, **H** is symmetric.

:::{.callout-note}
## Caution: Not the Hat Matrix!

Don't confuse the Hessian matrix **H** (from calculus) with the hat matrix **H** (from projections). Context makes it clear which is meant.
:::

---

### Basic Derivative Rules {#sec-derivative-rules}

Here are the essential matrix derivative rules for linear models:

#### Rule Table

| Expression | $\frac{\partial}{\partial \mathbf{x}}$ | Dimensions | Notes |
|------------|---------------------------------------|------------|-------|
| $\mathbf{a}'\mathbf{x}$ | $\mathbf{a}$ | n × 1 | **a** constant n × 1 |
| $\mathbf{x}'\mathbf{a}$ | $\mathbf{a}$ | n × 1 | Same as above |
| $\mathbf{x}'\mathbf{x}$ | $2\mathbf{x}$ | n × 1 | Quadratic |
| $\mathbf{x}'\mathbf{A}\mathbf{x}$ | $2\mathbf{A}\mathbf{x}$ | n × 1 | **A** symmetric n × n |
| $\mathbf{x}'\mathbf{A}\mathbf{x}$ | $(\mathbf{A} + \mathbf{A}')\mathbf{x}$ | n × 1 | **A** not symmetric |
| $\mathbf{A}\mathbf{x}$ | $\mathbf{A}$ | m × n | **A** is m × n |
| $\mathbf{x}'\mathbf{A}$ | $\mathbf{A}'$ | n × m | Result w.r.t. **x** |

**Most important for linear models**: $\frac{\partial}{\partial \boldsymbol{\beta}}[\mathbf{y}'\mathbf{X}\boldsymbol{\beta}] = \mathbf{X}'\mathbf{y}$

where:
- $\mathbf{y}$ is n × 1 (data, constant w.r.t. $\boldsymbol{\beta}$)
- $\mathbf{X}$ is n × p (design matrix, constant)
- $\boldsymbol{\beta}$ is p × 1 (parameters, variable)
- Result is p × 1

#### Detailed Derivations

**1. Linear form: $f(\mathbf{x}) = \mathbf{a}'\mathbf{x}$**

$$
f(\mathbf{x}) = a_1 x_1 + a_2 x_2 + \cdots + a_n x_n
$$

$$
\frac{\partial f}{\partial \mathbf{x}} = \begin{bmatrix} a_1 \\ a_2 \\ \vdots \\ a_n \end{bmatrix} = \mathbf{a}
$$ {#eq-deriv-linear}

**2. Quadratic form: $f(\mathbf{x}) = \mathbf{x}'\mathbf{x}$**

$$
f(\mathbf{x}) = x_1^2 + x_2^2 + \cdots + x_n^2
$$

$$
\frac{\partial f}{\partial \mathbf{x}} = \begin{bmatrix} 2x_1 \\ 2x_2 \\ \vdots \\ 2x_n \end{bmatrix} = 2\mathbf{x}
$$ {#eq-deriv-quadratic}

**3. Quadratic form with matrix: $f(\mathbf{x}) = \mathbf{x}'\mathbf{A}\mathbf{x}$ where **A** symmetric**

First, note that:
$$
f(\mathbf{x}) = \sum_{i=1}^n \sum_{j=1}^n a_{ij} x_i x_j
$$

Taking derivative with respect to $x_k$:
$$
\frac{\partial f}{\partial x_k} = \sum_{i=1}^n a_{ik} x_i + \sum_{j=1}^n a_{kj} x_j
$$

Since **A** is symmetric ($a_{ij} = a_{ji}$):
$$
\frac{\partial f}{\partial x_k} = 2\sum_{i=1}^n a_{ki} x_i = 2[\mathbf{A}\mathbf{x}]_k
$$

Therefore:
$$
\frac{\partial (\mathbf{x}'\mathbf{A}\mathbf{x})}{\partial \mathbf{x}} = 2\mathbf{A}\mathbf{x}
$$ {#eq-deriv-qf}

**4. Product rule for $\mathbf{y}'\mathbf{X}\boldsymbol{\beta}$ with respect to $\boldsymbol{\beta}$**

Treating **y** and **X** as constants:
$$
\mathbf{y}'\mathbf{X}\boldsymbol{\beta} = \sum_{i=1}^n \sum_{j=1}^p y_i X_{ij} \beta_j = \sum_{j=1}^p \left(\sum_{i=1}^n y_i X_{ij}\right) \beta_j
$$

This is linear in $\boldsymbol{\beta}$, so:
$$
\frac{\partial (\mathbf{y}'\mathbf{X}\boldsymbol{\beta})}{\partial \boldsymbol{\beta}} = \begin{bmatrix}
\sum_{i=1}^n y_i X_{i1} \\
\sum_{i=1}^n y_i X_{i2} \\
\vdots \\
\sum_{i=1}^n y_i X_{ip}
\end{bmatrix} = \mathbf{X}'\mathbf{y}
$$ {#eq-deriv-xty}

#### R Verification with Numerical Derivatives

```{r}
#| label: numerical-derivatives

# Verify derivative rules numerically using finite differences

# Function to compute numerical gradient
numerical_gradient <- function(f, x, h = 1e-8) {
  n <- length(x)
  grad <- numeric(n)
  for (i in 1:n) {
    x_plus <- x
    x_plus[i] <- x[i] + h
    x_minus <- x
    x_minus[i] <- x[i] - h
    grad[i] <- (f(x_plus) - f(x_minus)) / (2 * h)
  }
  return(grad)
}

# Example 1: f(x) = a'x
a <- c(2, 3, 4)
f1 <- function(x) sum(a * x)

x0 <- c(1, 2, 3)
analytical1 <- a
numerical1 <- numerical_gradient(f1, x0)

cat("Example 1: f(x) = a'x\n")
cat("Analytical gradient:", analytical1, "\n")
cat("Numerical gradient:", numerical1, "\n")
cat("Agreement:", all.equal(analytical1, numerical1), "\n\n")

# Example 2: f(x) = x'x
f2 <- function(x) sum(x^2)

analytical2 <- 2 * x0
numerical2 <- numerical_gradient(f2, x0)

cat("Example 2: f(x) = x'x\n")
cat("Analytical gradient:", analytical2, "\n")
cat("Numerical gradient:", numerical2, "\n")
cat("Agreement:", all.equal(analytical2, numerical2), "\n\n")

# Example 3: f(x) = x'Ax with A symmetric
A <- matrix(c(2, 1, 1, 3), 2, 2)  # 2x2 symmetric
f3 <- function(x) c(t(x) %*% A %*% x)

x0_2 <- c(1, 2)
analytical3 <- 2 * A %*% x0_2
numerical3 <- numerical_gradient(f3, x0_2)

cat("Example 3: f(x) = x'Ax (A symmetric)\n")
cat("Analytical gradient:", analytical3, "\n")
cat("Numerical gradient:", numerical3, "\n")
cat("Agreement:", all.equal(c(analytical3), numerical3), "\n")
```

---

### Deriving Normal Equations {#sec-deriving-normal-equations}

This is **THE fundamental derivation** in linear models. We'll go step-by-step from minimizing sum of squared errors to the normal equations.

#### Setup

Given:
- Response vector $\mathbf{y}$: n × 1 (observed data)
- Design matrix $\mathbf{X}$: n × p (predictors, full rank)
- Parameter vector $\boldsymbol{\beta}$: p × 1 (unknown)
- Model: $\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}$

**Goal**: Find $\boldsymbol{\beta}$ that minimizes sum of squared errors.

#### Step 1: Define the Sum of Squares Function

The **residual vector** is:
$$
\mathbf{e} = \mathbf{y} - \mathbf{X}\boldsymbol{\beta}
$$ {#eq-residual-vector}

The **sum of squared errors** is:
$$
S(\boldsymbol{\beta}) = \mathbf{e}'\mathbf{e} = (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})'(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})
$$ {#eq-sse-function}

This is a scalar-valued function of the p × 1 vector $\boldsymbol{\beta}$.

#### Step 2: Expand the Sum of Squares

Expand using $(\mathbf{a} - \mathbf{b})'(\mathbf{a} - \mathbf{b}) = \mathbf{a}'\mathbf{a} - 2\mathbf{a}'\mathbf{b} + \mathbf{b}'\mathbf{b}$:

$$
\begin{align}
S(\boldsymbol{\beta}) &= (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})'(\mathbf{y} - \mathbf{X}\boldsymbol{\beta}) \\
&= \mathbf{y}'\mathbf{y} - \mathbf{y}'\mathbf{X}\boldsymbol{\beta} - \boldsymbol{\beta}'\mathbf{X}'\mathbf{y} + \boldsymbol{\beta}'\mathbf{X}'\mathbf{X}\boldsymbol{\beta}
\end{align}
$$

Since $\mathbf{y}'\mathbf{X}\boldsymbol{\beta}$ is a scalar, it equals its transpose:
$$
\mathbf{y}'\mathbf{X}\boldsymbol{\beta} = (\mathbf{y}'\mathbf{X}\boldsymbol{\beta})' = \boldsymbol{\beta}'\mathbf{X}'\mathbf{y}
$$

Therefore:
$$
S(\boldsymbol{\beta}) = \mathbf{y}'\mathbf{y} - 2\boldsymbol{\beta}'\mathbf{X}'\mathbf{y} + \boldsymbol{\beta}'\mathbf{X}'\mathbf{X}\boldsymbol{\beta}
$$ {#eq-sse-expanded}

#### Step 3: Take the Derivative with Respect to $\boldsymbol{\beta}$

Apply derivative rules to each term:

1. $\frac{\partial}{\partial \boldsymbol{\beta}}[\mathbf{y}'\mathbf{y}] = \mathbf{0}$ (constant)

2. $\frac{\partial}{\partial \boldsymbol{\beta}}[-2\boldsymbol{\beta}'\mathbf{X}'\mathbf{y}] = -2\mathbf{X}'\mathbf{y}$ (linear in $\boldsymbol{\beta}$)

3. $\frac{\partial}{\partial \boldsymbol{\beta}}[\boldsymbol{\beta}'\mathbf{X}'\mathbf{X}\boldsymbol{\beta}] = 2\mathbf{X}'\mathbf{X}\boldsymbol{\beta}$ (quadratic, $\mathbf{X}'\mathbf{X}$ is symmetric)

Combining:
$$
\frac{\partial S(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}} = \mathbf{0} - 2\mathbf{X}'\mathbf{y} + 2\mathbf{X}'\mathbf{X}\boldsymbol{\beta} = -2\mathbf{X}'\mathbf{y} + 2\mathbf{X}'\mathbf{X}\boldsymbol{\beta}
$$ {#eq-sse-derivative}

#### Step 4: Set Derivative to Zero

For a minimum, set the gradient to zero:
$$
-2\mathbf{X}'\mathbf{y} + 2\mathbf{X}'\mathbf{X}\boldsymbol{\beta} = \mathbf{0}
$$

Divide by 2:
$$
-\mathbf{X}'\mathbf{y} + \mathbf{X}'\mathbf{X}\boldsymbol{\beta} = \mathbf{0}
$$

Rearrange:
$$
\mathbf{X}'\mathbf{X}\boldsymbol{\beta} = \mathbf{X}'\mathbf{y}
$$ {#eq-normal-equations}

These are the **normal equations**!

#### Step 5: Solve for $\boldsymbol{\beta}$

If $\mathbf{X}'\mathbf{X}$ is invertible (i.e., **X** has full column rank):
$$
\mathbf{b} = (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{y}
$$ {#eq-ls-solution}

This is the **least squares estimator**.

#### Step 6: Verify it's a Minimum (Second Derivative Test)

The Hessian (matrix of second derivatives) is:
$$
\frac{\partial^2 S(\boldsymbol{\beta})}{\partial \boldsymbol{\beta} \partial \boldsymbol{\beta}'} = 2\mathbf{X}'\mathbf{X}
$$ {#eq-hessian-sse}

For a minimum, the Hessian must be **positive definite**. Since $\mathbf{X}'\mathbf{X}$ is:
- Symmetric (always)
- Positive semi-definite (always): For any $\mathbf{v}$, $\mathbf{v}'\mathbf{X}'\mathbf{X}\mathbf{v} = (\mathbf{X}\mathbf{v})'(\mathbf{X}\mathbf{v}) = ||\mathbf{X}\mathbf{v}||^2 \geq 0$
- Positive definite if **X** has full rank: $\mathbf{v}'\mathbf{X}'\mathbf{X}\mathbf{v} > 0$ for all $\mathbf{v} \neq \mathbf{0}$

Therefore, $S(\boldsymbol{\beta})$ is convex and has a unique global minimum at $\mathbf{b} = (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{y}$.

:::{.callout-important}
## Key Result

The normal equations $\mathbf{X}'\mathbf{X}\boldsymbol{\beta} = \mathbf{X}'\mathbf{y}$ are derived by:

1. **Minimizing** $S(\boldsymbol{\beta}) = (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})'(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})$
2. **Taking derivative** with respect to $\boldsymbol{\beta}$
3. **Setting to zero**: $\frac{\partial S}{\partial \boldsymbol{\beta}} = \mathbf{0}$

This is the foundation of least squares estimation!
:::

#### Complete Example: Simple Regression

```{r}
#| label: deriving-simple-regression

# Simple regression: y = β₀ + β₁x + e
# Derive normal equations step-by-step

# Data: Broiler weight (kg) vs age (days)
age <- c(21, 28, 35, 42, 49)
weight <- c(0.5, 0.9, 1.4, 1.9, 2.3)
n <- length(age)

# Design matrix
X <- cbind(1, age)
y <- weight
p <- ncol(X)

cat("Step 1: Set up\n")
cat("y:", y, "\n")
cat("X:\n")
print(X)

# Step 2: Define S(β) = (y - Xβ)'(y - Xβ)
# We'll evaluate S at several β values to visualize

# For visualization, use grid of β values
beta0_seq <- seq(-2, 2, length = 50)
beta1_seq <- seq(0, 0.08, length = 50)
S_grid <- matrix(NA, length(beta0_seq), length(beta1_seq))

for (i in 1:length(beta0_seq)) {
  for (j in 1:length(beta1_seq)) {
    beta <- c(beta0_seq[i], beta1_seq[j])
    e <- y - X %*% beta
    S_grid[i, j] <- sum(e^2)
  }
}

cat("\n\nStep 3: Compute X'X and X'y\n")
XtX <- t(X) %*% X
Xty <- t(X) %*% y

cat("X'X =\n")
print(XtX)
cat("\nX'y =\n")
print(Xty)

cat("\n\nStep 4: Solve normal equations X'Xb = X'y\n")
b <- solve(XtX) %*% Xty
cat("b = (X'X)^(-1) X'y =\n")
print(b)

cat("\n\nStep 5: Verify this minimizes S(β)\n")
e <- y - X %*% b
SSE <- sum(e^2)
cat("SSE at b:", SSE, "\n")

# Check derivative at solution is zero
grad_at_b <- -2 * Xty + 2 * XtX %*% b
cat("\nGradient at b (should be 0):\n")
print(grad_at_b)
cat("Max absolute value:", max(abs(grad_at_b)), "\n")

# Verify second derivative is positive definite
hess <- 2 * XtX
cat("\nHessian = 2X'X =\n")
print(hess)
cat("Eigenvalues (should be > 0):", eigen(hess)$values, "\n")

# Compare with lm()
fit <- lm(weight ~ age)
cat("\n\nComparison with lm():\n")
cat("Our b:", c(b), "\n")
cat("lm() coefficients:", coef(fit), "\n")
cat("Agreement:", all.equal(c(b), unname(coef(fit))), "\n")
```

#### Numerical Verification: Gradient is Zero at Solution

```{r}
#| label: verify-gradient-zero

# Define S(β) as a function
S_function <- function(beta, X, y) {
  e <- y - X %*% beta
  return(sum(e^2))
}

# Compute numerical gradient at solution
h <- 1e-8
grad_numerical <- numeric(p)
for (i in 1:p) {
  b_plus <- b
  b_plus[i] <- b[i] + h
  b_minus <- b
  b_minus[i] <- b[i] - h
  grad_numerical[i] <- (S_function(b_plus, X, y) - S_function(b_minus, X, y)) / (2*h)
}

cat("Numerical gradient at b:", grad_numerical, "\n")
cat("Should be approximately zero\n")
cat("Max absolute value:", max(abs(grad_numerical)), "\n")

# Analytical gradient at solution (should be exactly 0)
grad_analytical <- -2 * t(X) %*% y + 2 * t(X) %*% X %*% b
cat("\nAnalytical gradient:", c(grad_analytical), "\n")
```

:::{.callout-tip}
## Understanding the Derivation

The normal equations emerge from three fundamental calculus steps:

1. **Express the objective**: $S(\boldsymbol{\beta}) = (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})'(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})$
2. **Take the gradient**: $\nabla S = -2\mathbf{X}'\mathbf{y} + 2\mathbf{X}'\mathbf{X}\boldsymbol{\beta}$
3. **Set to zero**: $\mathbf{X}'\mathbf{X}\boldsymbol{\beta} = \mathbf{X}'\mathbf{y}$

This is why least squares "works" - it's a direct application of calculus to find the minimum of a convex function.
:::

---

### Useful Identities Table {#sec-calculus-identities}

Here are additional matrix calculus identities useful in linear models:

#### General Identities

| Expression | Derivative w.r.t. $\mathbf{x}$ | Notes |
|------------|-------------------------------|-------|
| $\mathbf{c}$ | $\mathbf{0}$ | Constant vector |
| $\mathbf{A}\mathbf{x}$ | $\mathbf{A}$ | **A** constant m × n |
| $\mathbf{x}'\mathbf{A}$ | $\mathbf{A}'$ | **A** constant |
| $\mathbf{a}'\mathbf{x}$ | $\mathbf{a}$ | Linear |
| $\mathbf{x}'\mathbf{A}\mathbf{x}$ | $(\mathbf{A} + \mathbf{A}')\mathbf{x}$ | General **A** |
| $\mathbf{x}'\mathbf{A}\mathbf{x}$ | $2\mathbf{A}\mathbf{x}$ | **A** symmetric |
| $(\mathbf{A}\mathbf{x})'\mathbf{B}\mathbf{x}$ | $\mathbf{A}'\mathbf{B}\mathbf{x} + \mathbf{B}'\mathbf{A}\mathbf{x}$ | Product rule |
| $\mathbf{x}'\mathbf{A}\mathbf{x}\mathbf{b}$ | $2\mathbf{A}\mathbf{x}\mathbf{b}'$ | Outer product |

#### Specific to Least Squares

| Quantity | Formula | Derivative w.r.t. $\boldsymbol{\beta}$ |
|----------|---------|---------------------------------------|
| $\mathbf{e}$ | $\mathbf{y} - \mathbf{X}\boldsymbol{\beta}$ | $-\mathbf{X}$ |
| $\mathbf{e}'\mathbf{e}$ | SSE | $-2\mathbf{X}'\mathbf{e} = -2\mathbf{X}'\mathbf{y} + 2\mathbf{X}'\mathbf{X}\boldsymbol{\beta}$ |
| $\hat{\mathbf{y}}$ | $\mathbf{X}\boldsymbol{\beta}$ | $\mathbf{X}$ |
| $\mathbf{y}'\mathbf{X}\boldsymbol{\beta}$ | Linear in $\boldsymbol{\beta}$ | $\mathbf{X}'\mathbf{y}$ |
| $\boldsymbol{\beta}'\mathbf{X}'\mathbf{X}\boldsymbol{\beta}$ | Quadratic in $\boldsymbol{\beta}$ | $2\mathbf{X}'\mathbf{X}\boldsymbol{\beta}$ |

#### Chain Rule

If $\mathbf{u} = \mathbf{u}(\mathbf{x})$ and $f = f(\mathbf{u})$, then:

$$
\frac{\partial f}{\partial \mathbf{x}} = \frac{\partial \mathbf{u}}{\partial \mathbf{x}} \frac{\partial f}{\partial \mathbf{u}}
$$

where $\frac{\partial \mathbf{u}}{\partial \mathbf{x}}$ is the Jacobian matrix.

**Example**: If $f = \mathbf{e}'\mathbf{e}$ where $\mathbf{e} = \mathbf{y} - \mathbf{X}\boldsymbol{\beta}$:

$$
\frac{\partial f}{\partial \boldsymbol{\beta}} = \frac{\partial \mathbf{e}}{\partial \boldsymbol{\beta}} \frac{\partial (\mathbf{e}'\mathbf{e})}{\partial \mathbf{e}} = (-\mathbf{X})' (2\mathbf{e}) = -2\mathbf{X}'\mathbf{e}
$$

:::{.callout-note}
## Cross-References

- **Week 5: Least Squares Theory** - Uses these derivatives extensively
- **Week 6: Multiple Regression** - Applies to multiple predictors
- **Week 8: Contrasts** - Derivatives of contrast functions
:::

:::{.callout-important}
## Summary: Matrix Calculus Essentials

The three key results for linear models are:

1. $\frac{\partial}{\partial \boldsymbol{\beta}}[\mathbf{a}'\boldsymbol{\beta}] = \mathbf{a}$ - Linear terms
2. $\frac{\partial}{\partial \boldsymbol{\beta}}[\boldsymbol{\beta}'\mathbf{A}\boldsymbol{\beta}] = 2\mathbf{A}\boldsymbol{\beta}$ - Quadratic terms (**A** symmetric)
3. $\frac{\partial}{\partial \boldsymbol{\beta}}[(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})'(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})] = -2\mathbf{X}'\mathbf{y} + 2\mathbf{X}'\mathbf{X}\boldsymbol{\beta}$ - Normal equations

Master these three, and you understand the calculus foundation of linear models!
:::

---

## Kronecker Products {#sec-kronecker}

:::{.callout-note}
## Foundation for Advanced Topics

This section prepares you for Animal models and multi-trait analysis covered in future courses. Kronecker products provide the mathematical structure for variance-covariance matrices in multi-trait genetic evaluations.
:::

### Definition and Basic Properties {#sec-kronecker-definition}

**Definition:**

The **Kronecker product** (also called direct product or tensor product) of matrix **A** (m × n) and matrix **B** (p × q) is:

$$
\mathbf{A} \otimes \mathbf{B} = \begin{bmatrix}
a_{11}\mathbf{B} & a_{12}\mathbf{B} & \cdots & a_{1n}\mathbf{B} \\
a_{21}\mathbf{B} & a_{22}\mathbf{B} & \cdots & a_{2n}\mathbf{B} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1}\mathbf{B} & a_{m2}\mathbf{B} & \cdots & a_{mn}\mathbf{B}
\end{bmatrix}
$$ {#eq-kronecker-product}

The result is an (mp) × (nq) matrix formed by replacing each element a~ij~ of **A** with the block a~ij~**B**.

**Simple Example:**

$$
\begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{bmatrix} \otimes \begin{bmatrix} b_{11} & b_{12} \\ b_{21} & b_{22} \end{bmatrix} =
\begin{bmatrix}
a_{11}b_{11} & a_{11}b_{12} & a_{12}b_{11} & a_{12}b_{12} \\
a_{11}b_{21} & a_{11}b_{22} & a_{12}b_{21} & a_{12}b_{22} \\
a_{21}b_{11} & a_{21}b_{12} & a_{22}b_{11} & a_{22}b_{12} \\
a_{21}b_{21} & a_{21}b_{22} & a_{22}b_{21} & a_{22}b_{22}
\end{bmatrix}
$$ {#eq-kronecker-example}

**Basic Properties:**

1. **Dimension**: If **A** is m × n and **B** is p × q, then $\mathbf{A} \otimes \mathbf{B}$ is (mp) × (nq)
2. **NOT commutative**: $\mathbf{A} \otimes \mathbf{B} \neq \mathbf{B} \otimes \mathbf{A}$ (generally)
3. **Associative**: $(\mathbf{A} \otimes \mathbf{B}) \otimes \mathbf{C} = \mathbf{A} \otimes (\mathbf{B} \otimes \mathbf{C})$
4. **Distributive**: $(\mathbf{A} + \mathbf{B}) \otimes \mathbf{C} = \mathbf{A} \otimes \mathbf{C} + \mathbf{B} \otimes \mathbf{C}$

**R Implementation:**

```{r}
#| label: kronecker-basic

# Simple 2×2 matrices
A <- matrix(c(1, 2, 3, 4), nrow = 2, ncol = 2, byrow = TRUE)
B <- matrix(c(5, 6, 7, 8), nrow = 2, ncol = 2, byrow = TRUE)

cat("Matrix A (2×2):\n")
print(A)

cat("\nMatrix B (2×2):\n")
print(B)

# Kronecker product
C <- kronecker(A, B)  # or A %x% B

cat("\nA ⊗ B (4×4):\n")
print(C)

cat(sprintf("\nDimension: %d×%d matrix (A is 2×2, B is 2×2 → product is 4×4)\n",
            nrow(C), ncol(C)))
```

**Non-Commutativity:**

```{r}
#| label: kronecker-noncommutative

# A ⊗ B
AkronB <- A %x% B

# B ⊗ A
BkronA <- B %x% A

cat("A ⊗ B:\n")
print(AkronB)

cat("\nB ⊗ A:\n")
print(BkronA)

cat("\nA ⊗ B ≠ B ⊗ A (not commutative!)\n")
```

**Kronecker Product with Vectors:**

```{r}
#| label: kronecker-vectors

a <- c(1, 2)
b <- c(10, 20, 30)

# Kronecker product of vectors
c <- kronecker(a, b)

cat("Vector a:", a, "\n")
cat("Vector b:", b, "\n")
cat("\na ⊗ b:", c, "\n")

cat("\nResult is a vector of length 2 × 3 = 6\n")
cat("Interpretation: [1*10, 1*20, 1*30, 2*10, 2*20, 2*30]\n")
```

**Identity Property:**

$\mathbf{I}_m \otimes \mathbf{I}_n = \mathbf{I}_{mn}$

```{r}
#| label: kronecker-identity

I2 <- diag(2)
I3 <- diag(3)

I_kron <- I2 %x% I3

cat("I₂ ⊗ I₃:\n")
print(I_kron)

cat("\nResult is I₆ (6×6 identity matrix)\n")
all.equal(I_kron, diag(6))
```

---

### Kronecker Product Rules {#sec-kronecker-rules}

The Kronecker product has many useful algebraic properties that simplify calculations in multi-trait models.

**Comprehensive Property Table:**

| Property | Rule | Condition |
|----------|------|-----------|
| **Transpose** | $(\mathbf{A} \otimes \mathbf{B})' = \mathbf{A}' \otimes \mathbf{B}'$ | Always |
| **Product** | $(\mathbf{A} \otimes \mathbf{B})(\mathbf{C} \otimes \mathbf{D}) = (\mathbf{AC}) \otimes (\mathbf{BD})$ | Dimensions compatible |
| **Sum** | $(\mathbf{A} + \mathbf{B}) \otimes \mathbf{C} = \mathbf{A} \otimes \mathbf{C} + \mathbf{B} \otimes \mathbf{C}$ | A, B same size |
| **Scalar** | $(c\mathbf{A}) \otimes \mathbf{B} = \mathbf{A} \otimes (c\mathbf{B}) = c(\mathbf{A} \otimes \mathbf{B})$ | Always |
| **Inverse** | $(\mathbf{A} \otimes \mathbf{B})^{-1} = \mathbf{A}^{-1} \otimes \mathbf{B}^{-1}$ | A, B invertible |
| **Trace** | $\text{tr}(\mathbf{A} \otimes \mathbf{B}) = \text{tr}(\mathbf{A}) \cdot \text{tr}(\mathbf{B})$ | A, B square |
| **Determinant** | $\det(\mathbf{A} \otimes \mathbf{B}) = \det(\mathbf{A})^q \cdot \det(\mathbf{B})^m$ | A (m×m), B (q×q) |
| **Rank** | $r(\mathbf{A} \otimes \mathbf{B}) = r(\mathbf{A}) \cdot r(\mathbf{B})$ | Always |
| **Eigenvalues** | If λ eigenvalue of A, μ eigenvalue of B, then λμ eigenvalue of A ⊗ B | A, B square |

**Verification of Key Properties:**

```{r}
#| label: kronecker-properties

A <- matrix(c(2, 1, 1, 2), nrow = 2, ncol = 2, byrow = TRUE)
B <- matrix(c(3, 0, 0, 1), nrow = 2, ncol = 2, byrow = TRUE)

cat("Matrix A (2×2):\n")
print(A)
cat("\nMatrix B (2×2):\n")
print(B)

# Property 1: Transpose
AkronB <- A %x% B
trans_AkronB <- t(A %x% B)
A_trans_kron_B_trans <- t(A) %x% t(B)

cat("\n1. Transpose property:\n")
cat("(A ⊗ B)' equals A' ⊗ B'?", all.equal(trans_AkronB, A_trans_kron_B_trans), "\n")

# Property 2: Trace
tr_AkronB <- sum(diag(A %x% B))
tr_A_times_tr_B <- sum(diag(A)) * sum(diag(B))

cat("\n2. Trace property:\n")
cat(sprintf("tr(A ⊗ B) = %.1f\n", tr_AkronB))
cat(sprintf("tr(A) × tr(B) = %.1f × %.1f = %.1f\n",
            sum(diag(A)), sum(diag(B)), tr_A_times_tr_B))
cat("Equal?", all.equal(tr_AkronB, tr_A_times_tr_B), "\n")

# Property 3: Determinant
det_AkronB <- det(A %x% B)
det_formula <- det(A)^2 * det(B)^2  # A is 2×2, B is 2×2

cat("\n3. Determinant property:\n")
cat(sprintf("det(A ⊗ B) = %.1f\n", det_AkronB))
cat(sprintf("det(A)² × det(B)² = %.1f² × %.1f² = %.1f\n",
            det(A), det(B), det_formula))
cat("Equal?", all.equal(det_AkronB, det_formula), "\n")

# Property 4: Rank
rank_AkronB <- qr(A %x% B)$rank
rank_product <- qr(A)$rank * qr(B)$rank

cat("\n4. Rank property:\n")
cat(sprintf("r(A ⊗ B) = %d\n", rank_AkronB))
cat(sprintf("r(A) × r(B) = %d × %d = %d\n",
            qr(A)$rank, qr(B)$rank, rank_product))
cat("Equal?", rank_AkronB == rank_product, "\n")
```

**Inverse Property (Most Important!):**

```{r}
#| label: kronecker-inverse

A <- matrix(c(4, 1, 1, 3), nrow = 2, ncol = 2, byrow = TRUE)
B <- matrix(c(2, 0, 0, 5), nrow = 2, ncol = 2, byrow = TRUE)

cat("Invertible matrix A:\n")
print(A)
cat("\nInvertible matrix B:\n")
print(B)

# Form A ⊗ B
AkronB <- A %x% B

# Inverse of A ⊗ B (direct calculation)
AkronB_inv <- solve(A %x% B)

# Using property: (A ⊗ B)^(-1) = A^(-1) ⊗ B^(-1)
Ainv_kron_Binv <- solve(A) %x% solve(B)

cat("\n(A ⊗ B)^(-1) computed directly:\n")
print(round(AkronB_inv, 4))

cat("\nA^(-1) ⊗ B^(-1):\n")
print(round(Ainv_kron_Binv, 4))

cat("\nVerify they're equal:\n")
all.equal(AkronB_inv, Ainv_kron_Binv)

cat("\nThis property is CRUCIAL for inverting multi-trait covariance matrices!\n")
```

**Product Rule (Mixed-Product Property):**

$(\mathbf{A} \otimes \mathbf{B})(\mathbf{C} \otimes \mathbf{D}) = (\mathbf{AC}) \otimes (\mathbf{BD})$

```{r}
#| label: kronecker-product-rule

A <- matrix(c(1, 2, 3, 4), nrow = 2, ncol = 2, byrow = TRUE)
B <- matrix(c(5, 6, 7, 8), nrow = 2, ncol = 2, byrow = TRUE)
C <- matrix(c(2, 0, 0, 1), nrow = 2, ncol = 2, byrow = TRUE)
D <- matrix(c(1, 1, 1, 1), nrow = 2, ncol = 2, byrow = TRUE)

# Left side: (A ⊗ B)(C ⊗ D)
left_side <- (A %x% B) %*% (C %x% D)

# Right side: (AC) ⊗ (BD)
right_side <- (A %*% C) %x% (B %*% D)

cat("(A ⊗ B)(C ⊗ D) equals (AC) ⊗ (BD)?\n")
all.equal(left_side, right_side)

cat("\nThis property simplifies matrix multiplications dramatically!\n")
```

**Eigenvalue Property:**

```{r}
#| label: kronecker-eigenvalues

A <- matrix(c(3, 1, 1, 3), nrow = 2, ncol = 2, byrow = TRUE)
B <- matrix(c(2, 0, 0, 5), nrow = 2, ncol = 2, byrow = TRUE)

# Eigenvalues of A and B
eigen_A <- eigen(A)$values
eigen_B <- eigen(B)$values

cat("Eigenvalues of A:", round(eigen_A, 4), "\n")
cat("Eigenvalues of B:", round(eigen_B, 4), "\n")

# Eigenvalues of A ⊗ B should be all products λ_i * μ_j
AkronB <- A %x% B
eigen_AkronB <- eigen(AkronB)$values

cat("\nEigenvalues of A ⊗ B:\n")
print(round(sort(eigen_AkronB, decreasing = TRUE), 4))

# All products
all_products <- sort(c(outer(eigen_A, eigen_B)), decreasing = TRUE)
cat("\nAll products λ_i × μ_j:\n")
print(round(all_products, 4))

cat("\nThey match! ✓\n")
```

**Vec Operator Property:**

The vec operator stacks matrix columns into a vector. Key property:

$\text{vec}(\mathbf{ABC}) = (\mathbf{C}' \otimes \mathbf{A})\text{vec}(\mathbf{B})$

```{r}
#| label: kronecker-vec

# Small example
A <- matrix(c(1, 2, 3, 4), nrow = 2, ncol = 2, byrow = TRUE)
B <- matrix(c(5, 6, 7, 8), nrow = 2, ncol = 2, byrow = TRUE)
C <- matrix(c(2, 0, 0, 1), nrow = 2, ncol = 2, byrow = TRUE)

# Compute ABC
ABC <- A %*% B %*% C

# vec(ABC) - stack columns
vec_ABC <- c(ABC)

cat("Matrix ABC:\n")
print(ABC)

cat("\nvec(ABC) (stacking columns):\n")
print(vec_ABC)

# Using property: vec(ABC) = (C' ⊗ A) vec(B)
vec_B <- c(B)
result <- (t(C) %x% A) %*% vec_B

cat("\n(C' ⊗ A) vec(B):\n")
print(result)

cat("\nVerify they're equal:\n")
all.equal(vec_ABC, c(result))

cat("\nThis property is used in vectorizing matrix equations\n")
```

:::{.callout-tip}
## Key Properties for Linear Models

The most important properties for linear models are:

1. **Inverse property**: $(\mathbf{A} \otimes \mathbf{B})^{-1} = \mathbf{A}^{-1} \otimes \mathbf{B}^{-1}$
   - Used for inverting multi-trait covariance matrices

2. **Mixed-product property**: $(\mathbf{A} \otimes \mathbf{B})(\mathbf{C} \otimes \mathbf{D}) = (\mathbf{AC}) \otimes (\mathbf{BD})$
   - Simplifies matrix multiplications in mixed model equations

3. **Vec operator**: $\text{vec}(\mathbf{ABC}) = (\mathbf{C}' \otimes \mathbf{A})\text{vec}(\mathbf{B})$
   - Used in derivative calculations and equation manipulation
:::

---

### Applications in Linear Models {#sec-kronecker-applications}

Kronecker products are essential for structuring variance-covariance matrices in advanced linear models.

**1. Multi-Trait Genetic Evaluation:**

For t traits measured on n animals, the variance-covariance structure is often:

$$
\mathbf{Var}(\mathbf{y}) = \mathbf{G} \otimes \mathbf{A}
$$ {#eq-multitrait-variance}

where:
- **G** is t × t genetic covariance matrix between traits
- **A** is n × n additive relationship matrix between animals
- Result is (nt) × (nt) covariance matrix

```{r}
#| label: kronecker-multitrait

# Simple example: 2 traits, 3 animals
# Genetic covariance matrix (2×2)
G <- matrix(c(
  10,  3,   # Var(trait1) = 10, Cov(trait1, trait2) = 3
   3,  5    # Var(trait2) = 5
), nrow = 2, ncol = 2, byrow = TRUE)

# Relationship matrix (3×3) - simplified
A <- matrix(c(
  1.0, 0.5, 0.0,  # Animal 1 related to itself (1.0) and animal 2 (0.5)
  0.5, 1.0, 0.25, # Animal 2 related to animals 1, 2, 3
  0.0, 0.25, 1.0  # Animal 3
), nrow = 3, ncol = 3, byrow = TRUE)

cat("Genetic covariance matrix G (2×2):\n")
print(G)

cat("\nRelationship matrix A (3×3):\n")
print(A)

# Full variance-covariance matrix
V <- G %x% A

cat("\nFull covariance matrix V = G ⊗ A (6×6):\n")
print(round(V, 2))

cat("\nInterpretation:\n")
cat("  - 6×6 matrix for 3 animals × 2 traits\n")
cat("  - Block structure reflects genetic correlations\n")
cat("  - Animals related by pedigree have correlated breeding values\n")
```

**2. Repeated Measures / Longitudinal Data:**

For measurements repeated over time, the covariance structure might be:

$$
\mathbf{Var}(\mathbf{y}) = \mathbf{I}_n \otimes \mathbf{\Sigma}_t
$$ {#eq-repeated-measures}

where:
- **I**~n~ is n × n identity (animals are independent)
- **Σ**~t~ is t × t covariance matrix across time points

```{r}
#| label: kronecker-repeated

# 4 animals, 3 time points each
n_animals <- 4
n_times <- 3

# Time covariance (AR(1) structure: correlation decreases with time)
rho <- 0.7  # Autocorrelation
Sigma_t <- matrix(c(
  1.0,  rho,  rho^2,
  rho,  1.0,  rho,
  rho^2, rho, 1.0
), nrow = 3, ncol = 3, byrow = TRUE)

cat("Time covariance matrix Σ_t (3×3):\n")
print(round(Sigma_t, 3))

# Full covariance: animals independent, but measurements within animal correlated
V <- diag(n_animals) %x% Sigma_t

cat("\nFull covariance V = I_4 ⊗ Σ_t (12×12):\n")
cat("(Showing first 6×6 block)\n")
print(round(V[1:6, 1:6], 3))

cat("\nBlock-diagonal structure: each 3×3 block is Σ_t for one animal\n")
```

**3. Multi-Trait Animal Model Preview:**

The mixed model equations for multi-trait evaluation are:

$$
\begin{bmatrix}
\mathbf{X}'(\mathbf{R} \otimes \mathbf{I})^{-1}\mathbf{X} & \mathbf{X}'(\mathbf{R} \otimes \mathbf{I})^{-1}\mathbf{Z} \\
\mathbf{Z}'(\mathbf{R} \otimes \mathbf{I})^{-1}\mathbf{X} & \mathbf{Z}'(\mathbf{R} \otimes \mathbf{I})^{-1}\mathbf{Z} + (\mathbf{G} \otimes \mathbf{A})^{-1}
\end{bmatrix}
\begin{bmatrix}
\hat{\mathbf{b}} \\
\hat{\mathbf{u}}
\end{bmatrix} =
\begin{bmatrix}
\mathbf{X}'(\mathbf{R} \otimes \mathbf{I})^{-1}\mathbf{y} \\
\mathbf{Z}'(\mathbf{R} \otimes \mathbf{I})^{-1}\mathbf{y}
\end{bmatrix}
$$ {#eq-multitrait-mme}

The inverse property is crucial: $(\mathbf{G} \otimes \mathbf{A})^{-1} = \mathbf{G}^{-1} \otimes \mathbf{A}^{-1}$

```{r}
#| label: kronecker-animal-model

# Simplified example: 2 traits, 4 animals
# Genetic covariance
G <- matrix(c(10, 3, 3, 5), nrow = 2, ncol = 2, byrow = TRUE)

# Relationship matrix (4 animals)
A <- matrix(c(
  1.00, 0.50, 0.25, 0.00,
  0.50, 1.00, 0.50, 0.25,
  0.25, 0.50, 1.00, 0.50,
  0.00, 0.25, 0.50, 1.00
), nrow = 4, ncol = 4, byrow = TRUE)

cat("G (2×2 genetic covariance):\n")
print(G)

cat("\nA (4×4 relationship matrix):\n")
print(A)

# Form G ⊗ A
G_kron_A <- G %x% A

cat("\nG ⊗ A (8×8):\n")
cat("(Showing first 4×4 block)\n")
print(round(G_kron_A[1:4, 1:4], 2))

# Inverse using property
G_inv <- solve(G)
A_inv <- solve(A)
inv_direct <- solve(G_kron_A)
inv_property <- G_inv %x% A_inv

cat("\nVerify (G ⊗ A)^(-1) = G^(-1) ⊗ A^(-1):\n")
cat("Max difference:", max(abs(inv_direct - inv_property)), "\n")

cat("\nThis property makes inversion computationally feasible!\n")
cat("Instead of inverting 8×8, we invert 2×2 and 4×4 separately.\n")
```

**4. Residual Covariance Structure:**

For heterogeneous residual variances across traits:

$$
\mathbf{R} = \mathbf{R}_0 \otimes \mathbf{I}_n
$$ {#eq-residual-structure}

where **R**~0~ is the residual covariance between traits.

```{r}
#| label: kronecker-residual

# 3 traits with different residual variances and covariances
R0 <- matrix(c(
  4.0, 0.5, 0.2,  # Trait 1: var = 4.0
  0.5, 2.0, 0.3,  # Trait 2: var = 2.0
  0.2, 0.3, 3.0   # Trait 3: var = 3.0
), nrow = 3, ncol = 3, byrow = TRUE)

n <- 5  # 5 animals

cat("Residual covariance between traits R₀ (3×3):\n")
print(R0)

# Full residual covariance
R <- R0 %x% diag(n)

cat("\nFull residual covariance R = R₀ ⊗ I₅ (15×15):\n")
cat("(Showing first 6×6 block)\n")
print(round(R[1:6, 1:6], 2))

cat("\nStructure: Traits are correlated, but residuals across animals are independent\n")
```

**5. Kronecker Structure in Design Matrices:**

For factorial designs, the design matrix can often be written using Kronecker products.

```{r}
#| label: kronecker-design

# 2×3 factorial: 2 levels of factor A, 3 levels of factor B
# 2 replicates per cell

# Factor A design (2 levels)
X_A <- matrix(c(1, 0,
                0, 1), nrow = 2, ncol = 2, byrow = TRUE)

# Factor B design (3 levels)
X_B <- matrix(c(1, 0, 0,
                0, 1, 0,
                0, 0, 1), nrow = 3, ncol = 3, byrow = TRUE)

cat("Factor A design:\n")
print(X_A)

cat("\nFactor B design:\n")
print(X_B)

# Full factorial design (one observation per cell)
X_full <- X_A %x% X_B

cat("\nFull factorial design X_A ⊗ X_B (6×6):\n")
print(X_full)

cat("\nEach row represents one cell in the 2×3 factorial\n")
```

**Livestock Example - Multi-Trait Dairy Evaluation:**

```{r}
#| label: kronecker-dairy-example

# Simplified dairy evaluation: 2 traits (milk yield, fat %), 6 cows
n_cows <- 6
n_traits <- 2

# Genetic covariance matrix
# Milk and fat % are genetically correlated
G <- matrix(c(
  100,  -5,   # Var(milk) = 100, Cov = -5 (negative: higher milk → lower fat %)
   -5,   4    # Var(fat%) = 4
), nrow = 2, ncol = 2, byrow = TRUE)

# Pedigree relationship (6 cows, 3 half-sib families)
A <- matrix(c(
  1.00, 0.00, 0.00, 0.25, 0.25, 0.00,
  0.00, 1.00, 0.50, 0.25, 0.25, 0.00,
  0.00, 0.50, 1.00, 0.25, 0.25, 0.00,
  0.25, 0.25, 0.25, 1.00, 0.50, 0.50,
  0.25, 0.25, 0.25, 0.50, 1.00, 0.50,
  0.00, 0.00, 0.00, 0.50, 0.50, 1.00
), nrow = 6, ncol = 6, byrow = TRUE)

cat("Genetic covariance G:\n")
print(G)

cat("\nAdditive relationship matrix A (6 cows):\n")
print(round(A, 2))

# Variance of breeding values
Var_u <- G %x% A

cat("\nVariance of breeding values (12×12):\n")
cat("(Showing correlation structure for first 4 observations)\n")

# Convert to correlation for easier interpretation
D <- diag(1/sqrt(diag(Var_u)))
Cor_u <- D %*% Var_u %*% D

print(round(Cor_u[1:4, 1:4], 3))

cat("\nInterpretation:\n")
cat("  - Breeding values for same cow's traits are correlated (genetic correlation)\n")
cat("  - Breeding values for related cows are correlated (pedigree)\n")
cat("  - Kronecker structure captures both effects simultaneously\n")
```

:::{.callout-important}
## Why Kronecker Products Matter

In advanced linear models, Kronecker products:

1. **Provide compact notation** for complex covariance structures
2. **Enable efficient inversion**: $(\mathbf{A} \otimes \mathbf{B})^{-1} = \mathbf{A}^{-1} \otimes \mathbf{B}^{-1}$
3. **Simplify mixed model equations** for multi-trait analysis
4. **Extend naturally** to Animal models and genetic evaluation
5. **Reduce storage** requirements through structured sparsity

Without Kronecker products, multi-trait genetic evaluation would be computationally infeasible!
:::

:::{.callout-note}
## Cross-References

Kronecker products are foundational for:

- **Week 14**: Preview of mixed models with random effects
- **Future courses**: Multi-trait Animal models
- **Future courses**: Genomic selection with multiple traits
- **Section @sec-computation**: Efficient computation with Kronecker structure
:::

---

### Computational Considerations {#sec-kronecker-computation}

**CRITICAL RULE: Never Form Kronecker Products Explicitly!**

For large matrices, explicitly computing $\mathbf{A} \otimes \mathbf{B}$ is:
- **Memory intensive**: (m×n) ⊗ (p×q) creates (mp) × (nq) matrix
- **Computationally expensive**: O(mnpq) operations
- **Usually unnecessary**: Can work implicitly using properties

**Example of the Problem:**

```{r}
#| label: kronecker-memory-problem

# Small matrices
A <- diag(100)  # 100×100
B <- diag(50)   # 50×50

# Their Kronecker product
# Would be 5000×5000 = 25 million elements!

cat(sprintf("A: %d×%d = %s elements\n",
            nrow(A), ncol(A), format(nrow(A)*ncol(A), big.mark=",")))
cat(sprintf("B: %d×%d = %s elements\n",
            nrow(B), ncol(B), format(nrow(B)*ncol(B), big.mark=",")))
cat(sprintf("A ⊗ B: %d×%d = %s elements!\n",
            nrow(A)*nrow(B), ncol(A)*ncol(B),
            format(nrow(A)*nrow(B)*ncol(A)*ncol(B), big.mark=",")))

cat("\nForming explicitly would use excessive memory and time.\n")
cat("Solution: Use implicit operations!\n")
```

**Strategy 1: Implicit Matrix-Vector Multiplication**

Instead of forming $\mathbf{C} = \mathbf{A} \otimes \mathbf{B}$ and computing $\mathbf{Cx}$, use:

$$
(\mathbf{A} \otimes \mathbf{B})\mathbf{x} = \text{vec}(\mathbf{BXA}')
$$ {#eq-kronecker-matvec}

where **X** is the matrix formed by "unvectorizing" **x**.

```{r}
#| label: kronecker-implicit-matvec

# Smaller example for demonstration
A <- matrix(c(2, 1, 1, 3), nrow = 2, ncol = 2, byrow = TRUE)
B <- matrix(c(1, 2, 3, 4), nrow = 2, ncol = 2, byrow = TRUE)
x <- c(1, 2, 3, 4)  # Vector to multiply

# Method 1: Explicit (bad for large matrices!)
C_explicit <- A %x% B
result_explicit <- C_explicit %*% x

cat("Explicit method result:\n")
print(result_explicit)

# Method 2: Implicit (efficient!)
X <- matrix(x, nrow = nrow(B), ncol = nrow(A))  # Reshape x
result_implicit <- c(B %*% X %*% t(A))  # vec(BXA')

cat("\nImplicit method result:\n")
print(result_implicit)

cat("\nVerify they're equal:\n")
all.equal(c(result_explicit), result_implicit)

cat("\nImplicit method avoids forming the 4×4 matrix A ⊗ B!\n")
```

**Strategy 2: Exploit Inverse Property**

Instead of forming $(\mathbf{A} \otimes \mathbf{B})^{-1}$, use:

$$
(\mathbf{A} \otimes \mathbf{B})^{-1} = \mathbf{A}^{-1} \otimes \mathbf{B}^{-1}
$$ {#eq-kronecker-inv-property}

Invert **A** and **B** separately (much cheaper!).

```{r}
#| label: kronecker-implicit-inverse

A <- matrix(c(4, 1, 1, 3), nrow = 2, ncol = 2, byrow = TRUE)
B <- matrix(c(5, 2, 2, 2), nrow = 2, ncol = 2, byrow = TRUE)

cat("Size comparison:\n")
cat(sprintf("Inverting A (2×2): ~%d operations\n", 2^3))
cat(sprintf("Inverting B (2×2): ~%d operations\n", 2^3))
cat(sprintf("Total for A^(-1) ⊗ B^(-1): ~%d operations\n", 2^3 + 2^3))

cat(sprintf("\nInverting A ⊗ B (4×4) directly: ~%d operations\n", 4^3))

cat("\nSavings factor: ", 4^3 / (2^3 + 2^3), "×\n")

# For larger matrices, savings are enormous
n_A <- 100
n_B <- 50
n_kron <- n_A * n_B

cat(sprintf("\nFor 100×100 and 50×50 matrices:\n"))
cat(sprintf("Direct inversion of %d×%d: ~%s operations\n",
            n_kron, n_kron, format(n_kron^3, big.mark=",")))
cat(sprintf("Separate inversions: ~%s operations\n",
            format(n_A^3 + n_B^3, big.mark=",")))
cat(sprintf("Savings: %.0f×\n", n_kron^3 / (n_A^3 + n_B^3)))
```

**Strategy 3: Solving Linear Systems**

To solve $(\mathbf{A} \otimes \mathbf{B})\mathbf{x} = \mathbf{b}$:

1. Reshape **b** into matrix **B_mat**
2. Solve $\mathbf{BYA}' = \mathbf{B_{mat}}$ for **Y** (use Sylvester equation solvers)
3. Vectorize **Y** to get **x**

```{r}
#| label: kronecker-solve

library(MASS)  # For ginv if needed

# Small example
A <- matrix(c(2, 1, 1, 2), nrow = 2, ncol = 2, byrow = TRUE)
B <- matrix(c(3, 1, 1, 3), nrow = 2, ncol = 2, byrow = TRUE)
b <- c(1, 2, 3, 4)

# Method 1: Explicit (form A ⊗ B and solve)
AkronB <- A %x% B
x_explicit <- solve(AkronB, b)

cat("Explicit solution:\n")
print(x_explicit)

# Method 2: Using inverse property
Ainv <- solve(A)
Binv <- solve(B)
B_mat <- matrix(b, nrow = nrow(B), ncol = nrow(A))
Y <- Binv %*% B_mat %*% t(Ainv)
x_implicit <- c(Y)

cat("\nImplicit solution:\n")
print(x_implicit)

cat("\nVerify they're equal:\n")
all.equal(x_explicit, x_implicit)
```

**Strategy 4: Sparse Kronecker Products**

If **A** or **B** is sparse, the Kronecker product is also sparse. Use sparse matrix representations.

```{r}
#| label: kronecker-sparse

library(Matrix)  # For sparse matrices

# Sparse matrices (diagonal)
A_sparse <- Diagonal(n = 100, x = 1:100)
B_sparse <- Diagonal(n = 50, x = 1:50)

cat("Sparse matrix A:\n")
cat(sprintf("  Dimension: %d×%d\n", nrow(A_sparse), ncol(A_sparse)))
cat(sprintf("  Non-zero elements: %d (out of %s)\n",
            length(A_sparse@x), format(nrow(A_sparse)^2, big.mark=",")))

cat("\nSparse matrix B:\n")
cat(sprintf("  Dimension: %d×%d\n", nrow(B_sparse), ncol(B_sparse)))
cat(sprintf("  Non-zero elements: %d (out of %s)\n",
            length(B_sparse@x), format(nrow(B_sparse)^2, big.mark=",")))

# Kronecker product of sparse matrices is sparse!
# (But still don't form explicitly unless necessary)

cat("\nA ⊗ B would be:\n")
cat(sprintf("  Dimension: %d×%d\n",
            nrow(A_sparse)*nrow(B_sparse), ncol(A_sparse)*ncol(B_sparse)))
cat(sprintf("  Non-zero elements: ~%s (sparse!)\n",
            format(length(A_sparse@x) * length(B_sparse@x), big.mark=",")))
cat(sprintf("  Total elements: %s\n",
            format(nrow(A_sparse)*nrow(B_sparse)*ncol(A_sparse)*ncol(B_sparse),
                   big.mark=",")))
cat(sprintf("  Sparsity: %.6f%%\n",
            100 * length(A_sparse@x) * length(B_sparse@x) /
              (nrow(A_sparse)*nrow(B_sparse)*ncol(A_sparse)*ncol(B_sparse))))
```

**Strategy 5: Block Operations**

When you must work with Kronecker products, exploit block structure:

$$
\mathbf{A} \otimes \mathbf{B} = \begin{bmatrix}
a_{11}\mathbf{B} & a_{12}\mathbf{B} & \cdots \\
a_{21}\mathbf{B} & a_{22}\mathbf{B} & \cdots \\
\vdots & \vdots & \ddots
\end{bmatrix}
$$ {#eq-kronecker-blocks}

Work with blocks **a~ij~B** rather than individual elements.

```{r}
#| label: kronecker-blocks

# Example: Extract specific block
A <- matrix(1:4, nrow = 2, ncol = 2, byrow = TRUE)
B <- matrix(5:8, nrow = 2, ncol = 2, byrow = TRUE)

cat("Matrix A:\n")
print(A)
cat("\nMatrix B:\n")
print(B)

# Don't form A ⊗ B
# Instead, if we need block (i,j), compute a_ij * B directly

i <- 1  # Row block
j <- 2  # Column block

# Block (1,2) of A ⊗ B = a_12 * B
block_12 <- A[i, j] * B

cat(sprintf("\nBlock (%d,%d) of A ⊗ B:\n", i, j))
print(block_12)

# Verify by forming explicitly (for small example only!)
AkronB <- A %x% B
block_rows <- ((i-1)*nrow(B) + 1):(i*nrow(B))
block_cols <- ((j-1)*ncol(B) + 1):(j*ncol(B))

cat("\nVerify by extracting from explicit A ⊗ B:\n")
print(AkronB[block_rows, block_cols])

cat("\nSame result, but block method avoids forming full matrix!\n")
```

**Livestock Example - Multi-Trait Mixed Model Equations:**

```{r}
#| label: kronecker-mme-efficient

# Realistic scenario: 2 traits, 1000 animals
n_traits <- 2
n_animals <- 1000

# Genetic covariance (2×2)
G <- matrix(c(10, 3, 3, 5), nrow = 2, ncol = 2, byrow = TRUE)

# We need (G ⊗ A)^(-1) where A is 1000×1000
# NEVER form the 2000×2000 matrix explicitly!

# Instead, use: (G ⊗ A)^(-1) = G^(-1) ⊗ A^(-1)

G_inv <- solve(G)

cat("Genetic covariance G (2×2):\n")
print(G)

cat("\nG^(-1) (2×2):\n")
print(round(G_inv, 4))

cat(sprintf("\nFor mixed model equations with %d traits and %d animals:\n",
            n_traits, n_animals))
cat(sprintf("  - (G ⊗ A)^(-1) would be %d×%d matrix\n",
            n_traits*n_animals, n_traits*n_animals))
cat(sprintf("  - Storing explicitly: %.1f GB (at 8 bytes/element)\n",
            (n_traits*n_animals)^2 * 8 / 1e9))

cat("\nInstead:\n")
cat(sprintf("  - Store G^(-1): %d elements\n", n_traits^2))
cat(sprintf("  - Store A^(-1): %d elements (sparse!)\n", n_animals^2))
cat("  - Use implicit operations: vec(G^(-1) * A^(-1) * X)\n")

cat("\nThis makes large-scale genetic evaluation computationally feasible!\n")
```

**Summary of Computational Best Practices:**

```{r}
#| label: kronecker-summary

cat("Kronecker Product Computational Best Practices:\n\n")

cat("✓ DO:\n")
cat("  1. Use inverse property: (A ⊗ B)^(-1) = A^(-1) ⊗ B^(-1)\n")
cat("  2. Use implicit matrix-vector products: (A ⊗ B)x = vec(BXA')\n")
cat("  3. Exploit sparsity when A or B is sparse\n")
cat("  4. Work with blocks: a_ij * B instead of full matrix\n")
cat("  5. Use specialized solvers for Sylvester equations\n\n")

cat("✗ DON'T:\n")
cat("  1. Form A ⊗ B explicitly for large matrices\n")
cat("  2. Invert A ⊗ B directly - use the inverse property\n")
cat("  3. Store full dense Kronecker products\n")
cat("  4. Ignore block structure\n\n")

cat("Key insight: Kronecker structure is for NOTATION and THEORY.\n")
cat("For COMPUTATION, exploit the structure without forming the product!\n")
```

:::{.callout-warning}
## Never Form Large Kronecker Products Explicitly!

For **G** (2×2) and **A** (10,000×10,000):
- **G ⊗ A** would be 20,000 × 20,000 = 400 million elements
- At 8 bytes per element: **3.2 GB of memory!**
- And that's just for storage - operations would be even worse

**Solution**: Use properties to avoid explicit formation:
- Inverse: $(\mathbf{G} \otimes \mathbf{A})^{-1} = \mathbf{G}^{-1} \otimes \mathbf{A}^{-1}$
- Matrix-vector: $(\mathbf{G} \otimes \mathbf{A})\mathbf{x} = \text{vec}(\mathbf{AXG}')$
- Exploit sparsity of **A**

This is why modern genetic evaluation software can handle millions of animals!
:::

:::{.callout-note}
## Cross-References

Computational considerations appear in:

- **Week 11**: Numerical stability and efficient algorithms
- **Week 12**: Sparse matrix methods for large systems
- **Section @sec-computation**: General computational best practices
- **Future courses**: Sparse matrix methods in genomic evaluation
:::

---

## Matrix Identities for Linear Models {#sec-identities}

### Identities for X, H, and Projections {#sec-projection-identities}

This section provides a comprehensive reference table of matrix identities frequently used in linear models. These identities are essential for deriving least squares properties (Week 5), understanding ANOVA decompositions (Weeks 7-9), and working with rank-deficient models (Week 12).

::: {.callout-note}
## Why These Identities Matter

These identities allow you to:

1. Simplify complex matrix expressions
2. Prove properties of least squares estimators
3. Derive variance formulas without tedious algebra
4. Verify computational results

**Pro tip**: Keep this table handy during theoretical derivations!
:::

#### Key Projection Matrix Identities

Let $\mathbf{X}$ be an $n \times p$ design matrix with full column rank, $\mathbf{H} = \mathbf{X}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'$ be the hat matrix, and $\mathbf{M} = \mathbf{I}_n - \mathbf{H}$ be the residual maker.

**Table of Fundamental Identities:**

| Identity | Name/Description | Reference |
|----------|-----------------|-----------|
| $\mathbf{H}' = \mathbf{H}$ | $\mathbf{H}$ is symmetric | @sec-projection-matrices |
| $\mathbf{H}^2 = \mathbf{H}$ | $\mathbf{H}$ is idempotent | @sec-projection-matrices |
| $\mathbf{H}\mathbf{X} = \mathbf{X}$ | Projects $\mathbf{X}$ onto itself | @sec-projection-matrices |
| $\mathbf{M}' = \mathbf{M}$ | $\mathbf{M}$ is symmetric | @sec-projection-matrices |
| $\mathbf{M}^2 = \mathbf{M}$ | $\mathbf{M}$ is idempotent | @sec-projection-matrices |
| $\mathbf{M}\mathbf{X} = \mathbf{0}$ | Residuals orthogonal to $\mathbf{X}$ | Week 5 |
| $\mathbf{H}\mathbf{M} = \mathbf{0}$ | Orthogonal projections | @sec-projection-matrices |
| $\mathbf{M}\mathbf{H} = \mathbf{0}$ | Orthogonal projections | @sec-projection-matrices |
| $\text{tr}(\mathbf{H}) = p$ | Trace equals rank | @sec-trace |
| $\text{tr}(\mathbf{M}) = n - p$ | Trace equals error df | @sec-trace |
| $\mathbf{H} + \mathbf{M} = \mathbf{I}_n$ | Partition of identity | @sec-projection-matrices |
| $r(\mathbf{H}) = p$ | Rank of hat matrix | @sec-rank |
| $r(\mathbf{M}) = n - p$ | Rank of residual maker | @sec-rank |

#### Identities Involving Normal Equations

For $\mathbf{X}'\mathbf{X}\mathbf{b} = \mathbf{X}'\mathbf{y}$:

| Identity | Interpretation | When Used |
|----------|----------------|-----------|
| $\mathbf{b} = (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{y}$ | LS solution | Week 4, 5, 6 |
| $\hat{\mathbf{y}} = \mathbf{X}\mathbf{b} = \mathbf{H}\mathbf{y}$ | Fitted values are projection | Week 5 |
| $\mathbf{e} = \mathbf{y} - \hat{\mathbf{y}} = \mathbf{M}\mathbf{y}$ | Residuals are projection | Week 5 |
| $\mathbf{X}'\mathbf{e} = \mathbf{0}$ | Normal equations satisfied | Week 5 |
| $\mathbf{X}'\hat{\mathbf{y}} = \mathbf{X}'\mathbf{y}$ | Key orthogonality | Week 5 |
| $\hat{\mathbf{y}}'\mathbf{e} = \mathbf{0}$ | Fitted values ⊥ residuals | Week 5 |

#### Rank Identities

These are critical for understanding non-full rank models (Week 12):

| Identity | Condition | Reference |
|----------|-----------|-----------|
| $r(\mathbf{X}'\mathbf{X}) = r(\mathbf{X})$ | Always | @sec-rank |
| $r(\mathbf{X}'\mathbf{X}) \leq \min(n, p)$ | Always | @sec-rank |
| $r(\mathbf{AB}) \leq \min(r(\mathbf{A}), r(\mathbf{B}))$ | General rule | @sec-rank |
| $r(\mathbf{A} + \mathbf{B}) \leq r(\mathbf{A}) + r(\mathbf{B})$ | Sum of matrices | @sec-rank |
| $r(\mathbf{A}'\mathbf{A}) = r(\mathbf{A})$ | Gram matrix | @sec-rank |

#### R Code to Verify Identities

```{r}
#| label: verify-projection-identities
#| echo: true

# Simple regression example: Broiler weight vs. age
age <- c(21, 28, 35, 42, 49, 56)  # days
weight <- c(0.5, 0.9, 1.4, 1.9, 2.5, 3.0)  # kg

n <- length(weight)
X <- cbind(1, age)  # Design matrix
p <- ncol(X)

# Compute projection matrices
XtX <- t(X) %*% X
XtX_inv <- solve(XtX)
H <- X %*% XtX_inv %*% t(X)
M <- diag(n) - H

# Verify symmetry
cat("H is symmetric:", all.equal(H, t(H)), "\n")
cat("M is symmetric:", all.equal(M, t(M)), "\n")

# Verify idempotence
cat("H is idempotent:", all.equal(H %*% H, H), "\n")
cat("M is idempotent:", all.equal(M %*% M, M), "\n")

# Verify orthogonality
cat("HM = 0:", all.equal(H %*% M, matrix(0, n, n), check.attributes = FALSE), "\n")
cat("MH = 0:", all.equal(M %*% H, matrix(0, n, n), check.attributes = FALSE), "\n")

# Verify trace properties
cat("tr(H) =", sum(diag(H)), "(should be", p, ")\n")
cat("tr(M) =", sum(diag(M)), "(should be", n - p, ")\n")

# Verify partition of identity
cat("H + M = I:", all.equal(H + M, diag(n)), "\n")

# Verify rank (using tolerance for numerical issues)
cat("r(H) =", qr(H)$rank, "(should be", p, ")\n")
cat("r(M) =", qr(M)$rank, "(should be", n - p, ")\n")

# Verify MX = 0
cat("MX = 0:", all.equal(M %*% X, matrix(0, n, p), check.attributes = FALSE), "\n")

# Verify HX = X
cat("HX = X:", all.equal(H %*% X, X), "\n")
```

#### Practical Application: Verifying Orthogonality

**Example**: Dairy cow milk yield regression on days in milk.

```{r}
#| label: verify-orthogonality-dairy
#| echo: true

# Simulated data: n=8 Holstein cows
dim <- c(30, 60, 90, 120, 150, 180, 210, 240)  # Days in milk
milk <- c(35, 38, 36, 33, 30, 27, 24, 21)  # kg/day (lactation curve)

# Fit model
X <- cbind(1, dim)
y <- milk
n <- length(y)

# Solve normal equations
b <- solve(t(X) %*% X) %*% t(X) %*% y

# Compute fitted values and residuals
y_hat <- X %*% b
e <- y - y_hat

# Verify key identities
cat("X'e should be zero:\n")
print(t(X) %*% e)

cat("\ny_hat'e should be zero:", t(y_hat) %*% e, "\n")

cat("\nX'y_hat should equal X'y:\n")
cat("X'y_hat:\n")
print(t(X) %*% y_hat)
cat("X'y:\n")
print(t(X) %*% y)
cat("Equal?", all.equal(t(X) %*% y_hat, t(X) %*% y), "\n")
```

::: {.callout-warning}
## Numerical Precision

When verifying identities computationally, use `all.equal()` instead of `==` because:

1. Floating-point arithmetic introduces small errors
2. Matrix operations accumulate rounding errors
3. `all.equal()` uses tolerance (default: $1.5 \times 10^{-8}$)

**Example of what NOT to do**:
```{r}
#| label: bad-comparison
#| echo: true
#| eval: false

# BAD - Too strict!
if (sum(t(X) %*% e) == 0) {
  cat("X'e is zero\n")  # Might fail due to rounding
}

# GOOD - Tolerant comparison
if (all.equal(t(X) %*% e, matrix(0, 2, 1), check.attributes = FALSE)) {
  cat("X'e is zero (within tolerance)\n")
}
```
:::

#### Useful Compound Identities

These appear frequently in variance derivations (see @sec-variance-formulas):

| Identity | Expanded Form | Used For |
|----------|--------------|----------|
| $\mathbf{y}'\mathbf{H}\mathbf{y}$ | $\mathbf{b}'\mathbf{X}'\mathbf{y}$ | SSR (model sum of squares) |
| $\mathbf{y}'\mathbf{M}\mathbf{y}$ | $\mathbf{y}'\mathbf{y} - \mathbf{b}'\mathbf{X}'\mathbf{y}$ | SSE (error sum of squares) |
| $\mathbf{y}'(\mathbf{H} + \mathbf{M})\mathbf{y}$ | $\mathbf{y}'\mathbf{y}$ | Total sum of squares |
| $\text{E}(\mathbf{y}'\mathbf{H}\mathbf{y})$ | $\mathbf{X}\boldsymbol{\beta})'\mathbf{H}(\mathbf{X}\boldsymbol{\beta}) + \sigma^2 p$ | Expected SSR |
| $\text{E}(\mathbf{y}'\mathbf{M}\mathbf{y})$ | $\sigma^2(n-p)$ | Expected SSE |

**Derivation example** (Expected SSE):

$$
\begin{align}
\text{E}(\mathbf{y}'\mathbf{M}\mathbf{y}) &= \text{E}[(\mathbf{X}\boldsymbol{\beta} + \mathbf{e})'\mathbf{M}(\mathbf{X}\boldsymbol{\beta} + \mathbf{e})] \\
&= \text{E}[\mathbf{e}'\mathbf{M}\mathbf{e}] \quad \text{(since } \mathbf{M}\mathbf{X} = \mathbf{0}) \\
&= \text{E}[\text{tr}(\mathbf{e}'\mathbf{M}\mathbf{e})] \\
&= \text{E}[\text{tr}(\mathbf{M}\mathbf{e}\mathbf{e}')] \quad \text{(trace is cyclic)} \\
&= \text{tr}(\mathbf{M} \cdot \text{E}[\mathbf{e}\mathbf{e}']) \\
&= \text{tr}(\mathbf{M} \cdot \sigma^2\mathbf{I}_n) \\
&= \sigma^2 \text{tr}(\mathbf{M}) \\
&= \sigma^2(n - p)
\end{align}
$$

This derivation appears in Week 5 when proving that $\hat{\sigma}^2 = \text{SSE}/(n-p)$ is unbiased.

---

### Variance and Covariance Formulas {#sec-variance-formulas}

This section presents key variance and covariance formulas for linear models. These formulas are derived in Week 5 and used throughout the course for constructing confidence intervals (Week 6), testing contrasts (Week 8), and understanding precision of estimates.

#### General Variance Formula

For a linear model $\mathbf{y} \sim N(\mathbf{X}\boldsymbol{\beta}, \sigma^2\mathbf{I}_n)$, and any matrix $\mathbf{A}$:

$$
\text{Var}(\mathbf{Ay}) = \mathbf{A} \cdot \text{Var}(\mathbf{y}) \cdot \mathbf{A}' = \mathbf{A} \cdot \sigma^2\mathbf{I}_n \cdot \mathbf{A}' = \sigma^2 \mathbf{A}\mathbf{A}'
$$

This single formula generates all variance formulas below!

::: {.callout-tip}
## Memory Aid

To find the variance of ANY linear combination $\mathbf{Ay}$:

1. Write down $\mathbf{A}$
2. Compute $\mathbf{A}\mathbf{A}'$
3. Multiply by $\sigma^2$

**That's it!**
:::

#### Variance of Least Squares Estimator

For $\mathbf{b} = (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{y}$:

$$
\begin{align}
\text{Var}(\mathbf{b}) &= \text{Var}[(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{y}] \\
&= (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}' \cdot \sigma^2\mathbf{I}_n \cdot \mathbf{X}(\mathbf{X}'\mathbf{X})^{-1} \\
&= \sigma^2 (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{X}(\mathbf{X}'\mathbf{X})^{-1} \\
&= \sigma^2 (\mathbf{X}'\mathbf{X})^{-1}
\end{align}
$$

**Key properties**:

- Standard errors: $\text{se}(b_j) = \sqrt{\hat{\sigma}^2 [(\mathbf{X}'\mathbf{X})^{-1}]_{jj}}$
- Diagonal elements of $(\mathbf{X}'\mathbf{X})^{-1}$ give variances
- Off-diagonal elements give covariances between estimates

#### Variance of Fitted Values

For $\hat{\mathbf{y}} = \mathbf{H}\mathbf{y}$:

$$
\text{Var}(\hat{\mathbf{y}}) = \text{Var}(\mathbf{H}\mathbf{y}) = \sigma^2 \mathbf{H}\mathbf{H}' = \sigma^2 \mathbf{H}
$$

(using $\mathbf{H}' = \mathbf{H}$ and $\mathbf{H}^2 = \mathbf{H}$)

**For a specific fitted value** $\hat{y}_i$:

$$
\text{Var}(\hat{y}_i) = \sigma^2 h_{ii}
$$

where $h_{ii}$ is the $i$-th diagonal element of $\mathbf{H}$ (the "leverage" of observation $i$).

#### Variance of Residuals

For $\mathbf{e} = \mathbf{M}\mathbf{y}$:

$$
\text{Var}(\mathbf{e}) = \text{Var}(\mathbf{M}\mathbf{y}) = \sigma^2 \mathbf{M}\mathbf{M}' = \sigma^2 \mathbf{M}
$$

**For a specific residual** $e_i$:

$$
\text{Var}(e_i) = \sigma^2 (1 - h_{ii})
$$

Note: Residuals have **different** variances (heteroscedastic), even though errors are homoscedastic!

#### Variance of Predictions

For a **new** observation with predictor values $\mathbf{x}_0$ (a $p \times 1$ vector):

$$
\hat{y}_0 = \mathbf{x}_0'\mathbf{b}
$$

**Variance**:

$$
\text{Var}(\hat{y}_0) = \text{Var}(\mathbf{x}_0'\mathbf{b}) = \mathbf{x}_0' \cdot \text{Var}(\mathbf{b}) \cdot \mathbf{x}_0 = \sigma^2 \mathbf{x}_0'(\mathbf{X}'\mathbf{X})^{-1}\mathbf{x}_0
$$

**Prediction interval** accounts for both estimation uncertainty AND observation error:

$$
\text{Var}(y_0 - \hat{y}_0) = \sigma^2 [1 + \mathbf{x}_0'(\mathbf{X}'\mathbf{X})^{-1}\mathbf{x}_0]
$$

::: {.callout-note}
## Confidence Interval vs. Prediction Interval

- **Confidence interval** for $\text{E}(y_0)$: Uses $\text{Var}(\hat{y}_0) = \sigma^2 \mathbf{x}_0'(\mathbf{X}'\mathbf{X})^{-1}\mathbf{x}_0$
- **Prediction interval** for new observation $y_0$: Adds $\sigma^2$ for observation error

Prediction intervals are ALWAYS wider!
:::

#### Variance of Linear Contrasts

For a contrast $\mathbf{c}'\boldsymbol{\beta}$ estimated by $\mathbf{c}'\mathbf{b}$ (Week 8):

$$
\text{Var}(\mathbf{c}'\mathbf{b}) = \mathbf{c}' \cdot \text{Var}(\mathbf{b}) \cdot \mathbf{c} = \sigma^2 \mathbf{c}'(\mathbf{X}'\mathbf{X})^{-1}\mathbf{c}
$$

**Standard error**:

$$
\text{se}(\mathbf{c}'\mathbf{b}) = \sqrt{\hat{\sigma}^2 \mathbf{c}'(\mathbf{X}'\mathbf{X})^{-1}\mathbf{c}}
$$

#### Covariance Properties

**Key covariances**:

| Quantity 1 | Quantity 2 | Covariance | Interpretation |
|-----------|-----------|------------|----------------|
| $\mathbf{b}$ | $\mathbf{e}$ | $\mathbf{0}$ | Estimates independent of residuals |
| $\hat{\mathbf{y}}$ | $\mathbf{e}$ | $\mathbf{0}$ | Fitted values independent of residuals |
| $b_j$ | $b_k$ | $\sigma^2 [(\mathbf{X}'\mathbf{X})^{-1}]_{jk}$ | Estimates may be correlated |

**Proof that** $\text{Cov}(\mathbf{b}, \mathbf{e}) = \mathbf{0}$:

$$
\begin{align}
\text{Cov}(\mathbf{b}, \mathbf{e}) &= \text{Cov}[(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{y}, \mathbf{M}\mathbf{y}] \\
&= (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}' \cdot \text{Var}(\mathbf{y}) \cdot \mathbf{M}' \\
&= \sigma^2 (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{M} \\
&= \sigma^2 (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'(\mathbf{I} - \mathbf{H}) \\
&= \sigma^2 (\mathbf{X}'\mathbf{X})^{-1}(\mathbf{X}' - \mathbf{X}'\mathbf{H}) \\
&= \sigma^2 (\mathbf{X}'\mathbf{X})^{-1}(\mathbf{X}' - \mathbf{X}') \\
&= \mathbf{0}
\end{align}
$$

(using $\mathbf{X}'\mathbf{H} = \mathbf{X}'$)

#### R Implementation: Computing Variances

**Example**: Swine litter size regression on parity number.

```{r}
#| label: variance-formulas-swine
#| echo: true

# Data: n=10 sows
parity <- c(1, 1, 2, 2, 3, 3, 4, 4, 5, 5)  # Parity number
litter_size <- c(9, 10, 11, 12, 12, 13, 13, 14, 14, 15)  # Piglets born alive

n <- length(litter_size)
X <- cbind(1, parity)
y <- litter_size
p <- ncol(X)

# Solve for estimates
XtX <- t(X) %*% X
XtX_inv <- solve(XtX)
b <- XtX_inv %*% t(X) %*% y

# Compute residuals and variance estimate
y_hat <- X %*% b
e <- y - y_hat
SSE <- sum(e^2)
sigma2_hat <- SSE / (n - p)

# Variance-covariance matrix of b
Var_b <- sigma2_hat * XtX_inv
cat("Var(b):\n")
print(Var_b)

# Standard errors of coefficients
se_b <- sqrt(diag(Var_b))
cat("\nStandard errors:\n")
cat("se(b0) =", se_b[1], "\n")
cat("se(b1) =", se_b[2], "\n")

# Compare with lm()
fit <- lm(litter_size ~ parity)
cat("\nlm() standard errors:\n")
print(summary(fit)$coefficients[, "Std. Error"])

# Variance of fitted values
H <- X %*% XtX_inv %*% t(X)
Var_y_hat <- sigma2_hat * H
cat("\nVariance of fitted values (diagonal of Var(y_hat)):\n")
print(diag(Var_y_hat))

# Leverage values
h <- diag(H)
cat("\nLeverage values (h_ii):\n")
print(h)

# Variance of residuals
Var_e <- sigma2_hat * (diag(n) - H)
cat("\nVariance of residuals (diagonal of Var(e)):\n")
print(diag(Var_e))

# Verify: Var(e_i) = sigma2 * (1 - h_ii)
cat("\nVerify Var(e_i) = sigma2 * (1 - h_ii):\n")
print(sigma2_hat * (1 - h))
```

#### Prediction Example: Predicting Future Litter Size

```{r}
#| label: prediction-variance-swine
#| echo: true

# Predict litter size for parity 6
x0 <- c(1, 6)  # Intercept and parity=6

# Point prediction
y0_hat <- t(x0) %*% b
cat("Predicted litter size at parity 6:", y0_hat, "\n")

# Variance of prediction (for mean response)
var_pred_mean <- sigma2_hat * t(x0) %*% XtX_inv %*% x0
se_pred_mean <- sqrt(var_pred_mean)
cat("SE for mean response:", se_pred_mean, "\n")

# Variance of prediction (for new observation)
var_pred_obs <- sigma2_hat * (1 + t(x0) %*% XtX_inv %*% x0)
se_pred_obs <- sqrt(var_pred_obs)
cat("SE for new observation:", se_pred_obs, "\n")

# 95% confidence interval for mean response
t_crit <- qt(0.975, df = n - p)
ci_lower <- y0_hat - t_crit * se_pred_mean
ci_upper <- y0_hat + t_crit * se_pred_mean
cat("\n95% CI for mean at parity 6: [", ci_lower, ",", ci_upper, "]\n")

# 95% prediction interval for new observation
pi_lower <- y0_hat - t_crit * se_pred_obs
pi_upper <- y0_hat + t_crit * se_pred_obs
cat("95% PI for new obs at parity 6: [", pi_lower, ",", pi_upper, "]\n")

# Compare with predict()
cat("\nCompare with predict():\n")
new_data <- data.frame(parity = 6)
pred_mean <- predict(fit, newdata = new_data, interval = "confidence", level = 0.95)
pred_obs <- predict(fit, newdata = new_data, interval = "prediction", level = 0.95)
print(pred_mean)
print(pred_obs)
```

#### Contrast Example: Comparing Two Parities

```{r}
#| label: contrast-variance-swine
#| echo: true

# Contrast: difference between parity 4 and parity 2
# E(y|parity=4) - E(y|parity=2) = (β0 + 4β1) - (β0 + 2β1) = 2β1
c <- c(0, 2)  # Contrast vector

# Estimate
contrast_est <- t(c) %*% b
cat("Estimated difference (parity 4 - parity 2):", contrast_est, "\n")

# Variance
var_contrast <- sigma2_hat * t(c) %*% XtX_inv %*% c
se_contrast <- sqrt(var_contrast)
cat("SE of difference:", se_contrast, "\n")

# Test H0: no difference
t_stat <- contrast_est / se_contrast
p_value <- 2 * pt(abs(t_stat), df = n - p, lower.tail = FALSE)
cat("t-statistic:", t_stat, "\n")
cat("p-value:", p_value, "\n")

# 95% CI
ci_lower_c <- contrast_est - t_crit * se_contrast
ci_upper_c <- contrast_est + t_crit * se_contrast
cat("95% CI for difference: [", ci_lower_c, ",", ci_upper_c, "]\n")
```

::: {.callout-tip}
## Practical Insight: Variance Depends on X

All these variances depend on $(\mathbf{X}'\mathbf{X})^{-1}$, which means:

1. **More data** (larger $n$) → smaller variance
2. **Better spread** in $x$ values → smaller variance
3. **Collinearity** (ill-conditioned $\mathbf{X}'\mathbf{X}$) → larger variance
4. **Predictions far from** $\bar{x}$ → larger variance

This explains why:
- Balanced designs are efficient
- Extreme extrapolation is risky
- Collinearity inflates standard errors
:::

#### Summary Table of Key Variance Formulas

| Quantity | Formula | Dimensions | Used In |
|----------|---------|------------|---------|
| $\text{Var}(\mathbf{b})$ | $\sigma^2(\mathbf{X}'\mathbf{X})^{-1}$ | $p \times p$ | Week 5, 6, 8 |
| $\text{Var}(\hat{\mathbf{y}})$ | $\sigma^2\mathbf{H}$ | $n \times n$ | Week 5, 11 |
| $\text{Var}(\mathbf{e})$ | $\sigma^2\mathbf{M}$ | $n \times n$ | Week 5, 11 |
| $\text{Var}(\hat{y}_i)$ | $\sigma^2 h_{ii}$ | scalar | Week 11 (diagnostics) |
| $\text{Var}(e_i)$ | $\sigma^2(1 - h_{ii})$ | scalar | Week 11 (diagnostics) |
| $\text{Var}(\hat{y}_0)$ | $\sigma^2\mathbf{x}_0'(\mathbf{X}'\mathbf{X})^{-1}\mathbf{x}_0$ | scalar | Week 6 (prediction) |
| $\text{Var}(y_0 - \hat{y}_0)$ | $\sigma^2[1 + \mathbf{x}_0'(\mathbf{X}'\mathbf{X})^{-1}\mathbf{x}_0]$ | scalar | Week 6 (prediction) |
| $\text{Var}(\mathbf{c}'\mathbf{b})$ | $\sigma^2\mathbf{c}'(\mathbf{X}'\mathbf{X})^{-1}\mathbf{c}$ | scalar | Week 8 (contrasts) |

---

### Sum of Squares Identities {#sec-ss-identities}

Sum of squares decompositions are the foundation of ANOVA (Weeks 7-9) and regression analysis (Weeks 4-6). This section presents the key identities in matrix form.

#### Basic Decomposition: SST = SSM + SSE

For the linear model $\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \mathbf{e}$:

$$
\underbrace{\sum_{i=1}^n (y_i - \bar{y})^2}_{\text{SST}} = \underbrace{\sum_{i=1}^n (\hat{y}_i - \bar{y})^2}_{\text{SSM}} + \underbrace{\sum_{i=1}^n (y_i - \hat{y}_i)^2}_{\text{SSE}}
$$

**Matrix form**:

$$
\mathbf{y}'\mathbf{C}\mathbf{y} = \mathbf{b}'\mathbf{X}'\mathbf{C}\mathbf{y} + \mathbf{e}'\mathbf{e}
$$

where $\mathbf{C} = \mathbf{I}_n - n^{-1}\mathbf{1}_n\mathbf{1}_n'$ is the **centering matrix**.

::: {.callout-note}
## Alternative Matrix Formulations

There are several equivalent ways to write sum of squares:

**SST (Total Sum of Squares)**:
$$
\text{SST} = \mathbf{y}'\mathbf{C}\mathbf{y} = \mathbf{y}'\mathbf{y} - n\bar{y}^2 = \sum_{i=1}^n y_i^2 - \frac{(\sum_{i=1}^n y_i)^2}{n}
$$

**SSM (Model Sum of Squares)**:
$$
\text{SSM} = \hat{\mathbf{y}}'\mathbf{C}\hat{\mathbf{y}} = \mathbf{b}'\mathbf{X}'\mathbf{y} - n\bar{y}^2 = \mathbf{y}'\mathbf{H}\mathbf{C}\mathbf{y}
$$

**SSE (Error Sum of Squares)**:
$$
\text{SSE} = \mathbf{e}'\mathbf{e} = \mathbf{y}'\mathbf{y} - \mathbf{b}'\mathbf{X}'\mathbf{y} = \mathbf{y}'\mathbf{M}\mathbf{y}
$$
:::

#### Proof of Orthogonal Decomposition

**Key insight**: The decomposition works because $\hat{\mathbf{y}}'\mathbf{e} = \mathbf{0}$ (orthogonality).

$$
\begin{align}
\mathbf{y}'\mathbf{y} &= (\hat{\mathbf{y}} + \mathbf{e})'(\hat{\mathbf{y}} + \mathbf{e}) \\
&= \hat{\mathbf{y}}'\hat{\mathbf{y}} + 2\hat{\mathbf{y}}'\mathbf{e} + \mathbf{e}'\mathbf{e} \\
&= \hat{\mathbf{y}}'\hat{\mathbf{y}} + \mathbf{e}'\mathbf{e} \quad \text{(since } \hat{\mathbf{y}}'\mathbf{e} = \mathbf{0})
\end{align}
$$

Subtracting $n\bar{y}^2$ from both sides:

$$
\mathbf{y}'\mathbf{y} - n\bar{y}^2 = (\hat{\mathbf{y}}'\hat{\mathbf{y}} - n\bar{y}^2) + \mathbf{e}'\mathbf{e}
$$

$$
\text{SST} = \text{SSM} + \text{SSE}
$$

#### Projection Matrix Form

Using $\mathbf{H}$ and $\mathbf{M}$:

| Sum of Squares | Formula 1 | Formula 2 | Formula 3 |
|---------------|-----------|-----------|-----------|
| SST | $\mathbf{y}'\mathbf{C}\mathbf{y}$ | $\mathbf{y}'(\mathbf{I} - n^{-1}\mathbf{J})\mathbf{y}$ | $\mathbf{y}'\mathbf{y} - n\bar{y}^2$ |
| SSM | $\mathbf{y}'\mathbf{H}\mathbf{C}\mathbf{y}$ | $\mathbf{b}'\mathbf{X}'\mathbf{y} - n\bar{y}^2$ | $\hat{\mathbf{y}}'\mathbf{C}\hat{\mathbf{y}}$ |
| SSE | $\mathbf{y}'\mathbf{M}\mathbf{y}$ | $\mathbf{y}'\mathbf{y} - \mathbf{b}'\mathbf{X}'\mathbf{y}$ | $\mathbf{e}'\mathbf{e}$ |

where $\mathbf{J} = \mathbf{1}_n\mathbf{1}_n'$ (matrix of all ones).

#### Degrees of Freedom

Each sum of squares has associated degrees of freedom equal to the **rank** of its projection matrix:

| Source | Sum of Squares | df | Projection Matrix | Rank |
|--------|---------------|----|------------------|------|
| Total | SST | $n-1$ | $\mathbf{C}$ | $r(\mathbf{C}) = n-1$ |
| Model | SSM | $p-1$ | $\mathbf{H}\mathbf{C}$ | $r(\mathbf{H}\mathbf{C}) = p-1$ |
| Error | SSE | $n-p$ | $\mathbf{M}$ | $r(\mathbf{M}) = n-p$ |

**Key property**: $(n-1) = (p-1) + (n-p)$ (df add up!)

#### Identities for Sums of Squares

**Computational shortcuts**:

| Identity | Why It's Useful |
|----------|----------------|
| $\mathbf{b}'\mathbf{X}'\mathbf{y} = \hat{\mathbf{y}}'\mathbf{y}$ | Avoids computing $\hat{\mathbf{y}}$ explicitly |
| $\text{SSE} = \mathbf{y}'\mathbf{y} - \mathbf{b}'\mathbf{X}'\mathbf{y}$ | Compute SSE without residuals |
| $\mathbf{e}'\mathbf{e} = (\mathbf{y} - \mathbf{X}\mathbf{b})'(\mathbf{y} - \mathbf{X}\mathbf{b})$ | Alternative for SSE |
| $\text{SST} = \mathbf{y}'\mathbf{y} - n\bar{y}^2$ | Correction for the mean |

**Verification identities**:

| Identity | Check |
|----------|-------|
| $\text{SST} = \text{SSM} + \text{SSE}$ | Always true (orthogonal decomposition) |
| $\text{SSM} = \mathbf{b}'(\mathbf{X}'\mathbf{y} - n\bar{y}\mathbf{1}_p)$ | Alternative computation |
| $R^2 = \text{SSM}/\text{SST} = 1 - \text{SSE}/\text{SST}$ | Both should give same $R^2$ |

#### Multi-Factor ANOVA Decompositions

For two-way ANOVA (Week 9), the sum of squares partition extends:

$$
\text{SST} = \text{SS(A)} + \text{SS(B)} + \text{SS(AB)} + \text{SSE}
$$

**Matrix representation**: Each source corresponds to a projection matrix.

For model $\mathbf{y} = \mathbf{X}_A\boldsymbol{\alpha} + \mathbf{X}_B\boldsymbol{\beta} + \mathbf{X}_{AB}\boldsymbol{\gamma} + \mathbf{e}$:

- $\text{SS(A)} = \mathbf{y}'(\mathbf{H}_A - \mathbf{H}_\mu)\mathbf{y}$
- $\text{SS(B)} = \mathbf{y}'(\mathbf{H}_B - \mathbf{H}_\mu)\mathbf{y}$
- $\text{SS(AB)} = \mathbf{y}'(\mathbf{H}_{full} - \mathbf{H}_A - \mathbf{H}_B + \mathbf{H}_\mu)\mathbf{y}$
- $\text{SSE} = \mathbf{y}'(\mathbf{I} - \mathbf{H}_{full})\mathbf{y}$

where:
- $\mathbf{H}_\mu$ = projection onto intercept only
- $\mathbf{H}_A$ = projection onto intercept + A
- $\mathbf{H}_B$ = projection onto intercept + B
- $\mathbf{H}_{full}$ = projection onto full model

::: {.callout-warning}
## Type I vs. Type III Sums of Squares

The formulas above give **Type III SS** (each effect adjusted for all others).

**Type I SS** (sequential) depend on the order:
- $\text{SS(A|}\mu) = \mathbf{y}'(\mathbf{H}_A - \mathbf{H}_\mu)\mathbf{y}$
- $\text{SS(B|}\mu, A) = \mathbf{y}'(\mathbf{H}_{AB} - \mathbf{H}_A)\mathbf{y}$

**For balanced designs**, Type I = Type III. **For unbalanced designs**, they differ!
:::

#### R Implementation: Sum of Squares Computations

**Example**: Beef cattle weight gain (ADG) by breed.

```{r}
#| label: sum-of-squares-beef
#| echo: true

# One-way ANOVA: ADG by breed (3 breeds, n=4 per breed)
breed <- factor(rep(c("Angus", "Hereford", "Charolais"), each = 4))
adg <- c(1.2, 1.3, 1.1, 1.4,    # Angus
         1.1, 1.2, 1.0, 1.3,    # Hereford
         1.5, 1.6, 1.4, 1.7)    # Charolais

n <- length(adg)
y <- adg

# Design matrix (cell means model)
X <- model.matrix(~ breed - 1)
p <- ncol(X)

# Solve normal equations
b <- solve(t(X) %*% X) %*% t(X) %*% y
y_hat <- X %*% b
e <- y - y_hat

# Method 1: Using formulas
y_bar <- mean(y)
SST <- sum((y - y_bar)^2)
SSM <- sum((y_hat - y_bar)^2)
SSE <- sum(e^2)

cat("Method 1 (direct computation):\n")
cat("SST =", SST, "\n")
cat("SSM =", SSM, "\n")
cat("SSE =", SSE, "\n")
cat("SSM + SSE =", SSM + SSE, "(should equal SST)\n\n")

# Method 2: Matrix formulas
SST_mat <- t(y) %*% y - n * y_bar^2
SSM_mat <- t(b) %*% t(X) %*% y - n * y_bar^2
SSE_mat <- t(y) %*% y - t(b) %*% t(X) %*% y

cat("Method 2 (matrix formulas):\n")
cat("SST =", SST_mat, "\n")
cat("SSM =", SSM_mat, "\n")
cat("SSE =", SSE_mat, "\n\n")

# Method 3: Using projection matrices
C <- diag(n) - (1/n) * matrix(1, n, n)  # Centering matrix
H <- X %*% solve(t(X) %*% X) %*% t(X)
M <- diag(n) - H

SST_proj <- t(y) %*% C %*% y
SSM_proj <- t(y) %*% H %*% C %*% y
SSE_proj <- t(y) %*% M %*% y

cat("Method 3 (projection matrices):\n")
cat("SST =", SST_proj, "\n")
cat("SSM =", SSM_proj, "\n")
cat("SSE =", SSE_proj, "\n\n")

# Degrees of freedom
df_total <- n - 1
df_model <- p - 1
df_error <- n - p

cat("Degrees of freedom:\n")
cat("df(Total) =", df_total, "\n")
cat("df(Model) =", df_model, "\n")
cat("df(Error) =", df_error, "\n")
cat("Check:", df_model + df_error, "= df(Total)\n\n")

# Mean squares and F-statistic
MSM <- SSM / df_model
MSE <- SSE / df_error
F_stat <- MSM / MSE
p_value <- pf(F_stat, df_model, df_error, lower.tail = FALSE)

# ANOVA table
cat("ANOVA Table:\n")
cat("Source    df    SS       MS       F        p-value\n")
cat("-------------------------------------------------------\n")
cat(sprintf("Model     %2d    %.4f   %.4f   %.4f   %.4f\n",
            df_model, SSM, MSM, F_stat, p_value))
cat(sprintf("Error     %2d    %.4f   %.4f\n", df_error, SSE, MSE))
cat(sprintf("Total     %2d    %.4f\n", df_total, SST))

# Compare with lm()
cat("\n\nCompare with lm():\n")
fit <- lm(adg ~ breed)
print(anova(fit))

# R-squared
R2 <- SSM / SST
cat("\nR-squared:", R2, "\n")
```

#### Two-Way ANOVA Example

**Example**: Dairy milk yield by breed and farm.

```{r}
#| label: two-way-anova-ss
#| echo: true

# Balanced 2×3 factorial: 2 breeds, 3 farms, n=2 per cell
breed <- factor(rep(c("Holstein", "Jersey"), each = 6))
farm <- factor(rep(rep(c("A", "B", "C"), each = 2), 2))
milk <- c(32, 33,  # Holstein, Farm A
          30, 31,  # Holstein, Farm B
          28, 29,  # Holstein, Farm C
          25, 26,  # Jersey, Farm A
          24, 25,  # Jersey, Farm B
          22, 23)  # Jersey, Farm C

y <- milk
n <- length(y)
y_bar <- mean(y)

# Fit models
fit_null <- lm(milk ~ 1)
fit_breed <- lm(milk ~ breed)
fit_farm <- lm(milk ~ farm)
fit_full <- lm(milk ~ breed + farm + breed:farm)

# Extract SS using anova()
cat("Type I (Sequential) SS:\n")
anova_seq <- anova(fit_full)
print(anova_seq)

# Type III SS (using car package)
library(car)
cat("\n\nType III SS:\n")
anova_type3 <- Anova(fit_full, type = 3)
print(anova_type3)

# Manual computation of Type III SS
# SS(Breed | everything else)
fit_no_breed <- lm(milk ~ farm + breed:farm)
SS_breed_III <- sum(residuals(fit_no_breed)^2) - sum(residuals(fit_full)^2)

# SS(Farm | everything else)
fit_no_farm <- lm(milk ~ breed + breed:farm)
SS_farm_III <- sum(residuals(fit_no_farm)^2) - sum(residuals(fit_full)^2)

# SS(Interaction | everything else)
fit_no_int <- lm(milk ~ breed + farm)
SS_int_III <- sum(residuals(fit_no_int)^2) - sum(residuals(fit_full)^2)

cat("\n\nManual Type III SS:\n")
cat("SS(Breed | Farm, Interaction) =", SS_breed_III, "\n")
cat("SS(Farm | Breed, Interaction) =", SS_farm_III, "\n")
cat("SS(Interaction | Breed, Farm) =", SS_int_III, "\n")
```

::: {.callout-tip}
## Computational Strategy

**For efficiency**:

1. **Never** form projection matrices explicitly for large $n$
2. Use $\mathbf{b}'\mathbf{X}'\mathbf{y}$ instead of $\hat{\mathbf{y}}'\hat{\mathbf{y}}$
3. Compute SSE as $\mathbf{y}'\mathbf{y} - \mathbf{b}'\mathbf{X}'\mathbf{y}$ (avoids residual vector)
4. Use incremental SS: $\text{SS(Full)} - \text{SS(Reduced)}$

**Example**:
```r
# Efficient
SSE <- t(y) %*% y - t(b) %*% t(X) %*% y

# Inefficient (for large n)
y_hat <- X %*% b
SSE <- t(y - y_hat) %*% (y - y_hat)
```
:::

#### Identity Summary

**Key sum of squares identities**:

| Identity | Formula | Usage |
|----------|---------|-------|
| Decomposition | $\text{SST} = \text{SSM} + \text{SSE}$ | Always holds |
| SST | $\mathbf{y}'\mathbf{y} - n\bar{y}^2$ | Correction for mean |
| SSM | $\mathbf{b}'\mathbf{X}'\mathbf{y} - n\bar{y}^2$ | Model SS |
| SSE | $\mathbf{y}'\mathbf{y} - \mathbf{b}'\mathbf{X}'\mathbf{y}$ | Error SS |
| $R^2$ | $\text{SSM}/\text{SST} = 1 - \text{SSE}/\text{SST}$ | Goodness of fit |
| Incremental SS | $\text{SS(Full)} - \text{SS(Reduced)}$ | Testing nested models |
| Orthogonality | $\hat{\mathbf{y}}'\mathbf{e} = \mathbf{0}$ | Key to decomposition |

---

### Computational Shortcuts {#sec-computational-shortcuts}

R provides specialized functions that are faster and more numerically stable than explicit matrix operations. This section shows you how to write efficient code for linear models computations.

::: {.callout-important}
## Why Computational Efficiency Matters

For large datasets (n > 10,000 or p > 1,000):

- Naive implementations can be **100× slower**
- Memory usage can explode
- Numerical errors accumulate

**Learning efficient coding now** will pay dividends when you work with real breeding datasets!
:::

#### Efficient Cross-Product Functions

**Instead of** `t(X) %*% X`, **use** `crossprod(X)`:

| Operation | Slow (Don't Use) | Fast (Use This) | Speedup |
|-----------|-----------------|----------------|---------|
| $\mathbf{X}'\mathbf{X}$ | `t(X) %*% X` | `crossprod(X)` | ~2× |
| $\mathbf{X}'\mathbf{y}$ | `t(X) %*% y` | `crossprod(X, y)` | ~2× |
| $\mathbf{X}\mathbf{X}'$ | `X %*% t(X)` | `tcrossprod(X)` | ~2× |
| $\mathbf{X}\mathbf{y}'$ | `X %*% t(y)` | `tcrossprod(X, y)` | ~2× |

**Why faster?**

1. No explicit transpose created (saves memory and time)
2. Optimized BLAS routines called directly
3. Better cache utilization

**Example**:

```{r}
#| label: crossprod-example
#| echo: true

# Generate larger dataset
set.seed(123)
n <- 5000
p <- 10
X <- matrix(rnorm(n * p), n, p)
y <- rnorm(n)

# Method 1: Slow
system.time({
  XtX_slow <- t(X) %*% X
  Xty_slow <- t(X) %*% y
})

# Method 2: Fast
system.time({
  XtX_fast <- crossprod(X)
  Xty_fast <- crossprod(X, y)
})

# Verify same result
cat("Results identical?", all.equal(XtX_slow, XtX_fast), "\n")
cat("Results identical?", all.equal(Xty_slow, Xty_fast), "\n")
```

#### Solving Normal Equations Efficiently

**Three methods**, ranked by efficiency:

1. **QR decomposition** (best for most cases)
2. **Cholesky decomposition** (if you know $\mathbf{X}'\mathbf{X}$ is well-conditioned)
3. **Direct inverse** (avoid if possible!)

```{r}
#| label: solve-normal-equations
#| echo: true

# Setup
X <- matrix(rnorm(100 * 5), 100, 5)
y <- rnorm(100)

# Method 1: Direct inverse (AVOID!)
XtX <- t(X) %*% X
Xty <- t(X) %*% y
b1 <- solve(XtX) %*% Xty  # Slow and unstable

# Method 2: Solve without forming inverse (BETTER)
XtX <- crossprod(X)
Xty <- crossprod(X, y)
b2 <- solve(XtX, Xty)  # Faster, more stable

# Method 3: Cholesky (GOOD for well-conditioned X'X)
XtX <- crossprod(X)
Xty <- crossprod(X, y)
R <- chol(XtX)  # Upper triangular
b3 <- backsolve(R, forwardsolve(t(R), Xty))  # Two triangular solves

# Method 4: QR decomposition (BEST)
qr_decomp <- qr(X)
b4 <- qr.coef(qr_decomp, y)  # Uses QR, most stable

# Method 5: Using lm.fit (EASIEST, nearly as fast as QR)
b5 <- lm.fit(X, y)$coefficients

cat("Method 1 (inverse):", b1[1:3], "\n")
cat("Method 2 (solve):", b2[1:3], "\n")
cat("Method 3 (Cholesky):", b3[1:3], "\n")
cat("Method 4 (QR):", b4[1:3], "\n")
cat("Method 5 (lm.fit):", b5[1:3], "\n")
```

#### Vectorized Operations

**Avoid loops** when possible. Use vectorized functions:

| Task | Slow (Loop) | Fast (Vectorized) |
|------|------------|------------------|
| Column sums | `apply(X, 2, sum)` | `colSums(X)` |
| Row sums | `apply(X, 1, sum)` | `rowSums(X)` |
| Column means | `apply(X, 2, mean)` | `colMeans(X)` |
| Row means | `apply(X, 1, mean)` | `rowMeans(X)` |

**Example**:

```{r}
#| label: vectorized-operations
#| echo: true

X <- matrix(rnorm(1000 * 50), 1000, 50)

# Slow
system.time({
  col_sums_slow <- apply(X, 2, sum)
})

# Fast
system.time({
  col_sums_fast <- colSums(X)
})

cat("Speedup:",
    system.time(apply(X, 2, sum))[3] / system.time(colSums(X))[3], "×\n")
```

#### Centering and Scaling with sweep()

**Task**: Center columns of X (subtract column means).

```{r}
#| label: sweep-centering
#| echo: true

X <- matrix(rnorm(100 * 5), 100, 5)

# Method 1: Manual (slow)
X_centered1 <- X
for (j in 1:ncol(X)) {
  X_centered1[, j] <- X[, j] - mean(X[, j])
}

# Method 2: Using sweep (fast)
X_centered2 <- sweep(X, 2, colMeans(X), "-")

# Method 3: Using scale (easiest for centering/scaling)
X_centered3 <- scale(X, center = TRUE, scale = FALSE)

# Verify
cat("Method 1 = Method 2?", all.equal(X_centered1, X_centered2), "\n")
cat("Method 2 = Method 3?",
    all.equal(X_centered2, as.matrix(X_centered3), check.attributes = FALSE), "\n")

# Column means should be ~0
cat("Column means after centering:", colMeans(X_centered2), "\n")
```

#### Computing Fitted Values Efficiently

Don't form the hat matrix $\mathbf{H} = \mathbf{X}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'$ explicitly!

```{r}
#| label: fitted-values-efficient
#| echo: true

n <- 1000
p <- 10
X <- matrix(rnorm(n * p), n, p)
y <- rnorm(n)

# NEVER do this (forms n×n matrix!)
# H <- X %*% solve(t(X) %*% X) %*% t(X)
# y_hat <- H %*% y

# Method 1: Solve and multiply (good)
b <- solve(crossprod(X), crossprod(X, y))
y_hat1 <- X %*% b

# Method 2: Using QR (better)
qr_decomp <- qr(X)
y_hat2 <- qr.fitted(qr_decomp, y)

# Method 3: Using lm.fit (easiest)
fit <- lm.fit(X, y)
y_hat3 <- fit$fitted.values

cat("All methods agree?",
    isTRUE(all.equal(y_hat1, y_hat2)) && isTRUE(all.equal(y_hat2, y_hat3)), "\n")
```

#### Quadratic Forms: $\mathbf{y}'\mathbf{A}\mathbf{y}$

**Efficient computation** without forming full matrix product:

```{r}
#| label: quadratic-forms-efficient
#| echo: true

n <- 500
A <- matrix(rnorm(n * n), n, n)
A <- A + t(A)  # Make symmetric
y <- rnorm(n)

# Method 1: Naive (forms n-vector twice)
quad1 <- t(y) %*% A %*% y

# Method 2: One multiplication (faster)
quad2 <- t(y) %*% (A %*% y)

# Method 3: Avoid transpose (fastest)
quad3 <- sum(y * (A %*% y))

# Method 4: If A is sparse, use Matrix package
library(Matrix)
A_sparse <- as(A, "dgCMatrix")
quad4 <- sum(y * (A_sparse %*% y))

cat("Method 1:", quad1, "\n")
cat("Method 2:", quad2, "\n")
cat("Method 3:", quad3, "\n")
cat("All equal?",
    isTRUE(all.equal(as.numeric(quad1), quad2)) && isTRUE(all.equal(quad2, quad3)), "\n")
```

#### Livestock Example: Efficient Solver for Large Dataset

**Scenario**: Beef cattle dataset with n=5000 animals, p=20 fixed effects.

```{r}
#| label: efficient-solver-beef
#| echo: true

# Simulate realistic beef cattle dataset
set.seed(42)
n <- 5000
p <- 20

# Design matrix: breed, sex, age, farm, pen effects, etc.
X <- matrix(rnorm(n * p), n, p)
X[, 1] <- 1  # Intercept
y <- rnorm(n, mean = 350, sd = 50)  # Carcass weight, kg

# Efficient normal equations solver
system.time({
  XtX <- crossprod(X)         # X'X
  Xty <- crossprod(X, y)      # X'y
  b <- solve(XtX, Xty)        # Solve X'Xb = X'y
  y_hat <- X %*% b            # Fitted values
  SSE <- sum((y - y_hat)^2)   # Error SS
  sigma2_hat <- SSE / (n - p) # Variance estimate

  # Standard errors
  Var_b <- solve(XtX) * sigma2_hat
  se_b <- sqrt(diag(Var_b))
})

cat("\nFirst 5 estimates:\n")
print(b[1:5])
cat("\nFirst 5 standard errors:\n")
print(se_b[1:5])

# Compare with lm()
cat("\nVerify against lm():\n")
system.time({
  fit_lm <- lm(y ~ X - 1)  # -1 to use X as-is
})

cat("Coefficients match?",
    all.equal(as.numeric(b), coef(fit_lm), check.attributes = FALSE), "\n")
```

#### Memory-Efficient Computations

For **very large** datasets, avoid creating unnecessary copies:

```{r}
#| label: memory-efficient
#| echo: true
#| eval: false

# BAD: Creates many temporary objects
XtX <- t(X) %*% X
XtX_inv <- solve(XtX)
Xty <- t(X) %*% y
b <- XtX_inv %*% Xty
y_hat <- X %*% b
e <- y - y_hat
SSE <- t(e) %*% e

# GOOD: Minimizes temporaries, uses efficient functions
XtX <- crossprod(X)
Xty <- crossprod(X, y)
b <- solve(XtX, Xty)  # Don't form inverse!
SSE <- sum(y^2) - sum(Xty * b)  # Compute SSE without forming residuals
```

#### Benchmark Summary

**Rules of thumb** (for modern computers):

| Operation | Slow | Fast | Speedup |
|-----------|------|------|---------|
| $\mathbf{X}'\mathbf{X}$ | `t(X) %*% X` | `crossprod(X)` | ~2× |
| Solve $\mathbf{Ab} = \mathbf{c}$ | `solve(A) %*% c` | `solve(A, c)` | ~3× |
| Column sums | `apply(X, 2, sum)` | `colSums(X)` | ~50× |
| Centering | `for` loop | `sweep()` or `scale()` | ~10× |
| Normal equations | Form $(\mathbf{X}'\mathbf{X})^{-1}$ | Use `solve()` or QR | ~5× |

**For very large problems** (n > 100,000 or p > 10,000):
- Use sparse matrix methods (`Matrix` package)
- Consider iterative solvers
- Use parallel computing (`parallel` package)
- Stream data from disk if doesn't fit in RAM

#### Best Practices Summary

::: {.callout-tip}
## Efficient Coding Checklist

**Always**:
- ✅ Use `crossprod()` instead of `t(X) %*% X`
- ✅ Use `solve(A, b)` instead of `solve(A) %*% b`
- ✅ Use vectorized functions (`colSums`, `rowSums`, etc.)
- ✅ Use `sweep()` or `scale()` for centering/scaling
- ✅ Use `lm.fit()` or QR decomposition for solving

**Never**:
- ❌ Form $(\mathbf{X}'\mathbf{X})^{-1}$ explicitly unless you need it
- ❌ Form hat matrix $\mathbf{H}$ for large $n$
- ❌ Use loops where vectorization works
- ❌ Create unnecessary copies of large matrices
- ❌ Use `apply()` when specialized functions exist

**Profile your code** with `system.time()` or `microbenchmark` package!
:::

#### Quick Reference Card

```{r}
#| label: quick-reference
#| echo: true
#| eval: false

# Efficient normal equations solution
XtX <- crossprod(X)              # X'X
Xty <- crossprod(X, y)           # X'y
b <- solve(XtX, Xty)             # Solve (don't invert!)
y_hat <- X %*% b                 # Fitted values
SSE <- sum(y^2) - sum(Xty * b)   # SSE without forming residuals
sigma2 <- SSE / (n - p)          # Variance
Var_b <- solve(XtX) * sigma2     # Var(b) - only if you need SE
se_b <- sqrt(diag(Var_b))        # Standard errors

# Even better: use lm.fit() for production code
fit <- lm.fit(X, y)
b <- fit$coefficients
y_hat <- fit$fitted.values
SSE <- sum(fit$residuals^2)
```

---

## Computational Considerations {#sec-computation}

This section addresses the practical realities of computer arithmetic. Even mathematically correct formulas can produce incorrect results due to rounding errors and numerical instability.

::: {.callout-warning}
##Why This Matters

**Just because the math is correct doesn't mean the computer will get it right!**

Ill-conditioned problems, poor algorithms, and rounding errors can produce:
- Wrong parameter estimates
- Negative variances (!!)
- Failed convergence
- Nonsensical results

Learning these issues NOW will save you hours of debugging later.
:::

### Numerical Stability {#sec-numerical-stability}

**Numerical stability** refers to how sensitive a computation is to rounding errors. An algorithm is **numerically stable** if small errors in input produce small errors in output.

#### Condition Numbers

The **condition number** of a matrix $\mathbf{A}$ measures how sensitive $\mathbf{A}^{-1}$ is to perturbations:

$$
\kappa(\mathbf{A}) = \|\mathbf{A}\| \cdot \|\mathbf{A}^{-1}\|
$$

For symmetric positive definite matrices:

$$
\kappa(\mathbf{A}) = \frac{\lambda_{\max}(\mathbf{A})}{\lambda_{\min}(\mathbf{A})}
$$

**Interpretation**:

| Condition Number | Matrix Status | Inversion Accuracy |
|-----------------|---------------|-------------------|
| $\kappa < 10$ | Well-conditioned | Excellent |
| $10 \leq \kappa < 10^3$ | Moderate | Good |
| $10^3 \leq \kappa < 10^6$ | Ill-conditioned | Poor, use caution |
| $\kappa \geq 10^6$ | Severely ill-conditioned | Unreliable, avoid if possible |

**Rule of thumb**: You lose about $\log_{10}(\kappa)$ digits of precision.

::: {.callout-note}
## Example: Collinearity Creates Ill-Conditioning

In linear models, **collinearity** makes $\mathbf{X}'\mathbf{X}$ ill-conditioned:

- If predictors are highly correlated, $\mathbf{X}'\mathbf{X}$ has small eigenvalues
- Small eigenvalues → large condition number → unstable inversion
- Result: Huge standard errors, unstable estimates
:::

#### Computing Condition Numbers in R

```{r}
#| label: condition-numbers
#| echo: true

# Well-conditioned matrix
A_good <- matrix(c(4, 1, 1, 3), 2, 2)
kappa_good <- kappa(A_good)
cat("Well-conditioned matrix:\n")
print(A_good)
cat("Condition number:", kappa_good, "\n\n")

# Ill-conditioned matrix (high collinearity)
A_bad <- matrix(c(1.0, 0.99,
                  0.99, 1.0), 2, 2, byrow = TRUE)
kappa_bad <- kappa(A_bad)
cat("Ill-conditioned matrix:\n")
print(A_bad)
cat("Condition number:", kappa_bad, "\n\n")

# Severely ill-conditioned (nearly singular)
A_terrible <- matrix(c(1.0, 0.9999,
                       0.9999, 1.0), 2, 2, byrow = TRUE)
kappa_terrible <- kappa(A_terrible)
cat("Severely ill-conditioned matrix:\n")
print(A_terrible)
cat("Condition number:", kappa_terrible, "\n\n")

# For design matrix X'X
set.seed(123)
n <- 100
x1 <- rnorm(n)
x2 <- x1 + rnorm(n, sd = 0.01)  # Highly correlated!
X <- cbind(1, x1, x2)
XtX <- crossprod(X)
kappa_XtX <- kappa(XtX)
cat("X'X with collinearity:\n")
cat("Condition number:", kappa_XtX, "\n")
cat("Correlation between x1 and x2:", cor(x1, x2), "\n")
```

#### Sources of Numerical Instability

**1. Subtracting Nearly Equal Numbers**

```{r}
#| label: subtraction-instability
#| echo: true

# Computing variance: Var(x) = E(x²) - [E(x)]²
# Two formulas, mathematically equivalent

x <- rnorm(100, mean = 1e8, sd = 1)  # Large mean, small variance

# Formula 1: Textbook (UNSTABLE for large mean)
mean_x <- mean(x)
var1 <- mean(x^2) - mean_x^2
cat("Method 1 (textbook, unstable):", var1, "\n")

# Formula 2: Deviations from mean (STABLE)
var2 <- mean((x - mean_x)^2)
cat("Method 2 (deviations, stable):", var2, "\n")

# R's built-in (uses stable algorithm)
var3 <- var(x) * (length(x) - 1) / length(x)  # Adjust for n vs n-1
cat("Method 3 (R's var()):", var3, "\n")

cat("\nTrue variance (from simulation):", 1, "\n")
```

**2. Forming $(\mathbf{X}'\mathbf{X})^{-1}$ Explicitly**

```{r}
#| label: explicit-inverse-instability
#| echo: true

# Generate X'X with moderate collinearity
set.seed(456)
n <- 100
p <- 5
X <- matrix(rnorm(n * p), n, p)
X[, 2] <- X[, 1] + rnorm(n, sd = 0.1)  # Collinear columns
y <- rnorm(n)

XtX <- crossprod(X)
Xty <- crossprod(X, y)

# Method 1: Explicit inverse (potentially unstable)
XtX_inv <- solve(XtX)
b1 <- XtX_inv %*% Xty

# Method 2: Solve directly (more stable)
b2 <- solve(XtX, Xty)

cat("Estimates match?", all.equal(b1, b2), "\n")
cat("Condition number of X'X:", kappa(XtX), "\n")

# Check: how well do we solve X'X * b = X'y?
residual1 <- XtX %*% b1 - Xty
residual2 <- XtX %*% b2 - Xty
cat("||X'Xb - X'y|| using inverse:", sqrt(sum(residual1^2)), "\n")
cat("||X'Xb - X'y|| using solve():", sqrt(sum(residual2^2)), "\n")
```

**3. Large Range in Data Values**

```{r}
#| label: scaling-instability
#| echo: true

# Predictors with vastly different scales
n <- 50
age_days <- sample(200:400, n, replace = TRUE)  # Scale: O(100)
weight_mg <- runif(n, 5000, 8000)               # Scale: O(1000)
y <- rnorm(n)

X_unscaled <- cbind(1, age_days, weight_mg)

# Condition number without scaling
kappa_unscaled <- kappa(crossprod(X_unscaled))
cat("Condition number (unscaled):", kappa_unscaled, "\n")

# Scale predictors (mean 0, sd 1)
X_scaled <- scale(X_unscaled[, -1])  # Don't scale intercept
X_scaled <- cbind(1, X_scaled)

kappa_scaled <- kappa(crossprod(X_scaled))
cat("Condition number (scaled):", kappa_scaled, "\n")

cat("Improvement:", kappa_unscaled / kappa_scaled, "×\n")
```

#### Remedies for Numerical Instability

::: {.callout-tip}
## Strategies to Improve Numerical Stability

**1. Center and Scale Predictors**
```r
X_centered <- scale(X, center = TRUE, scale = TRUE)
```

**2. Use Stable Algorithms**
- QR decomposition instead of normal equations
- Cholesky instead of explicit inverse (if well-conditioned)
- SVD for rank-deficient problems

**3. Avoid Explicit Matrix Inversion**
```r
# BAD
b <- solve(XtX) %*% Xty

# GOOD
b <- solve(XtX, Xty)
```

**4. Check Condition Numbers**
```r
if (kappa(XtX) > 1e6) {
  warning("X'X is severely ill-conditioned!")
}
```

**5. Ridge Regression for Collinearity**
```r
# Add small value to diagonal
lambda <- 0.01
XtX_ridge <- XtX + lambda * diag(ncol(XtX))
b_ridge <- solve(XtX_ridge, Xty)
```

**6. Use QR Decomposition**
```r
qr_obj <- qr(X)
b <- qr.coef(qr_obj, y)  # Most numerically stable
```
:::

#### Livestock Example: Collinearity in Dairy Data

**Scenario**: Predicting milk yield from fat%, protein%, and lactose% (highly correlated).

```{r}
#| label: dairy-collinearity-example
#| echo: true

# Simulated dairy data
set.seed(789)
n <- 50
# These are compositionally constrained (sum to ~100%)
fat_pct <- runif(n, 3.5, 4.5)
protein_pct <- runif(n, 3.0, 3.5)
lactose_pct <- 100 - fat_pct - protein_pct - runif(n, 85, 89)  # Highly constrained!

milk_yield <- 30 + 2*fat_pct - 1*protein_pct + 3*lactose_pct + rnorm(n, sd = 2)

# Check correlations
comp <- cbind(fat_pct, protein_pct, lactose_pct)
cat("Correlation matrix:\n")
print(cor(comp))

# Design matrix
X <- cbind(1, fat_pct, protein_pct, lactose_pct)
XtX <- crossprod(X)

# Check condition number
kappa_comp <- kappa(XtX)
cat("\nCondition number:", kappa_comp, "\n")

if (kappa_comp > 1000) {
  cat("WARNING: X'X is ill-conditioned due to collinearity!\n")
}

# Fit model anyway
b <- solve(XtX, crossprod(X, milk_yield))
Var_b <- solve(XtX) * (sum((milk_yield - X %*% b)^2) / (n - 4))
se_b <- sqrt(diag(Var_b))

cat("\nEstimates and SE:\n")
results <- data.frame(
  Parameter = c("Intercept", "Fat %", "Protein %", "Lactose %"),
  Estimate = b,
  SE = se_b
)
print(results)

cat("\nNote the HUGE standard errors due to collinearity!\n")

# VIF (Variance Inflation Factor)
library(car)
fit <- lm(milk_yield ~ fat_pct + protein_pct + lactose_pct)
vif_values <- vif(fit)
cat("\nVIF (>10 indicates severe collinearity):\n")
print(vif_values)
```

#### Detecting Numerical Problems

**Warning signs**:

1. **Huge standard errors** (compared to estimates)
2. **Estimates change drastically** when you add/remove observations
3. **Opposite signs** from what you expect biologically
4. **`solve()` fails** with "system is computationally singular"
5. **VIF > 10** for any predictor
6. **Condition number > 10^6**

**Diagnostic checklist**:

```{r}
#| label: diagnostic-checklist
#| echo: true
#| eval: false

# 1. Check condition number
kappa_value <- kappa(XtX)
if (kappa_value > 1e6) warning("Severely ill-conditioned!")

# 2. Check correlations
cor_matrix <- cor(X[, -1])  # Exclude intercept
if (any(abs(cor_matrix[upper.tri(cor_matrix)]) > 0.95)) {
  warning("High correlations detected!")
}

# 3. Check VIF
library(car)
fit <- lm(y ~ X - 1)
vif_values <- vif(fit)
if (any(vif_values > 10)) warning("High VIF detected!")

# 4. Check eigenvalues
eigenvalues <- eigen(XtX)$values
if (min(eigenvalues) < 1e-10) warning("Near-zero eigenvalue!")

# 5. Examine SE/estimate ratios
se_ratio <- se_b / abs(b)
if (any(se_ratio > 1, na.rm = TRUE)) {
  warning("Standard error exceeds estimate!")
}
```

::: {.callout-warning}
## When in Doubt, Use QR Decomposition

The QR decomposition is **numerically stable** even for ill-conditioned problems:

```r
qr_obj <- qr(X)
b <- qr.coef(qr_obj, y)
```

**Why QR is better**:
- Avoids forming $\mathbf{X}'\mathbf{X}$ (condition number squared!)
- Uses orthogonal transformations (preserve lengths, numerically stable)
- Automatic detection of rank deficiency
- Used by `lm()` under the hood
:::

---

### Efficient Computation {#sec-efficient-computation}

Choosing the right algorithm can make the difference between a computation taking seconds versus hours. This section presents the best algorithms for common linear models tasks.

#### Algorithm Comparison: Solving $\mathbf{X}'\mathbf{X}\mathbf{b} = \mathbf{X}'\mathbf{y}$

**Five methods**, ranked by numerical stability and speed:

| Method | Operation Count | Stability | When to Use |
|--------|----------------|-----------|-------------|
| **QR decomposition** | $O(np^2)$ | Excellent | **Default choice** |
| **Cholesky** | $O(p^3/3)$ | Good (if well-conditioned) | X'X already computed, well-conditioned |
| **solve(XtX, Xty)** | $O(p^3)$ | Good | Simple, readable code |
| **SVD** | $O(np^2 + p^3)$ | Excellent | Rank-deficient, collinear |
| **Direct inverse** | $O(p^3)$ | Poor | **AVOID** |

#### Method 1: QR Decomposition (Recommended)

**Factorization**: $\mathbf{X} = \mathbf{QR}$ where $\mathbf{Q}' \mathbf{Q} = \mathbf{I}$ and $\mathbf{R}$ is upper triangular.

**Solution**:
$$
\mathbf{X}'\mathbf{X}\mathbf{b} = \mathbf{X}'\mathbf{y} \implies \mathbf{R}'\mathbf{R}\mathbf{b} = \mathbf{R}'\mathbf{Q}'\mathbf{y} \implies \mathbf{R}\mathbf{b} = \mathbf{Q}'\mathbf{y}
$$

Solve by back-substitution (fast and stable).

**Advantages**:
- Never forms $\mathbf{X}'\mathbf{X}$ (avoids squaring condition number)
- Detects rank deficiency automatically
- Used by `lm()` in R
- Most numerically stable

**R Implementation**:

```{r}
#| label: qr-method
#| echo: true

set.seed(100)
n <- 100
p <- 5
X <- matrix(rnorm(n * p), n, p)
y <- rnorm(n)

# QR decomposition
qr_obj <- qr(X)

# Extract Q and R (for illustration)
Q <- qr.Q(qr_obj)
R <- qr.R(qr_obj)

# Verify X = QR
cat("X = QR?", all.equal(X, Q %*% R), "\n")

# Verify Q'Q = I
cat("Q'Q = I?", all.equal(crossprod(Q), diag(ncol(Q))), "\n")

# Solve for b
b_qr <- qr.coef(qr_obj, y)

# Get fitted values and residuals
y_hat_qr <- qr.fitted(qr_obj, y)
resid_qr <- qr.resid(qr_obj, y)

cat("\nFirst 3 coefficients:", b_qr[1:3], "\n")

# Compare with lm()
fit_lm <- lm(y ~ X - 1)
cat("Match lm()?", all.equal(as.numeric(b_qr), coef(fit_lm)), "\n")
```

#### Method 2: Cholesky Decomposition

**Factorization**: $\mathbf{X}'\mathbf{X} = \mathbf{L}\mathbf{L}'$ where $\mathbf{L}$ is lower triangular (or $\mathbf{R}\mathbf{R}'$ with $\mathbf{R}$ upper triangular).

**Solution**:
1. Compute $\mathbf{X}'\mathbf{X}$ and $\mathbf{X}'\mathbf{y}$
2. Factor: $\mathbf{X}'\mathbf{X} = \mathbf{R}'\mathbf{R}$
3. Solve $\mathbf{R}'\mathbf{w} = \mathbf{X}'\mathbf{y}$ (forward substitution)
4. Solve $\mathbf{R}\mathbf{b} = \mathbf{w}$ (back substitution)

**Advantages**:
- Faster than QR if $p \ll n$
- Exploits symmetry and positive definiteness
- Good for repeated solves with same $\mathbf{X}'\mathbf{X}$

**Disadvantages**:
- Requires $\mathbf{X}'\mathbf{X}$ to be positive definite
- Less stable than QR for ill-conditioned matrices

**R Implementation**:

```{r}
#| label: cholesky-method
#| echo: true

# Same data as above
XtX <- crossprod(X)
Xty <- crossprod(X, y)

# Cholesky factorization
R_chol <- chol(XtX)  # Upper triangular

# Verify X'X = R'R
cat("X'X = R'R?", all.equal(XtX, t(R_chol) %*% R_chol), "\n")

# Solve R'w = X'y
w <- forwardsolve(t(R_chol), Xty)

# Solve Rb = w
b_chol <- backsolve(R_chol, w)

cat("QR = Cholesky?", all.equal(b_qr, b_chol), "\n")

# Shortcut: chol2inv() to get (X'X)^(-1)
XtX_inv <- chol2inv(R_chol)
b_chol2 <- XtX_inv %*% Xty
cat("Alternative Cholesky:", all.equal(b_chol, b_chol2), "\n")
```

#### Method 3: Direct solve()

**R's solve()** uses LU decomposition with partial pivoting.

```{r}
#| label: solve-method
#| echo: true

# Method 1: Form inverse (BAD!)
XtX_inv_bad <- solve(XtX)
b_bad <- XtX_inv_bad %*% Xty

# Method 2: Solve directly (GOOD!)
b_solve <- solve(XtX, Xty)

cat("Both methods agree:", all.equal(b_bad, b_solve), "\n")

# But Method 2 is faster and more accurate
# Benchmark
library(microbenchmark)
mbm <- microbenchmark(
  inverse = solve(XtX) %*% Xty,
  direct = solve(XtX, Xty),
  times = 100
)
print(summary(mbm)[, c("expr", "median")])
```

#### Method 4: SVD (for rank-deficient problems)

**Singular Value Decomposition**: $\mathbf{X} = \mathbf{U}\mathbf{D}\mathbf{V}'$

- $\mathbf{U}$: $n \times p$ orthogonal
- $\mathbf{D}$: $p \times p$ diagonal (singular values)
- $\mathbf{V}$: $p \times p$ orthogonal

**Moore-Penrose inverse**: $\mathbf{X}^+ = \mathbf{V}\mathbf{D}^+\mathbf{U}'$

```{r}
#| label: svd-method
#| echo: true

# Create rank-deficient X
X_rank_def <- X
X_rank_def[, 5] <- X_rank_def[, 1] + X_rank_def[, 2]  # Perfect collinearity

# QR will detect rank deficiency
qr_obj_def <- qr(X_rank_def)
cat("Rank of X:", qr_obj_def$rank, "(should be 4, not 5)\n")

# SVD approach
svd_obj <- svd(X_rank_def)
d <- svd_obj$d
u <- svd_obj$u
v <- svd_obj$v

# Threshold small singular values
threshold <- 1e-10
d_inv <- ifelse(d > threshold, 1/d, 0)

# Moore-Penrose inverse
X_pinv <- v %*% diag(d_inv) %*% t(u)
b_svd <- X_pinv %*% y

cat("\nSingular values:", d, "\n")
cat("Small singular value indicates rank deficiency\n")

# Compare with MASS::ginv()
library(MASS)
b_ginv <- ginv(X_rank_def) %*% y
cat("SVD = ginv()?", all.equal(b_svd, b_ginv, check.attributes = FALSE), "\n")
```

#### When to Use Which Method?

**Decision tree**:

```
Is X'X rank deficient?
├─ YES → Use SVD or generalized inverse
│         (See Week 12 on non-full rank models)
│
└─ NO → Is X'X already computed?
        ├─ YES → Is condition number < 1000?
        │        ├─ YES → Use Cholesky
        │        └─ NO  → Use QR
        │
        └─ NO → Always use QR
```

::: {.callout-tip}
## Practical Recommendations

**For most linear models work**:
```r
# Just use lm() or lm.fit()
fit <- lm(y ~ X)
# OR
fit <- lm.fit(X, y)
```

**If you must code your own**:
```r
# Use QR
qr_obj <- qr(X)
b <- qr.coef(qr_obj, y)
```

**For repeated solves** (same $\mathbf{X}$, different $\mathbf{y}$):
```r
# Factor once
qr_obj <- qr(X)

# Solve many times
b1 <- qr.coef(qr_obj, y1)
b2 <- qr.coef(qr_obj, y2)
b3 <- qr.coef(qr_obj, y3)
```
:::

#### Beef Cattle Example: Algorithm Comparison

**Scenario**: Compare methods for n=500, p=10.

```{r}
#| label: beef-algorithm-comparison
#| echo: true

# Realistic beef cattle data
set.seed(999)
n <- 500
p <- 10

# Design matrix: breed, sex, sire, dam, pen effects, etc.
X <- matrix(rnorm(n * p), n, p)
X[, 1] <- 1  # Intercept
y <- rnorm(n, mean = 450, sd = 50)  # Carcass weight, kg

# Method 1: QR (best)
system.time({
  qr_obj <- qr(X)
  b1 <- qr.coef(qr_obj, y)
})

# Method 2: Cholesky (fast if well-conditioned)
system.time({
  XtX <- crossprod(X)
  Xty <- crossprod(X, y)
  R <- chol(XtX)
  b2 <- backsolve(R, forwardsolve(t(R), Xty))
})

# Method 3: Direct solve (easy)
system.time({
  XtX <- crossprod(X)
  Xty <- crossprod(X, y)
  b3 <- solve(XtX, Xty)
})

# Method 4: lm.fit() (easiest)
system.time({
  fit <- lm.fit(X, y)
  b4 <- fit$coefficients
})

# Verify all agree
cat("QR = Cholesky?", all.equal(b1, b2), "\n")
cat("QR = solve()?", all.equal(b1, b3), "\n")
cat("QR = lm.fit()?", all.equal(b1, b4, check.attributes = FALSE), "\n")

# Check condition number
cat("\nCondition number of X'X:", kappa(crossprod(X)), "\n")
```

#### Computing Standard Errors Efficiently

**Don't compute** $(\mathbf{X}'\mathbf{X})^{-1}$ unless you need the full variance-covariance matrix!

```{r}
#| label: efficient-standard-errors
#| echo: true

# Suppose we only need SE for one coefficient (e.g., β₁)

# Method 1: Full inverse (SLOW if you only need one SE)
XtX <- crossprod(X)
XtX_inv <- solve(XtX)
sigma2_hat <- sum((y - X %*% b1)^2) / (n - p)
se_full <- sqrt(diag(XtX_inv) * sigma2_hat)

# Method 2: Solve for specific columns (FASTER)
# Var(b_j) = [(X'X)^(-1)]_jj * σ²
# To get [(X'X)^(-1)]_jj, solve X'X * v_j = e_j
e1 <- c(0, 1, 0, 0, 0, 0, 0, 0, 0, 0)  # Second coefficient (β₁)
v1 <- solve(XtX, e1)
var_b1_method2 <- v1[2] * sigma2_hat
se_b1_method2 <- sqrt(var_b1_method2)

cat("SE(β₁) via full inverse:", se_full[2], "\n")
cat("SE(β₁) via single solve:", se_b1_method2, "\n")
cat("Match?", all.equal(se_full[2], se_b1_method2), "\n")
```

::: {.callout-note}
## Memory Considerations

**For very large $n$**:

- **QR decomposition**: Stores $n \times p$ matrix $\mathbf{Q}$ → $O(np)$ memory
- **Cholesky**: Only stores $p \times p$ matrix $\mathbf{R}$ → $O(p^2)$ memory
- **Trade-off**: Cholesky uses less memory but requires forming $\mathbf{X}'\mathbf{X}$

**Rule of thumb**:
- If $n > 10p$, Cholesky saves memory
- If worried about stability, always use QR
:::

#### Summary: Algorithm Selection

| Situation | Best Algorithm | R Function |
|-----------|---------------|-----------|
| General linear models | QR decomposition | `lm()` or `qr.coef()` |
| Well-conditioned X'X | Cholesky | `chol()` then `backsolve()` |
| Rank deficient | SVD or g-inverse | `svd()` or `MASS::ginv()` |
| Need full Var(b) | QR or Cholesky | `lm()` then `vcov()` |
| Just need estimates | QR | `qr.coef()` |
| Repeated solves | Factor once, solve many | Store `qr_obj` |
| Large $n$, small $p$ | Cholesky (memory) | `chol()` |
| Ill-conditioned | QR or SVD | `qr()` or `svd()` |

---

### Checking Your Work {#sec-verification}

**Trust, but verify.** Even careful programmers make mistakes. This section presents systematic strategies for verifying matrix computations.

::: {.callout-important}
## The Cardinal Rule of Numerical Computing

**Always verify your results!**

Check your work by:
1. Testing matrix properties
2. Comparing with known solutions
3. Verifying against `lm()`
4. Checking mathematical identities
5. Using `all.equal()` for tolerant comparisons
:::

#### Using all.equal() Correctly

**Never use** `==` for floating-point comparisons!

```{r}
#| label: all-equal-usage
#| echo: true

# Example: Two ways to compute the same thing
x <- c(1, 2, 3)
result1 <- sum(x) / length(x)
result2 <- mean(x)

# BAD: Exact comparison (may fail due to rounding!)
if (result1 == result2) {
  cat("Equal!\n")
} else {
  cat("Not equal!\n")  # Might happen even though they're the same!
}

# GOOD: Tolerant comparison
if (all.equal(result1, result2)) {
  cat("Equal (within tolerance)!\n")
}

# Show the default tolerance
tolerance <- sqrt(.Machine$double.eps)  # ≈ 1.5e-8
cat("Default tolerance:", tolerance, "\n")

# Custom tolerance
cat("Equal with tighter tolerance?",
    all.equal(result1, result2, tolerance = 1e-15), "\n")

# Checking matrices
A <- matrix(c(1, 2, 3, 4), 2, 2)
B <- A + 1e-10  # Tiny difference

cat("\nA == B (exact)?", all(A == B), "\n")
cat("all.equal(A, B)?", all.equal(A, B), "\n")
```

#### Verification Checklist for Linear Models

When you implement a least squares solver, verify these properties:

**1. Normal equations satisfied**: $\mathbf{X}'\mathbf{X}\mathbf{b} = \mathbf{X}'\mathbf{y}$

```{r}
#| label: verify-normal-equations
#| echo: true

# Setup
set.seed(111)
n <- 50
p <- 3
X <- matrix(rnorm(n * p), n, p)
y <- rnorm(n)

# Solve
b <- solve(crossprod(X), crossprod(X, y))

# Check: X'Xb should equal X'y
LHS <- crossprod(X) %*% b
RHS <- crossprod(X, y)

cat("Normal equations satisfied?", all.equal(LHS, RHS), "\n")
cat("Max absolute difference:", max(abs(LHS - RHS)), "\n")
```

**2. Residuals orthogonal to X**: $\mathbf{X}'\mathbf{e} = \mathbf{0}$

```{r}
#| label: verify-orthogonality
#| echo: true

# Compute residuals
y_hat <- X %*% b
e <- y - y_hat

# Check orthogonality
Xte <- crossprod(X, e)

cat("X'e = 0?", all.equal(Xte, matrix(0, p, 1), check.attributes = FALSE), "\n")
cat("Max |X'e|:", max(abs(Xte)), "\n")
```

**3. Fitted values orthogonal to residuals**: $\hat{\mathbf{y}}'\mathbf{e} = 0$

```{r}
#| label: verify-fitted-residual-orthogonality
#| echo: true

inner_product <- t(y_hat) %*% e

cat("y_hat'e = 0?", all.equal(inner_product, matrix(0, 1, 1), check.attributes = FALSE), "\n")
cat("|y_hat'e| =", abs(inner_product), "\n")
```

**4. Sum of squares decomposition**: $\text{SST} = \text{SSM} + \text{SSE}$

```{r}
#| label: verify-ss-decomposition
#| echo: true

y_bar <- mean(y)
SST <- sum((y - y_bar)^2)
SSM <- sum((y_hat - y_bar)^2)
SSE <- sum(e^2)

cat("SST =", SST, "\n")
cat("SSM =", SSM, "\n")
cat("SSE =", SSE, "\n")
cat("SSM + SSE =", SSM + SSE, "\n")
cat("SST = SSM + SSE?", all.equal(SST, SSM + SSE), "\n")
```

**5. Compare with lm()**

```{r}
#| label: verify-against-lm
#| echo: true

# Your implementation
my_results <- list(
  coefficients = b,
  fitted = y_hat,
  residuals = e,
  sigma = sqrt(SSE / (n - p))
)

# R's lm()
fit_lm <- lm.fit(X, y)

# Compare
cat("Coefficients match?",
    all.equal(as.numeric(my_results$coefficients),
              fit_lm$coefficients), "\n")
cat("Fitted values match?",
    all.equal(as.numeric(my_results$fitted),
              fit_lm$fitted.values), "\n")
cat("Residuals match?",
    all.equal(as.numeric(my_results$residuals),
              fit_lm$residuals), "\n")
```

#### Testing Matrix Properties

**Template function** for verifying matrix properties:

```{r}
#| label: test-matrix-properties
#| echo: true

test_matrix_properties <- function(A, name = "A", tolerance = 1e-10) {
  cat("\n=== Testing", name, "===\n")

  # 1. Check dimensions
  cat("Dimensions:", nrow(A), "×", ncol(A), "\n")

  # 2. Check if square
  is_square <- nrow(A) == ncol(A)
  cat("Square?", is_square, "\n")

  if (is_square) {
    # 3. Check symmetry
    is_symmetric <- all.equal(A, t(A), tolerance = tolerance)
    cat("Symmetric?", isTRUE(is_symmetric), "\n")

    # 4. Check if diagonal
    is_diag <- all(A[row(A) != col(A)] < tolerance)
    cat("Diagonal?", is_diag, "\n")

    # 5. Check idempotence (A² = A)
    A2 <- A %*% A
    is_idempotent <- all.equal(A, A2, tolerance = tolerance)
    cat("Idempotent?", isTRUE(is_idempotent), "\n")

    # 6. Rank
    cat("Rank:", qr(A)$rank, "\n")

    # 7. Condition number
    kappa_A <- kappa(A)
    cat("Condition number:", kappa_A, "\n")
    if (kappa_A > 1000) {
      cat("  WARNING: Ill-conditioned!\n")
    }

    # 8. Eigenvalues
    eigenvalues <- eigen(A, only.values = TRUE)$values
    cat("Eigenvalues:", eigenvalues, "\n")

    # 9. Positive definite?
    is_pd <- all(eigenvalues > tolerance)
    cat("Positive definite?", is_pd, "\n")

    # 10. Trace
    cat("Trace:", sum(diag(A)), "\n")

    # 11. Determinant
    cat("Determinant:", det(A), "\n")
  }

  invisible(NULL)
}

# Test on hat matrix
X <- matrix(rnorm(50 * 3), 50, 3)
H <- X %*% solve(crossprod(X)) %*% t(X)
test_matrix_properties(H, "Hat matrix H")

# Test on residual maker
M <- diag(nrow(X)) - H
test_matrix_properties(M, "Residual maker M")
```

#### Debugging Numerical Issues

**Common problems and how to diagnose them**:

```{r}
#| label: debug-numerical-issues
#| echo: true

debug_linear_model <- function(X, y) {
  cat("=== Debugging Linear Model ===\n\n")

  n <- nrow(X)
  p <- ncol(X)

  # 1. Check for NAs
  cat("1. Checking for missing values:\n")
  cat("  NAs in X:", sum(is.na(X)), "\n")
  cat("  NAs in y:", sum(is.na(y)), "\n")

  # 2. Check dimensions
  cat("\n2. Checking dimensions:\n")
  cat("  n =", n, "\n")
  cat("  p =", p, "\n")
  cat("  n > p?", n > p, "(need this for full rank)\n")

  # 3. Check rank
  cat("\n3. Checking rank:\n")
  rank_X <- qr(X)$rank
  cat("  rank(X) =", rank_X, "\n")
  cat("  Full rank?", rank_X == p, "\n")

  # 4. Check condition number
  cat("\n4. Checking condition number:\n")
  XtX <- crossprod(X)
  kappa_XtX <- kappa(XtX)
  cat("  κ(X'X) =", kappa_XtX, "\n")
  if (kappa_XtX > 1e6) {
    cat("  WARNING: Severely ill-conditioned!\n")
  } else if (kappa_XtX > 1000) {
    cat("  WARNING: Ill-conditioned!\n")
  } else {
    cat("  OK: Well-conditioned\n")
  }

  # 5. Check for perfect collinearity
  cat("\n5. Checking for collinearity:\n")
  if (p > 1) {
    cor_matrix <- cor(X)
    max_cor <- max(abs(cor_matrix[upper.tri(cor_matrix)]))
    cat("  Max |correlation| =", max_cor, "\n")
    if (max_cor > 0.99) {
      cat("  WARNING: Very high correlation detected!\n")
    }
  }

  # 6. Try to solve
  cat("\n6. Attempting to solve:\n")
  tryCatch({
    b <- solve(XtX, crossprod(X, y))
    cat("  SUCCESS: Solution found\n")
    cat("  Coefficients:", b, "\n")

    # Check residuals
    e <- y - X %*% b
    SSE <- sum(e^2)
    sigma2 <- SSE / (n - p)
    cat("  σ² =", sigma2, "\n")

    # Check if any SE are huge
    se_b <- sqrt(diag(solve(XtX)) * sigma2)
    max_se_ratio <- max(abs(se_b / b), na.rm = TRUE)
    cat("  Max SE/estimate ratio:", max_se_ratio, "\n")
    if (max_se_ratio > 1) {
      cat("  WARNING: Some SEs exceed estimates!\n")
    }
  }, error = function(e) {
    cat("  ERROR:", e$message, "\n")
    cat("  System is computationally singular!\n")
  })
}

# Test on good data
cat("### Test 1: Good data ###\n")
X_good <- matrix(rnorm(50 * 3), 50, 3)
y_good <- rnorm(50)
debug_linear_model(X_good, y_good)

# Test on collinear data
cat("\n\n### Test 2: Collinear data ###\n")
X_bad <- matrix(rnorm(50 * 3), 50, 3)
X_bad[, 3] <- X_bad[, 1] + X_bad[, 2]  # Perfect collinearity
y_bad <- rnorm(50)
debug_linear_model(X_bad, y_bad)
```

#### Unit Testing Template

**Create a test suite** for your solver:

```{r}
#| label: unit-testing-template
#| echo: true
#| eval: false

# Function to test
my_lm_solver <- function(X, y) {
  XtX <- crossprod(X)
  Xty <- crossprod(X, y)
  b <- solve(XtX, Xty)
  return(b)
}

# Test suite
test_lm_solver <- function() {
  all_pass <- TRUE

  # Test 1: Simple case
  cat("Test 1: Simple case... ")
  X <- matrix(c(1, 1, 1, 1, 2, 3), 3, 2)
  y <- c(1, 2, 2)
  b <- my_lm_solver(X, y)
  b_expected <- solve(crossprod(X), crossprod(X, y))
  if (all.equal(b, b_expected)) {
    cat("PASS\n")
  } else {
    cat("FAIL\n")
    all_pass <- FALSE
  }

  # Test 2: Compare with lm()
  cat("Test 2: Match lm()... ")
  X <- matrix(rnorm(100 * 5), 100, 5)
  y <- rnorm(100)
  b_mine <- my_lm_solver(X, y)
  b_lm <- lm.fit(X, y)$coefficients
  if (all.equal(as.numeric(b_mine), b_lm)) {
    cat("PASS\n")
  } else {
    cat("FAIL\n")
    all_pass <- FALSE
  }

  # Test 3: Residuals orthogonal to X
  cat("Test 3: X'e = 0... ")
  e <- y - X %*% b_mine
  Xte <- crossprod(X, e)
  if (all.equal(Xte, matrix(0, 5, 1), check.attributes = FALSE)) {
    cat("PASS\n")
  } else {
    cat("FAIL\n")
    all_pass <- FALSE
  }

  # Test 4: SS decomposition
  cat("Test 4: SST = SSM + SSE... ")
  y_bar <- mean(y)
  y_hat <- X %*% b_mine
  SST <- sum((y - y_bar)^2)
  SSM <- sum((y_hat - y_bar)^2)
  SSE <- sum(e^2)
  if (all.equal(SST, SSM + SSE)) {
    cat("PASS\n")
  } else {
    cat("FAIL\n")
    all_pass <- FALSE
  }

  # Summary
  cat("\n")
  if (all_pass) {
    cat("ALL TESTS PASSED ✓\n")
  } else {
    cat("SOME TESTS FAILED ✗\n")
  }

  return(all_pass)
}

# Run tests
test_lm_solver()
```

#### Quick Verification Functions

**Handy functions to keep in your toolkit**:

```{r}
#| label: verification-functions
#| echo: true

# Check if matrix is symmetric
is_symmetric <- function(A, tol = 1e-10) {
  isTRUE(all.equal(A, t(A), tolerance = tol))
}

# Check if matrix is idempotent
is_idempotent <- function(A, tol = 1e-10) {
  isTRUE(all.equal(A, A %*% A, tolerance = tol))
}

# Check if two vectors are orthogonal
is_orthogonal <- function(x, y, tol = 1e-10) {
  abs(sum(x * y)) < tol
}

# Check normal equations
check_normal_equations <- function(X, y, b, tol = 1e-10) {
  LHS <- crossprod(X) %*% b
  RHS <- crossprod(X, y)
  max(abs(LHS - RHS)) < tol
}

# Check projection property
check_projection <- function(P, tol = 1e-10) {
  is_symmetric(P, tol) && is_idempotent(P, tol)
}

# Use them
X <- matrix(rnorm(50 * 3), 50, 3)
H <- X %*% solve(crossprod(X)) %*% t(X)

cat("H is symmetric?", is_symmetric(H), "\n")
cat("H is idempotent?", is_idempotent(H), "\n")
cat("H is projection?", check_projection(H), "\n")
```

::: {.callout-tip}
## Best Practices Summary

**Always**:
1. ✅ Use `all.equal()` for comparisons, not `==`
2. ✅ Verify normal equations are satisfied
3. ✅ Check residual orthogonality
4. ✅ Verify SS decomposition
5. ✅ Compare with `lm()` output
6. ✅ Check condition numbers
7. ✅ Test on known cases first

**Debugging strategy**:
1. Start with small, simple test cases
2. Check dimensions and ranks
3. Verify mathematical properties
4. Compare with trusted implementations
5. Use diagnostic functions systematically
6. Profile code for performance bottlenecks

**When something goes wrong**:
- Print intermediate results
- Check for NAs, Infs, or NaNs
- Verify matrix dimensions match
- Test on well-conditioned data first
- Use `debug()` and `browser()` in R
:::

#### Complete Verification Example

**Putting it all together** for a livestock example:

```{r}
#| label: complete-verification-swine
#| echo: true

# Swine litter size example
set.seed(2024)
n <- 30
parity <- sample(1:5, n, replace = TRUE)
X <- cbind(1, parity)
y <- 8 + 1.2 * parity + rnorm(n, sd = 1.5)

cat("=== Complete Verification Example ===\n\n")

# Solve
b <- solve(crossprod(X), crossprod(X, y))
cat("Estimates: Intercept =", b[1], ", Slope =", b[2], "\n\n")

# Verification checklist
cat("1. Normal equations: X'Xb = X'y?\n")
cat("   ", check_normal_equations(X, y, b), "\n\n")

cat("2. Residuals orthogonal to X?\n")
e <- y - X %*% b
cat("   Max |X'e| =", max(abs(crossprod(X, e))), "\n\n")

cat("3. Fitted values orthogonal to residuals?\n")
y_hat <- X %*% b
cat("   |y_hat'e| =", abs(sum(y_hat * e)), "\n\n")

cat("4. SS decomposition: SST = SSM + SSE?\n")
y_bar <- mean(y)
SST <- sum((y - y_bar)^2)
SSM <- sum((y_hat - y_bar)^2)
SSE <- sum(e^2)
cat("   SST =", SST, "\n")
cat("   SSM + SSE =", SSM + SSE, "\n")
cat("   Match?", all.equal(SST, SSM + SSE), "\n\n")

cat("5. Compare with lm():\n")
fit <- lm(y ~ parity)
cat("   Coefficients match?",
    all.equal(as.numeric(b), coef(fit), check.attributes = FALSE), "\n\n")

cat("6. Projection matrix properties:\n")
H <- X %*% solve(crossprod(X)) %*% t(X)
cat("   H symmetric?", is_symmetric(H), "\n")
cat("   H idempotent?", is_idempotent(H), "\n")
cat("   tr(H) = p?", all.equal(sum(diag(H)), ncol(X)), "\n\n")

cat("7. Condition number:\n")
cat("   κ(X'X) =", kappa(crossprod(X)), "\n\n")

cat("✓ ALL CHECKS PASSED\n")
```

---

## Cross-Reference Table {#sec-cross-reference}

This comprehensive table maps matrix algebra concepts to their locations in the course and provides quick reference for R implementations.

::: {.callout-note}
## How to Use This Table

- **Concept**: Matrix algebra term or operation
- **Section**: Where it's defined in this appendix
- **Key Formula**: Most important identity or formula
- **Used in Weeks**: Course weeks where this concept appears
- **R Function**: Primary R function(s) for computation
- **Notes**: Critical usage notes or warnings
:::

### Core Matrix Operations

| Concept | Section | Key Formula | Used in Weeks | R Function | Notes |
|---------|---------|-------------|---------------|------------|-------|
| **Matrix transpose** | @sec-transpose | $(\mathbf{A}')' = \mathbf{A}$ | 1-15 | `t(A)` | Use `crossprod()` instead of `t(X) %*% X` |
| **Matrix multiplication** | @sec-multiplication | $(\mathbf{AB})' = \mathbf{B}'\mathbf{A}'$ | 1-15 | `%*%` | Not commutative! |
| **Cross-product** | @sec-computational-shortcuts | $\mathbf{X}'\mathbf{X}$ | 4-15 | `crossprod(X)` | 2× faster than `t(X) %*% X` |
| **Outer product** | @sec-computational-shortcuts | $\mathbf{X}\mathbf{X}'$ | 5-11 | `tcrossprod(X)` | Avoid for large $n$ |
| **Kronecker product** | @sec-kronecker | $\mathbf{A} \otimes \mathbf{B}$ | 14 (preview) | `kronecker(A, B)` | Never form explicitly for large matrices! |

### Special Matrices

| Concept | Section | Key Formula | Used in Weeks | R Function | Notes |
|---------|---------|-------------|---------------|------------|-------|
| **Identity matrix** | @sec-identity | $\mathbf{I}_n\mathbf{A} = \mathbf{A}$ | 1-15 | `diag(n)` | |
| **Diagonal matrix** | @sec-diagonal | $\mathbf{D} = \text{diag}(d_1, \ldots, d_n)$ | 2-15 | `diag(x)` | Creates diagonal or extracts diagonal |
| **Symmetric matrix** | @sec-symmetric | $\mathbf{A}' = \mathbf{A}$ | 2-15 | `isSymmetric(A)` | $\mathbf{X}'\mathbf{X}$ always symmetric |
| **Orthogonal matrix** | @sec-orthogonal | $\mathbf{Q}'\mathbf{Q} = \mathbf{I}$ | 2, 5, 11 | `qr.Q()` | Preserves lengths, numerically stable |
| **Idempotent matrix** | @sec-idempotent | $\mathbf{P}^2 = \mathbf{P}$ | 5, 6, 11 | Custom function | Projection matrices are idempotent |
| **Hat matrix** | @sec-projection-matrices | $\mathbf{H} = \mathbf{X}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'$ | 5, 6, 11 | `hatvalues()` | NEVER form explicitly for large $n$ |
| **Residual maker** | @sec-projection-matrices | $\mathbf{M} = \mathbf{I} - \mathbf{H}$ | 5, 6, 11 | Use implicitly | $\mathbf{e} = \mathbf{M}\mathbf{y}$ |
| **Centering matrix** | @sec-special-matrices | $\mathbf{C} = \mathbf{I} - n^{-1}\mathbf{1}\mathbf{1}'$ | 4-10 | `scale(center=TRUE)` | For computing SST |

### Matrix Properties

| Concept | Section | Key Formula | Used in Weeks | R Function | Notes |
|---------|---------|-------------|---------------|------------|-------|
| **Rank** | @sec-rank | $r(\mathbf{X}'\mathbf{X}) = r(\mathbf{X})$ | 2, 7, 12 | `qr()$rank` | Critical for estimability |
| **Trace** | @sec-trace | $\text{tr}(\mathbf{H}) = p$ | 2, 5, 6 | `sum(diag(A))` | Sum of eigenvalues |
| **Determinant** | @sec-determinant | $\det(\mathbf{AB}) = \det(\mathbf{A})\det(\mathbf{B})$ | 2, 5 | `det(A)` | Zero if singular |
| **Condition number** | @sec-numerical-stability | $\kappa(\mathbf{A}) = \lambda_{\max}/\lambda_{\min}$ | 6, 11, 12 | `kappa(A)` | $>10^6$ is severely ill-conditioned |
| **Eigenvalues** | @sec-eigenvalues | $\mathbf{A}\mathbf{v} = \lambda\mathbf{v}$ | 2, 7 | `eigen()` | For spectral decomposition |
| **Singular values** | @sec-svd | $\mathbf{X} = \mathbf{UDV}'$ | 2, 11, 12 | `svd()` | Most numerically stable decomposition |

### Matrix Inverses

| Concept | Section | Key Formula | Used in Weeks | R Function | Notes |
|---------|---------|-------------|---------------|------------|-------|
| **Regular inverse** | @sec-inverses | $\mathbf{A}\mathbf{A}^{-1} = \mathbf{I}$ | 2-15 | `solve(A)` | Only for full-rank square matrices |
| **Solve linear system** | @sec-efficient-computation | $\mathbf{Ab} = \mathbf{c}$ | 4-15 | `solve(A, c)` | NEVER use `solve(A) %*% c` |
| **Generalized inverse** | @sec-generalized-inverse | $\mathbf{A}\mathbf{A}^-\mathbf{A} = \mathbf{A}$ | 2, 12, 13 | `MASS::ginv()` | For rank-deficient systems |
| **Moore-Penrose inverse** | @sec-generalized-inverse | Unique g-inverse | 12 | `MASS::ginv()` | Uses SVD |
| **Cholesky decomposition** | @sec-efficient-computation | $\mathbf{A} = \mathbf{R}'\mathbf{R}$ | 11 | `chol(A)` | Requires positive definite |
| **QR decomposition** | @sec-efficient-computation | $\mathbf{X} = \mathbf{QR}$ | 2, 11 | `qr()` | Most stable for normal equations |

### Projection and Quadratic Forms

| Concept | Section | Key Formula | Used in Weeks | R Function | Notes |
|---------|---------|-------------|---------------|------------|-------|
| **Projection matrix** | @sec-projection-matrices | $\mathbf{P}^2 = \mathbf{P} = \mathbf{P}'$ | 5, 6 | Custom | Symmetric + idempotent |
| **Fitted values** | @sec-projection-matrices | $\hat{\mathbf{y}} = \mathbf{H}\mathbf{y}$ | 4-15 | `fitted()` or `X %*% b` | Don't form $\mathbf{H}$ |
| **Residuals** | @sec-projection-matrices | $\mathbf{e} = \mathbf{M}\mathbf{y}$ | 4-15 | `residuals()` | $\mathbf{X}'\mathbf{e} = \mathbf{0}$ |
| **Quadratic form** | @sec-quadratic-forms | $\mathbf{y}'\mathbf{A}\mathbf{y}$ | 5-10 | `t(y) %*% A %*% y` or `sum(y * (A %*% y))` | Sum of squares |
| **Leverage** | @sec-projection-matrices | $h_{ii} = [\mathbf{H}]_{ii}$ | 11 | `hatvalues()` | Influence of observation $i$ |
| **Mahalanobis distance** | @sec-quadratic-forms | $D^2 = (\mathbf{x} - \boldsymbol{\mu})'\boldsymbol{\Sigma}^{-1}(\mathbf{x} - \boldsymbol{\mu})$ | 11 | `mahalanobis()` | For outlier detection |

### Linear Models Identities

| Concept | Section | Key Formula | Used in Weeks | R Function | Notes |
|---------|---------|-------------|---------------|------------|-------|
| **Normal equations** | @sec-projection-identities | $\mathbf{X}'\mathbf{X}\mathbf{b} = \mathbf{X}'\mathbf{y}$ | 4-15 | `solve(crossprod(X), crossprod(X, y))` | Foundation of LS |
| **LS solution** | @sec-projection-identities | $\mathbf{b} = (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{y}$ | 4-15 | `lm.fit()` or `qr.coef()` | Use QR, not inverse |
| **Residual orthogonality** | @sec-projection-identities | $\mathbf{X}'\mathbf{e} = \mathbf{0}$ | 5-15 | Check: `crossprod(X, e)` | Always verify! |
| **Var(b)** | @sec-variance-formulas | $\text{Var}(\mathbf{b}) = \sigma^2(\mathbf{X}'\mathbf{X})^{-1}$ | 5-15 | `vcov()` | For standard errors |
| **Var(y_hat)** | @sec-variance-formulas | $\text{Var}(\hat{\mathbf{y}}) = \sigma^2\mathbf{H}$ | 5, 11 | `sigma2 * H` | Heteroscedastic! |
| **Var(e)** | @sec-variance-formulas | $\text{Var}(\mathbf{e}) = \sigma^2\mathbf{M}$ | 5, 11 | `sigma2 * M` | Not constant variance |
| **Contrast variance** | @sec-variance-formulas | $\text{Var}(\mathbf{c}'\mathbf{b}) = \sigma^2\mathbf{c}'(\mathbf{X}'\mathbf{X})^{-1}\mathbf{c}$ | 8 | Custom | For testing hypotheses |

### Sum of Squares

| Concept | Section | Key Formula | Used in Weeks | R Function | Notes |
|---------|---------|-------------|---------------|------------|-------|
| **SST** | @sec-ss-identities | $\sum(y_i - \bar{y})^2$ | 4-10 | `sum((y - mean(y))^2)` | Total variation |
| **SSE** | @sec-ss-identities | $\mathbf{e}'\mathbf{e} = \mathbf{y}'\mathbf{M}\mathbf{y}$ | 4-15 | `sum(residuals^2)` or `deviance()` | Error SS |
| **SSM/SSR** | @sec-ss-identities | $\mathbf{b}'\mathbf{X}'\mathbf{y} - n\bar{y}^2$ | 4-10 | From `anova()` | Model/Regression SS |
| **SS decomposition** | @sec-ss-identities | $\text{SST} = \text{SSM} + \text{SSE}$ | 5-10 | Always verify! | Orthogonal decomposition |
| **R²** | @sec-ss-identities | $R^2 = \text{SSM}/\text{SST}$ | 4-6 | `summary()$r.squared` | Proportion explained |
| **Adjusted R²** | @sec-ss-identities | $\bar{R}^2 = 1 - \frac{\text{SSE}/(n-p)}{\text{SST}/(n-1)}$ | 6 | `summary()$adj.r.squared` | Penalizes complexity |

### Matrix Calculus

| Concept | Section | Key Formula | Used in Weeks | R Function | Notes |
|---------|---------|-------------|---------------|------------|-------|
| **Derivative scalar wrt vector** | @sec-matrix-calculus | $\partial(\mathbf{a}'\mathbf{x})/\partial\mathbf{x} = \mathbf{a}$ | 5 | Manual | For deriving normal equations |
| **Derivative quadratic form** | @sec-matrix-calculus | $\partial(\mathbf{x}'\mathbf{A}\mathbf{x})/\partial\mathbf{x} = 2\mathbf{Ax}$ | 5 | Manual | For minimizing SSE |
| **Hessian** | @sec-matrix-calculus | $\partial^2 S/\partial\boldsymbol{\beta}\partial\boldsymbol{\beta}' = 2\mathbf{X}'\mathbf{X}$ | 5 | Manual | For checking convexity |

### Computational Topics

| Concept | Section | Key Formula | Used in Weeks | R Function | Notes |
|---------|---------|-------------|---------------|------------|-------|
| **VIF** | @sec-numerical-stability | $\text{VIF}_j = 1/(1 - R_j^2)$ | 6, 11, 12 | `car::vif()` | $>10$ indicates collinearity |
| **Centering predictors** | @sec-numerical-stability | $\mathbf{X}_c = \mathbf{X} - \bar{\mathbf{X}}$ | 6, 10, 11 | `scale(X, center=TRUE)` | Improves conditioning |
| **Scaling predictors** | @sec-numerical-stability | Standardize to mean 0, sd 1 | 6, 11 | `scale(X)` | For comparing effects |
| **all.equal()** | @sec-verification | Tolerant comparison | 2-15 | `all.equal(a, b)` | NEVER use `==` for floats |
| **Numerical tolerance** | @sec-verification | Default $\approx 1.5 \times 10^{-8}$ | 2-15 | `.Machine$double.eps` | For checking zeros |

### Week-Specific Applications

| Week | Primary Topics | Key Matrix Operations | Critical Sections |
|------|---------------|----------------------|-------------------|
| **1** | Overview, matrix basics | Transpose, multiplication | @sec-basic-operations |
| **2** | Linear algebra essentials | Rank, inverse, eigenvalues | @sec-rank, @sec-inverses, @sec-eigenvalues |
| **3** | Design matrices | Building $\mathbf{X}$, coding schemes | @sec-basic-operations |
| **4** | Simple regression | $\mathbf{X}'\mathbf{X}$, $\mathbf{X}'\mathbf{y}$, $\mathbf{b}$ | @sec-projection-identities |
| **5** | Least squares theory | $\mathbf{H}$, $\mathbf{M}$, projections, Gauss-Markov | @sec-projection-matrices |
| **6** | Multiple regression | $\text{Var}(\mathbf{b})$, collinearity, VIF | @sec-variance-formulas, @sec-numerical-stability |
| **7** | One-way ANOVA | Cell means model, SS decomposition | @sec-ss-identities |
| **8** | Contrasts | $\mathbf{c}'\mathbf{b}$, estimability | @sec-variance-formulas |
| **9** | Two-way ANOVA | Multiple factors, interactions | @sec-ss-identities |
| **10** | ANCOVA | Adjusted means, homogeneity of slopes | @sec-variance-formulas |
| **11** | Diagnostics | Leverage ($h_{ii}$), Cook's D | @sec-projection-matrices, @sec-quadratic-forms |
| **12** | Non-full rank | Generalized inverse, constraints | @sec-generalized-inverse |
| **13** | Special topics I | Unbalanced data, Type I/II/III SS | @sec-ss-identities |
| **14** | Special topics II | Polynomial regression, WLS, mixed model preview | @sec-kronecker |
| **15** | Capstone | All concepts integrated | All sections |

::: {.callout-tip}
## Quick Lookup Strategy

1. **Find your week** in the Week-Specific Applications table
2. **Identify the matrix concept** you need
3. **Look up the concept** in the detailed tables above
4. **Navigate to the section** for full details
5. **Use the R function** provided for implementation
:::

::: {.callout-warning}
## Common Mistakes to Avoid

Based on the course material, students commonly make these errors:

| Mistake | Correct Approach | Reference |
|---------|-----------------|-----------|
| Using `==` for float comparison | Use `all.equal()` | @sec-verification |
| Forming $\mathbf{H}$ explicitly | Use $\mathbf{X}\mathbf{b}$ instead | @sec-computational-shortcuts |
| Using `solve(A) %*% c` | Use `solve(A, c)` | @sec-efficient-computation |
| Ignoring rank deficiency | Check `qr()$rank`, use g-inverse | @sec-numerical-stability |
| Not centering predictors | Use `scale()` for ill-conditioned $\mathbf{X}'\mathbf{X}$ | @sec-numerical-stability |
| Forming large Kronecker products | Use inverse property, implicit operations | @sec-kronecker |
| Using `t(X) %*% X` | Use `crossprod(X)` | @sec-computational-shortcuts |
:::

---

## R Code Templates {#sec-templates}

This section provides production-ready R functions for common linear models tasks. Copy and adapt these templates for your own analyses.

::: {.callout-note}
## How to Use These Templates

Each template includes:
- **Complete, tested function code**
- **Input/output specifications**
- **Usage examples with livestock data**
- **Error checking and validation**
- **Documentation comments**

**Installation**: Copy the function to your R script or save in a separate file to `source()`.
:::

### Matrix Property Checker {#sec-template-property-checker}

**Purpose**: Comprehensive diagnostic function for checking matrix properties.

```{r}
#| label: template-property-checker
#| echo: true
#| eval: false

#' Check Matrix Properties
#'
#' Comprehensive diagnostic function for matrices in linear models
#'
#' @param A Matrix to check
#' @param name Character string for display (default: "A")
#' @param tolerance Numerical tolerance for checks (default: 1e-10)
#' @param verbose Logical, print detailed output? (default: TRUE)
#'
#' @return List with logical test results (invisible)
#'
#' @examples
#' X <- matrix(rnorm(50*3), 50, 3)
#' H <- X %*% solve(t(X) %*% X) %*% t(X)
#' check_matrix_properties(H, "Hat Matrix")
#'
check_matrix_properties <- function(A, name = "A", tolerance = 1e-10, verbose = TRUE) {

  # Initialize results list
  results <- list()

  if (verbose) cat("\n=== Checking", name, "===\n\n")

  # Basic properties
  dims <- dim(A)
  results$dimensions <- dims
  if (verbose) cat("Dimensions:", dims[1], "×", dims[2], "\n")

  is_square <- dims[1] == dims[2]
  results$is_square <- is_square
  if (verbose) cat("Square:", is_square, "\n")

  if (!is_square) {
    if (verbose) cat("(Additional checks require square matrix)\n")
    return(invisible(results))
  }

  # Symmetry
  is_sym <- isTRUE(all.equal(A, t(A), tolerance = tolerance))
  results$is_symmetric <- is_sym
  if (verbose) cat("Symmetric:", is_sym, "\n")

  # Diagonal
  off_diag <- A[row(A) != col(A)]
  is_diag <- all(abs(off_diag) < tolerance)
  results$is_diagonal <- is_diag
  if (verbose) cat("Diagonal:", is_diag, "\n")

  # Idempotent
  A2 <- A %*% A
  is_idem <- isTRUE(all.equal(A, A2, tolerance = tolerance))
  results$is_idempotent <- is_idem
  if (verbose) cat("Idempotent (A² = A):", is_idem, "\n")

  # Rank
  rank_A <- qr(A)$rank
  results$rank <- rank_A
  full_rank <- rank_A == min(dims)
  results$full_rank <- full_rank
  if (verbose) {
    cat("Rank:", rank_A, "/", min(dims))
    if (full_rank) cat(" (full rank)")
    cat("\n")
  }

  # Condition number
  kappa_A <- kappa(A)
  results$condition_number <- kappa_A
  if (verbose) {
    cat("Condition number:", sprintf("%.2e", kappa_A))
    if (kappa_A > 1e6) {
      cat(" [SEVERELY ILL-CONDITIONED]")
    } else if (kappa_A > 1000) {
      cat(" [ILL-CONDITIONED]")
    } else if (kappa_A > 10) {
      cat(" [MODERATE]")
    } else {
      cat(" [WELL-CONDITIONED]")
    }
    cat("\n")
  }

  # Eigenvalues
  eigen_vals <- eigen(A, only.values = TRUE)$values
  results$eigenvalues <- eigen_vals
  if (verbose) {
    cat("Eigenvalues:", paste(sprintf("%.4f", Re(eigen_vals)), collapse=", "))
    if (any(abs(Im(eigen_vals)) > tolerance)) {
      cat(" [COMPLEX]")
    }
    cat("\n")
  }

  # Positive definite
  is_pd <- all(Re(eigen_vals) > tolerance) && all(abs(Im(eigen_vals)) < tolerance)
  results$positive_definite <- is_pd
  if (verbose) cat("Positive definite:", is_pd, "\n")

  # Trace
  tr_A <- sum(diag(A))
  results$trace <- tr_A
  if (verbose) cat("Trace:", sprintf("%.4f", tr_A), "\n")

  # Determinant
  det_A <- det(A)
  results$determinant <- det_A
  results$singular <- abs(det_A) < tolerance
  if (verbose) {
    cat("Determinant:", sprintf("%.4e", det_A))
    if (abs(det_A) < tolerance) cat(" [SINGULAR]")
    cat("\n")
  }

  # Projection matrix check
  if (is_sym && is_idem) {
    if (verbose) cat("\n✓ This is a PROJECTION MATRIX\n")
    results$is_projection <- TRUE
  } else {
    results$is_projection <- FALSE
  }

  return(invisible(results))
}

# Example usage
X <- matrix(rnorm(50 * 3), 50, 3)
H <- X %*% solve(t(X) %*% X) %*% t(X)
check_matrix_properties(H, "Hat Matrix")
```

---

### Build Projection Matrices {#sec-template-projection}

**Purpose**: Safely construct projection matrices with numerical stability checks.

```{r}
#| label: template-projection
#| echo: true
#| eval: false

#' Build Projection Matrices
#'
#' Construct hat matrix H and residual maker M with stability checks
#'
#' @param X Design matrix (n × p)
#' @param method Character: "qr" (default, most stable) or "cholesky"
#' @param check_properties Logical, verify projection properties? (default: TRUE)
#'
#' @return List with components:
#'   - H: Hat matrix (n × n) - NOTE: only computed if n < 1000
#'   - M: Residual maker (n × n) - NOTE: only computed if n < 1000
#'   - qr_obj: QR decomposition object (for fitted values)
#'   - rank: Rank of X
#'   - is_full_rank: Logical
#'
#' @details For large n, H and M are NOT computed explicitly. Use qr_obj instead.
#'
#' @examples
#' X <- cbind(1, rnorm(50), rnorm(50))
#' proj <- build_projection_matrices(X)
#' y_hat <- proj$qr_obj$qr %*% qr.coef(proj$qr_obj, y)  # Efficient
#'
build_projection_matrices <- function(X, method = "qr", check_properties = TRUE) {

  n <- nrow(X)
  p <- ncol(X)

  # Check dimensions
  if (n < p) {
    stop("n must be >= p for projection matrices")
  }

  # QR decomposition (always computed, most stable)
  qr_obj <- qr(X)
  rank_X <- qr_obj$rank
  is_full_rank <- rank_X == p

  if (!is_full_rank) {
    warning(paste("X is not full rank. Rank =", rank_X, "but p =", p))
  }

  # Build projection matrices (only for small n)
  H <- NULL
  M <- NULL

  if (n < 1000) {
    if (method == "qr") {
      Q <- qr.Q(qr_obj)
      H <- tcrossprod(Q)  # Q %*% t(Q)
    } else if (method == "cholesky") {
      if (!is_full_rank) {
        stop("Cholesky method requires full rank X")
      }
      XtX <- crossprod(X)
      XtX_inv <- chol2inv(chol(XtX))
      H <- X %*% XtX_inv %*% t(X)
    } else {
      stop("method must be 'qr' or 'cholesky'")
    }

    M <- diag(n) - H

    # Verify properties
    if (check_properties) {
      # Check symmetry
      if (!isTRUE(all.equal(H, t(H)))) {
        warning("H is not symmetric (numerical issue)")
      }

      # Check idempotence
      if (!isTRUE(all.equal(H, H %*% H, tolerance = 1e-8))) {
        warning("H is not idempotent (numerical issue)")
      }

      # Check trace
      tr_H <- sum(diag(H))
      if (!isTRUE(all.equal(tr_H, rank_X, tolerance = 1e-6))) {
        warning(paste("tr(H) =", tr_H, "but should be", rank_X))
      }
    }
  } else {
    message("n >= 1000: H and M not computed explicitly (too large)")
    message("Use qr_obj for efficient operations instead")
  }

  return(list(
    H = H,
    M = M,
    qr_obj = qr_obj,
    rank = rank_X,
    is_full_rank = is_full_rank,
    n = n,
    p = p
  ))
}

# Example: Small dataset
X_small <- cbind(1, rnorm(50), rnorm(50))
proj <- build_projection_matrices(X_small)
cat("Rank:", proj$rank, "\nFull rank?:", proj$is_full_rank, "\n")

# Example: Large dataset (won't form H explicitly)
X_large <- cbind(1, rnorm(5000), rnorm(5000))
proj_large <- build_projection_matrices(X_large)
```

---

### Solve Normal Equations {#sec-template-solve}

**Purpose**: Flexible solver for normal equations with multiple methods and diagnostics.

```{r}
#| label: template-solve
#| echo: true
#| eval: false

#' Solve Normal Equations
#'
#' Solve X'Xb = X'y using specified method with full diagnostics
#'
#' @param X Design matrix (n × p)
#' @param y Response vector (length n)
#' @param method Character: "qr" (default), "cholesky", "svd", or "ginv"
#' @param compute_diagnostics Logical, compute full diagnostics? (default: TRUE)
#'
#' @return List with components:
#'   - coefficients: Estimated coefficients
#'   - fitted.values: Fitted values
#'   - residuals: Residuals
#'   - sigma2: Variance estimate
#'   - vcov: Variance-covariance matrix of coefficients
#'   - se: Standard errors
#'   - df: Degrees of freedom
#'   - rank: Rank of X
#'   - method: Method used
#'   - diagnostics: List of diagnostic measures (if requested)
#'
#' @examples
#' X <- cbind(1, rnorm(50), rnorm(50))
#' y <- rnorm(50)
#' fit <- solve_normal_equations(X, y, method = "qr")
#' summary(fit)
#'
solve_normal_equations <- function(X, y, method = "qr", compute_diagnostics = TRUE) {

  n <- nrow(X)
  p <- ncol(X)

  # Input validation
  if (length(y) != n) {
    stop("Length of y must equal nrow(X)")
  }

  # Initialize result
  result <- list(method = method)

  # Solve based on method
  if (method == "qr") {
    qr_obj <- qr(X)
    result$rank <- qr_obj$rank
    b <- qr.coef(qr_obj, y)
    y_hat <- qr.fitted(qr_obj, y)
    e <- qr.resid(qr_obj, y)

  } else if (method == "cholesky") {
    XtX <- crossprod(X)
    Xty <- crossprod(X, y)
    result$rank <- qr(X)$rank

    R <- chol(XtX)
    b <- backsolve(R, forwardsolve(t(R), Xty))
    y_hat <- X %*% b
    e <- y - y_hat

  } else if (method == "svd") {
    svd_obj <- svd(X)
    result$rank <- sum(svd_obj$d > 1e-10)

    d_inv <- ifelse(svd_obj$d > 1e-10, 1/svd_obj$d, 0)
    X_pinv <- svd_obj$v %*% diag(d_inv) %*% t(svd_obj$u)
    b <- X_pinv %*% y
    y_hat <- X %*% b
    e <- y - y_hat

  } else if (method == "ginv") {
    library(MASS)
    result$rank <- qr(X)$rank
    X_pinv <- ginv(X)
    b <- X_pinv %*% y
    y_hat <- X %*% b
    e <- y - y_hat

  } else {
    stop("method must be 'qr', 'cholesky', 'svd', or 'ginv'")
  }

  # Remove names if present
  b <- as.numeric(b)
  y_hat <- as.numeric(y_hat)
  e <- as.numeric(e)

  # Basic results
  result$coefficients <- b
  result$fitted.values <- y_hat
  result$residuals <- e
  result$df <- n - result$rank

  # Variance estimation
  SSE <- sum(e^2)
  sigma2 <- SSE / result$df
  result$sigma2 <- sigma2
  result$sigma <- sqrt(sigma2)

  # Variance-covariance matrix
  XtX <- crossprod(X)
  if (result$rank == p) {
    vcov_mat <- solve(XtX) * sigma2
  } else {
    vcov_mat <- ginv(XtX) * sigma2
  }
  result$vcov <- vcov_mat
  result$se <- sqrt(diag(vcov_mat))

  # Compute diagnostics
  if (compute_diagnostics) {
    diag_list <- list()

    # Sum of squares
    y_bar <- mean(y)
    SST <- sum((y - y_bar)^2)
    SSM <- sum((y_hat - y_bar)^2)
    diag_list$SST <- SST
    diag_list$SSM <- SSM
    diag_list$SSE <- SSE
    diag_list$MSE <- sigma2
    diag_list$R2 <- SSM / SST
    diag_list$adj_R2 <- 1 - (SSE / result$df) / (SST / (n - 1))

    # Check normal equations
    check_ne <- max(abs(crossprod(X, e)))
    diag_list$max_Xte <- check_ne
    diag_list$normal_eqs_satisfied <- check_ne < 1e-8

    # Condition number
    diag_list$condition_number <- kappa(XtX)

    # F-statistic
    if (result$rank > 1) {
      MSM <- SSM / (result$rank - 1)
      F_stat <- MSM / sigma2
      diag_list$F_statistic <- F_stat
      diag_list$F_pvalue <- pf(F_stat, result$rank - 1, result$df, lower.tail = FALSE)
    }

    result$diagnostics <- diag_list
  }

  class(result) <- "my_lm"
  return(result)
}

# Print method
print.my_lm <- function(x, ...) {
  cat("\nCall: solve_normal_equations(method =", paste0("'", x$method, "')\n\n"))
  cat("Coefficients:\n")
  print(round(x$coefficients, 4))
  cat("\nResidual standard error:", round(x$sigma, 4), "on", x$df, "degrees of freedom\n")
  if (!is.null(x$diagnostics)) {
    cat("Multiple R-squared:", round(x$diagnostics$R2, 4), "\n")
    cat("Adjusted R-squared:", round(x$diagnostics$adj_R2, 4), "\n")
  }
  invisible(x)
}

# Example usage
set.seed(123)
n <- 50
X <- cbind(1, rnorm(n), rnorm(n))
y <- X %*% c(10, 2, -1) + rnorm(n, sd = 2)

fit <- solve_normal_equations(X, y, method = "qr")
print(fit)

# Compare methods
fit_qr <- solve_normal_equations(X, y, "qr", compute_diagnostics = FALSE)
fit_chol <- solve_normal_equations(X, y, "cholesky", compute_diagnostics = FALSE)
cat("QR vs Cholesky:", all.equal(fit_qr$coefficients, fit_chol$coefficients), "\n")
```

---

### Compute Quadratic Forms {#sec-template-quadratic}

**Purpose**: Efficient computation of quadratic forms and sums of squares.

```{r}
#| label: template-quadratic
#| echo: true
#| eval: false

#' Compute Quadratic Form
#'
#' Efficiently compute y'Ay for various matrix structures
#'
#' @param y Numeric vector (length n)
#' @param A Matrix (n × n), or NULL for identity
#' @param type Character: "general", "diagonal", "symmetric", or "identity"
#'
#' @return Scalar value of quadratic form
#'
#' @examples
#' y <- rnorm(100)
#' A <- crossprod(matrix(rnorm(100*100), 100, 100))
#' qf <- compute_quadratic_form(y, A, type = "symmetric")
#'
compute_quadratic_form <- function(y, A = NULL, type = "general") {

  n <- length(y)

  if (type == "identity" || is.null(A)) {
    # y'y
    return(sum(y^2))

  } else if (type == "diagonal") {
    # y'diag(d)y = sum(d * y^2)
    d <- diag(A)
    return(sum(d * y^2))

  } else if (type == "symmetric") {
    # Exploit symmetry: only compute upper triangle
    # More efficient: sum(y * (A %*% y))
    return(sum(y * (A %*% y)))

  } else if (type == "general") {
    # Full computation: t(y) %*% A %*% y
    return(as.numeric(t(y) %*% A %*% y))

  } else {
    stop("type must be 'general', 'diagonal', 'symmetric', or 'identity'")
  }
}

#' Decompose Sums of Squares
#'
#' Compute SST, SSM, SSE decomposition with verification
#'
#' @param y Response vector
#' @param y_hat Fitted values vector
#' @param verify Logical, verify decomposition? (default: TRUE)
#'
#' @return List with SST, SSM, SSE, R2, verification status
#'
decompose_sums_of_squares <- function(y, y_hat, verify = TRUE) {

  n <- length(y)
  y_bar <- mean(y)

  # Compute components
  SST <- sum((y - y_bar)^2)
  SSM <- sum((y_hat - y_bar)^2)
  SSE <- sum((y - y_hat)^2)

  # R-squared
  R2 <- SSM / SST

  # Verify decomposition
  decomp_ok <- TRUE
  if (verify) {
    sum_check <- SSM + SSE
    decomp_ok <- isTRUE(all.equal(SST, sum_check, tolerance = 1e-10))
    if (!decomp_ok) {
      warning(paste("SS decomposition failed: SST =", SST, "but SSM + SSE =", sum_check))
    }
  }

  return(list(
    SST = SST,
    SSM = SSM,
    SSE = SSE,
    R2 = R2,
    decomposition_verified = decomp_ok
  ))
}

# Example usage
y <- rnorm(100, mean = 50, sd = 10)
y_hat <- rnorm(100, mean = 50, sd = 8)

ss_decomp <- decompose_sums_of_squares(y, y_hat)
print(ss_decomp)
```

---

### Kronecker Product Functions {#sec-template-kronecker}

**Purpose**: Efficient operations with Kronecker product structure (never form explicitly!).

```{r}
#| label: template-kronecker
#| echo: true
#| eval: false

#' Kronecker Product Matrix-Vector Multiplication
#'
#' Compute (A ⊗ B)x efficiently WITHOUT forming A ⊗ B
#'
#' @param A Matrix (m × n)
#' @param B Matrix (p × q)
#' @param x Vector (length nq)
#'
#' @return Vector (length mp), result of (A ⊗ B)x
#'
#' @details Uses the identity: (A ⊗ B)vec(X) = vec(BXA')
#'
kronecker_matvec <- function(A, B, x) {

  m <- nrow(A)
  n <- ncol(A)
  p <- nrow(B)
  q <- ncol(B)

  # Check dimensions
  if (length(x) != n * q) {
    stop(paste("x must have length", n * q, "for this Kronecker product"))
  }

  # Reshape x to matrix X (q × n)
  X <- matrix(x, nrow = q, ncol = n)

  # Compute BXA' and vectorize
  result <- B %*% X %*% t(A)

  return(as.vector(result))
}

#' Solve (A ⊗ B)x = y Efficiently
#'
#' Solve Kronecker structured system WITHOUT forming A ⊗ B
#'
#' @param A Matrix (m × m), must be invertible
#' @param B Matrix (p × p), must be invertible
#' @param y Vector (length mp)
#'
#' @return Vector x such that (A ⊗ B)x = y
#'
#' @details Uses: (A ⊗ B)^(-1) = A^(-1) ⊗ B^(-1)
#'
solve_kronecker_system <- function(A, B, y) {

  m <- nrow(A)
  p <- nrow(B)

  # Check dimensions
  if (length(y) != m * p) {
    stop(paste("y must have length", m * p))
  }

  # Check invertibility
  if (abs(det(A)) < 1e-10) stop("A is singular")
  if (abs(det(B)) < 1e-10) stop("B is singular")

  # Compute inverses separately
  A_inv <- solve(A)
  B_inv <- solve(B)

  # Apply (A^(-1) ⊗ B^(-1)) to y
  x <- kronecker_matvec(A_inv, B_inv, y)

  return(x)
}

#' Example: Multi-Trait Genetic Evaluation Structure
#'
#' Demonstrates Kronecker product in genetic evaluation context
#'
multi_trait_var_structure <- function(G, A, n_traits = 2, n_animals = 5) {

  # G: genetic covariance between traits (2×2)
  # A: additive relationship matrix (5×5)
  # Full covariance: V = G ⊗ A (10×10)

  cat("=== Multi-Trait Variance Structure ===\n\n")
  cat("G (genetic covariance matrix):\n")
  print(G)
  cat("\nA (relationship matrix):\n")
  print(A)

  # Size of full system
  total_dim <- n_traits * n_animals
  cat("\nFull system size:", total_dim, "×", total_dim, "\n")
  cat("Would require", format(total_dim^2, big.mark=","), "elements\n")

  # DON'T form explicitly!
  if (total_dim <= 20) {
    V_explicit <- kronecker(G, A)
    cat("\nV = G ⊗ A (formed explicitly for illustration):\n")
    cat("Dimensions:", dim(V_explicit), "\n")
  } else {
    cat("\n[V not formed - too large!]\n")
  }

  # For solving (G ⊗ A)^(-1)y, use:
  cat("\nTo solve (G ⊗ A)^(-1)y:\n")
  cat("1. Compute G^(-1) (", n_traits, "×", n_traits, ")\n")
  cat("2. Compute A^(-1) (", n_animals, "×", n_animals, ")\n")
  cat("3. Apply (G^(-1) ⊗ A^(-1)) implicitly\n")

  # Demonstrate
  G_inv <- solve(G)
  A_inv <- solve(A)

  y_test <- rnorm(total_dim)
  x_solution <- solve_kronecker_system(G_inv, A_inv, y_test)

  cat("\nTest: Solved system of size", total_dim, "without forming", total_dim, "×", total_dim, "matrix!\n")

  return(invisible(list(G = G, A = A, G_inv = G_inv, A_inv = A_inv)))
}

# Example: 2 traits, 5 animals
G <- matrix(c(10, 3, 3, 5), 2, 2)  # Genetic covariance
A <- matrix(c(1.0, 0.5, 0.0, 0.0, 0.25,
              0.5, 1.0, 0.25, 0.0, 0.125,
              0.0, 0.25, 1.0, 0.5, 0.375,
              0.0, 0.0, 0.5, 1.0, 0.25,
              0.25, 0.125, 0.375, 0.25, 1.0), 5, 5)

result <- multi_trait_var_structure(G, A)
```

---

## References {.unnumbered}

Key references for matrix algebra in linear models:

- Searle, S. R. (1971). *Linear Models*. John Wiley & Sons.
- Searle, S. R. (1982). *Matrix Algebra Useful for Statistics*. John Wiley & Sons.
- Henderson, C. R. (1984). *Applications of Linear Models in Animal Breeding*. University of Guelph.
- Harville, D. A. (1997). *Matrix Algebra From a Statistician's Perspective*. Springer.

:::{.callout-tip}
## What's Next?

After reviewing matrix algebra concepts here, you'll be well-prepared to:

- Understand the derivations in **Week 5: Least Squares Theory**
- Work with rank-deficient systems in **Week 12: Non-Full Rank Models**
- Explore advanced topics in future Animal models and multi-trait analysis courses
:::
